course_title,lesson_num,start_seconds,topic,transcript
Full Stack Deep Learning - Spring 2021,1,0,Intro,so josh talked about why we're doing this course and what's coming up this is the first lecture deep learning fundamentals that actually has the i guess the content but there's a lot of content in deep learning fundamentals and most of it is going to be a review for most of you that's what we assume that's the purpose of our requirements for enrolling but if what i'm going to talk about in this lecture real quick is not going to be mostly review for you then i highly recommend that you go through this uh online textbook neural networks and deeplearning.com it's something you can probably do in like a focused day or two it's not like a big textbook it's more of a website so i highly recommend that and uh our weekly reading is actually from this uh textbook it's a chapter from this book so today what we're going to talk about is neural networks universality of approximation the types of learning problems that neural networks can be used to solve loss functions and and minimizing them gradient descent being a method for that back propagation um architectural considerations and the concept of i guess gpu compute cuda cores stuff like that
Full Stack Deep Learning - Spring 2021,1,85,Neural Networks,so let's kick it off with neural networks um and you guys see the screen that has like the slide and nothing else on it right okay so neural networks are called neural because they are biologically inspired by neurons right which do all the computing in our bodies and the the kind of mental model of a neuron is that it's a cell that has things coming out of the main part called dendrites and you can think of them as like receptors of information and then if enough stimulation has been received by the dendrites then the whole neuron does a thing called firing it's basically an electrical impulse that begins you know in in the cell and then propagates down this long branch called the axon and the axon terminates in little branches that are basically adjoining other neurons as dendrites okay and so it's like a network of these neurons getting stimulated if they get stimulated enough they fire and the electrical potential travels down this long branch and stimulates other neurons in turn mathematically you can think about this as a pretty simple function called you know which is often called the perceptron and it really dates back to the 1950s so it's a pretty old concept in computing but you can think of the stimulation arriving at the dendrites as uh basically inputs right so x sub naught x sub one x sub two and so on the dendrite itself like where it meets the the input that's coming in how exactly much it gets stimulated by that input is determined by a weight so w sub zero or w sub one w sub two and you sum all that up right so you have that sum over i of w sub i x times x sub i and that's really like the neuron getting stimulated by the input and then there's b which is just the bias because this is a linear function you kind of want a little offset for the y-intercept basically and then the whole thing is enclosed in some kind of activation function because the way that a neuron works is right it's either fully on or fully off right if it's stimulated enough it fires and if it's not stimulated enough then it doesn't fire and so there's an activation function that basically is a threshold function like if if enough of the sum if the sum kind of exceeds the threshold it passes it on otherwise it remains off mostly what are some good activation functions well classical neural network literature really used the sigmoid function which is over on the left and it's a simple function that kind of squashes everything into you know no matter what the input the output is going to be between 0 and 1 and it's going to be it kind of asymptotes at 0 for negative inputs and that asymptotes at one for positive inputs and then in between around zero it quickly changes from zero to one so that's that's kind of what you want to see in an activation function it has a nice derivative right which can also be called the gradient uh g prime which is displayed in orange here on hyperbolic tangent i'll just skip over that's another one that people have used but in recent times people have mostly used the activation function on the right called the rectified linear unit or the value also known as a max function right because it's literally just saying whatever input comes in if it's above zero then pass it on and then if it's not above zero if it's if it's less than zero then then don't pass it on it's zero um and the gradient for it has a discontinuity but that's fine so basically the gradient is one if the input was was zero was greater than zero or it's or it's zero otherwise and this is part of the innovation actually that really kicked off the deep learning revolution in 2013 the relu so what's a network like what makes so we talked about neurons individually we call them perceptrons but what makes a neural network so if you arrange perceptrons in layers like you do here that's where the the terminology of networks comes from and usually there's an input layer that's whatever your input data and then the input layer connects to a hidden layer that hidden layer connects to another hidden layer there's some number of those and then finally there's an output layer now each one of these perceptrons right that make up this network has its own uh weight and has its own bias and that's really the setting of these weights and biases determines how the neural network responds to input so the next thing we want to talk about
Full Stack Deep Learning - Spring 2021,1,408,Universality,is universality which is you know this neural network represents some function y right y equals f of of x the input and then w the setting of all the weights but what can that function be right let's look at this function on the left f of x very you know lots of peaks and valleys in here how can we know if there's a neural network that and a choice of weights for it that can basically represent this function and to summarize you know some theoretical results you can prove that any two layer neural networks that's one hidden layer right so inputs to one hidden layer two outputs if given enough hidden units can be found to have you know some set of weights that can approximate any function so that's known as the universal approximation you know theorem and a little bit of intuition about why that should be true can be obtained if you go to that neural networks and deeplearning.com website go to chapter four uh basically you can think of each one of the perceptrons so you have a hidden layer of maybe thousands of perceptrons or millions right and each one of them you can think of as an impulse function that you know is either just kind of adds a column right uh here the columns are in orange the original function is in blue if there are enough of these columns and they kind of go up and down you can represent anything right it's almost like a fourier transform of this function so that's quite interesting i think i'll just pause for questions later um the takeaway is that neural networks are just incredibly general and like at least theoretically you can represent any function using in your network so what do we use neural
Full Stack Deep Learning - Spring 2021,1,528,Learning Problems,networks for well we do for machine learning what kind of machine learning problems are there there's three kind of big you know a breakdown of all the machine learnings out there you can have three categories supervised learning unsupervised learning and reinforcement learning there's also transfer learning meta learning you know imitation learning all these different types of learnings but these are the three big categories so supervised learning actually unsupervised learning you get unlabeled data x that means you know x can be maybe sound clips right or text like text strings but there's no nothing else associated with them it's just the sound clips and the text strings or images and the goal is really to learn the structure of that data so you learn x the reason you want to do it is because you can generate more of these types of data you can generate fake sound clips images or reviews but you can also obtain insights into what the data might hold so here's some fake text you know an amazon review that you can just generate using a neural net here is the concept of clustering so you have some data you don't know anything about it you don't have labels for it but just because of how it's structured you might infer that there are clusters right like some process gave rise to this data such that the data over here came from one process and the data over here came from another process supervised learning is you get both x and y so x being the raw kind of input data and y being a label for it so now you could have maybe an image and then a label a label could be you know does it have a cat in it and the goal is to learn a function that goes from x so an image to y a label for it and the goal for that is just to be able to make predictions so if i get an image i can say it's a cat if i get a sound clip i might be able to understand that it's a person you know speaking the words hey siri and untold other examples of that and then reinforcement learning the goal is to learn how to take actions in an environment so there's some kind of agent maybe a robot maybe a computer virus you know or or something it can take actions it can maybe move forward it can you know look somewhere and because when it takes an action you know reality provides some kind of input back to it because it's acting in an environment and you can interpret that environment as basically providing a reward or not to the agent and then changing the state that the agent is in so like if a robot was in this place and then it took a forward move in action well now it's in this place and maybe there's a reward associated with it or maybe not you can train uh for example game playing agents using reinforcement learning so here it's like you know the action is place down a piece on the go board and then the reward can be like did you eventually win the game or not and the state is obviously just the state of the of the go board so commercially viable is mostly supervised supervised learning right and then reinforcing learning is definitely up next and um and and peter you know is working at covariant.ai which is a company doing exactly that but also unsupervised learning i would say is also up next at this point with open ai for example productizing gpt3 and all the things that that enables that would fall under unsupervised learning i'm going to just skip this well maybe i won't skip the unsupervised learning so one example of an unsupervised learning problem might be to just predict the next character in a string of text so there's a maybe a blog post by andre carpathi you've seen called char rnn but it's basically using an rnn to feed in one character at a time and then the rnn can output also characters and what you end up learning is a language model that if you just kind of get it going with a word it'll just keep writing just by generating character after character and it's and it's very impressive what it can output just with that simple input type and and model uh another unsupervised learning problem might be just understanding word relationships so here the input would be actually words in a vocabulary so like imagine your vocabulary is 30 000 different words so then each word would be represented as a vector that's all zeros except for one in the place that corresponds to that word in in in like some you know list of words well if you feed in a bunch of those vectors into a system that's properly kind of set up and trained you can actually determine that there are certain relationships between words such as like man to woman is as king as the queen right in the in the canonical example which is quite interesting um in computer vision land you can try to predict the next pixel instead of the next character and text you can predict the next pixel so you can get something going with just a little bit of an image and then it auto completes it you can you can train this kind of stuff by for example trying to compress an image down to like a very small representation called the latent vector and then expand that latent vector back out to an image and that can can learn a very compressed representation of a kind of a rich input source and then lastly you know the culmination of this line of research maybe is generative adversarial networks gans so there the idea is that there's a generator which is like generating this kind of fake images or text or something but then also there's another model a neural network called the discriminator and the goal of the discriminator is to be able to tell apart the generated images or pieces of text from actually real images or pieces of text and the goal of the generator is to produce images or text that can fool the discriminator and if you set up the system and you train it you get very impressive results that are getting impressive like more impressive every year i mean we've all seen deep fake videos at this point and um if you go to this face does not or this what is it this person does not exist calm you'll see an infinite number of game generated faces that is really uh fun to look at i also saw this anime does not exist.com today and in reinforcement learning you have a lot of examples i think josh covered some of them so so next up let's talk about
Full Stack Deep Learning - Spring 2021,1,977,Empirical Risk Minimization / Loss Functions,um what's known as risk minimization and the concept of loss functions so let's talk about linear regression for a second um so linear regression is the so here i'm showing you what's known as one-dimensional data right so there's one dimension on the x-axis there's some number and then there's another dimension on the y-axis and that's the output so it's one-dimensional input data producing one-dimensional output and the question we may want to ask is like well if we get an input let's say it's 30 how can we predict what the output is likely to be right given that all this data that we've seen when we see a new data piece but we only see the input part of it not not what the output's supposed to be can we predict what the output is supposed to be and you can right and the mathematically kind of robust way to do it is to put a line of best fit through this data and the reason it's a line is because there's no reason to believe it should be anything other than align with like this kind of data but how do we find what that line should be right so the line will be able to tell us like if we feed in x it'll give us y because it'll multiply x by some number and then add another number right so it'll be like ax plus b but what should a and b be set to right how should we set this line well what we can do is we can minimize the squared error between all of the data points that we've observed and some candidate line right so given some line which is just defined by two numbers a and b we can compute the squared error on all the data that we've seen and it's done as in as in this formula right here um and then we can try to find the settings of the line you know a and b parameters a and b that minimize the squared error and that'll be the line of best fit and more generally we can call this squaredair function a loss function and our goal is to minimize the loss function okay so we find the setting of weights and biases like a and b that minimize the loss function and this is the whole idea of empirical risk minimization now in neural nets the function f given you know w and b weights and biases of x that's the neural net so weights and biases are the parameters of the neural net the loss function is can be you know mean squared air or maybe it's some other type of loss but that's basically how you would train or how you
Full Stack Deep Learning - Spring 2021,1,1155,Gradient Descent,would determine whether neural network is solving the problem or not and for classification all that changes so regression being you try to predict from the input to some real valued output classification you predict from input to some categorical output right so it's not it's never going to be like 2.3 as an output it'll be exactly zero or exactly one or exactly two and these things will correspond to like the label of the data point and for that we typically use cross entropy loss and you will learn a lot about this in the reading this week okay so we have the loss function so we can kind of see like all right if we have some weights we can understand how good or bad the the model is but how what do we actually do with that right well our goal is to find the weights and biases that optimize this function as in minimize the loss and it might be a crazy looking function right of the this loss this loss function given the data might be quite crazy looking what we can do is we can update each weight by setting it to the current setting of the weight okay minus some alpha which is can be called the learning rate and then the gradient of the loss function with regard to the weight so we have some random parameters we uh evaluate them on the data that we observed we obtain we we can compute the loss function and then in order to improve the fit of the neural net to the data we will update each weight by doing this we'll just subtract the gradient of the loss function with respect to that weight multiplied by some learning rate that's all there is to it and uh the way we want to do it is we always want to move in the direction of kind of steepest descent there's some tricks to making sure that if you know if if your data is lives in like some part of the space that's kind of smaller than it could be gradient descent will have a tougher time than if the data was what's known as well condition which means that it usually has kind of zero mean and equal variance in all dimensions because that gives the gradient to sound kind of the most signal to to follow so we can talk about weight initialization we can talk about normalization these are all first order methods the the gradient descent being you compute the gradient right first order gradient and then you just update the weight with that gradient there's also second order methods right where you can compute the second order derivative of the loss function with regard to the weight but we don't typically use them because they're very computationally intensive but there are some approximate second order methods that can play a role in training neural networks more quickly and if you just remember the name adam right that would be the optimizer we're going to use in the labs and that's what it's trying to do approximate second order and lastly we could look at all the data we've ever seen compute the loss update each weight but in practice it might be better to actually compute the gradient on just a subset of the data not the entire data so this is known as batch gradient descent or stochastic gradient descent and stochastic gradient descent might use a batch of just size one right so you you look at one data point you compute the loss on it and you update all weights with just that loss and then you look at the next example so that's known as the cast of gradient descent and the reason to do it is because it's a lot less compute per step right you don't have to go through all the data you have a million images on imagenet just to update the weights the first time you can just look at 32 images compute the loss update the weights and then the next batch of 32 you look at the model is going to be a little better suited to the data so it's going to train faster using less compute but it is more noisy um but basically it's what we do we use batch gradient descent the train
Full Stack Deep Learning - Spring 2021,1,1437,Backpropagation / Automatic Differentiation,so back propagation um is something that we can talk about now because we've reduced the whole concept of learning to just optimizing a loss function right we're just trying to find weights and biases that that minimize this loss function and we figured out how we can do it with stochastic gradient descent which we can do by basically taking the batch of data computing the loss function on it computing the gradient of every weight with respect to that loss function and then updating every weight with that gradient times some learning rate alpha but how do we efficiently compute these gradients you know it's easy to say well we'll compute the gradient how do we actually do it so gradients just another word for derivatives and derivatives you've seen in your calculus class right it's just it you know you can do it symbolically you can figure out given a function what its gradient is but the neural net is never just like you know e to the power of x or something like that it's never going to be that easy to compute its gradient but that's okay because the neural net is made up of computations where each of the computations does have a gradient because it's maybe just a linear function like ax plus b right that does have a gradient and then we can apply the chain rule for everything such that we can get the gradient of the loss function with respect to the weight way at the bottom of the neural network very far away from the loss function just by kind of chaining it through all of the layers of the neural net and that is called back propagation right and the good news is that we don't have to even code up the derivatives ourselves because we use automatic differentiation software so that's like pi torch or tensorflow or basically anything else you're likely to see will compute the gradients for you so all you need to do is just program the forward function like f of x you know given weights w and then pi torch will automatically compute the gradients for you
Full Stack Deep Learning - Spring 2021,1,1569,Architectural Considerations,so the simplest neural net architecture is what we've been talking about also known as a multi-layer perceptron it's literally just perceptrons arranged in layers um that's all it is sometimes this is called a fully connected layer instead of instead of a a perceptron layer um and we know that that's really all we theoretically need to represent any kind of function but we might need like an infinitely large such network and we might need an extremely large amount of data to actually learn the weights that will do the right thing so what we can do instead is we can encode knowledge that we have about the world into the architecture of the neural net so for example for computer vision we use convolutional networks and what that means is that it's a set of weights that are kind of tied together and so no matter where in the input they're applied they're always in a structure that's local which is actually what happens in our eye which we know from you know studies of the eye and the brain and it's also what makes sense for the world because the world is composed of objects that don't radically change as you move around them and as they get closer to you right it's an edge is going to be an edge even if it's closer to you it doesn't it doesn't change and for sequence processing like in natural language processing we use often recurrent networks which have temporal invariance which is something that's usually true for sequences like text the the rules of language don't change as the sequence wears on the rules are going to be the same no matter where in the sequence you are and so your neural network can kind of know that by being structured in a certain way and then when we optimize these neural networks well we can have let's say 10 layers where each layer is not so wide so maybe it only has like you know 10 channels but there's 100 layers or we might have 10 layers but each one has 100 channels that's like depth over width which one is better well that kind of depends on like empirically what we've observed there's no theory that really can help you out here but certain things work better in practice and so part of being a deep learning practitioner is just really kind of obtaining that knowledge by doing it by reading papers taking courses like this skip connections you can connect the input kind of around the layer that's processing it so that the output of the layer is added to the input itself and that tends to help a lot in in back propagation there's all kinds of tricks that we're going to you know cover
Full Stack Deep Learning - Spring 2021,1,1741,CUDA / Cores of Compute,and lastly why exactly you know did things kick off in 2013 we had bigger data sets but we also got good libraries for matrix computation on gpus particularly with nvidia cuda right and that's using graphical processing units which had you know until this point been only used for gaming but with this cuda library being released by nvidia you could use that graphical processing unit to just do general matrix computations which was very applicable to a lot of scientific computing including deep learning and the reason it's so crucial for deep learning is because all the computations in neural networks all the computations that we've seen are just matrix multiplications and matrix multiplications are easy to paralyze over the computational cores of a gpu
Full Stack Deep Learning - Spring 2021,3,0,Introduction,okay so so let's dive in and talk about recurrent neural networks um and so recurrent neural networks at a high level um this is sort of like an analog in some sense for convolutional neural networks uh for sequence data um and you know that idea of being similar right that we're going to try to exploit some structure about the data that's coming into the model um in this case the fact that the data is coming in in the sequence to build a more efficient neural net architecture um hang on a second all right so what we're going to cover today um first we'll talk about sequence problems and just what are some of the types of problems you can solve with these architectures then we'll talk about recurrent neural networks which are the main family of architectures we'll be talking about today and we'll talk about some of the problems with recurrent neural networks and some of the solutions that people have come up with to address those problems then we'll do a case study on machine translation and we'll talk about how um one of the earlier uh neural machine translation systems from google uh worked and i imagine this is probably not still not how it works because these days everything is transformers which we'll talk about next week then we'll talk about the ctc loss which is a loss function that's useful for sequence problems that we'll be using in the lab we'll talk about some of the pros and cons of for current neural network architectures as opposed to some other choices that you have and then finally um we'll do a very quick preview of non-recurrent models for sequences and then next week will be all about that
Full Stack Deep Learning - Spring 2021,3,94,Sequence Problems,okay so sequence problems like what are what are we even talking about here so um simple example of a sequence problem that you can have as a mental model is time series forecasting right and so in time series forecasting uh the input to your model is a time series so a history of um of a value of some data point and your goal in this task is to predict what the next value is going to be so the input is a sequence and the output is a single value you can also do this with other types of data other than just time series so for example you can have the input sequence be text and then predict the sentiment but sequence models can also handle cases where the output is not just a single value the output is itself a sequence so for example in translation the input to your model is a sequence of text and the output is the sequence of text in another language similarly speech generation speech recognition and speech generation you know your input is say an audio waveform and your output is maybe the text that is contained in that audio waveform um but sequences um don't have to just be the input of models like you can have uh problems where your input is a you know a single value or a single vector and then your output is a sequence of um maybe variable length so for example you can generate music using sequence models you can caption images right so you can take a single image as an output which you can view as a sequence or you can view as just a single input and then you can output a description which is a sequence or in question answering um it's maybe another example right where your question is some text or your input is some text and your output is a subset of the text that went into it okay so there's kind of a few broad groupings of sequence problems that we saw on the previous slide so one is once many where you're you have a single output and then a single input and then the output of your model is a sequence of outputs there's many to one problems where the input to the model is a sequence of inputs but there's a single output and then there are many to many problems where the input to your model is a sequence and the output is also a sequence maybe of different length um and so we will uh we'll talk about how to use kind of a similar architectural structure to solve all of these types of problems okay so what you know one question i think is worth asking just like we asked in the confnet lecture is why not just use feedforward networks for to solve these problems so one way you could do that is if you have um like let's say you're solving a many-to-many problem which is the most general case uh maybe you have three inputs you could concatenate those inputs pass those through a fully connected network to an output and then you could reshape that output so it has the shape that you eventually want um one problem with this is that this doesn't really handle sequences of arbitrary length well you can one way to deal with this problem is you can pad sequences to the max length right so you can add kind of empty characters in the sequence if the sequence is shorter than the max length that you allow but one challenge with this is that the memory requirement for um doing this computation scales linearly in the number of times time steps right so um if you increase the max length of the sequence then the matrix that you need to multiply with um also increases in size and so this is kind of undesirable scaling property for this type of model but maybe more importantly than that this matrix that's mapping your flattened sequence to your outputs is maybe overkill in some sense and so so the way to think about that is what is this this matrix doing right so this matrix is learning a mapping from um every single dimension in your input to every single dimension in your output and in particular if there's some like pattern that you're trying to recognize in your sequence so to give you an example um maybe a particular word um is somehow correlated with the output that you're trying to produce then um that model will need to learn weights for that particular uh pattern at every single position in the sequence um independently right and so it's in some sense it's very data inefficient because um if you're trying you know if um if having you know the word cat in your sentence is very predictive of the output then you need to see examples in the input where cat is at every single position um and so this kind of ignores some fundamental nature of the problem some structure that we expect the inputs of this model have which is patterns that repeat themselves over time okay so the next thing we'll talk about
Full Stack Deep Learning - Spring 2021,3,388,Review of RNNs,is kind of the core type of model that we'll be focusing on today and how this addresses the issue that we pointed out out in sequence problems the main idea behind recurrent neural networks is that instead of having a single massive matrix that you multiply that has independent weights for every position in the sequence instead we're going to do stateful computation um and so what this looks like is the output that your model produces depends on the input to the model at the current time step in the sequence as well as some state that the model maintains over time and so at each time step when you're fed a new input to the model what you produce is you produce the output for that time step as well as the next hidden state for the model one way to make this a little bit more concrete is to look at how this rolls out over time so before the first time step you have some starting hidden state for your model so that's h0 in this diagram then at the first time step you receive the first input and your model computes given h0 and the first input x1 it computes the first output y1 as well as the first um or really the second hidden state h1 and that at each subsequent time step it performs the same calculation um so using the same neural network weights it takes the input that it sees at that time step and whatever the value of the previous hidden state is and it uses that to reduce the next hidden state and the next output so what does this look like in code um let's focus first on the bottom uh the bottom function here and so this this uh this function step is the function that gets called every single time the model sees a new input and so what the model does is it can it first um computes the next hidden value and we'll come back in a second to how it does that but for now just think of that as a black box and then it uses that hidden value um and uh it has a matrix multiplies it with a set of weights to produce the the output for the model okay so now now let's focus on this um how this compute next h function works so the way the compute next h function works is um this thing in the top function here which is kind of like this 10h of the sum of two matrix multiplications and so to make this a little bit more concrete this is what's going on in compute next h so you have you have two inputs to this function one is a property of um of this class which is the previous hidden state ht minus one and then the second is the input of this timestamp xt and the way that this um this function works is it takes each of those inputs and matrix multiplies it with a um with a separate weight matrix that is a property of this recurrent neural network layer and so this um you know this h this nh dimensional hidden vector gets multiplied with a h by like nh by nh dimensional matrix and then this nx dimensional model input gets multiplied by a nh by nx dimensional weight matrix and so the outputs of both of those matrix multiplications are both nh dimensional and so we can add those two outputs together and then pass the output through an activation function of some kind which in the most basic recurrent neural network state is a 10h so coming back here the way that we compute the next hidden state is that we perform two matrix multiplications one with the previous hidden state and one with the input vector x we add those together and then we wrap them in an activation function like 10 h and so this is this is really um all you need to know for like the most basic kind of for current neural network that you can create okay and so one thing to note about this is the way that this model works is that we're calling the step function every time we have an input in the sequence and at each of those inputs it produces a different output um so this this model as we've laid it out so far works well when you have many to many problems where that are aligned right so where you're you want to produce exactly one output for each input to the model um but in general that's not always what you want to do so um the next case that we'll talk about is how to use this type of recurrent neural network to solve many to one problems right so where instead of having an output at each input we instead have one output at the end of the sequence um and so again like expanding this a little bit um what we're going to do is we're going to take this last um this last output value yt and then we're going to use that to produce the output for the entire sequence so again this this output value yt is kind of the state um at the last time step and that's what we're going to use to compute the the value for the entire sequence so more concretely like if the input to your current neural network is subtext like let's say a review then you could pass that text to um a to an rnn which we'll call an encoder and then we'll look at the last output and so the last output is just a vector then we could pass that vector to a classifier which you could also call it decoder and that that classifier will produce the the single value that you want to output which is in this case the sentiment positive or negative of the sequence um and so this is this is kind of one case of a more general architectural pattern called encoder decoder architectures and encoder decoder architectures are kind of the the architectural pattern that we'll study in this lecture so we talked about um the general um kind of recurrent neural network setup and we talked about how to use recurrent neural networks to solve many-to-one problems and we talked about kind of this architectural pattern which is having sort of one network that serves as an encoder that maps your input to a single vector and then a separate network that maps that um that vector to your output um and now we'll talk about how to use this encoder decoder architecture to solve one-to-many problems so um one example of a one-to-many problem though you might want to solve is uh you know we take as an input an image of something that's happening and we want our model to output some text that describes what's going on in that image so one way to solve that problem using recurrent neural networks and an encoder decoder architecture is um you can start by mapping that input through um some encoder which in the case of the input being an image might be a comp confident and then instead of having that comp net map all the way to the classification of the image you can take one of the vectors from the the last couple of fully connected layers um and so this is this is your hidden state of the model and so this vector now can be the um you now can serve as the initial hidden state for your recurrent neural network that's used to generate the text that describes this image [Music] one other thing to note here is um thinking of this as an encoder decoder architecture and thinking of the encoder and decoder as separate networks um is more is really like a um mental heuristic for understanding the architecture um the networks aren't really separate in any meaningful sense right so they're um they and in particular they're connected um during back propagation so when you when you train an architecture like this your loss function propagates all the way from the output back through the decoder to the hidden state and then back from the hidden state all the way through um the weights in your encoder okay so let's uh let's zoom in on this example a little bit and make it um slightly more concrete so um we've taken our input image we've passed it through a comp net we're looking at one of the last couple of hidden layers and that's a vector so then we're going to just use that vector to um we're going to we're going to treat that vector as the initial hidden state of the rnn um and then we're basically just gonna pass the rest of the um and then we're gonna continue propagating that rnn forward and one thing to note here is in the previous examples we've seen there's always been an input x at each step um and so like one question you might have is you know what is how like what is the input to each step in the sequence um in uh in an image captioning model and the answer is there's like kind of one trick that's pretty common for these architectures that are generating sequences which is you just feed the last output of the model back as the input at the next time timestamp so in this example these y's are forming are playing the role both of the output of the model as well as the input of the next step of the model and then there's one other trick to be aware of here for how these recurrent neural network decoder architectures work and that's you know how do you know like if if you're if the input of your model is just the output of the previous step how do you know when to stop generating right like there's always a new input because it's just the output of the previous step um and so the way that we often deal with that is by adding a special character to your vocabulary and that character is um uh plays the role of telling the generator that okay now it's time to stop generating new characters all right so in terms of how you could train something like this um you know the simplest thing that um could work is you could have a label for each step in the sequence um which is the you know the true sentence that describes this output and then the loss function that you'd use is um for each of those outputs um that's a character and so you can take a cross-entry loss between that between the predicted character and the actual character in the sequence and then to form the loss function for your entire model you can um you can just uh you can just sum all of those cross-entropy loss terms together and so that's something differentiable that you can back prop through the entire sequence and um uh and train your model that way so there's some problems with using this as a loss function and later on in lecture we'll talk about um a better loss function for these types of sequence models all right so we've talked about how to use this encoder decoder pattern to do uh many to one problems and to do one to many problems so now um pretty natural extension is to apply this to the most general case where the input to the model is a sequence and the output to the model is also a sequence but the sequences are not necessarily the same length so for example um the example that we'll talk about for for the next um little while is machine translation so in machine translation you take an input which is a sequence of characters in one language um so sequence of characters that form words in english and then the goal is to predict how that sentence should translate to another language like french and the kind of baseline architecture that we're going to talk about here is an encoder decoder architecture where both the encoder and the decoder are both recurring neural networks so to zoom in on this a little bit what this might look like is you start at time t equals 0 with the hidden state of the encoder network then at each time step time at each of the um the values of the input sequence so the sentence in english um you um you combine that input with the previous um uh hidden state to protect produce the next hidden state then at the end of that sentence you take the final hidden state and you use that as the initial hidden state for the decoder network so the decoder network starts with an initial hidden state that's created by the encoder network and then it does a similar recurrent decoding scheme that we saw in in the last section so at each at each step it takes the output of the previous step as input combines that with the previous hidden state to produce the next output so for the next few minutes we'll talk about some of the challenges with using this type of architecture to solve translation problems and we'll talk about some of the ways that people have improved on recurrent neural networks to address those problems and so just to give you a little bit of of a preview one of the problems with this setup is that you have you know you have the sentence and the sentence might be really long um but all the information about what the first sentence means needs to be encoded in this in the only thing that's passed between the encoder and the decoder which is this you know the the initial hidden state of the decoder and so um you know in practice you'll you'll choose the dimensionality of that hidden state in advance so maybe that's 128 dimensional and then no matter how long your input sequence is all of the information that's contained in that input sequence needs to somehow fit into that 128 dimensional vector um and so that's kind of the intuition for some of the problems with this encoder decoder architecture for a more complex problem with um with potentially longer input sequences like machine translation all right um so before we get into the case study the
Full Stack Deep Learning - Spring 2021,3,1320,Vanishing Gradient Issue,next thing that we're going to talk about is um we're going to we're going to upgrade our model for our mental model for how our current neural network works from kind of the simplest possible rnn setup that we described before to one that is actually more commonly used in practice called the lstm all right so let's um let's to motivate the lstm let's talk about some of the things that we might want in our current neural network architecture so one goal is we want to be able to handle long sequences of input data and in particular along those long sequences we want to be able to connect events from um from the past to outcomes in the future so you know another way of describing this is long-term dependencies and an example of what i mean here is um if you have a sentence and you're trying to describe what happens in the sentence at the end of the sentence even if that sentence is really long you might need to be able to remember a name of a character um like a character in a novel that occurred at the beginning of the sentence um so you need to be able to connect events in the future like what is a summary of of maybe this entire novel to things that happened near the beginning of the sequence so we need to be able to handle long-term dependencies in reality the neural net the recurrent neural network model that we described before does not do a very good job of handling long sequences so in practice kind of anything be more than like 10 or 20 times time steps um tends not to work very well with vanilla rnns and longer term dependencies really get lost and so the reason why is this idea of vanishing gradients so to explain how like what vanishing gradients do and why those prevent long-term dependencies from working first we'll talk about um a little bit about how we actually back propagate a loss function in recurrent neural networks um and so i'm gonna go through this piece relatively quickly so if you're not familiar with it it's worth reading up a little bit on back propagation through time um but you know the the main thing that you need to know is that back propagation through time works exactly the same as regular back propagation um you just have to sort of unroll the entire graph across time before you do your back propagation steps um and so in particular if we're trying to study the gradient of the loss function with respect to some of the weights of our network then the first thing that we need to do is we need to study the the gradient of the loss function with respect to the hidden state of the recurrent neural network and then we'll pass that gradient backward um we'll combine it with the um uh the the gradient of the the um the previous hidden state with respect to the current hidden state and then we'll use that um combined gradient to find the gradient of the loss function with respect to the model weights and so the the um the observation to make here without going to all the details about how this works is that the gradient of your loss function with respect to the weights that describe the transition between the previous hidden state and then the current hidden state depend on um the gradient speed of um uh the gradient between subsequent hidden states in um in the model and so if you're recalling kind of the vanilla um uh recurrent neural network setup that we had the way that we computed the next hidden state from the previous hidden state was by this 10h of a sum of two matrix multiplications and so um when you're computing this this factor of this like uh gradient of subsequent hidden states then each time you do that that increase it that introduces a gradient of a 10h so 10h prime into your um into into the gradient with respect to that hidden state and so the problem with this is that if you look at the derivatives of common activation functions here's here's a plot of sort of the three most common activation functions um uh as a function of the value of the input to those functions and so in particular both tanh and sigmoid have derivatives that are very close to zero when the values that are going into them are large and so what tends to happen in practice is that you um you have relatively large inputs to that 10 h prime that produces an output that's close to zero and the magnitude of um of the gradient where you have all these uh like 10 h primes multiplied by each other um eventually hits zero in numerical precision so after enough gradients enough time stamps the gradients are just too small and so one question you might ask is um well what about values right like values have a gradient that's equal to one any time the input is positive um and the answer there is that relu rnns in practice often have the opposite problem which is exploding gradients right so if you have many time steps um then uh like gradients tend to get too big
Full Stack Deep Learning - Spring 2021,3,1672,LSTMs and Its Variants,all right so next thing that we're going to cover is lstms and so when people talk about recurrent neural networks in practice usually what they're talking about is lstms or some variant on lscms and so the the key idea behind lstms is to use is to change our compute nextstage function and so instead of using the simple compute next h function that we used in the previous examples we're going to use something that is a little bit more complicated but helps in some way preserve gradients over time and so um what i'm going to do in the next few slides is i'm going to introduce you at a high level to how you can think about um the the architecture of what's going on inside of um inside of an lstm unit and but won't go through all the details but but again hopefully this will give you some intuition about how they work and then you can fill in the details on your own later [Music] so the main idea behind lstms is that in addition to having this hidden state that gets updated ever at every time step we introduce this this um this new state that also gets passed from time to time step to timestamp um called a cell state and that cell state has very particular rules about how it gets updated at every time step and so in particular there's kind of three um three main interactions that the hidden state has with the cell state so the first interaction is that um is called the forget gate and so that the like at a rough level the function of this forget gate is to decide which parts of the old cell state we want to get rid of as a result of seeing this new input to the model and so the way this works is you take the previous hidden state you combine that with the input to the model and then you pass that through a sigmoid so that produces a value between zero and one for each of the dimensions of the cell state and then when you element wise multiply that with the cell state um each of the values of the cell state is um you know either mapped to zero or left the same or scaled somewhere in between and so this this what this gate helps you do is kind of get rid of old information that no longer needs to be in the self state the next component of the lstm is the input gate and so this decides like how to actually incorporate new information into the cell state so the way this works is you again take as an input the the inputs to the model as well as the previous hidden state you map through those through a tanh to produce values between negative one and one and you combine those with uh values that are the output output of a sigmoid and so together you get um you get a a new vector that you just add to the previous value of the cell state so you've incorporated new information and then the last step is how do you use like how do you actually produce the next hidden state so the way that you produce the next hidden state is that you um again you combine the current input the previous hidden state um you sigmoid that um and then you 10 h the value of the cell state multiply that by that value between zero and one and then that's passed to the next time step and to the output as the hidden the hidden state for this time step okay so there there's there's a lot going on there um a lot of moving pieces and like in particular you might be wondering well why does this actually solve our problem and uh and to answer those questions i'm happy to talk more about this as well but um i i would actually highly recommend reading this blog post from chris ola which i think is kind of the most clear explanation for how lcms work and why they solve this problem but hopefully this gives you a little bit of intuition about what's happening under the hood so one other thing you might have heard of if you're like reading machine learning papers is like gru's and other uh you know other alternative recurrent neural network architectures and so i want to talk a little bit about about that like is is there something better than the lscm out there um two papers that i like that explore this question um these are both empirical papers where they compared different variants of the lstm including gru um so in the first one they compared nine lcm variants on three data sets and concluded you know empirically it's kind of hard to beat a regular lstm in terms of like overall performance across different data sets uh a later paper um went even deeper on this into this question so instead of just comparing like a uh you know measly nine lstm variants they use neural architecture search to generate um 10 000 architectural variants um they combine they compared those architectural variants on three different data sets and they concluded that gru's actually tend to perform a little bit better than lsms and they also found some architectures that they made up using this procedure that performed a little bit better than gru's and so what should you take away from this right like is should you be using gru's should you be using lstms should you be using like some other architecture so our advice here is um lstms if you're going to use recurrent neural network architecture which maybe not that many people do anymore um but if you're going to they tend to work very well for a wide variety of tasks and um gru's might be a little bit better might be a little bit easier to train and so if you know if you if you uh if your trusty lstm doesn't seem to be performing well then you could also give gru's a try um but if you're if you're starting to um dive deeper into recurrent neural network uh units than that then maybe there's a bigger problem in how you're setting up your problem um up next is we're gonna talk more
Full Stack Deep Learning - Spring 2021,3,2050,Bidirectionality and Attention from Google's Neural Machine Translation,about machine translation and we're gonna introduce two ideas um one is by bi-directionality and the other is attention so these are sort of the big ideas to take away from this section and um this is all from this one paper from back in 2016. so at one point this was running in production at google um i highly doubt that it is anymore um but it's it's worth knowing about anyway [Music] so um one one thing i want to talk about is just in general if you're like reading machine learning application papers what what are the questions that you might want to ask yourself to understand what's going on these papers um so here here are some questions i think are worth asking one is just like first of all what is the problem that they're trying to solve so what are they using the neural network for then what model architecture are they using to solve the problem um how are they training that model architecture so what is the loss function and what is the data set that's trained on and how do they actually perform training and then finally since we're thinking about applications of machine learning here the last question that i think is very much worth considering is how inference works for this model so are there any tricks that you need in order to do inference on this model in production so starting from the problem again we're working on machine translation so the goal is to translate a sentence from one language to another um and the architecture that they use is motivated by the same encoder decoder architecture that we talked about earlier [Music] um and again this is kind of um how this would work if this is how this would look if if this is all they did like encoder decoder with a with a regular lcm but there's some problems with just doing a single layer lcm as your machine translation model and one of the biggest problems is that this is just gonna underfit this task pretty badly so one thing that you could try is you can try stacking lstm layers and so this is a diagram of what this looks like you have multiple lcms each lstm is reading from the lstm before it as well as its own previous hidden state to produce the next hidden state which is the input to the next lstm um but stacked lcms are hard to train so this is this is a quote from the paper which says that they you know they were able to get them to work pretty well up to four layers and by the time you got up to eight layers this is just very unreal unwieldy so one solution here is to add something similar to a a resnet here so to add residual connections and again this works very similarly to how it does in a resnet where um between the the um between the layers of the lstm you uh you have this skip connection so that the input to the lscm at the first layer is passed directly via some to the lcdm in the second layer in addition to the output of the first lstm layer so that's one way to make it easier to train these large stacks of lstms there's another problem though um and this is one that we alluded to earlier right and so the problem is that like when you're encoding an entire sentence there's a lot of information that has to be encoded in that last time step so there's a bottleneck here there's an information bottleneck between the encoder and the decoder [Music] and so the solution that they use is um is a is a class of techniques called attention which we'll be talking a lot about next week but the key idea behind attention is that instead of compressing all the previous time steps into a single hidden state instead you want to give the decoder access to the entire history of of time steps um but you know that's a lot of information for the decoder to uh to take into account and so we're really only going to pay attention to a subset of those past factors so to make this a little bit more concrete let's say that we're translating a sentence from english to french and let's ask the question about like which parts of the sentence in english do we really care about when we're predicting each character in the french sentence right so maybe for the first um the first sent the first word in the french sentence um we only really care about the first few words in the in the english sentence and for the second word maybe we care about the same words uh maybe for this third word this is this means brown in french so um our network might only really care about the word brown in english to translate this word and uh and so on so one way we can formalize this into something that we can build into a network is um instead of having a subset of the of the the input to the model that we pay attention to to produce the output instead we're going to have a distribution over all of the inputs to the model and so the the um the way that we're going to compute the the attention value for this particular word in the sentence is we're going to use some relevant scores and these relevant scores tell us how relevant each word in the input sentence is to this particular word in the output sentence so there's a couple questions that this brings up one is like okay how do we how do we actually compute these relevant scores and then the second is how do we use the relevant scores to compute the um the value of of this word that we're trying to predict so let's focus on the second question first um so the way that you use relevant scores in an attention setup is that you compute a relevant score for each of um in many like in many cases the way that this works is you compute a relevant score for each of the hidden values um in the recurrent neural network encoder so each of those hidden states is given a relevance weight p p sub i and then the way that you use those is you produce an attention score um for um that combines all of those hidden values in a weighted sum according to the relevant scores and then that that weighted sum is passed as another input into the next prediction task that the model has to do so you're using um you're using you're using something in the decoder to compute some relevant scores for each hidden state in the encoder then you're combining all the hidden states in the encoder via a weighted sum which is weighted by the relevant scores and you're passing that as another input to the decoder so this is what it looks like over time so one thing that you can do to build a little bit more intuition for what's going on in attention models is you can look at attention maps and so these are basically ways of visualizing the the weight that each of the the input the the parts of the input sequence have on prediction of a particular uh word in this case in the output sentence and so this is this is one way of visualizing it so for example like um you know this french word the uh this means agreement in french i guess um so you can see that the weight is almost entirely on agreement in the english sentence um but you know some of these other words like this uh well let's let's find one that's more interesting than that um but like th this french word for example kind of depends equally on both this english word and this english word so the the model is paying attention to both of these things you know almost equally another way to visualize this for sequences is you can plot this on a heat map and so the the cells on the heat map um so you know these are the outputs of the model the cells on the heat map um you know lighter colors correspond to more weight being placed on the on that row this is another way to visualize what's happening in the intention inside of a network and then the other thing i'll mention on attention is just attention is is useful in many machine learning problems not just translation um so uh you know um uh turning like a raw audio waveform into the text that's contained in that um that's being spoken in that way waveform these models often use attention and it's not just for like what you would think of traditionally as sequence models either right it's it's also often used in image models so if you um if you're like for example like if you're if you're doing this task of trying to describe what's in an image so encoding the image and then decoding it um to produce a sentence that describes what's happening in the image then one thing that you can do is you can add an attention mechanism to that that allows the decoder to pay attention to certain parts of the image and so this is visualized here where you know um this is a this is the image that was an input this is the output that the model produced and here's a visualization where the lighter parts of the image are the parts of the image that the model paid more attention to when it's producing its outputs okay so this is kind of where we are where we're at with this um this model architecture so we have um we have the stacked lcm and we have this attention module that's allowing the decoder to communicate more with the encoder um another problem with lcms is they only consider backward context so why is why is that a problem um well when you're translating a sentence um you know and you're deciding how to translate the word quick quick can mean different things in different contexts so understanding how the rest of the sentence is going to unfold might be important to deciding how you translate the sentence so the simple idea that you that you can use to make order not matter or like directionality not matter in the sentence is to use two separate lstms one to process the sentence in the forward order and then the other to process the sentence in backward order um so this is called a bidirectional stm so what this looks like is you have sentence you have an lstm and then you have a separate lstm that is processing the same sentence but it's the hidden states move in the opposite direction and then at the output you just concatenate those two inputs and pass it on to to the next layer and so this is briefly what this looks like in the architecture [Music] okay um what loss function was used um and what data set it was trained on and all this stuff i'll skip over but worth uh we're taking a look at those in the paper [Music] all right so to summarize the way that this um kind of like early landmark results in machine translation worked was they um it's built off of this recurrent neural network encoder decoder architecture that we talked about at the beginning but with some improvements so instead of rnns it's lcms instead of a single lstm it's stacked lcms with residual connections and an attention mechanism that's added to it so that longer term connections can form and then to encode future information we also made this model bi-directional and it's trained using a standard cross-entry loss on a large data set um and there's some tricks that they use to speed up inference and training but um that's kind of that's the recipe for training a state of the art neural machine translation system in as of 2016 or 2017.
Full Stack Deep Learning - Spring 2021,3,2798,CTC Loss,so the next thing that we're going to talk about is what's called the ctc loss so what's the goal of ctc loss um so in some of the labs one of the problems that we're going to be working on is taking some head written text and turning that into what is the text in this handwriting so our the input is like kind of an image of a line of text and then the output is a sentence so the question is how how would you actually model this right so um based on what we talked about before one way you could do this is you could have an encoder decoder architecture right where the the encoder takes a particular window in that sentence passes it through a comnet passes the output of the comnat to an lstm that l stand produces uh a character and then we use that character as like what is the character at that position in the continent and so what this in an ideal world what this could look like is you know we slide this comp net window over this input image and at each position that we slide it over we um combine the output of that convent with um with the previous hidden state of the lstm we produce the character that we expect to see at this position on the image so in a perfect world you know this just works great and we're done the problem is like what if we're dealing with an input that's scaled differently so we might even take the same sentence and just stretch it out more and then like let's see what happens if we try to apply the same window size to this problem well as we slide this comnet window um you know for the for the first two positions that the the that convolutional filter is being slid over it's uh looking at the same character so we might produce the same character twice um and you know the same thing might happen like in different parts of the sentence so the question is like how do we deal with the fact that the the inputs and outputs might be scaled differently or they might be misaligned um and so more generally like this kind of misalignment problem can happen in um lots of different sequence of sequence problems and um yeah and and throughout this i'll be using images from this uh blog post below that covers the ctc loss in more details and so again would recommend checking that out if you want to understand a little bit better how this is working but the intuition for the solution is let's say that we take some input and we slide our window over that input and we produce some characters in the output but those characters um we might have repeated characters there um the intuition for how the ctc loss works is like if we see a character you know c or a two times in a row then let's just assume that that corresponds to the same character in the output sentence and so we'll combine subsequent characters that have the same value um and so in this case like we combine the two c's we'd combine the three a's and we produce the output that we want um one problem with this is how do we deal with sentences that actually do have are like supposed to have two of the same letter in a row and so that's kind of the one extra wrinkle that will add to the ctc loss and that's that we add these um kind of extra epsilon tokens as as one of the possible things that the model can output so the way this works is we'll slide our window over the entire sentence we'll produce the um and then the the model will produce either a character or an epsilon token and then we'll have a rule for how we merge characters and epsilon tokens together in order to produce the final output sentence and so the rules that we'll follow are the first thing that we'll do is we'll merge all repeated characters and then we'll remove any epsilon tokens and the remaining characters are the output and so the reason that this solves our problem is that if you actually want to output two of the same character in a row then all the model has to do is instead of outputting those two characters in a row in the raw outputs it said instead has to output the character and then epsilon and then the character and then our merging rules allow us to have those two characters that remain at the end so there's more technical details that are important to know here if you actually want to implement and train this for example um how do we know this is actually differential um or how do we make it differentiable and uh again refer you to this great blog post for when you wanna understand those details um but yeah this is kind of like what this works like what what the what this looks like when you're decoding a raw input sequence and um we'll talk a little bit more about this in a second for lab three because we'll actually implement ctc loss for our rnn model
Full Stack Deep Learning - Spring 2021,3,3132,Pros and Cons of Encoder-Decoder LSTM Architectures,i wanna talk quickly about some of the pros and cons of the the neural net architectures that we've been talking about so far today um what are some of the pros well one of the pros of these recurrent neural net architectures is that which um hopefully we hopefully is is clear from some of the examples that we've looked at um but this this architectural pattern this encoder decoder pattern with you know um lstms or stacked bi-directional lcms of the tension or whatever variants of your current neural network you want um as either the input or the encoder or the decoder or both it's a very flexible architectural pattern and it can solve a lot of different problems right so again it works for one's many problems it works for many to one problems and it works for many too many problems even if the inputs and outputs are misaligned and another pro is that there's kind of a long history of these models being successful in nlp and other applications and for a long time really like up until 2000 and what 2019 is that right um most of the like state-of-the-art sequence applications used some variant of a recurrent neural network architecture but really since 2018 2019 um almost all of those applications have been replaced with um transformers and variants of transformers which is what we'll be talking about next week [Music] so what are the cons well um recurrent neural network training is inherently not as paralyzable as other types of models and the reason for that is that you need to go in sequence right so in order to know what the value of the um you know the hidden state at like the end of the sequence is you first need to compute the hidden states in all the previous steps in the sequence um and so there's something that's inherently sequential about that um and so um you know parallelism is one of the things that makes training larger and larger models more feasible so we have somehow less parallelism in our current networks than we do in other types of networks and so in practice these things tend to be a lot slower to train and in practice they're also often very finicky to train so lstms and other rnn variants tend to require a lot of tuning and there's a lot of tricks that goes into training these things well so it's uh it's not a particularly easy thing to just kind of pick up and start working with off the shelf
Full Stack Deep Learning - Spring 2021,3,3295,WaveNet,all right so the last thing i want to talk about in this section is just a quick preview of what sergey is going to talk about next week which is non-recurrent sequence models [Music] so the first insight here is that just because you're working with sequence data it doesn't necessarily mean that you need to use your current network model and so today we're going to look at a convolutional approach to sequence data and next week as i mentioned we'll expand and talk about transformers and so the approach that we'll discuss is from this paper which is a early success in generating raw audio and for at least for a while was used in google assistant and google google cloud text-to-speech and um i wonder if it still is or if it's just like google is transformers all the way down these days i'm not sure um but for at least a few years this was used in production and so the main idea that's introduced here is convolutional sequence models [Music] so again what problem are we trying to solve here in the wavenet model the problem that they're trying to solve is taking some input text so what do you want the model to say and then outputting an audio waveform of speaking that text so you want you want to um produce some like realistic sounding speech of you know the model reading that text a lot what model architecture is used um so again our baseline model architecture is this rnn encoder decoder architecture but there's some limitations to lstms that make audio audio modeling difficult and these are some of the same limitations that we've uh talked about before right so long term dependencies and this bottleneck of sequential training and so in audio data in particular you tend to have like very very long sequences so the insight that was applied in this paper is that convolutions are not just for images convolutions can be applied to sequences as well um so the way to visualize this is kind of like this right so at each time step um in the input uh you um you map kind of a history of time steps um so a window of time steps to produce the next input in the next layer of the model and then that layer also takes a window of time steps to produce the next layer and so on and so this is what this looks like right so for a particular output the way it's produced is by looking at a window of of the previous hidden layer and each of those inputs from the previous hidden layer is produced from a window of inputs in the hidden layer before that all the way back down to the input layer one other detail to note here is typically in convolutions you think of a of uh like so the question is like where do we sample this window from and um this particular scheme that's used in this paper and for a lot of sequence modeling problems is called a causal convolution um the reason this causes uh called a causal convolution is that the entire window is sampled from the past so you're not looking at any data in the future in order to produce the output at the next hidden layer and the reason that's important is because when you're sampling from this model right so when someone is typing text into the model in real time we want to be able to also produce the um the output in real time so we don't want to be able to look into the future and see what the person is going to type next in order to produce the sound at this time step and yeah so this this is kind of like the math behind how causal convolution works um again it's just a weighted sum of values in a window of input values so one challenge with models like this is getting a large enough receptive field right so just like in some of the other sequence problems that we talked about in um text-to-speech one thing that that matters is that like the output waveform at a particular time step might not just depend on a very very small like narrow window of text around that time step it might also depend on things that were said in or things that were typed in the distant past because those might have an influence for example on like the inflection that you use when you say a particular word and so one of the challenges is you know how do we actually increase the receptive field of a particular output in this model the approach here is to use like just like we talked about dilated convolutions for computer vision applications there's also a notion of dilated convolutions um in the one-dimensional world so these are called um dilated causal convolutions and the way it works is similar to how dilation works in image models where instead of sampling subsequent uh inputs in the previous hidden layer to produce the next output you instead skip some inputs so you um you you know instead of looking at uh like the the value at the current time step and the value at the previous time step you might look at the value of the current time step and then the time step that was like n time steps ago so this is kind of the other one of the other key ideas that made this paper work so there's other important details about like um how this is implemented how to optimize these types of models that um are important to know if you actually want to implement this but are maybe like less conceptually important for understanding how it works and so it's it's uh if you're interested in this it's worth checking out the original paper on this um so the loss function for wavenet is very similar to the loss function that you'll use for a lot of other classification tasks which is you're just trying to minimize the log likelihood of the outputs given the inputs this is what this looks like in uh you know in math and the way that you actually implement this is you break it out right so y is your sequence of outputs and so you decompose this into a log likelihood on each of the individual outputs um not just uh uh yeah and that's how you'd actually compute this and each of those log likelihoods is just your standard cross entropy loss [Music] so one other thing that's worth mentioning just because i think it's it's pretty important to understanding how a lot of these like really successful large um uh like production language models and text-to-speech models and other big deep learning models work is that one of the keys to make them work really well is that they tend to use massive data sets that are not necessarily publicly available and so this is true for the wavenet paper and it used internal google sets uh google data sets that um you know maybe are not that many hours of speech but have like many many samples per second so these at the time in 2016 were huge data sets and now the models that do this are uh much much larger and they're trained on way more data than this so in order to make this work so in order to make training on these humongous data sets work there's as you can imagine a bunch of tricks that go into um into doing this training um but you might imagine wrong and that's kind of like one of the amazing things about the wavenet paper which is that uh since wavenet training can be done in parallel there weren't like a ton of complicated tricks that were needed in order to train these models just really you know basically paralyze it on gpus on the other hand inference on these models is very tricky right because you at each uh at each um time step in the output right these time steps are very small they're like a you know a small fraction of a second you need to compute stuff that depends on potentially long sequence of input and so uh actually doing inference on these models is very expensive so even though trading is simple inference on these models is complicated and there was actually uh a separate paper a second paper that was introduced that um uh had some ideas about how to do more efficient inference on the wavenet architecture okay so to summarize wavenet approach so wavenet um or these kinds of convolutional sequence models are another way of dealing with long-term dependencies and the way that we deal with long-term dependencies is we just say you know what recurrent neural networks don't deal with long-term dependencies well so let's just get rid of neural net recurrent neural networks [Music] and instead we're going to use a one-dimensional form of convolution so convolution in the sense that each output is produced by looking at a window of inputs and in particular the types of 1d convolutions that we use are causal convolutions so where the windows of inputs that we're using to predict the next output depend only on the past and not in the future and that's important if we want to be able to do inference on these kinds of networks in real time and then to increase the receptive field of these causal convolutions so if uh you know if a particular output depends on a wide range of input data then we can dilate these convolutions so that each output looks at a wider array of inputs and then you know in addition to like handling these longer term dependencies more gracefully and more effectively in practice the other big advantage of wavenet is that it's just inherently parallel to train and so training it is very fast and doesn't require many tricks um but the trade-off that you make when you implement a model like this is it's these models are slow at inference time because the the next time step that you're trying to compute depends on you know values computed from all of the past and so there's there's some work that needs to go into making these models usable for online inference if that's your goal
Full Stack Deep Learning - Spring 2021,4,0,Introduction,so today we're going to talk about transfer learning and transformers transfer learning got started in computer vision we're going to talk about how it's applied in language and natural language processing specifically starting with embeddings and language models talk about nlp's imagenet moment and then transformers the uh foundational mechanism of of transformers is attention so we'll cover that in detail and we'll talk about the specific you know birds gpt twos and threes and t5s and what have you all the transformers but first i think
Full Stack Deep Learning - Spring 2021,4,42,Transfer Learning in Computer Vision,it's good to start in computer vision and kind of take a step back to a couple of lectures ago so let's say we want to classify birds and we have just 10 000 labeled images so in imagenet where we have a million images the best performing methods are deep neural networks like resnet50 and we know they work really well but the problem is resnet50 is so large with so many layers that it would actually over fit on the data that we have which is just 10 000 images so one solution we can have is we can actually train the neural net on imagenet on the full you know one million images and then do something called fine tuning on our small bird data and this will result in better performance than basically anything else so this is the concept of transfer learning right in traditional machine learning on the left you use a lot of data and you produce a large model out of that in transfer learning you apply your large model you train it on the on the big data and then you kind of train on much less data [Music] and just add some new layers or replace some layers of the large model and that takes less time and it works really well [Music] so imagine that we have a pre-trained model we're going to keep the layers like up until the the last few and we're going to replace the last few right so in this model which i believe is vgg g16 we take all the convolutional layers and then just cut out the fully connected layer and final softmax so we just learn the weights of just those layers this is actually pretty easy to do in in in tensorflow but also pytorch so i'm showing you pythor's lightning code here and the thing to look at is is right here so transfer learning if you know if you say pre-trained equals true this will use the resnet 18 from torch vision package with the weights that were trained on imagenet which are provided for us and then we freeze the weights of of that what we call feature extractor by setting the evaluation mode to true so we say dot eval and this will make it so that when we run this network it won't store gradients on these on these weights and so when we take our training steps it's not going to actually learn or change these weights it's only going to change the weights that we're going to add on top of it which is maybe actually just nothing maybe we just have a just a soft max or something on top of it but here we have a single linear layer the self.classifier [Music] and there's model zooz for both pytorch and tensorflow and other deep learning frameworks which make these pre-trained model easily available so in pytorch you have torch vision and you have basically all the menagerie of of models that we covered in lecture two same thing with tensorflow
Full Stack Deep Learning - Spring 2021,4,240,Embeddings and Language Models,so moving on to the world of language the first thing we should figure out is you know our inputs are words usually but in deep learning we deal with vectors so we need to convert words to vectors and one simple way we can do it is what's known as one hot encoding and so that basically means you have some kind of dictionary right so like you have ten thousand words let's say in your dictionary and whatever word you need to encode you just look up in the dictionary and you make an all zero vector except there's a one at the position where that word is in the dictionary and there's problems with this i mean it does work but it scales poorly with vocabulary size so the more the larger your vocabulary the larger your vectors become by necessity and the vectors are very sparse and very high dimensional so that makes a lot of neural network assumptions kind of not correct and so the neural networks don't work as well as if they were lower dimensional and dense and it also just logically kind of violates what we know about word similarity right so like a word like run is going to be as far away from a word like running or walk as it is from tertiary or poetry or you know like any other totally unrelated word so what we could do instead is we could map basically these one hot vectors two dance vectors so now instead of sparse one hotend coatings you have dance smaller encodings and all you need to do for that is to have an embedding matrix so you have your vocabulary size but vocabulary size one hot encoding matrix and then you have your v by e where e is your embedding dimension uh embedding matrix and then out of that you get the embedding of every word so that begs the question how do we set the values of the embedding matrix so one solution is to just learn it as part of the task so you just you know that your words are coming in as words or you know one-hot vectors you just slap a you know and then that embedding layer before anything else and it'll actually learn the weights just in the same way that it can learn a linear layer or a cone play or anything else but another solution could be that we actually learn the embedding matrix first and essentially pre-train it right for our task maybe we can learn a really good word embedding that's going to be useful to a lot of tasks well how could we do this we could do it by training on a very general task on a large corpus of text so kind of in the same way that imagenet can be pre-trained on a large corpus of images and then apply it to a lot of different tasks we can maybe pre-train a language model on a lot of text and then apply it to different tasks and basically this can be done by a large corpus of text like wikipedia and then a task of predicting the next word given some number of words so our our inputs are words our task is to predict the next word and so our output is going to be like a softmax vector over our vocabulary we can train it with cross entropy just do the stuff we know how to do to form the data set for training this we can basically you can think of it as sliding a window through all the text on a corpus and forming rows right right so like thou shalt not so this would be a an m gram of size three because there's three words in it or maybe this is a two gram not sure but anyway you slide your window of size three over the data and then it makes rows like thou shalt not and then shalt not make not make a and so on there's also something you can improve you can improve this by doing skip grams instead of just m grams which are so anagrams you just look at preceding words skip grams you look at both preceding and following words so the goal is you know the for the word not your targets are not just thou shalt it's going to be thou shalt and also make a in order to speed up this task we can make it a binary classifier or classification problem instead of a multi-class classification problem so multi-class is predicting the next word the number of classes is the number of words in your vocabulary and you have to predict over basically that whole vocabulary sized vector but if you make it a binary classification then your prediction is always going to be a zero or one and so your input so you know if to predict neighboring word your input is just a order or an engram and the prediction is another word now your input is going to be two words and then your prediction is yes or no like are these words neighbors or not basically in order to train the embedding of let's say the word not we look at all the surrounding words and all the words that don't surround it and that forms our data set the words that surround it have a target of one right which means they they are neighbors the words that never happen next to it are targets of zero and you know vector math is something everyone's familiar with but basically maybe the insight here is that you can view these embeddings as vectors right in embedding dimension size space and so you can do vector math on them there's other relationships like verb tense so like if you apply the kind of transition that gets you from walking to walked you apply that transition to swimming then you get a vector that's very close to the word swam in the vocabulary and you have relationships between you know proper nouns like countries and capitals so we talked about the imagine that kind
Full Stack Deep Learning - Spring 2021,4,609,"NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE",of based pre-training that became popular in computer vision we talked about kind of old school language model stuff that people did up until maybe like 2017 with embeddings and stuff and now we're going to talk about mlp's image in that moment which is really combining these two trends and that happened around 2017. so the idea is yeah you have word to vac and glove i just mentioned this in an answer to a question these embeddings became popular and their boosted accuracy and basically everything by like you know less than ten percent but that's very sizable in all of these tasks but the problem with these representations is that they're shallow which means that only the first layer of your model right so you're building some kind of deep learning model and the first layer is going to take advantage of that pre-trained embedding but that's the only layer that's going to take advantage of it only that first layer has seen all of wikipedia for example and the rest of your deep net is going to be trained only on your data which is going to be less than wikipedia so why not just pre-train more layers right so instead of just only training and embedding why why not try pre-training and embedding followed by an lstm followed by another lstm layer and so on and so the reason you might want to do it is because there's certain things in language that are just impossible to disambiguate without a lot more context potentially more context than an uh skip gram can provide so like some simple stuff is like you know the word rule can be a verbal noun there's grammar rules that would be nice to know like there's actually even informal grammar rules like how to order adjectives describing you know color size like weight and whatever have you seen do you guys know what i'm talking about it's like you say pretty little thing or something you'd never say like little pretty thing because it just doesn't sound right so all these kind of things you can pick up if you had a more powerful model and that's what happened one of the first ones is called elmo from 2018 they basically just had a bi-directional stacked lstm same thing that we did in our lab 3 right pretty easy model and it's for the purpose of predicting a word in in the middle of a sentence basically it's from the allen institute for ai here in seattle i'm in seattle and it improved on state-of-the-art on a bunch of different tasks pretty substantially on some of them [Music] particularly let's look at the squad data set so there's a whole you know menagerie of data sets in nlp here's a couple of slides just explaining some of them so we're not totally lost so squad is a q a data set there's a hundred thousand question answer pairs and the answers are always fans in the question so on the right hand side you can see um at the top like immediate meteor meteorology precipitation is any product whatever that's the source text and then there's a question like what causes precipitation to fall and there's an answer to it which is gravity and that board has to be part of the source text so that's what squad is as a data set there's another data set called snli that for natural language inference and what that refers to is basically uh identifying what relation exists between a piece of text and a hypothesis and there's 570 000 pairs of these kind of things so an example here on the top row one piece of text is a man inspects the uniform of a figure in some east asian country another piece of text is the man is sleeping right and then the correct relation that that the model is supposed to infer is that it's a contradiction because you can't both be sleeping and inspecting a uniform so once again elmo improved on a lot of these there's another big data set which a lot of people use called glue which is a really nine different tasks some of them entailment some of them just similarity some of them paraphrasing so there's nine different tasks and the model is supposed to give performance on all of them and then you average the performance across all nine tasks so that's glue so you just real briefly like it's is the sense grammatical or ungrammatical is the sentiment of the sentence positive negative or neutral is sentence b a paraphrase of sense a how similar are sons a and b from you know one to ten are the two questions similar binary judgment does sense a entail or contradict sentence b does sentence b contain the answer to the question in sentence a does sentence a entail sends b and sends b replaces sentence a's ambiguous pronoun with one of the nouns is this the correct noun so it's a number of different tasks so if a model performs well on this it really has a good understanding of language that's the theory another one of these imagenet moment models is called ulm fit it was trained by the researchers behind fast.ai which is a great course that we recommend i think on our course website they take like a hacker's approach to to deep neural to deep learning and they were among the first and maybe the first to to apply this kind of image that pre-training to that natural language their model is very similar to elma it's it's also just a bi-directional stacked lstm there's also an embedding [Music] and so in summer 2018 people declared that nlp's image net moment has arrived because this was clearly going to be the the way to do nlp tasks of all sorts going forward this is reflected in the model use right so if you look at the pi torch and the tensorflow model zoos or model gardens you now see a lot of natural language processing pre-trained model like albert bird and h nat transformer
Full Stack Deep Learning - Spring 2021,4,1009,Rise of Transformers,all right well let's move on to transformers so one thing you might have noticed in this tensorflow model garden screenshot is burt you know is bi-directional encoder representation from transformers nhnet is a transformer based sequence to sequence model there's a thing called transformer which is a transformer model and so all of these are actually transformers everywhere you look you know so what what what are they well they get started in 2017 with a paper called attention is all you need which is an encoder decoder architecture like josh talked about in lecture three but with no lstms or rnns of any sort no recurrent neural lab works only attention and fully connected layers so attention is all you need and that approach sets state of the art on particularly translation data sets and got people's attention so let's go through what the mechanisms actually are but for simplicity we can focus on just the encoder so i just zoomed in on just the encoder part of the model and actually a lot of these transformer models that in the model zoo like bert bird is actually just the encoder of this particular paper so the components we're going to look at are one self-attention two positional encoding and three layer normalization first let's talk about attention in
Full Stack Deep Learning - Spring 2021,4,1100,"Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization",detail so the basic self-attention you have an input which is a sequence of tensors or vectors your output is a sequence of tensors each one is a weighted sum of the input sequence okay so you have some x sub 1 x of 2 x sub t inputs and your outputs are going to be y sub 1 y sub 2 y sub t but each y is just a combination of it's a weighted comp it's a weighted sum of the inputs okay and furthermore let's just say for now that the weight that we're going to apply to each one of the inputs is not even going to be learned it's just going to be a pure function of x sub i and x sub j right so we're summing over j and then we're trying to assemble y sub i so we're going to have some weight w sub i and j and we can actually just say like well that weight is just going to be the the dot product of these two vectors x sub y and x sub j and what we needed to do is we needed to sum to 1 over j so that it's a proper weighted sum well we can do that and so then let's just kind of reason through it right so we have our inputs on the bottom the cat is yawning we have our outputs at the top which are going to be just some weighted combinations of all these inputs so far we have no learned weights and the order of the sequence actually doesn't affect the result of the computations we're going to do well we can improve that first part of no learned weights by learning some weights okay so to learn weights let's think about how we're actually going to use each vector x sub i we're going to use it in three different ways first we're going to compare it to every other vector to compute attention weights for its own output x sub or y sub i we can call that query second we're going to compare it to every other vector to compute the attention weight w sub i j for output y sub j and so we can think of that as a key and then third we're gonna sum it with all the other input vectors to form the result of the attention weighted sum and so it serves as like a value for this and it's the same vector but we can kind of process it in three different ways with matrix multiplication such that it can fulfill these three different roles so specifically there's query key and value so we're going to say okay q sub i is going to be weight matrix sub q times x sub i the key vector k sub i is going to be weight matrix of k times x sub i and so on and then we can introduce the softmax to make sure it sums to one but that's basically it so like all we have to do is just multiply the input vector each input vector by these three matrices and then combine the three results in the different ways to produce the output y sub i this is part of the weekly reading so you'll have to go through this in more detail i'm actually not going to stop for questions yet there was a word called multi-head attention you might have noticed in the diagram of the of the attention is all you need paper and so what does that mean well that just means like the multiple heads of attention just means that each head is like a set of the three weight matrices the query weights keyways and and value weights if we have multiple heads then we're going to learn multiple of these sets simultaneously but honestly it's going to be implemented as just a single matrix that just becomes you know if you have two heads that matrix just becomes twice as big if you have three heads it's three times as big as one head so it's it's really quite simple but it just means we can learn three different or you know any number of different of these transformations at once tends to work better in practice and so okay so we have the self-attention we kind of explain that so you have inputs they go through the self-attention layer the next thing that happens is they go through this layer normalization layer which we'll talk about and then they go through a dense layer which is uh called mlp here multi-layer perceptron so what is layer normal normalization so layer normalization is motivated by the observation that neural networks really work best when inputs to a layer have uniform mean and standard deviation in each dimension we talked about this in the fundamentals of deep learning lecture you basically don't want uh your input data to have like variants in one direction one you know one of the dimensions has a lot of variants and then some other dimensions have very little variance that's not going to work as well as the variance is uniform across all dimensions so one way to do it when you have you know at the you can process your input data by rescaling it so that it has zero mean and uniform standard deviation you can initialize your weights in such a way that their out their outputs also have this property but as inputs kind of flow through the network no matter how good your weight is in this initialization is your means and standard deviations are going to get blown out kind of later on in the network so layer normalization is really just a hacky way to like hard reset things to where we want them which is uniform mean and standard deviation in between layers that's kind of all you need to understand about it at this point it's like just the hack that we insert to just reset mean and variance to uniform and you can insert it in between layers however you want and it tends to improve things in practice but i'm not aware of much theory behind it okay so so far we understand that we can combine input vectors to produce attention weighted output vectors and we can do this in a good way by learning these query key and value weights and we can do it even better by learning multiple sets of such weights that's called multiple heads but we still have the problem where the order of the inputs has no effect at all on the result of the computations so what we can do is we can encode each vector with its position so we have a sense this movie is great each word is embedded as a as a dense vector here of size three and you go through transformers and you you produce an output sequence okay so position embedding is something that we're going to literally add to the word embedding so word embedding does not depend on position right it only depends on what the word actually is the position of heading only depends on the position in the sequence it does not look at what the word actually is and then if you combine the two now you have a vector that has both content signal from the word embedding and position signal from the position embedding and so now when you have when you run those vectors through the transformer blocks now the transformer attention can actually reason about position as well as content and it really seems like way too simple but that's really all it is it just happens to work pretty well okay and then lastly there's the last trick in transformers which is if we want to train something that predicts the next word or hear the next character in the sequence there's a problem because we actually send all of our inputs to the transformer at the same time and so if you wanted to predict something that you know that's part of the input that that's you need to do something extra which is basically mask the future so it's called adding a mask so you have your inputs the inputs get computed or you can compute attention weights over the inputs through the transformer blocks and then you can apply a future blocking mask to those weights and this way the transformer is only able to use attention from the past right so it only can look at things that happen previously to what it's trying to predict when it predicts it okay that was a lot you guys are really going to have to do the reading yeah attention is kind of hard to
Full Stack Deep Learning - Spring 2021,4,1653,"Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc.",understand and really helps just uh doing the reading this week it's called transformers from scratch just really do you know that whole kind of tutorial in depth but i really it's like one of those things that when you realize that it's not it's like simpler than you thought that's when you kind of try to start to get it at least for me okay so attention is all you need from 2017 is a paper that applied this architecture for the first time they applied it to the task of translation right so like english to french and so on and they have this encoder decoder architecture where there's an encoder what we've been looking at and then the output of the encoder goes into the middle of the decoder which you can see on the right and then also the correct outputs go into the decoder and then they get kind of added together via transformer and then you can predict the how to translate so later models made it mostly either just the decoder or just the encoder except for t5 which we'll talk about in a second which is back to having both an encoder and a decoder but gpt and gpt2 and gpt3 stands for generative pre-trained transformer why generative well because it learns to predict the next word in the sequence right it's generating text basically just like elmo and ulm fit except elmo and ulm fit used lstms they use the embedding layer and then lstms gpt uses an embedding layer and then transformer layers so they just switch that transformer for lstm and it conditions only on preceding words just like an engram not a skip gram and so it uses this mask that we discussed to basically mask the future from the transformer so we can only pay attention to preceding words when it tries to predict the next word the original gpt was trained on eight million web pages or no maybe that's gpt2 i'm not sure there's also all these variants there's gpt too small gpt to medium large extra large there's obviously no gpt3 which i'll i'll have a slide on later but they're called large or extra large because the number of parameters that are in them i think they're all trained on the same data which are a bunch of web pages on reddit particularly pages and wikipedia you can use gpt2 pretty easily like i think even on collab you can run it there's also talk to transformer that uses the 1.5 billion parameter model which is the extra large i just typed it today full stack deep learning is a uc berkeley course about says deep learning and neural networks first taught at hcs and fall 2016 and then at hcs in spring 2017 which is not true this course requires a light to moderate understanding of computer science preferably cs for computer science majors so it's really not that good right it doesn't sound like great text okay then there's a paper called burt which stands for bi-directional encoder representation from transformers so as you can imagine gpt is not bi-directional right it's unidirectional it uses masking to mask out future tokens so that it can predict them burt is bi-directional so there's no masking right when you feed it input it uses the whole all the tokens you give it and how do you train it then well what you do is you mask out certain words so you can mask out words in them so you you have your text like wikipedia you take let's say you know five token sequences and you mask out the middle one so now you have an actual word an actual word then a mask token then an actual word an actual then an actual word and you feed that through the network and the goal is to predict the content of the masked word and then they switch it up they also have bird predict whether one sentence follows another and i guess that's just so that it kind of like trains on a variety of tasks and not just this one mask prediction bird had 340 million parameters 24 transformer blocks embedding size of 1024 16 attention heads [Music] and in general you know all these transformers are growing in size right so starting in spring 2018 we had elmo which is not even a transform model but we had gpt kind of the beginning of summer 2018 for 10 million parameters and then gpt2 had 1.5 billion parameters and then you have megatron lm from nvidia t5 from google touring nlg with 17 billion parameters that's a person saying you know the the number of parameters too damn high but t5 is worth a look came out in february 2020 so uh sorry 2020 it's the last year about a year ago and basically what they did is they evaluated like all the things that people tried in all the different transformer papers they ran like a big hyper parameter search over all of the things that they identified that were different between papers and they have both the input and the output as text strings so that that might mean that the so the the input task definition is actually provided also as test as text not as some special token or anything right it'll just actually say like what the model is supposed to do as part of the input text so it could say like translate english to french then the english language sends and then the output is going to be in french or it can say does sends b entail from sentence a sentence a text sends b text and then the output is going to be like t or f like true or false it had 11 billion parameters it's trained on this colossal clean crawled corpus c4 which is 100 times larger than wikipedia this is work from google by the way and it said state of the art on glue which we've seen squad which is which we've seen and then this thing called super glue which is another data set that's supposed to solve some problems that glue has so it's really cool you like literally put in the task definition f text and you get all kinds of behaviors out of the same model so i recommend t5 if you're interested in this line of research these are just some quotes about what the researchers found in the paper so they actually found that the encoder and decoder architecture together is is better than either just encoder or just decoder so that's why it's formed that way so if we look at gpt-2 it had 1.5 billion parameters t5 now has 11 billion parameters and came out about a year ago gpt-3 came out at the beginning of last summer and it had what is that so it's 200 000 million so 200 it all it had about 175 billion parameters so microsoft tiering energy had 11 billion gpg3 had 175 billion parameters and openai published the paper about it but they did not release the pre-trained weights citing societal concerns right like this technology they believe can be misused for for ill and so they only allow access through their https api which you can request access to of course it's not going to stay you know that way forever there's open source efforts like this luther ai to basically train the gpt3 architecture on the same data that open ai did but then release the weights publicly i think they target this summer for like releasing the weights so it will happen
Full Stack Deep Learning - Spring 2021,4,2180,GPT3 Demos,it is really good text generation here's a funny example that we fed in a couple of actual instructor quotes about gradescope and then we had gpt3 provide you know some fake instructor quotes about great scope which are actually really good you know like the gpg3 writing is is not in bold so it says administrative contact for grade scope license at university of kentucky i'm fairly new to the role but i had several people tell me the features for grade scope are amazing and how flexible the grading works for our classes grade scope has saved a lot of time for our staff and helps us communicate with students when grades go up which is like something that a person could say so it's very impressive i like the wind grades go on yeah i mean there are problems there are problems with like the quality there's also problems with the content of what gpg3 puts out which is part of the reason why open ai you know held back on the on the trained weights and so this is a quote this is a tweet from the head of the head of ai research at facebook jerome byzanti and yeah he just seeded gpt3 with some words like like jews and then they got stuff like jews love money which is is perpetuating things that we don't want to perpetuate right that are present in the in the source data that gpt3 was trained on but you know we as a community need to make more progress and understand how to not have our models basically put out stuff that we don't want them putting out and that's a hot topic of research right now and is like a big reason for why openai didn't release the weights and we'll see what happens when the open source weights will be released i think a reasonable mental model of gpt3 is you know it kind of performs like a clever student who like can put together words in a way that reads fine mostly and is actually kind of factual right so it's like they have you know 30 seconds to just like google something and then bs their way through plus some exam question so you'll get things that are actually true right like things that gpt3 knows from being trained on basically the internet like factual information about for example grade scope right or you can even try your name and like know certain things about you but and it reads fine at first but then you kind of get into stuff like ah that doesn't really quite make sense what's really interesting though is that it might be useful for more than text so here is someone describing a leia so they fine-tune gbg3 or it's basically fine-tuning they provide examples of like how like layouts how descriptions of layouts map to basically html code and now they're able to describe a layout and gpt3 will generate stuff well it'll generate actual code that they then render on the right and it looks fine so this like really blew a lot of people's minds when when this came out in the summer oops one thing that i really like as an idea is using gpd3 as like a spreadsheet function so here's a little demo okay so what happens here so the way gpg3 works is you can provide basically beginning text like source text and then a prompt and then it'll continue writing and so the purpose of that beginning text is to kind of like key it into the type of output that you expect right so what this person did is they pretty cleverly connected it to a spreadsheet so first they provide this this is kind of like the source text and then this is michigan is the prompt and then the gpg3 output is going to be in the cell that's loading currently and it's 10.31 million and so now they can change the prompt leaving the source text alone and it changes the output and so gpg3 knows a lot of facts because it's trained on a corpus that contains a lot of facts and when prompted in this way it's actually able to like give you back those facts in a way that makes sense would i bet my life on whether these answers are correct [Music] absolutely you know no i you know i wouldn't because that's one part of the problem you don't know it doesn't give you like a confidence right it just gives you the answer and the answer might be right or might not be so we're kind of far away from like putting it in a production use case for stuff like this but it's a very compelling demo and we're definitely you know moving in that direction with our language models and furthermore you can actually use transformers for image generation so just recently i think like a month ago openai released what they called dolly a 12 billion parameter version of gpt3 trained to generate images from text descriptions so they had a data set of text to image pairs and they found that the model that was trained has ability to generate images that have never been seen before in the data set so for example a storefront that has the word open ai written on it like that is not in the data set because that doesn't exist and yet dolly is able to generate images that are quite realistic looking that match that description
Full Stack Deep Learning - Spring 2021,4,2573,Future Directions,so seems a little scary right like are we barreling towards like strong ai with these transformer-based models kinda there's real there's no sign of like performance slowdown this is this is a plot from the gpt3 paper and so on the on the y-axis is accuracy on the x-axis is the number of examples given to the model and then the different colors are for the different models right so you have a 1.3 billion parameter model a 13 billion parameter model and then gpt 3 the 175 billion parameter model and as you can see performance like is increasing substantially between these order of magnitude improvements in the number of parameters nothing else about the architecture is changing right it's just making the model larger and you get these crazy accuracy improvements and you can imagine that it's only at you know 60 accuracy now if you make it a 175 trillion parameter model you know i don't know we haven't seen a slowdown just yet and in fact just recently you know google released this model called the switch transformer which i don't even know what that is i think it's like one point 1.6 trillion parameters and the point of the switch transformer is that a lot of the weights are sparse i believe and i i'm not aware of like really impressive demos just yet of this model you guys should speak up if you are but the point is like we are now able at least to train these models another thing i should mention about gpt3 is that it took it potentially like it's it's unclear exactly how expensive it is to train but it takes several thousand gpus to train it takes definitely like a month or or more to train it to completion so that's very expensive in cloud compute costs which probably explains the you know exclusive partnership that microsoft has with openai you know access to cheaper compute i think the estimate that we've seen is like several million dollars per training run of gpg3 so that's a little bit of a downer right because the way these things are trending it seems like only big companies can afford to compete in these tasks right in natural language processing and now also potentially vision and you can see that even from like a year ago if you look at the glue benchmark leaderboard if you just look at who the teams are it's baidu alibaba google microsoft huawei microsoft facebook ai and so on right so where are the research labs there's another direction to go in you can try to do more with less and so one example i like to give of this is called distilbert from hugging face so the idea here is to use a technique called knowledge distillation which is training a larger model on a large dataset and then training a small model not on the data but on the output of the larger model and we talked about this when we talked about potentially defeating adversarial examples this could be one of the paths to take it's also a way to just have smaller models and so distilbert is able to retain 97 of bird performance at something like you know it has 66 million parameters and bird has 340 million parameters so substantial you know decrease the number of parameters but basically no drop in performance so this could be one research direction that people without crazy huge resources can pursue if you want to keep up with the stuff i always recommend papers with code there's also nlp progress by a sebastian rooter that kind of tracks all these recent state of the arts and then transformers is the undisputed king of just implementing all the transformer models and making them available often with pre-trained weights on their website through their python package it's based on i think it's it's compatible with both pi torch and tensorflow i think it's mostly based on pi torch at this point but there's a lot of pre-trained transformer models here in hugging phase so definitely check that out so all this stuff about scaling up with you know scaling up accuracy with just increasing the number of parameters is what rich sutton who's a reinforcement learning pioneer and the professor at university of alberta he called the bitter lesson right so the bitter lesson of of of ai research is that it seems like you don't need that much human ingenuity in the methodology like once you find the thing that that works you can continue scaling that thing up and that thing for us right now is neural networks and specifically transformers so you just scale that up and that beats like any kind of ingenuity improvement that we've been able to devise over the last like decades
Full Stack Deep Learning - Spring 2021,5,0,Introduction,this week really starts the kind of the meat of the content of this class which is really focused on like everything that you need to know about turning machine learning projects into like working production systems other than training models and so to start off with we're going to talk about setting up machine learning projects right so how to how to pick projects and how to set them up for success so i kind of like this characterization of machine learning projects in part because it's so out of date so this is an xkcd cartoon from not that long ago which kind of makes this this clip that like you know if you if you try to articulate to someone outside of computer science like what's what problems are still hard in cs it can be really challenging right because why why like why should it be so hard to recognize a photo of a bird but kind of the funnier thing about this to me is that this is now so comically out of date right like i think for for all of us who have uh spent time in the machine learning world like this is just one api call away and so i think it articulates how difficult it could be to understand what problems are going to be challenging in machine learning another statistic i'll point you to if you spend time looking around the production machine learning world and blog posts and stuff like that you'll see this statistic come up 85 percent of ai projects fail i've seen it attributed to different places and that sometimes people say 87 sometimes they say 85 so i think you know maybe not so important to actually you know know that the exact numbers and the statistic because i'm not like not even totally sure how they came up with it but i think it kind of captures the sentiment that is out there which is that it's really really hard to make machine learning projects work in the real world and the more interesting question is you know whether it's 85 or 50 or whatever why are so many machine learning projects failing so i think there's kind of like one core
Full Stack Deep Learning - Spring 2021,5,105,Why Do ML Projects Fail?,reason which is that you know machine learning to a large extent there's a lot that we can do to make it feel more like engineering but in reality machine learning projects are still to some degree some varying degree research projects and so 100 success rate shouldn't really be the goal right like you should be trying some stuff that isn't gonna work um otherwise you're probably not being ambitious enough but i do think that many machine learning projects are doomed to fail from the start because for example they're they were never technically feasible to begin with or they weren't very well thought out and they're they're poorly scoped you know the projects kind of get stuck in proof of concept phase they have a cool working demo but the demo never makes it to production or you know the demo makes it to production but it was it's never really clear like what the success criteria for the project is so never gets greenlit or you know all this just kind of falls apart because the team is managed poorly to begin with and so you know so we're gonna we're gonna talk about some of these things and how you can avoid some of these problems when you're picking and scoping machine learning projects
Full Stack Deep Learning - Spring 2021,5,173,Lecture Overview and Running Case Study,so what we'll cover today is we're going to start by talking about the life cycle of a machine learning project and so this is really just meant to like get your help you wrap your mind around all of the activities that you're going to need to think about if you're building an end-to-end machine learning project then we'll talk about how to pick projects right so how to prioritize which projects you should work on which will boil out down to assessing the feasibility of projects and their and their potential for impact and then we'll talk about a few different archetypes of projects so different categories of mission learning projects that you might embark on and what the implications of those categories are for how you might think about managing those projects then we'll talk about metrics and so metrics are kind of the numbers that you look at when you're optimizing the machine learning model how to pick why it's important to pick a single metric and how to pick that metric and then finally we'll talk about baselines and so and so baselines are essentially a way of really understanding whether your model is performing well or not and so these last two are are things that you should have in place before you even start you know training models as part of the model development process and throughout today we'll we'll keep coming back to this running case study so the case study is pose estimation and this is kind of inspired by a project i actually worked on an open ai but the idea is that you know we're we're hypothetical robotics company and what we're trying to do is we're trying to predict the pose of objects from an image of those objects so the input to our model is you know one or more images um of some indoor scene that the robots can interact with and what we're trying to predict is the position and the orientation of all the objects in that image and so why why is this important right so you know throughout this running example the we're working on this company called full stack robotics and we're working on robotic grasping and the way our system works is that we have two different models that that we're using the first is the perception model which is the model that takes images as input and produces you know state of the world so the positions and orientations of objects in our case and then the second component of the system is the grasping model which is you know takes those raw positions and orientations as input and produces motor commands for the robot to actually go and execute in the world and you know you might ask why why is this split into two components instead of learning this model end to end and this is a bit of an aside but one of one of the reasons that a lot of robotic system systems in particular self-driving car systems etc are split and are not just actually learned end-to-end you know using reinforcement learning or something like that is you know a couple of the main reasons are data efficiency so it can be it can be cheaper in terms of the amount of data that you need to have in label to build a system like this and then auditability right so if you have a model that just regresses images against motor commands then if something goes wrong right if the robot takes the wrong action it can be really hard to diagnose what actually happened and so in the real world a lot of robotic systems are split like this where there's a task of state estimation where you try to predict like what is the state of the world and then on top of that there's a task of control right so given the state of the world what action do we want the robot to take and and so that's why the system is broken up like this
Full Stack Deep Learning - Spring 2021,5,374,Lifecycle (Thinking about the activities in an ML project),all right so so diving into the the main content of the lecture so first thing that we're going to talk about is how to think of the life cycle of a machine learning project so all of the activities that go into creating a project with this so the life cycle of a machine learning project really starts with this planning and project setup base right so this is the phase where we might you know decide to even work on close estimation at all determine what our requirements are and what the goals are for the project allocate resources to the project you know maybe this is less relevant for this project but in other projects we might consider the ethical implications of the work that we're doing et cetera and so once we have a plan then we'll move into the to the data collection and labeling phase and this is where we might collect the objects that we want to train our model on set up our sensors like our cameras and start capturing images of the objects and then figure out some way to annotate those images of ground truth which in this task might be kind of difficult one thing to know about the life cycle of machine learning projects is that i think the right way to think about it is that machine learning projects are not a linear flow from you know project planning all the way to deployment they're actually a loop and each of the stages there's a bunch of opportunities to loop back to earlier stages as you learn more about your project or as you collect more data so you know you might loop back from the data collection phase to the planning and project setup phase because for example maybe you find out it's actually you know it's really too hard to get data for the tasks that we're working on so maybe let's actually like try to refine this task or pose the task differently so that we can make our data collection and data labeling process easier once you've collected and labeled data then you move on to the trading and debugging phase and so this is what i think most people think of as machine learning but you know here you might do things that are outside of the you know what you what you normally think of in training deep learning models so you might actually implement a baseline model that is not using machine learning at all maybe it's just calling some opencv functions but you also might do things like figuring out what the state of the art is and trying to reproduce that you'll probably spend a lot of time debugging in this space and we'll have a whole lecture on how to make that process less painful and then thinking about how to improve the model right so once you have something that kind of works trying to come up with better better model architecture or something along those lines but also fit into this space so trading and debugging can loop back into the data collection phase for example you might need to collect more data if your model is is overfitting or you might realize that for example like something about your data labeling process is unreliable so you're able unable to get good results and one of the reasons why might be that your labels are inconsistent you can also loop all the way back to the planning and project setup phase so you could realize that the task itself was just way too hard to begin with you could also realize that some of the requirements that you specified in the the planning and project setup phase trade off with each other so for example a lot of times accuracy trades off with latency so you might actually need to get a model that's too big in order to in order like to run in real time in order to solve the tasks for the accuracy or environment that you had and so it might be important to go back and revisit which of those requirements were important and then finally once you have a trained model that you think is good enough then you'll move into the deployment and testing phase and so in this phase you you know you might run a pilot so in our case we might run our grasping system in the lab and then you'll also test your model in this space so we might write regression tests to to prevent you know any any future changes from breaking things that are good about our model and we might also write tests to evaluate our model for biases and then once you know once you have your your pilot done and your tests in place then finally you'll roll your model out of production but your job is actually not done here so you could you know in a lot of cases you'll need to loop back to the training debugging phase so you might realize that even though your model worked on your evaluation set just fine it doesn't actually work in your pilot it doesn't work in the lab so you might actually need to keep improving the accuracy of your model you could loop back to the data collection and labeling phase so maybe there's some data mismatched between the data you trained on and the data that's actually seen in production there's some distribution shift and you could go back and fix that you could realize that you actually need more data you were you know you were overfitting somehow and so you need to go collect more data or there might be some edge cases that come up when you're deployed in production that you didn't know about in advance and so you could go try to mine hard cases and and maybe collect edge cases to retrain your model on and in some cases you actually have to loop all the way back to the project planning and setup phase so for example if the metric that you picked doesn't actually drive downstream performance so for example like if the metric that we picked was was accuracy of the grasping model but it turns out that you know having the the grasping model be accurate is is not really enough like that doesn't actually make the accuracy of the pose estimation model isn't really enough like that doesn't actually make our grasp more successful because for for example maybe we need more robust estimates of the uncertainty of that model you might have to go back and rethink the the planning of your project due to that or you might realize that the performance in the real world just isn't that great and so you could you could revisit your requirements and say like okay what did we get wrong about those original requirements that we should take into consideration so these are activities that you'll do during a machine learning project that i would think of as like on a per project basis but there's stuff that as a machine learning team you'll need to solve across all of your projects as well and so there's you know building the team and kind of hiring great people managing those people and then there's setting up the infrastructure and tooling that you need in order to actually do this stuff repeatedly in that scale and so we'll have we'll have lectures in the course that cover each of those as well all right so that's also not all that you need to know it's also important to to have some sense of what is state of the art in your domain right so what's what's actually possible to solve using machine learning right now and you know if you're really deep in your domain you might this might be how you decide what to try next and so we'll also introduce some of the most promising research areas in you know we talked a little bit about kind of basics of some of the most common deep learning tasks in the past few weeks and towards the end of the class peter will give a lecture on promising research areas that you that are worth keeping up with if you want to understand what's possible in machine learning okay but to summarize this is kind of how we think of the life cycle of a machine learning project and more or less the rest of the course will be oriented around trying to give you um tools and techniques and ideas for dealing with each of these different phases
Full Stack Deep Learning - Spring 2021,5,773,Prioritizing Projects (Assessing the feasibility and impact of the projects),so next we're going to talk about how to prioritize machine learning projects and so what this is going to boil down to is finding problems that are potentially high impact to work on and then trying to assess what the cost of those projects might be so to double click on that a little bit this is you know a general framework that you can think about for how to prioritize projects like nothing machine learning specific here but one way to think about project prioritization is that you want to look for things that are high impact or potentially high impact and are also relatively feasible right so those are the two axes that you might measure potential projects along and you know you might plot your projects on on this kind of two by two and then the ones that are in the upper right corner there's the ones to go and trap right so it's not necessarily an exercise you actually need to do but one way to think about picking projects is looking at what makes projects potentially high impact and what makes them feasible so i think there's no kind of silver bullet answer to finding high impact machine learning projects it depends on your use case and it's just generally hard but i do want to give you a few mental models for how to think about what types of machine learning projects might be high impact and so the four i want to talk about are where can you take advantage of cheap prediction where can you automate complicated where's their friction in your product that you might be able to automate away where can you automate complicated manual processes and then you know the the easiest one maybe is like what are other people doing so one lens to look at this problem with is through economics so there's this book called prediction machines that lays out the case that at a high level what ai enables you to do is it reduces the cost of prediction right so you know where a certain type of prediction might have required like an expert to spend their time to make this prediction you know machine learning allows you to like basically automate that expert into a system that you can run very cheaply and so prediction is essential for decision making and that means that if you have really cheap prediction then predictions give me a lot more places even for problems where it's too expensive before right like most people couldn't like hire a private driver before and so the implication of this if you like take this lens to project selection is to look for projects where cheap prediction could have a huge business impact right so it's a mental rule that you can think that you can think about when you're looking for high priority projects another lens i think is worth looking at is thinking about it from the lens of like what does your product actually need and so this is from an article by spotify which i think of this kind of being like one of the best machine learning driven products and i'll i'll link to this article in a few slides but the thesis that they lay out is that you should really be thinking about machine learning projects from a product perspective and you should be looking for parts of your product experience that are high friction and automating those points of high friction are exactly like where there's a lot of impact for machine learning to to make your business better a third heuristic that you might use is you know in addition to thinking about like the abstract economics of what ai enables and like what could make your product better maybe we should just think about like what is machine learning actually good at like where you know what what are the nails that this that this hammer can actually hit and so for this i like this this software 2.0 blog post by andre kurpathi and you know i think he sums it up nicely in a tweet which is uh gradient descent can write better code than you it's very nice to apologize for for insulting my code like that but to dive into this a little bit more the case that andre lays on this blog post is that what he frames as software 1.0 is kind of what most of us would think of as software right so traditional programs that have explicit instructions that are in most cases written by a human software 2.0 is a different programming paradigm where humans instead of writing code specify goals and then an algorithm that searches for a program that solves those goals and instead of working with code software 2.0 you know programmers instead work with data sets and those data sets get compiled into programs via optimization and you might ask like why do this right and and the answer at least according to andre is that it just works better you know gradient descent can write better code than you but in addition to that it's it's also more general in some sense because we can teach computers to do things that we can't easily articulate as rules ourselves and there are also computational advantages so if if instead of having like all these complicated control flows in our programs they're all just matrix multiplications that we can design better hardware to to suit those types of computations better and so you know putting on our project selection hat here the the implication of this mental model is that you should look for places in your product where or you know the whatever you're working on where there's kind of complicated uh rule-based software where instead of having people design these complicated roles themselves we can instead just learn these rules via data so that's another another way you can think about picking machine learning projects or which projects might be high impact another way i think is like you know that is worth mentioning is that just the copycat approach right like why why reinvent the wheel like if you're if you're working a company then in most cases like uh your company has a lot of similarities with other companies so like let's just look at what other companies are doing so a couple of resources on this i like netflix research published a talk where they talk about a lot of the different use cases of machine learning within netflix some of which you can see here which i recommend checking out this is from an industry report from a machine learning tools company called algorithmia where they they surveyed a bunch of machine learning practitioners like comp people in bigger companies mostly about which use cases of machine learning they're actually they're actually deploying and one interesting thing to note here is how low all these numbers are right so that the largest one is 38 37 so the the overall penetration of machine learning they found to be pretty low which which i think is interesting but also the categories that people are working on are interesting as well so there's a lot around you know reducing costs which i imagine is probably mostly stuff like eliminating manual work around like document review and things like that generating customer insights and intelligence so things like you know trying to predict whether you're whether your customers are going to churn or not improving customer experience this is like to me kind of the most exciting category right because this is like actually building better products through machine learning but then also a lot around like kind of internal process automation and more around like this customer this this efficiency thing so this is this is another lens into what other other people are able to get to work in the real world you can also look at papers i think you know the caveat here is that very a very small percentage of papers are actually like productionized and deployed in the world but you can look at papers from kind of like the big tech companies many of which are actually used in the real systems that they deploy you can also look at blog posts mostly from earlier stage companies most of these companies aren't really publishing a ton of papers about what they're doing but often their engineering blogs have good case studies and so this is this is a list of 10 case studies on you know making machine learning work in the real world different companies that chip win compiled and i would recommend reading through some of these as well to get to get ideas about what types of things might be useful all right coming back to our framework for prioritizing projects so we talked about some ways that you might think about assessing the impact of projects and then the other the other access here is feasibility right so how do you know how feasible a machine learning project is going to be or how much is it going to cost so i like to think of the three main kind of cost drivers of a machine learning project as you know in order of importance being data availability most important accuracy requirement of your project and then the you know the intrinsic difficulty of the problem itself so let's let's talk more about each of these so within data availability the main things to consider to tell whether you know data availability is going to be a is going to be a big problem for your project is you know how hard is it to acquire data in the first place how is how expensive is it to get that data labeled how much data are you going to need to to solve your problem which can be a hard thing to assess but you know and sometimes benchmarks can help you there how stable is the data right so if you're working with data that is never really going to change like your you know your your model is going to always be deployed in the same environment and people are always going to interact with it in the same way that's easier than if you're like in many machine learning use cases your data is constantly evolving right so in recommender systems for example users behavior changes over time because you know the world changes and maybe in some cases their behavior changes because your model made predictions that cause them to change their behavior so unstable data makes makes machine learning projects a lot harder and then lastly one other thing i'd consider is this data security requirements so if you're if you're working on a use case where you're able to just get all of the data that's going through your model back into some local place and you're able to look at all that data then that's a lot easier than if you're never able to look at your users or your customers data at all so for accuracy requirements uh requirement the main drivers here are how costly are wrong predictions so if you're building a self-driving car then wrong predictions can be catastrophic but if you're building a recommender system then you know a wrong prediction might just you know slightly annoy one of your users so if wrong predictions are expensive that can really drive up the cost of your project and then how frequently does the system need to be right to be useful so if you know if if you're able to if like your recommender system you know like just getting one right recommendation out of every 10 that it shows that's gonna be a lot easier to build than if you know users will like log out of your app and discuss if they see one wrong prediction and then lastly like one other thing to consider here is that the ethical implications of the accuracy of your model so if if there's if there's concerns about fairness or sort of you know differentiated value to different different classes of users for example then that can make projects a lot more challenging as well and then in terms of like the intrinsic difficulty of the problem itself like the technical difficulty of the problem i think probably like the main problem here in practice is just is your problem well defined at all right like have you have you scoped it out is it is it really is it really like structured as a machine learning problem but there's other things to look at here so is there good published work on similar problems right if there's not a lot of published work on similar problems then that means that there's probably more risk involved and more technical effort involved when you're looking at the similar work it's important also not to just look at you know whether those those those works like achieve the accuracy that you need but also what are the compute requirements both in terms of training the models that they trained a lot of state-of-the-art models now are incredibly expensive to train and also in terms of how much computers required to do inference on those models so in the real world you know one common constraint is that the the the model in the paper like takes uh a second or two to do inference and so you can't actually run it in real time and then lastly like one other thing to consider when you're assessing problem difficulty if you don't if none of these other heuristics give you a clear answer is just can a human do this right so just try doing it yourself and if you're able to solve the task pretty well then there's a reasonable chance that a machine learning system will be able to solve the task as well if you can't solve the problem given the same inputs that your machine learning system would have access to then you know you should ask yourself whether this problem is really feasible all right i want to double click on this accuracy requirement for a second because i think it can be a little bit counterintuitive that this is so important but i think the reason why this is so important is that accuracy requirements for machine learning models tend to scale super linearly in or project costs tend to scale super linearly in your required accuracy so you know as you increase the number of nines in your required accuracy then that tends to increase your factor your the your project costs by like you know as a very very rough estimate by like maybe a factor of 10 or something like that so if you really need a model that's you know 99.99 accuracy that's going to be a lot more expensive to build that model than if you only need 99.9 accuracy and the the fundamental reason for that is just that you typically need a lot more data and you need a lot higher quality labels to achieve these really high accuracy numbers all right i also want to talk a little bit about assessing the kind of intrinsic problem difficulty of machine learning projects so i want to talk a little bit about what's still what's still hard to do in machine learning right like we see there's tons of success stories there's a lot of things that seem possible what's still hard to do before i kind of give my answer to that i want to just strongly caveat this by saying that it's historically very challenging to predict what types of problems are going to be difficult for machine learning to solve in the future so you know this is an article from the new york times in the late 90s saying you know it might be a hundred years before humans be computers in or computers be humans and go right this is just this is just uh this is just such a hard problem like computers are never gonna be able to do this but you know fast forward less than 20 years and deep mind was the first built the first machine learning system to beat humans and go right so these predictions about what's going to be challenging in the future about machine learning can can be are notoriously tricky to make that being said i i still think there's there's some there's some interesting stuff that we can that we can say about this so one one heuristic that you'll hear is this is summed up in this tweet by andering which is that like pretty much anything that a normal person can do in less than one second we can now automate with ai right so i like to include this because i actually don't like this heuristic at all but it's something that you'll see quite often so the reason i don't like it is because i think it's like pretty easy to come up with counter examples right so i mean some some examples um of this are true right so it's getting relatively easy for machine learning systems to recognize the content of images or understand speech or translate speech or increasingly grasp objects and other things like that but there's also quite a few counter examples actually anyone have counter examples in mind to this this claim that anything that you can do in less than a second we can now automate with ai a couple that came to mind for me so understanding humor or sarcasm right like pretty easy for for humans to do or at least pretty easy for a lot of humans to do very very difficult still for machine learning systems to do in hand robotic manipulation right so for humans our hands are pretty dexterous we can you know we can move objects around in our hands relatively easily but that's still incredibly hard for for machine learning systems to do generalizing to new scenarios right so if you if someone shows you you know one example of a like a shark giraffe or something and then you see a picture of another shark giraffe you can say like yeah that's a shark giraffe but for machine learning systems that still tends to be pretty hard although their example there are success stories of you know in isolation solving some problems like that etc so again this is this is something i like i wanted to include because it is a heuristic that you'll see for assessing the feasibility of machine learning projects but i think it can be useful but just you know take this with a grain of salt like if you if you blank if you blanket apply this rule you're gonna you're gonna end up working on some near impossible problems i think a few things that i think are fair to say are still hard in machine learning are basically everything other than supervised learning so unsupervised learning you know despite the success of like self-supervised learning in in nlp tasks is uh like pure unsupervised learning is still relatively challenging to make work in the real world reinforcement learning a lot of progress in academia not a lot of real world applications at this point so both of these it's not to say that you can't apply these they're both showing promise in relatively little limited domains where you actually have a ton of data and compute available and the sort of domain itself is relatively constrained but in general you know if if someone if you're proposing to do a reinforcement learning project in industry you have to know that you know that's that's uh probably more research project than a product than something that is immediately going to be production ready so let's let's actually zoom in on supervised learning and let's talk about what's still hard there so a few examples that kind of came to mind for me question answering is still relatively hard summarizing text although i think actually these two over the past year or two have become a lot more feasible to do predicting video so given given some some sequence of video what's gonna happen in the next few frames building 3d models of the world recognizing speech in the real world you know in the presence of noise and accents and all those sorts of things resisting adversarial example adversarial examples doing math is still something that's like kind of surprisingly tricky to get neural nets to do well solving word puzzles solving bond guard problems so bond guard problems are kind of these visual reasoning tests where you you have like you're given some like visual sequence of like shapes and colors and you have to like reason about what's what's missing on the other side all these things are still things that are relatively hard to do even in supervised learning so let's try to like let's try to let's try to boil this down to some some principles that we can think about in terms of what problems are still hard in supervised learning so one category of things is where the the output itself that you're trying to predict is complex so maybe it's high dimensional or maybe it's ambiguous like in in the case of video prediction and so some examples of this are 3d reconstruction and video prediction but also dialog systems and open-ended recommender systems right like where you want your recommender system to be able to recommend any product that like users that user's input rather than trying to recommend something from a finite set of products that you define in advance another category problems that i think are still really hard are problems where you really need reliability from the system so where you need your system to be super super precise or very robust to kind of perturbations or changes to the outside world so one example a couple examples here are you know models that fail safely on out of distribution data or are robust to adversarial attacks or you know just need to do something to very high precision and then a third category that i think is still really hard is tasks where generalization is required so where you need to be able to perform well out of distribution data or you need to be able to do something that looks like reasoning or planning or causality and so there's a couple examples here from self-driving cars so edge cases are an example of out of distribution data and then control is you know increasingly learned but historically was done used using traditional planning algorithms in self-driving car companies and i think small data like just just generally data where you don't have large data sets still can be very tricky uh to work with with deep learning in particular all right so let's let's let's make this a little bit more concrete let's come back to our full stack robotics example and let's talk about why we're focusing on pose estimation so let's start by talking about the impact of pose estimation so our goal as a company is to do grasping right and so in order to do that we need to do reliable pose estimation and let's say that you know we were using a traditional robotics pipeline to to do that pose estimation and we were doing that using hand design heuristics and maybe some online optimization so that you know that causes some problems right it's it's slow it can be very brittle um it's complex so it might be a good candidate for software 2.0 let's talk about the feasibility of pose estimation so in terms of data availability it's relatively easy to collect data for this task we can just run our robot in a lab labeling data could be a challenge right because we need to understand what the pose of objects in the 3d world is so it might be hard for for a labeler to do that but we can get around that by just instrumenting our lab with enough sensors to be able to tell us the ground truth position of all the objects in terms of the accuracy requirement we do require pretty high accuracy to grasp an object so maybe we need the the pose to be correct to within less than half a centimeter but the cost of failure is pretty low right we we actually want our robot to be able to pick up a lot of objects per hour and if they fail a few times then that's not a big deal [Music] and then the the intrinsic problem difficulty this is you know very similar to to publish results that exist but you know we're using a different robot and maybe some different objects and so we'll need to adapt it so seems relatively low difficulty but there's still some some challenges there okay so the last thing i want to talk about in in this section is just how to think about running a feasibility assessment for a machine learning project right so what like what questions should you be asking yourself when you're trying to decide whether this project is feasible or not the first question i would always ask is are you are you really sure that you actually need machine learning at all right oftentimes like people do people throw machine learning into a project because it's like fun or it's cool but machine learning really increases the complexity of projects and so i think like making sure that you really need it before you embark on that project is a good idea once you once you're convinced that you actually need machine learning then i think one thing that is that people don't invest in enough typically is putting in the work up front to like really define what success would look like for this project so if there's other stakeholders in the project this means working with them to determine like all right how accurate do we really need this thing to be before you embark on it the next thing i think is pretty important to consider upfront is just what are the ethics of using ml to solve this problem right i think that a lot of times it can be tempting as engineers and researchers to say like okay it's our job to solve the problem not to to determine whether our solution is ethical and that can create problems downstream so think about this first once you've considered kind of those higher level upfront questions then what i would do is i would conduct a literature review i would look for other examples of people solving this problem in papers and you know see how hard it seems to be to implement those papers and you know once once you have a sense of what the techniques are then i would try to like kind of rapidly build a labeled benchmark data set so i would see like how and so this is trying to get at like how feasible your data collection and labeling process is going to be if it's really really hard to build this labeled benchmark data set then you know it's a there's a good chance that it's like data collection and data data labeling is always going to be a bottleneck throughout the project and then once you've kind of assessed the the feasibility of data collection and labeling then i would try to build like some minimal viable product right so you don't even necessarily need to train a model for this step you can just like use some rules um or maybe you train a very very simple model but this will this will just give you a sense of like whether this problem is solvable at all and how good your baselines are and then i think this kind of brings us back to the question of like the last thing i would once you've built this minimal viable product is i would ask yourself the question again like are you really sure are you really really really sure that you actually want to use machine learning to solve this problem or like maybe that heuristic that you came up with is good enough for now and so you should stick with the heuristic and move on and those are some of the main questions i would try to answer before you decide to embark on your ml project
Full Stack Deep Learning - Spring 2021,5,2211,Archetypes (Knowing the main categories of projects and implications for project,the next thing that i'm going to talk about is different different archetypes of the types of machine learning projects that you might see in industry and what are the implications of projects falling in those archetypes for how you how you actually go about building and managing those the models and the associated architecture around them so different archetypes that we'll talk about the first is i'll borrow the name software 2.0 to describe this right and so this is this is basically like any time that you take a part of your system that exists already and is governed by rules and you improve those rules using machine learning right so maybe you have an ide that does code completion and you want to improve your code completion algorithm using ml maybe you know you have a recommender system that's using you know matrix factorization approach and you want to like use more complicated machine learning to make that better or maybe you want to you know you have a video game ai that's like using heuristics and you want to build a better one using reinforcement learning second category of products that i want to talk about is human in the loop right and so this is where projects where the output of your model is going to be reviewed by a human before it's actually like executed in the real world so maybe you want to build a project to convert hand-drawn sketches into slides and then someone will review them before you know before they go in your pitch deck maybe you want to do email auto completion or help a radiologist do their job faster and then the last category that i want to cover is autonomous systems and so this is where the system itself is like making decisions in the real world with like no supervision from humans or limited supervision from humans so full self-driving or fully automated customer support or like instead of you know or like you know fully automating your website uh your website design these types of things are are autonomous and are gonna be more difficult to build so a few questions to ask yourself if you're building a project in each one of these each one of these categories so if you're building a software 2.0 product i think like one of the main questions to ask yourself is like are you actually improving performance of the system so it's important to measure that and does does that performance improvement actually translate to value for your business or your products along like whatever axes that you care about and then finally like do those performance improvements lead to a data flywheel i'll talk in a second about what a data flywheel is and why that's important if you're building a human in the loop system the questions i might ask myself are you know how useful does this system actually need to be in order to be useful or how good does it need to be to be useful how can you collect enough data to make it that good and those are kind of the main questions i would ask those are sort of the the main things that i think like drive project success for those types of projects and then for autonomous systems the main questions to consider is like what is an acceptable failure rate for the system how do you guarantee that your system won't exceed that failure rate like what guard rails can you have in place around it and how inexpensively can you label data from the system okay so i promised i was going to come back to this data flywheel concept so what is a dataflywheel dataflywheel is the idea that for certain machine learning projects or certain machine learning products you can create this positive feedback cycle where as you improve your model that improvement in your model performance leads to a better product and that better product leads to more users the more users you have the more data you're able to collect and the more data you're able to collect the better you're able to make your model and so i think this is kind of like this is like a gold standard to aim for when you're building machine learning enabled products like if you're able to create a data flywheel then your life is going to be a lot easier and you're more likely to build something that's going to get better over time and so the the kind of considerations here are you know when like where where can things fail in the data flywheel right so do more users necessarily lead to more data you you actually need to like set up a system that'll allow you to automatically collect data and ideally labels from your users do does more data lead to a better model it's kind of your job as the ml practitioner like if you if you can't figure out how to turn more data into a better model then you know that's that's on you and then does a better model lead to more users and you know the question here is do better predictions actually make you know does a more accurate model actually make your product better and this is something that i would i would try to measure very carefully and ideally try to assess before you even start the project so the the next point i want to make on this is that you know there's there's different trade-offs in terms of impact and feasibility of product projects in these different archetypes so autonomous systems have potential to be super super high impacts but are generally like very infeasible to create whereas like software 2.0 type projects right projects where you're just trying to make an existing system better you know the the potential for impact is lower because you do already have a system that's doing this job but they tend to be easier to do because you already have data available and then human in the loop is somewhere in between but one way to one other way to think about this is that the the kind of impacted feasibility of these categories of projects is not are not static so you can make software 2.0 projects higher impact and also more feasible by implementing a data loop right so by by building a system that allows you to collect data from the tasks that your model is solving so that you can improve the model going forward on this task and potentially on future tasks if you're collecting a broader set of data from your users as they interact with the model you can make human in the loop projects quite a bit more feasible and potentially even higher impact through good product design and i'll talk a little bit more about that in the next few slides but you can also make these these types of projects more feasible by you know just not trying to release something perfect from the beginning really you know release a good enough version get that out in the world as quickly as possible so that you can essentially turn this into a software 2.0 project so zooming in on this this good product design question if you design your product well that can actually reduce your need to have an incredibly accurate model so some examples of this are you know when facebook wants to tag you tag you in an image they don't just tag you right like because you know then you get like these wrong tags and that would be a horrible product experience so they would need to be very very accurate to do that they instead just suggest that you maybe tag yourself so that's the example of product design reducing the need for accuracy grammarly is a tool that kind of helps like correct grammar problems in your writing and you know again they one thing that they do is they produce like explanations for why they're highlighting certain things so it's not just like this is bad it's like here are some rules that you might want to follow to make this better and so again the users have a chance to apply their judgment as to whether the suggestion is correct or not and then recommender systems are another example of this right just just giving you like multiple suggestions for other things that you might um want to pick instead of just like taking you to the next one so there's a lot of i think there's a lot of like very underrated surface area and thinking about the the intersection between like making machine learning projects successful and good product design and so this this is not a product design class so we won't have time to do that topic justice but i do want to point you to a couple of resources that you can go to to learn more about this if you're interested so one i think actually like super underrated resource is apple's machine learning project product design guidelines and so they kind of they kind of urge you to ask like sort of three questions of yourself as you're designing your machine learning products the first is what role does machine learning play in your app right you know can you make it play less of a role to make it to make you know to make it more feasible and that sort of thing how can you actually learn from your users and so they have a couple of different sort of design paradigms for how to incorporate feedback from your users whether that's explicitly asking them for feedback like hey was this a good recommendation or not or implicitly asking them for feedback right like did they click on the recommendation or not by asking them to calibrate during setup right so when you when you get a new iphone it asks you to scan your face to create a model of your face for face id or you know you could ask users to input a few movies that they like before you start making them recommendations and then corrections is another another pattern that they suggest so actually manually going in and fixing mistakes that the model make these all create a better product experience and they also make our lives easier as model developers because they give us data to to assess the model's performance and potentially retrain then the third category of things that they that they have some suggestions about how to think through are how your app should handle mistakes so mistakes are an inevitable part of of the of you know what you're getting if you're building a machine learning enabled product so handling those mistakes gracefully is pretty critical and so they have suggestions like you know like show your users what the limitations of the model are like articulate to them where you shouldn't expect them all to perform well corrections so letting your users succeed even if the model itself fails helping them understand where those suggestions are coming from so they can build a mental model of whether they should trust them or not or explicitly telling them whether they should trust the results or not by outputting some sort of confidence so again kind of went through this pretty quickly but recommend checking out this this this resource from apple if you're interested in this and another resource that i'll point you to is this these guidelines for human ai interaction from microsoft and so they have like kind of a similar set of heuristics that you should think about when you're designing a product that has machine learning embedded into it and yeah one a couple that i'll highlight here are so you know they have they have some like potential patterns for how to handle wrong predictions for example which i think is one sort of design pattern that comes up both in like microsoft and apple's suggestions and then you know different ways of adapting to feedback and learning from your users over time so this is another resource that's worth checking out if you want to learn more about this and then last resource i i alluded to this blog post earlier but a blog post on machine learning product design from spotify which is a little bit higher level and less like tactical but i think has like some good mindsets and so their their philosophical approaches like find points in your product where there's friction and then and then basically like try to manually get rid of that friction and then try to automate the places where you're getting rid of friction using machine learning over time okay so that's that's some resources you can look at if you want to try to make your your human in the loop system more feasible by by like designing the product that sits around that machine learning model better and so autonomous systems you can also make more feasible and the way that you typically can do that is by making them less autonomous right so you can add humans in the loop you can add guardrails or maybe you just limit the initial scope of the problem that you're trying to solve so there's kind of examples of each of these tactics for making the problem more feasible in the self-driving car world humans in the loop is sort of the tesla approach right where like they're just you know they're just running the system in the background on all of your cars and they tell you that you're supposed to be paying attention and so when you like when you jump in and like take the wheel from the system that's you know that's that's turning the autonomous system into a human in the loop system guardrails around the system in the autonomous vehicle world are things like safety drivers right so most autonomous vehicle companies are taking this approach where they don't actually let the the av run in the real world on its own even if they mostly trust it there's always a safety driver there that's like ready to jump in and then limiting the initial scope of the problem one company that's taking this approach is voyage and their approach is instead of trying to solve the full self-driving problem from scratch instead let's just focus on like a narrow subset of this problem and for them that narrow subset is only focusing on senior living facilities where they can control their environment much more and so they've said like you know even though the goal is to eventually build fully autonomous systems we're going to constrain the problem and try to solve the constrained problem first before we broaden our scope [Music] so situating where we are
Full Stack Deep Learning - Spring 2021,5,2946,Metrics (Picking a single number to optimize),in the lecture and kind of the life cycle of scoping out your machine learning projects so we've talked about like some ideas for how you might think about picking a project and then some like overall considerations that you might think about if you're depending on what archetype your project falls into so the next couple of things that we'll talk about are a little bit more tactical and this is around metrics so how to pick a single number to optimize and then baselines so how to know if your performance on that number is good or not so the the main things that we'll cover in choosing a metric are you know the main things you really need to know are just that the you know the first thing is the real world is very messy and so in almost all cases you actually care about more than one metric like you don't just care about one accuracy of your model you also maybe care about things like the latency of the model or other metrics that describe your model's performance but that presents a challenge because when you're actually doing the process of optimizing a machine learning model training models evaluating them comparing them to each other that process works best when there's a single number that you're trying to drive down okay and so as a result we need to have some way of figuring out like among all the metrics that we care about how do we pick just a single number that we'll care about right now and you know the important thing to realize here is that this is sort of a pragmatic thing that that you'll need to do when you're when you're working on picking better better models for your task but that formula that like single number that you're driving down can and will change over time right so you know as you start to perform better against that metric you'll come back and revisit like okay what should we really be optimizing right now and so and the last thing i'll say on this is just you know this this sort of like single number view of the world i think is an important mindset to have when you're in the model development process but when you move on to kind of testing your model and evaluating it and deciding whether this model is actually like good enough to solve your tasks and go into production it's important to zoom back out and talk about the other metrics that you might care about and we'll talk about that we'll talk about that process in a separate lecture but for now our goal is to pick a single number that we're going to try to drive down and drive up so to review you know why is it important that we pick a single number so there's there's a few different numbers that if you're if you're doing like a binary classification task there's a few numbers that are like natural to consider so you know you have your accuracy which is the the number of correct predictions divided by the total number of total predictions that your model made and so in this case the accuracy would be like 50 then there's the precision which is another metric that you might care about which is defined as the number of true positives so where the actual answer was yes and the model predicted yes divided by the total number of positives that were predicted such both true and false positives then you have your recalls which is the the true positives divided by the pers the number of times that the answer was actually yes and so why is it important to choose a single metric well let's say that we're comparing three models and you know we care about both precision and recall for those three models and these are the precision and recall numbers that we get for those three models well which one is best like which one of those models should we pick to put into production or to you know run more hyperparameter optimization on and so that's why it's important to have like a single number in mind that you're trying to optimize at any given time so next let's talk about strategies for how you combine how you can combine the metrics that you care about the simplest thing which can work is that you might just like take a simple average or maybe a weighted average of the metrics that you're looking at so in this case like let's say that we just take the average of precision and recall and in this case that would sort of point us to model 3 as being the the model that's best in the real world this is i would say like less common to do what's more common is to like probably the most important of out of all these techniques to know is and i this idea of taking you know if you care about n metrics take n minus one of them where you're just gonna set a threshold on your performance according to that metric and then you're gonna always pick the model that does the best on the remaining metric and so often the way that this plays out is in practice is that like you'll you know you'll iterate on model architecture until you find a particular model that satisfies like your threshold on those n minus one metrics so maybe those are things like like latency or like total number of parameters or you know cost to retrain or something like that and then and then most of your optimization process will be spent um trying to drive a number like accuracy down on the end metric and but each time that you do that you'll kind of like choose perturbations to the model architecture and data and stuff that you're using that you think will like not make you much worse on the rest of the metrics and so we'll talk about this in a little bit more detail when we talk about edge deployment but you know for example like if you're trying to if you're trying to fit a model into the memory requirement of a particular edge device that you're working with then you might start with a model that you know fits into that memory requirement and then you know make changes to the model architecture but never like increase the size of that model too much so that you you know hopefully never get too far above that that model size metric that you care about so how should you choose which metrics to threshold and which metrics to actually optimize so you know one way to choose is just it's going to depend a little bit on your domain right so for example maybe there's some metrics that you can engineer around so like maybe you know maybe latency it's like important to have lower latency but if the latency is higher then like maybe there's some tricks like caching predictions or something that you can do so you can engineer around lower latency but maybe in your case not lower accuracy so this depends on the domain that you're working on another metric another way that you can choose which metrics the threshold are looking at which metrics are least sensitive to model choice so maybe you know maybe your accuracy varies a lot between all the models that you care about but your your your latency let's say doesn't really seem to vary that much between the different models that you're picking so you might choose a threshold latency and then lastly at any given point in your kind of model optimization process you'll for all the metrics that you care about you'll have some that are like relatively close to their desired values and then you'll have some that are further away and so a pragmatic decision that you can make is to say like okay let's just let's just let's just threshold the metrics that are already pretty close and let's just try to not make them too much worse and then let's really focus on driving this one that's still pretty far away from our desired value down and then once you've driven that down then you can reconsider other question you'll need to answer is choosing the value of the threshold so like what constitutes you know performance on that metric that's bad enough to just throw that all out entirely again domain judgment is going to be important here so what is an acceptor acceptable tolerance downstream and what's achievable for the task but another another way that you can assess this is by is by looking at how well the baseline model that you're working with does on this metric and we'll we'll talk in the next section about baselines and why they're important and how to choose them and then finally you know you might have some sense of how important different metrics are at this stage in your project right so again like if you you know if your model doesn't work at all then getting the model to work at all might be more important than you know getting it to work within the actual production constraints that you're working with so you know one like one way we could apply this to this problem is we could just say like okay let's you know most of these models have a recall that's above 60 percent so let's just set that as the threshold and we'll throw out any models that have recall less than 60 and then let's choose the precision let's choose the the model based on the one that has the highest precision subject to that constraint and so in this case model 2 would be that model then the last strategy that's worth considering for thinking about combining metrics is for some domains like for some problem spaces there's you know more complex or like the main specific formula that you can use for combining different metrics that you care about so in binary classification there's this metric called called the the mean average position so the way that the mean average precision works is you can plot the the trade-off between precision and recall on a curve right so at a given recall like at a recall of 100 what is your precision or a recall 50 what is your position you can take the area under this curve and that's your average precision for like let's say this particular class and then the mean average precision is the average precision averaged over all the classes that you're trying to perform a prediction on so that's the map and that's like an example of a domain specific metric that you can use and so you know for the sake of example here maybe the map would tell us that model one is actually the best model so you might pick that so again this is why it's important to put effort into thinking about how you're going to trade off between metrics up front because in this in this example that we made up each one of the metrics that you chose gave you a different choice for which model is performing best all right so let's let's come back to our running example of uh full stack robotics and working on pose estimation so let's introduce another metric that we might care about which is prediction time and so how we might go about choosing our the metric that we're going to optimize first is we might start by just like listing what are our requirements like what what do we think we're going to need to have in order for the system to work in production and so again our goal is to do real-time robotic grasping and so we think that you know maybe the position error i mean it seems like it probably has to be less than one centimeter um not really sure how precise it needs to be like maybe it actually needs to be much more precise than that but for sure it needs to be less than one centimeter and similarly like maybe the angular error we think that that might need to be within like five degrees or so in order for in order for the grass to succeed and we also might have we also might have a requirement that we think in order for this to really run in real time the predictions must come in like less than 100 milliseconds so the next thing that we might do is once we have a sense of the requirements like let's let's figure out what the what our current performance against those requirements is so we might train a few models we might like kind of just come up with a couple of hypotheses as to what our model architecture would look like you know not put a not put too much effort into optimizing those things up front but just train them and see where they fall on the performance curve so in our example let's say that you know they all tend to have a position error between you know or maybe around a centimeter a little below or a little above depending on the hyper parameters but the angular errors that we're seeing are huge they're like around 60 degrees and the inference time is also way above what's acceptable for our system this is actually kind of representative of where where we were when we started working on some grasping tasks that open ais like we had our position error we got that down relatively quickly but the angular error was really bad for some reason and so the question is like which of these things should we work on so i think a pragmatic decision in this case would be let's prioritize working on the angular error because that's really far away from what we think we're going to need in order to make this system work in the real world and let's threshold position error at one centimeter because we we feel like pretty sure that if it's above one centimeter it's not going to work so let's throw out models where our position error gets much worse than that and you know since we're so far away from actually getting the system to work in production let's just completely ignore runtime of the model right let's let's get a model that solves the task even if the runtime even if the inference time is is too long and then we'll figure out how to how to drive inference time down once we know that we can solve the problem and then the last step that we would do here is we would just you know as those numbers improve we would revisit what our metric is right so if like if we figure out a bug in our in how we're you know how we're labeling the angular the the angle and that causes us to be able to drive angular air down to 10 degrees or something then we might come back and say like okay now it seems like we're pretty close to to what we what we thought our requirements are were so maybe we'll focus on latency so we can actually try to run this on the robot okay so to review choosing a metric the real world is messy and you in the real world you almost always care about lots of metrics but our process of like iterating on models and building better and better models over time tends to work best when at any given time there's a single metric that we're focused on single number that we're trying to drive down or drive up and so as a result of that we need some like some rule for combining all of our metrics into a single number that we're going to work on right now and we covered some techniques for how to do that and then the last thing which i've emphasized a few times but it's very very important is that like this formula can and will change and it's natural to go back and and revisit that as you make progress on the metric that you're optimizing last thing i want to cover today is
Full Stack Deep Learning - Spring 2021,5,3756,Baselines (Figuring out if your model is performing well),baselines and so baselines the goal of baselines is to just to know how well your model's performing at any given time so key points here are you know why we need baselines the reason we need bass lines is that they give you a lower bound on how well you should expect your model to be able to perform and the tighter that lower bound the more useful the bass line is so so bass lines that are that are better but that are more that are closer to like the true best case performance are going to be more useful so let's dive into this a little bit let's say that let's say that you have some lost curves that look like this right so you know you have you know you're training a resnet and this is what your training loss and your validation loss looks like as you train the model so the question is like what what is the implication for like what you should do next in your model building process right so if you actually just look at these at these loss numbers that actually doesn't tell you enough to know what the next step is for for what you should do in this in your model building process so for example let's say that you have a baseline that you know human performance on this task is like basically already exactly the same as how you're performing on the training set or in another case maybe your baseline says that like you're actually nowhere near human performance on this data set and the the thing i want to point out is just that if you have the same exact model the same exact loss curves but you have a different baseline then that implies different next steps for your model building process right so in the first case where your baseline is basically exactly already what you're achieving on the training set the thing that you need to probably need to fix is the gap between your training loss and your validation loss right so you're over fitting here and so the next step would be like to try to reduce that over fitting somehow on the other hand if your baseline is much if if your training loss is nowhere near your baseline then the problem that you should work on first is addressing underfitting right so you should try to figure out like how to how to even make your training loss closer to human performance and you should do that before you try to address the gap between training and validation so same exact model same exact exact loss curves but different baseline imply different next steps so that's why it's really important to choose good baselines so what are your options like where can you go to look for baselines so some baselines i would call like external baselines so external to your model building process so you might have some requirements you could use those requirements as a baseline or you could also like go go and do a literature review and and look for published results on maybe similar problems this can be a good way to get baselines there's kind of a caveat here though which is that it's it's often easier to easy to delude yourself into thinking that you should be able to perform exactly as well as some published results but in reality you know your data set is probably different and so it's it's not always going to be a fair comparison so just just be careful when you're comparing to publish results [Music] then there's some internal baselines so in baselines that you can develop yourself one of the most important categories here which we've mentioned a couple times already are scripted baselines so just come up with some like heuristics or rules for solving your problem and maybe use opencv and maybe like you know try to try to tune an algorithm yourself and this is this can be a really powerful baseline and just just give you a sense of like how important this is when opening i was starting to work on the the project to beat the best humans at dota one of the first things that they did is like one of the best engineers on the team spent literally months just trying to build like the best dotabot that that he could build by hand and it turns out that he was able to build a better dota bot than the dota developers were able to build and so once they had that then they were confident they had like a reasonable baseline where if the model was better than that that that sort of hand design dotabot then that would mean that the model is actually like learning something you can also use a similar machine learning model as your baseline so like you can use a linear regression or something like that to give you a baseline and you know if your model is not able to if your deep learning model is not able to perform better than that then it's pretty good sign that there's you know there's something wrong in how you're building your model maybe you don't have enough data maybe there's a bug something like that and then lastly human performance and so human performance oh one other thing i want to mention on simple bass lines these don't actually have to be ml at all right like one you know even if nothing else like a reasonable baseline might be like the average of the outputs in your data set and i've i've had cases where you know you're working on training a model and you know it seems like the model's not performing that well and then you just compare its accuracy to just like taking taking the average across your data set and it does worse than that right so even that as a baseline is like pretty reasonable way to to give you some sense of whether your model is learning anything but coming back to human performance so human performance can be a really powerful baseline because it can give you a sense of you know it can be it could be a very tight bait very tight lower bound in the sense that it can be you know it could be almost as well as you can expect the model to perform especially if the especially if you're reliant on labels from humans to to create your performance so there's better ways and there's worse ways to create human baselines and there's kind of a trade-off in measuring human performance on a data set between how good the baseline is like how like how close it is to like the true baseline on the task and how easy it is to collect the data right so on one hand you can like you can just give the task to like random people on amazon turk or something and that's like super easy to do but generally low quality unless you really think through how to explain to people how to solve the task slightly better is to you know instead of just paying like one person per task you can pay a few people and you can like ensemble the results that those people produce so a little bit harder to collect because it's you know you're just a little more expensive but the quality of the baseline is better you can pull in domain experts which in some domains like medicine is absolutely necessary but in some cases like just training people a little bit more to do your task can make your baseline better you know you can maybe even pull better better experts like someone who's really a specialist and what you're trying to solve or you can like you can ensemble those experts you can do like a mixture over what those experts say and that's maybe like the most expensive way to create a baseline but will be the highest quality so you know how do you actually choose where on that trade-off makes sense for your task i think like one way to think about it is you want to you want to aim for the highest quality that you can that will allow you to like label more data relatively easily down the road so you know if you if you have like if you have a budget of 100 for labeling for your entire project then like don't spend all 100 of those dollars on labeling the initial data set you know because you wanted like the best doctors in the world to label it for you more specialized domains so like the the less generic your task is the more you're going to just need skilled labor labor labelers and you can be intelligent you can be more intelligent about how how you like which data you choose to label by looking at cases for example where your model performs worse and concentrating your data collection on those kinds of places and we'll talk about this more in the data lecture all right so so the main points that we covered on choosing baselines are the the reason why baselines are important is that they give you a lower bound on how well you should expect your model to perform on this task and so the better that lower bound is the more useful this is baseline because it gives you more confidence that your model is actually performing well and not just you know say memorizing the data or you know doing something stupid
Full Stack Deep Learning - Spring 2021,5,4221,Conclusion,okay so just to wrap up what did we cover today first thing that we talked about was the life cycle of machine learning projects right and so the main points to take away there are you know i think like maybe maybe the most important takeaway um for me in that piece is that you should think about the life cycle of machine learning project not as like a linear you know start from step one and go to step n but as an iterative process where at each step you're you're you know working on the task at hand but you're also maybe looping back to earlier steps as you learn more about your problem or build a better model but it's other takeaway from this is that the other implication of this is that the sooner that you can deploy something quickly to kind of complete the cycle the easier it's going to be to iterate on that so after that we talked about prioritizing projects and we talked about some heuristics for how to find high impact projects and and and projects that are relatively feasible and like the main takeaways there were that you really want to look for projects where you know it's it's relatively feasible to collect data and where the the cost of wrong predictions is not going to kill you so after that we talked about like a few different kind of high level archetypes that you can use to think about what type of machine learning project you're working on and maybe how to make it easier to solve your to solve the problem for that type of project and so you know one point that i want to emphasize here is that in many cases the secret sauce to like making these projects really work well like really improve over time is um to build automated or semi-automated data flywheels but another point that you might take away from this is that machine learning engineering is tightly coupled with doing good product engineering and product design and so there's there's a lot of different ways that you can make your project more feasible just by making better product choices about how your how your uh model is integrated with that product so we talked about metrics and you know the main point here is that in the real world there's a lot of metrics that you care about around your model but at any given point in time you should always have one and then you should revisit that as your project as your project evolves and rethink whether that's the right thing to be optimizing right now and then finally we talked about baselines and so the point of having good baselines is that good baselines help you invest your effort in improving your model in the right place and so if you have a better baseline then that gives you a sort of more granular sense of what the problem with your model right now might be and so that's that's why it's important to invest in good baselines and we talked about a few different ways of building baselines okay i also wanted to drop in a few more resources if you're interested in learning about this topic there's a lot to learn about here and so we you know we covered a lot but we also in some sense only scratched the surface and so here are some resources that i would recommend checking out all right and that is all happy to stick around and answer more questions
Full Stack Deep Learning - Spring 2021,6,0,Introduction,so today man let me show the dog again real quick okay so this is my dog she got shaved so she looks a little weird but she helped me develop these slides the slides are about infrastructure and tooling for deep learning
Full Stack Deep Learning - Spring 2021,6,24,The Dream vs. The Reality for ML Practitioners,and i want to start off by describing the dream that that practitioners have which is that we provide data and then we somehow just get a really good machine learning prediction system and it's available as a scalable api or an edge deployment and that deployment generates more data for us which we can use to improve our system and that's really the dream the reality is that you don't just grant the data to the system you have to find it aggregate it process it clean it you know label it version it then you have to write and debug model code then you have to somehow find compute run a lot of experiments review their results discover that you did something you know wrong or maybe just try different architectures to write more code provision more compute do it again at some point you're happy with it so you deploy your model and then you have to monitor the predictions that the model makes on production data such that you can gather some good examples aggregate and process them clean them label them add them to the data set version and then do it all over again i'm not the only one who's saying you know the what the dream is this is andre carpathi slide from a couple of years ago i guess called operation vacation so that tesla self-driving their dream right is to build a system that basically goes from the data they've gathered through their training evaluation and inference processes deployed on the cars and then as people drive gather more data and then just seamlessly add it back to their data set and then do the process again so that they can all go on vacation and see a model improve even in their absence and when you think about what the dream takes it's actually not mostly machine learning code right it's all the stuff around it to create the system for this machine learning system to improve and this observation was made a few years ago most famously in this paper called machine learning the high interest credit card of technical debt the observation basically is that the machine learning part of the code base is actually a very small part of it and in trying to deliver that part of it you take on a lot of debt and building up all these other components of the system that then have to be maintained and invested in and so on just for your little thing that you originally set out to do to be able to do its thing we can break down the landscape of all this infrastructure that's necessary into three broad buckets one has to do
Full Stack Deep Learning - Spring 2021,6,198,The 3 Buckets of ML Infrastructure/Tooling Landscape,with data the other has to do with the training and evaluation of the models and then the third one has to do with deployment in data we have the sources of the data logs databases file system you know images and stuff like that data lakes and data warehouses like data bricks snowflake bigquery redshift processing of the data via airflow or apache spark or dagster stuff like that exploring the data with pandas potentially transforming it into forms that are more amenable to exploration of sql with dbt versioning some final kind of data artifact and for a lot of the data that we deal with we also have to actually label it because it comes in a way that's not immediately useful for training on the training side we have the source of compute could be hardware it could be cloud providers of which there are a number of different vendors then once you've provisioned the compute you have to manage the resources that you now have in order to efficiently use them there's basic software engineering skills of course that play a role in all of this there's deep learning specific and machine learning specific frameworks which allow you to do distributed training more easily also all this training is generating a lot of data that you need to manage so there's things like tensorboard weights and biases for experiment management and running experiments blind is one thing but it'd be nice if we could be guided in like what experiments to actually run this is often called hyper parameter tuning and there's other solutions for that once we have a model we're happy with we want to be able to reproduce our results and ideally just automate the process so that's continuous integration we want to ensure that the model performs as well as we expect via testing if we're deploying on mobile or hardware like you know robotic hardware or some kind of sound series or anything like that often called edge computing we need specialized tools to to do that well if we're deploying to the web there's different deployment options available to us we will want to monitor our running model in order to catch certain problems with it and also maybe find good data points to add back to our data set around all of this you know i broke it into three buckets but in fact you can think of it as one kind of ammo ops process and there's a number of vendors offering like an all-in-one ammo op solution like amazon sagemaker domino data lab and then most recently also there is this concept of a feature store which plays a role in data but also in deployment so i put in the deployment bucket and today we're going to talk about the middle part of this diagram so training and evaluation so i'd like to start with with the most foundational thing here
Full Stack Deep Learning - Spring 2021,6,380,Software Engineering,which is just software engineering skills the programming language we use for machine learning and definitely deep learning is is python and the reason for that is actually because of the libraries not necessarily because of properties of the language but because of the libraries that have that have that have built up over time python's the clear winner in scientific and data computing there are some nice things about it it's very interoperable with with low level languages like c but that's also kind of a negative it'd be nice to just not have to leave python and be able to write really performing code as things stand today you kind of have to write the really performant scientific and data computing stuff in in c using cython and then integrate it or you know c plus plus or any other low-level language and then integrate it with python via a library but it's a nice experience for writing scripts and i think a lot of a lot of scientists and statisticians and people that aren't computer science first but are some other discipline first came to python because it's such a logical language to kind of write it almost reads as as english language and so now it's the scientific language to edit code we use text editors there's old school stuff you know old school options like vim emacs we can spark a little poll maybe if someone wants to post the poll right you know then versus emacs key bindings there's nodebooks which is an interesting way to do scientific computing where you execute one cell at a time and see the results of running a cell and you can execute cells out of order jupiter is the python solution for nodebooks there's new school editors of which i think the clear winner is vs code visual studio code open source project from microsoft and there's also python specific integrated development environments like pycharm which are also quite quite nice visual studio code makes for a very nice python experience and it's free it's easy to set up it has a powerful ecosystem so that's what we recommend i want to point out some nice things about it just in case you haven't used it so far there's built-in version control uh staging and diffing so you can see like the diff between the branch you're on and the code that you're you've actually edited and you can commit straight from your editor you can look at the history of commits you can do a lot of stuff without actually leaving your editor you can peek at documentation for different functions so as you start typing there's powerful autocomplete that it's called intellisense that analyzes your code base analyzes the code base of your dependencies and then is able to present helpful documentation autocompletes for you you can open projects remotely very easily which is quite nice so the farm2 is a machine that that i have access to and with just one click i'm able to get a text editor on another machine you can lend code as you're right so you can see certain highlights maybe problems you know potential problems syntax errors and other problems with your code this is another view of visual studio code that i just wanted to show that you can also use the terminal in the same window which is really the way i do things now i used to use the terminal separately from the from the editor but now i use actually the integrated terminal in the editor and it's just super nice and specifically for notebooks it's quite nice because or when you're working on another on a remote machine let's say you start a notebook in the terminal so you say jupiter notebook that starts a notebook on that server and what vs code does is it automatically forwards the port from the remote machine to your local host so you can open the notebook in your own browser just very seamlessly i want to make a note about winters and type hinting so linters are automated rules for your code so for example should there be a line length limit right some people say you know 80 character lines that's the max and if a line goes over that limit then it should be broken up into multiple lines well once you make that decision right is it 80 characters 100 characters whatever it is once you make that decision that should be codified and done automatically you shouldn't have to worry about it so and that goes for a lot of things how should variable variables be named right camel casing versus underscore casing snake casing everything that can be codified should be and additionally static analysis can catch some bugs so let's say you use a variable that was never defined you might not see that but a static analysis program like pyland will notice it and and highlight it in red so that it's very visible to you and then we can also if you see here in the train model function signature it says it expects a variable named model and then it says you know colon model that's that means the type of the variable that it expects is model epochs it expects an end batch size and end gpu index an end that can be none right so an optional end and this kind of thing both documents your code which is very helpful but also can catch bugs so if you if you call a function with an in but it expects a float or vice versa there might be some bugs there and we'll actually we'll we'll set this up for our code base in the labs in an upcoming lab so jupyter notebooks is something i'm sure you guys have seen because they've become really fundamental to data science and i think they're great as a first draft of a project and i think jeremy howard from fast that ai is really good to learn from like his q his uh fast that ai course videos he's just the notebook wizard flying around doing all kinds of stuff very inspirational but it is difficult to make scalable and reproducible and well-tested code bases with notebooks and the reason for that okay drawing on a blog post called five reasons why jupyter notebooks suck the reasons for why it's hard to make a good code base with notebooks is because they're hard to version they intersperse you know your actual code and the cell output and so you either have to not commit the cell output in which case it's a little less useful as a notebook or commit the cell output in which case it keeps changing whereas your logic might not have actually changed the the development environment of the notebook is pretty primitive it's in the browser it works okay but it's could be a lot better it's you know not vs code they're hard to test there's out of order execution can lead to artifacts where it just doesn't make sense like the state of your notebook doesn't make sense if you read it from top to bottom but you might have executed from bottom to top or something like that so it's just hard to reason about the state and then it's hard to run long or distributed tasks from a notebook interface the counterpoint to all of that is that some companies that do good machine learning have based their ml workflows on notebooks so there's some people committed to notebooks and once again jeremy howard from fastai uses notebooks for everything including developing his fast ai code base and there's a project called nbdev from from from that group of people that just shows you a way to develop you know good well-tested code still in a notebook i don't personally vipe with it but i want you i want everyone to know about it in case they do vibe with it an interesting recent project that's kind of an alternative to notebooks is called streamlit and uh it fulfills this need of interactive applets so notebooks can fulfill this need via widgets so sometimes you can you can drop a little widget as a cell output so for example a slider and then the slider might control the value of some variable but it's a little bit hacky to hook that up in a notebook whereas in streamlit it's like a first class citizen and so the idea is that you can make interactive applets like little apps just by writing python code and using the streamlet primitives like buttons sliders maps charts and such things and it's achieved just by decorating normal python code and what they're working on is making it easy to share these applets with your team or even just you know worldwide on the web that's not quite there yet but but it's an exciting direction so check out streamlit and then one kind of like boring administrative thing is defining all the dependencies right we have pie torch pie torch lightning you know pandas all that stuff we recommend using this project called conda for setting up the python and cuda and qdn environments so you know i want to use python version 3.8 cuda version 11. but not using conda for the actual python requirements like pytorch and my pi and stuff like that so for that we use pip tools and the reason for that is because pip tools has a really nice system for breaking up requirements into several files so you might have production requirements which are separate from development requirements which are separate from your test requirements and it's really easy to do that using pip tools and it also separates locking the exact version of a package from just saying that you want that package right so i want numpy i don't i don't care what version of numpy i get i just want numpy but for reproducibility i wanted to be stored what exact version of numpy i used and so just separating that out is what pip tools does really well so check out our sample project and this is also how we do it in the lab so the next thing we should talk about is the compute sources
Full Stack Deep Learning - Spring 2021,6,1026,Compute Hardware,what are the compute needs for for deep learning i like to kind of break it down into into a basic like you know early development and then scaled out training and evaluation so when we're doing early development we're writing code we're debugging the model we're looking at the results we want to you know quickly do things it'd be nice if we could use things that we're comfortable with like like guise and what could allow us to do it is actually having gpus in our desktop like in a computer we have access to or a compu or a computer we can easily ssh into and for you know rapid experimentation it'd be nice if it had more than one gpu so it could be on-prem could be in the cloud for scaling out your training we're going to want to run some kind of architecture hyperparameter search we will want to train models that just don't really fit on only four gpus and it should be easy to launch experiments on onto this you know compute and review all the results that are getting generated so here it'd be nice to have a cluster of machines could be on-prem could be in the cloud the reason this matters increasingly with each passing gear is because the results that are coming out of deep learning are using more and more compute right so i think we had the slide in the fundamentals lecture but it's you know notable deep learning results versus time versus the petaflops that you know of compute that they used it keeps on going up and you can see this in the transformer models that we discussed last week so jeep you know gpt3 still hasn't been trained by the community right only open ai train that using like microsoft cloud and millions of dollars we still haven't done that but you know google already came out with google switch transformer which is another couple of orders of magnitude larger than that so it's just it's a race to fit bigger and bigger models and run crunch through more and more data so one question we have is do we try to get our own hardware or just go straight to the cloud and to answer that we should look at the gpu basics we should look at the cloud options we should look at the on-prem options and then try to analyze what's going on make a recommendation so the basics of gpus is that basically it's an it's an nvidia game right it's a still really the only provider of good deep learning gpus they're not the fastest that's actually the google tpus tensor processing units which are available on the google cloud platform but it's worth you know unless you're committed to only using tpus which i don't think anyone is it's worth knowing about the nvidia gpus and so there's there's a number of them but we can make some sense of them so the first thing to notice is that there's a new nvidia architecture every year too and it went from kepler back in like 2014 when the when the deep learning revolution was just getting kicked off to pascal to volta to turing so it increases alphabetically and then it wrapped back to ampere they come out with a server version of the card first and then like an enthusiast version like you know a retail market card but that's still very expensive and then a consumer card that that most people are just gonna be gaming on and if you're using this for business purposes you're supposed to just use the server cards not the you know also possible to game on cards gpus have different amounts of ram this is very important because you can only compute on data that's on the memory of the gpu so if you can't get your data onto the gpu you can't compute on it the more data you can fit on the gpu the larger your batches are the faster your training goes there's so first of all no one does 64-bit computing and deep learning it's it's for some scientific computing stuff like you know predicting the weather or splitting the atom or whatever 64-bit scientific computing is is often done for deep learning 32 bits is what you use you don't you just don't need that much precision and in fact you actually don't even need 32 bits of precision i guess we'll talk about that next but starting with the volt architecture nvidia developed this technology of tensor cores which are specifically made for deep learning operations which are mixed precision between 32 and 16 bit so it might be you know 16-bit addition 32-bit multiplication or something like that they split up the bits in a very efficient way to make these matrix operations incredibly fast and this is really good for for the type of models that are prevalent today convolutional transformer you know fully connected models there's also 16-bit straight-up 16-bit training and that just means that you can fit larger batches on your gpus of the same data just because it's not as precise it works just about as well and it is a great speed up so going through the architectures and sequence we can talk about kepler maxwell first real quick it's a lot slower than the pascal volta architectures and basically you shouldn't buy it but you will see these in the cloud in the cloud offerings k80s and p100s in fact the k80 is what you're likely to get if you if you get a collab it's almost certainly a k80 gpu on there and yeah so they're cheap on the cloud because they bought many millions of them and now they're stuck with them then there's pascal architectures so the 1080 ti is from like 2017 it's still okay you know if you're buying used for cheap especially for recurrent networks where you don't get that much of a speed up from the tensor cores and p100 is a cloud server gpu that uses the pascal slash volt architecture then you have the voltairing architecture which is a preferred choice you know over the kepler and pascal because it supports the 16 bit mixed precision via the tensor cores which makes everything faster so the 2080 ti is like at least 30 faster than a 1080 ti in 32 bit but it's twice as fast if you're using 16-bit training and the titan rtx is even faster than the 2080ti and in the cloud you have the v100 which is the kind of well it's not the ultimate anymore but it used to be the ultimate for speed in the cloud 16 gigs 14 i think million 32-bit oh yeah 14 32-bit teraflops and then most recently as in like just in 2020 and really the the consumer versions are just coming out right now like the 30 90s and 80s and 70s is the ampere architecture so they have the most tensor cores so they have the most tensor t flops in this table it's like at least another third speed up over a turing like the 2080 ti here's an older benchmark that doesn't include the 30 the 3000 series but you can see that the v100 is fastest and then the tpu is still a little bit faster than that but the a100 which is the ampere version of the v100 is like two times as fast as the v100 for confnets and even faster than that for language models and this is according to benchmarks by the the folks at lambda labs which which build these machines a great resource for all this stuff is tim detmers a grad student at uw and they he has a post for which gpus to get for deep learning right now he's recommending the 38 or the 30 90 as the gpu to get because it's so much faster you know maybe even twice as fast as 2080 ti the problem with the 3000 series is their heat heat production and power consumption so you can fit four 20 80 ti's in a reasonable desktop rig but you can't do that with the 3000 series it would like use too much power like it literally your household electrical socket doesn't produce enough power to to power it and then they build too much heat so you can't really fit more than two into a normal desktop rig in the cloud you have amazon web services google cloud platform microsoft azure they're the heavyweights they're all pretty much similar in function you know what gpus they provide and how that all looks and price aws tends to be more expensive than the other two gcp is like a tiny bit cheaper it also has tpus which is quite nice and then azure i don't know people have really used it but it has the same kind of gpus at roughly the same price there's startups that are exciting one is core weave there's lame the labs so on amazon web services here's some pricing so you can get a k80 well you can get eight k 80s each one has 12 gigabytes ram that'll be 14 per hour you can get eight v100s each of these gpus has 16 gigs ram that'll be 24 per hour you can now get a special v100 with 32 gigs ram not 16 gigs ram so this is really nice for these big language models and that's 31 an hour and then now the the most recent gpu is the a100 and you can get eight of those and each one has 40 gigs ram for 32 an hour so this is quite this is quite wild so there's three generations of gpus they provide and we've been doing this course for a while and the prices like really don't change they're just kind of sticking to these prices so on google cloud platform you also have the k80 you have the v100 you also have the p100 which is a little bit cheaper and and slower and you have the a100 in some zones because it's so new but the real start of the show might be the tpu for you and it might be a reason to use uh gcp versus versus uh aws so the promise of tpus is that they're more expensive per hour but they're also like a lot faster and have a lot of memory so you can crank through training faster i haven't personally used them it's kind of exciting to get a tpu in a collab i don't know if that's ever happened to you but good to know about microsoft has k 80s p100s v100s has similar pricing then there's the startups so there's lambda labs cloud which you can get either four 20 adtis basically uh pascal based 11 gig gpus for 1.50 an hour which is pretty great or you can get an 8 gpu v100 machine for 12 dollars an hour so that's 12 an hour versus 24 an hour on aws so that's making a big difference right there's a startup called coreweave which i think pride themselves in having the lowest price as possible and so there you can get eight v100s for only four dollars eighty cents an hour it's the fourth row on this table so yeah each v100 has 24 gigs or sorry it has 32 gigs there's 24 cpus and it's 4.80 an hour so that's that's really good on-prem you have some options you can build your own machine which is actually pretty doable up to four you know 2080 ti's or two 30 90s or 30 80s you can buy a pre-built machine lame the labs they do the cloud offering but they also build machines that's your main business and video will sell you a machine and then there's standard kind of server pc builders like super micro stereo scale there's bison tech there's a bunch of them i built this one at some point 420 adtis uh i think it cost around seven thousand dollars and then today with the 30 90s you'd only be able to do two of them probably be around that eight thousand dollars the problem is you probably can't find the 3000 series gpus because they're all sold out i guess nvidia had trouble producing them at the scale they need and also people are using them for for like ethereum mining again but it's kind of fun to build a pc if you haven't done it it's painful to go beyond four gpus or two of the three thousand series gpus so don't try that and then once again tim that merge is your friend he'll you know he'll help you pick the components for this machine if you don't want to go through the trouble which is totally reasonable you can just buy it from lambda labs i configured one last night with the 230 90s 24 core cpu 256 gigs ram it's like 11 000. so that's probably like 20 more expensive than if you built it yourself but it also has a warranty and you know probably worth it and it's definitely worth it if you're trying to build a server grade machine because it's just not we can't we can't build them we have to buy them so if you want to you can buy an eight you know v100 server for around 100 000. so these prices are pretty big right like a hundred thousand that's that's a lot of money but if you compare them to the cloud it actually starts to make sense and so here i worked up a 4 x 2080 ti machine versus a 4x v100 cloud machine right so we can build a for 2080 ti machine for let's say ten thousand dollars it'll actually be cheaper but let's just say ten thousand dollars and we can use a forex v-100 machine on aws for twelve dollars so then if we run the machine that we build full time then in five weeks it's equivalent to running the v100 cloud instance full time if we don't run it full time but we run it for like 16 hours a day five days a week so that's like you know you come in you start running experiments you run them but then they finish at some point while you're sleeping and then you don't work on the weekend then it'll take 10 weeks to pay for the machine one way to see it is here's someone on reddit who built a deep learning bricks like six thousand dollars i i don't buy his claim that or her claim that it's that it's you know half as cheap i think it's 20 markup that lame the labs does not not like 100 but but still you know it costs a lot to train on imagenet on 2080 ti's so you save twelve hundred dollars which is the cost of a 2080 ti every time you run a full image in that training if you did it on your own build instead of using the google compute engine which i thought that's like an interesting way to think about it but another way to use the cloud is to launch a lot of experiments at once using specifically on-demand instances also called spot instances so i've been giving you per hour prices there's another way to to get instances which is spot pricing which are called preemptables that means you get an instance but at any second it might disappear from under you and for that risk you get a lower price which might be okay if you're using you know you're launching a whole armada of experiments at once so it's okay if some of them die and then get restarted later so using the cloud you can enable quicker experiments and so if time is a very important variable not just money then that's a reason to go with the cloud because you you can't like if you only have four gpus it'll take you 24 hours to run you know 16 trials if each trial takes six hours but on the cloud you can actually just finish all of them in six hours but the price is still pretty steep right so it's you know you can only run a hundred such experiments before you have paid the cost of a whole machine to the cloud so in practice you know the cloud is expensive but it's also hard to make on-prem scale past a certain point there's devops things that we have to do like provision instances you know make repeatable certain tasks that are just easier to do in the cloud than to like try to set it up yourself and then if a machine dies or needs some maintenance that's going to be a constant headache if if you're responsible for managing on-prem machines and in the cloud you wouldn't even know about it they just you know they they deal with it for you for a hobbyist our recommendation is to build your own pc you know maybe a 4x 2080 ti or 2x uh 30 80 machine and then if you want to scale out the your experimentation i mean you could just use the same machine just like run it overnight all the time but you could also then go to a cheaper kind of startup cloud like lambda or core weave and then get cloud instances there for a small company or startup i think you know let's buy a lambda labs machine that's sizeable for every machine learning scientist that we have and then to scale out let's buy more shared server machines and put them in a data center or use cloud instances like v100 instances or something and then for a larger company let's you know i think let's up that adax gpus per scientist or just go straight to the cloud with fast instances because presumably you know money is less of a less of a concern so next up let's talk about how do we actually manage all these
Full Stack Deep Learning - Spring 2021,6,2215,Resource Management,gpus and whatnot we have maybe multiple scientists using multiple machines with multiple gpus they might be running different environments and what we want them to be able to do is run many experiments with each experiment having its proper dependencies and the resources in terms of gpus and cpus and ram that it needs for the experiment and there's a number of solutions we could do we could kind of try to script our own solution to it we could use uh slurm which is a well-known cluster work workload manager we could go with docker and kubernetes or we could use software that's specialized for exactly this use case so the simplest thing is like just maybe hand coded stuff right so what we want to do is allocate resources that aren't being used to programs that will use them we could actually just write a simple script that does that and just checks like is a gpu available if i'm about to start using it then like lock it so that other people can't use it and that's you can do that even better you can use a cluster job scheduler often it's it's going to be slurm and so the way that basically works is you have a script that defines the resources it needs so this script needs one gpu 10 gigabytes ram and then specifies the dependencies it needs and then says what commands it needs to run so that's the last line it says python cnn.pie and then you submit that job definition using the slurm kind of cue runner and so you submit it to the queue and then slurm executes it on a machine that has the necessary resources and so then all of your scientists can if everyone agrees to use slurm they define their jobs using it and then the cluster scheduler correctly allocates resources to them if that's a little too old-school for you you can use docker and kubernetes so docker is a way to package up like an entire dependency stack so that you know cuda version certain binaries python version everything you need but exclude the operating system so a virtual machine includes the operating system in like it's packaging docker does not it only includes the the precise dependencies on top of the operating system and then the code we'll talk more about it in the deployment lecture and we'll use it in that lecture lab and then kubernetes is a way to run a lot of docker containers on top of a cluster so kubeflow specifically is an open source project that's been gathering steam originally from google but now it's it's it's a community project that lets you spawn and and get jupiter notebooks and have multi-step amount workflows which can include gpus and there's like plug-ins that people have that make certain things easier it's still kind of a pain to set up and this is actually an active area like it should be a lot easier to provision computer resources for deep learning there's all-in-one solutions that we're going to talk about like sage maker paper space gradient determined ai that also you know provision resources for you really nicely so we'll talk about that later also some recently you know recent startups have this specific thing as their main goal so for example there's any scale which came out of berkeley rise lab they had an open source project called ray which we'll also talk about and now it's a company called any scale and that's kind of their vision right it's like it should be really easy to provision the compute you need get some work on your laptop and then effortlessly scale it to the cloud and then from the makers of pytorch lightning there's a startup called grid.ai which has that same vision too right so it should be on your local machine you say python model.pi a learning rate you know 1 to the power of negative 6. can it be as easy as typing that we just type you know grid train say how many gpus we want still model that pi that's our code and then learning rate we can actually run a hyper parameter search right just that easily so that's the vision you know we'll see if it if it bears fruit but for now the recommendation is probably to use slurm let's talk about frameworks and how they can make distributed training
Full Stack Deep Learning - Spring 2021,6,2500,Frameworks and Distributed Training,easy so deep learning frameworks have existed for a while like over a decade with um thiano and torch really old projects actually and you can think of where each framework might fall on this on this chart where the axes are is it good for production use aka is it really fast and the other axis is is it good for development as in is it easy to write deep learning models in this framework and when tensorflow came out in 2015 it was you know billed as really good for production it makes the static optimized graph that's can be deployed to gpus cpus even mobile devices but the user experience of it wasn't very good because you had to like basically do this meta meta thinking where it's like you would just you would use a programming language to describe the execution graph of the deep learning model and it just didn't really fit in how people thought naturally cares made it a lot easier to use it's not a tensorflow only project but it is now part of tensorflow it's just a lot nicer of a way to define deep learning models and kind of hides a lot of the complexity of tensorflow behind the nice user interface pytorch came out a couple of years later with kind of a different pitch right it like wasn't supposed to be the fastest model produced out of out of dunagan pie torch but it was basically straight python and you guys have seen that in the lab right so it's a really good development experience so these are both these are still the two leading options tensorflow and keras or pytorch and you should use one of them unless you have a good reason not to and both have since converged to the same point which is tensorflow got eager execution and so tensorflow 2.0 you can write code that looks a lot like pytorch and then pytorch got a lot better at being fast because this project called cafe 2 got folded into it via torch grip so you can compile your model down to a really fast artifact most new projects today use pytorch i think because of just it's way more depth friendly for python users there's some libraries that have come out since that make it even better for development so fast.ai is a good library it builds on top of pytorch and it builds in a lot of good practices best practices and then pytorch lightning is what we use in labs similar vein right adds a very powerful training loop encodes best practices makes like distributed training really easy so here's some evidence that pythorgs dominates new development you know at least when you look at academic publications we i don't have more recent data than this but it's up you know upward of eighty percent of like submissions are in pytorch you can search for or you can see how users are searching for pi torture tensorflow looks like they're about equally popular right now and you know why do we need tensorflow or pytorch deep learning is actually like not a lot of code if you have a good matrix math library like numpy so we were able to code you know at least a linear layer in like 20 lines of code but we had to code our own backwards loop or backwards pass so we had to like do a manual derivative and yeah if you're able to do that then it's not a lot of code but we don't want to be able we don't want to have to do that we want audit differentiation which means we just write the forward pass and then we want the framework to figure out how to back propagate through it so how to differentiate the function that you wrote and also we use numpy but that doesn't actually use gpu so if we want things to run on cuda that's going to be another a lot of work because now you have to write actual cuda code also there's like all the layer types all the different convolutions and transpose convolutions and like lstms and whatnot optimizers interfaces to different data formats so all that stuff really built up so that's why we need frameworks except sometimes you really don't need all that stuff um you just need auto differentiation and the ability to run on gpu and this is a recent project from google called jaxx which is really just like numpy you know numpy matrix math plus auto differentiation and then compilation to code that can run on gpus or tpus it doesn't have all the deep learning layers and it's also not just for deep learning but it's just the way to like do really fast numerical computing on gpus and it's been picking up a lot of steam then there's kind of task specific frameworks like hugging face so hugging face is on top of both pytorch and tensorflow and they've defined they've written a lot of different model architectures mostly for nlp which is very valuable right so it's like yeah it's cool that we have a conf layer but can we have a whole bird model well with hugging face that's that's the level of abstraction that they're at and they also share pre-trained weights for all these models which is really useful so distributed training someone asked about and it's a big topic what that means is it's using multiple gpus and potentially multiple machines to train a single model so that's more complex than just using multiple gpus to train multiple models as in running different experiments but it's really a must do nowadays because the because the data sets are so big and the models are so large it really just takes forever to crunch through the data on a single gpu versus if you do data parallel training what you do is you split up essentially let's say you want a batch size of you know 100 and you have 10 gpus you can distribute that batch such that each gpus gets 10 examples they crunch through the computations and then they they do their backdrop updates and then they share what what their gradients were with some central place which is usually just the cpu or just one of the gpus and then the the gradients are averaged and then pushed back out to all of the gpus and this is almost a linear speed up so if you have two gpus you should expect like a 1.9 speed up if you have four gpus a 3.5 speed up this is borne out in benchmarks so data parallelism is what people usually mean when they talk about distributed training and it works well we'll see in the next couple of slides how to implement it model parallel training is a different beast and that's necessary when you can't even fit the weights of the model on a single gpu so data parallel assumes that all of the weights of the model are on each gpu and we're just splitting up the data to the gpus model parallel means that we're splitting up the weights over gpus okay and that means uh the data has to go through all the gpus this introduces a lot of complexity and if you can avoid it you should try to avoid it this is probably changing like the frameworks are making it easier and there's research coming out that that makes it that makes it better but for now it's better to just buy the largest gpu that you can or use the largest gpu in the cloud and then use tricks like gradient check pointing where you can you don't necessarily have to hold all the gradients in memory at once you can just write some out to disk and then kind of load them piece by piece so if you want to do data parallel training in in raw pi torch it's actually pretty easy so here we say your model equals model and then if torch.device count is greater than one then we're going to use several gpus and we say model equals nn.data parallel of the model so that does the secret sauce magic that's that now let's pytorch know that data to this model should flow to different gpus then we move we move the model to device except now it's actually multiple devices just you know auto magically and then when we train we will see that our batch size is 30 but inside of the gpu there's only 15 so there's two gpu so it splits up that batch of 30 onto two gpus so that each gpu gets 15. with pythor's lightning it's even easier you literally just say you know gpus equals whatever if you have a gpus equals eight and then you also say accelerator gdp distributed data parallel just so that it's fast and it's actually also possible to use slurm where you also say dash dash nodes equals four and then you define the task to say you know there's four nodes agps per node and so on then there's specialized i guess software for for distributing models over gpus horizon from uber yeah so instead of so here it says you know accelerated gdp so distributed data parallel that's a piece of pi torch code that implements you know syncing the syncing the weights sending the data to the gpus keeping track of all that so horebond has its own system that that you would use instead and they use the standard mpi framework instead of like some specialized code like pytorch distributed data parallel or in tensorflow like tensorflow distributed so i mentioned any scale so the prob the the project ray which is the original open source project the vision of that project was not deep learning specific but it was just to make stateful distributed computing really effortless in python so if you write your code and then you launch it with through ray then ray takes care of like basically syncing state across distributed system so it could be useful if if you're not specifically doing deep learning but you're just working in distributed computing let's talk about experiment management
Full Stack Deep Learning - Spring 2021,6,3168,Experiment Management,so just running one experiment at a time it's actually surprisingly easy to get lost into which code version and which hyper parameters and exactly what type of data or you know which version of the data generated the trained model that you produce and it gets a lot worse if you're running multiple experiments at once so a low co you know low tech way to deal with this is a spreadsheet and just here's what i did this is the data set you know this is the git commit this these are the hyper parameters these are the results that's what people used to do with tensorboard it's it's it's better it's automatically tracked by the way our labs with the pytorch lightning you automatically get tensorboard for all of your training runs and that's a fine solution for single experiments but as you run you know dozens and eventually hundreds of experiments it's just not a great interface for for looking at everything you've done and being able to make sense of it it's a it's a good way to look at a single experiment but not at many experiments so there's software that makes that part of it better so here's an open source project from databricks called mlflow and they do several things but kind of the central thing is this experiment management platform where you get just the log of everything that happened and then a table that you can kind of filter on and stuff specifically for you know to go further in that direction there's startups like comet or neptune which let you look at individual experiments but then also have a table of all the experiments you've ever done that are searchable have a chart where you can see the sensitivity of the different hyper parameters to the eventual accuracy that's that chart on top there even like see the diff of the code at the at the time that you ran this experiment you know to the most recent commit and and then store the results like the trained weights of the model weights and biases is another solution that that enables all of this and this is the one we're going to use in lab it's quite nice and yeah tracks gpu utilization all your runs on the table i mean you'll see it so i won't spend too much time on it and then another thing weights and biases lets you do is actually embed all of those charts and parameters and stuff into a nice report which you can write and then share with the world or with with your with your group or whoever else you want to share with so you can combine text and like machine learning artifacts in one document so
Full Stack Deep Learning - Spring 2021,6,3343,Hyperparameter Tuning,briefly that's about experiment management it's useful to have software that helps you decide what experiments to run and just to run them kind of for you and that could be as simple as just being able to say you know on the command line instead of saying you know learning rate equals 0.1 could we say learning rate equals between point zero zero zero one and point one uniform distribution and then half something just generate a lot of experiments and sample that space and it'd be even better if it wasn't actually random but if it was selected intelligently in some way and also like as you're running experiments some experiments are just clearly not going to be like they're not going to be as good as some experience you've already run you can see that from the beginning like the loss is always on top of the loss of another experiment it'd be cool if we could kill those runs early sig opt is one company that just that's their bread and butter it's an api where you can tell them you know what parameters you're about to run you know you tell them what parameters you want to kind of search over they'll tell you the values then you send them the output of the training and then they tell you the new values and they can tell that to like many experiments at once and and they suggest values in a way that's that's optimized so they use the bayesian optimization here if you don't want to use an api you can use your own you know local software so survey tune another project in the ray ecosystem can do this so they have state-of-the-art algorithms like population-based training hyper-band and basically you know they will give your training runs the resources that they believe are the most promising for the the hyper parameters that they believe are the most promising and they'll also kill training runs that are clearly not going to be the best weights and biases has this feature also and that's actually what we'll use in lab since we're already using it for experiment management and so the way that works is you define here on the right it's a yaml file that just decides that says okay use bayesian optimization i want to minimize validation character error rate i want to terminate early using the hyperbane protocol i wanna and then i wanna explore this parameter space of like different comp dimensions window width you know fully connected dimension whatever and then then the it's called the sweep in their language a sweep server is started on on their machine in the cloud and then on our local machine we start agents and the agents basically request a set of parameters from the server and then just execute everything belongs to wnb and then you get to record all of your experiments and see how they perform lastly all of these things right start even including data and deployment
Full Stack Deep Learning - Spring 2021,6,3540,All-In-One Solutions,and all the parts of training and evaluation we talked about there's solutions that kind of try to do all of them usually that takes the form of a notebook server for development being able to provision a lot of compute and launch a lot of experiments uh in the cloud then tracking the results and some kind of weights and biases type table sometimes even deploying the model as a rest api endpoint or getting some artifact that you can that you can deploy on edge and even maybe monitoring the performance of the deployed model the earliest that i can find of describing like this all-in-one machine learning system is probably fb learner from 2016. so it was an article that described facebook's ai backbone which starts with data there's a feature store that extracts features from the bay the data stores them somewhere then a training part which also uses gpus which produces a model which then gets deployed again on the cpu as uh for as an inference engine and then is displayed to the users in a way that as they interact with it that feeds back into the data store and the process repeats since then the cloud vendors like google microsoft amazon have come out with their all-in-one systems the google cloud ai platform has you know things for data data labeling bigquery for basically warehousing data they have ai platform notebook instances which give you like a really easy to use notebook training which you can use to spin up a lot of machines to do training for validation you know specific ways to look at the model that you've developed and try to get some visibility into it and then a way to deploy it amazon sagemaker is the that's the name that the amazon you know basically machine learning system has and once again it has stage maker ground truth for labeling data wrangler kind of like bigquery data processing then you have studio notebooks which gives you an easy notebook than than one-click training that gets you like provisioned hardware to do the experiments keeps the results of the experiments in sagemaker experiments deploys it and even has some model monitoring neptune is a startup that tries to do the same they kind of have that whole system also so does floyd jupiter notebooks deploy chain models you get gpus kind of at the click of a button gradient by paper space is another one so once again you got that one two three four develop one click jupyter notebooks train deploy model surveying and then scale determine ai is an open source solution everything up until now has been a paid system i think paper space gradient has a free tier but it's all public so if you want private experiments that you'll have to pay a little bit determine ai is open source it's actually also from people originally from uc berkeley and they don't do the data or the model deployment steps but within the model development and training they try to cover everything right so from resource management to to a really easy distributed training to tracking experiments to searching over hyper parameters in fact some of the founders of this company are are i guess most well known for the hyperband algorithm so this is definitely worth a look determined that ai among the startups i think the one that comes closest to covering everything is domino datalab you can provision compute you can track experiments you can deploy the trained model as a rest api you can monitor the predictions it makes you can publish like streamled like little applets for the you know leads they can monitor the spend of their data scientists they can see all the projects and kind of their statuses in one place it doesn't it's not quite targeted at deep learning it's uh more targeted at like just kind of more old school standard machine learning on structured data but i think this is the type of feature set that a lot of these vendors are approaching so i just wanted to show more of that and really it's a natural place to go for most machine learning operations companies so for example weights and biases they got started in experiment tracking giving you like the dashboard of experiment results they expanded to hyper parameter optimization via sweeps they also have data set versioning called artifacts right they have reports apparently production monitoring is coming soon so as we look at the you know ecosystem of the of all these vendors i think a lot of them have the inclination and tendency to like push out into the surrounding areas so breaking it down a little bit we can look at weights and biases they don't provision hardware for you so so that's not available they don't manage resources for you so they don't like allocate gpus to your experiments but they do help you have parameter optimization they will store the artifacts you produce they'll help you review experiments and they don't currently help you deploy or monitor paper space gradients uses paper space gpu hardware they have their own cloud they will help you do resource management they will not help you do hyper parameter optimization but they will help you do experiments and i guess they do help you deploy so i should fix that and then you can kind of look at the different vendors on this in this table the pricing of these vendors is something that often isn't totally clear and it kind of depends on your team size and exactly how much you'll be using it sage maker instances tend to be they definitely have a markup over the like if you just use basic ec2 instances that would probably be about 20 cheaper or 40 cheaper than using those same instances but provisioned via sagemaker so there's a premium for the ease of use that sagemaker charges you for domino data lab is pretty expensive determine ai has an open source solution so that might be worth starting in a research starting within a research environment and then some of these are free for personal use like weights and biases is free for personal use but your projects have to be public which is not usually a big deal if your data sets are public but if you're if you're training on some personal information in your data sets then then you need you know a private account so
Full Stack Deep Learning - Spring 2021,6,3984,Follow us on Twitter for #ToolingTuesday! (@full_stack_dl),someone asked like how do we stay up to date with this stuff like how do you find all this so one way that you can try to stay up to date is follow us on twitter and every tuesday which i guess is today we do tooling tuesdays where we talk about some some part of this ecosystem that is useful and we explain why it's useful and sometimes we explain like what other alternatives exist that you could be using instead of it so that's a good way to do it and then yeah i think twitter is a really good way to kind of build a community and follow some people that tweet a lot about these subjects we should probably at some point have a slide maybe towards the end of the class where we just have like a bunch of people that to follow you know suggestions to follow so that's it for the lecture i thank you just to make sure that there's no more questions
Full Stack Deep Learning - Spring 2021,7,0,Introduction,okay welcome back so this week we're going to talk about troubleshooting deep neural networks and just to place you for a second in where we are in kind of the life cycle of a machine learning project so where we are in the course so we talked a couple weeks ago about planning and project setup we we also talked about infrastructure and tooling last week we haven't talked about data collection and labeling yet but we will talk about that soon but you know this this part of the course becomes relevant like once you've collected your initial data set you've labeled it and now you're moving on to you know training models and debugging those models as you go and so you know why this is this is a class on like applied machine learning in some sense right like putting deep learning models into production so why have a whole lecture on training and debugging and troubleshooting i think this this xkcd comic kind of captures what it feels like a lot of times to to actually build a machine learning system right so you know you're you're pouring a big pile of data into a pile and you're stirring it around and some ants just come out the other side and you know if the answers are wrong then you know just keep stirring and maybe the answers will will turn out better and i think this kind of captures what it feels like a lot of times to iterate on machine learning systems i also like this tweet from andre which you know first it doesn't even compile and then it doesn't link then it says the seg faults and it gives the wrong answer and then only after all that then maybe maybe it actually works and so the the this i think captures a common sentiment that i that i hear among practitioners even really really top tier practitioners which is that you know it's it's when i first started in the field i felt like i was doing something wrong because i was spending eighty percent of my time or ninety percent of my time you know debugging things and tuning models and stuff like that but it turns out that that's you know even some of the top people in the field this is kind of how they spend their time as well and the goal of this lecture is not to try to eliminate the time that you're going to inevitably spend debugging and tuning your models but hopefully to give you some strategies so it feels a little bit less like you're you know like just spinning this this random pile of linear algebra and waiting for better answers to come out of the other side
Full Stack Deep Learning - Spring 2021,7,131,Why Is Deep Learning Troubleshooting Hard?,okay so the the first thing i want to talk about is i want to just give you some intuition about why this is such a hard problem right why are top practitioners spending so much of their time troubleshooting models so i'll use an example this is a chart from the original resnet paper and you know suppose that you're trying to reproduce this chart right this this this learning curve and let's say that your learning curve actually looks like this right so you know you're doing significantly worse than the model the paper is doing and you know and so the question is like what do you do after that right so you've implemented the model it's not doing as well as it does in the paper what's going on why is it not doing as well and so the insight here about why troubleshooting is so hard is that there's a bunch of different things actually that could be causing the same different performance so you could have an implementation bug and to make matters worse most deep learning bugs are actually completely invisible right so i once had a bug that looks something like this right so you know my model just wasn't training at all this is this is uh one of the snippets of code or a caricature of one of the snippets of code that i was using to train the model and there's a bug here any any ideas with that bug might be paste it in chat or chime in if you know okay so no no one else no no else knows this bug either yeah so okay so so nathan got it the order is random so it turns out that when you call glob in python it the the ordering that it returns files in is not deterministic and so i i didn't realize this i had this bug my model wasn't learning anything at all and it took me like half a day or a day or something just to even find that this is the place that i had but implementation bugs are not the only possible source of the same error so it could also be caused by hyperparameter choices and you know it it turns out in deep learning oftentimes our models are pretty sensitive to small differences and choices and hyper parameters so this is kind of a cartoon chart about what can happen to your model's performance if you choose different learning rates right so you know the difference between a a good learning rate and a low learning rate might not be that big but if you have a high learning rate or a very high learning rate your model might learn like might not get to as good an answer at all or might actually just not learn anything like it might actually the signal might go in the wrong direction and this is a chart from another i think this is a follow-on to the resnet paper talking about where they actually what like one of the tricks that made this paper work at all was the weight initialization and so the the the difference between what was the standard weight initialization at the time and the weight initialization that they used in this paper was the difference between you know the model not learning anything at all and the model being state of the art on the tasks that they were working on so models can be extremely sensitive to the small differences and choices of hyper parameters that you might make another cause of possible performance is what i would call like data model fit so maybe the data in the paper that you're trying to reproduce is imagenet let's say and yours could be something else like let's say it's self-driving car images and if you're if you're re-implementing the same model using a different data set than was used in the paper then you know you it's reasonable to believe that your model should be performed you know also perform well on your data set but there's no guarantees of that and so using data that's really different from the data in the paper could be the cause of degradation and performance and then lastly the the way the data set itself is constructed can also cause degradations and performance and so this is i think a pretty commonly shown chart at this point but this is from from a talk that andre carpathi gave not too long after he had taken over the reigns at a tesla and after he had left open ai and you know the the the insight here is that i guess the chart is a little a little fuzzy in in this picture but the inside is that you know when you're working on your phd when you're working on research then you know the vast majority of your time is spent thinking about models and algorithms not data sets but in industry in the real world it's actually flipped like most of your time is spent thinking about data sets and models and algorithms are actually a small portion of that i think actually even even this characterization that 25 of your time may be spent on models and algorithms is probably much lower in places other than other than tesla and there's a bunch of different things that could cause you know data set construction issues that affect model performance so maybe you don't have enough data maybe you have enough data but the classes are very imbalanced so you have some classes that are dramatically underrepresented maybe your labeling process itself could be noisy so you know your your labels could be giving you incorrect labels your train and test set could be from different distributions or any number of other different data set construction errors so to summarize why is it hard to troubleshoot the performance of your deep learning model well it's hard to tell that you actually have a bug to begin with and even if you have a bug even if you know that you have a bug so you're lucky enough to know that you have a bug there's a lot of different possible sources of the same degradation and performance and so it can be hard to to disambiguate which which cause is actually causing that degradation and performance and then lastly models can be very sensitive to small changes in your hyper parameters and how your data sets are constructed and so those those are sort of some of the main reasons why it's it's hard to troubleshoot models once you're building them and so the the bulk of what we're going to talk about today is a strategy that you can use to overcome this so strategy for for troubleshooting neural networks and yeah i think that like maybe if there's one takeaway that you have from this is that i think there's like one key mindset that you need to keep in mind as you're as you're building your models that is going to be one of the biggest sort of differences between being able to build models successfully or or you know kind of getting stuck in the endless cycle of lots of bugs and that's the mindset is pessimism okay so you know the key idea that i that i that i advocate for in troubleshooting models is you know since it can be really really hard to disambiguate what is the cause of this degradation and performance what is the cause of this error the strategy that we're going to take is we're going to start with something that's as simple as possible and then we're gradually going to ramp up the complexity so that at each step along the way we're only increasing the complexity by a little bit and so if if performance doesn't behave as well as we expect or the way that we expect then we can isolate that that small change that we made as the degradation and performance or the the unusual performance that we weren't expecting and we can uh start our debugging there okay so in a little bit more detail the strategy that we're going to talk through is you know starting starting very simple
Full Stack Deep Learning - Spring 2021,7,532,Decision Tree For Deep Learning Troubleshooting,so choosing a simple model simple data set and really paring down your problem to these to the essential components of it then you're going to implement that model that you chose and you're going to debug your implementation once you've have an implementation that you're you know reasonably certain is bug-free then you're going to evaluate your model and you'll use the results of that evaluation to make a decision about what to do next so it could be you know improving the model or improving the data or it could be tuning your hyper parameters and and we'll talk about how to make that decision and what the different ways that you can improve your models and tune your hyper parameters are and then you're going to iterate on this process kind of looping back to the implementation phase or the evaluation phase until you have a model that meets the requirements that you set out at the beginning of this exercise so really just quickly just to summarize all the stuff that we're going to cover starting simple you're going to choose the simplest model and simplest data possible then when you're implementing and debugging you're going to first get your model to run then you're going to overfit a single batch of data so just one batch a few data points and then you're going to try to reproduce the known results in evaluation we're going to the strategy that we're going to take here is we're going to apply the bias variance decomposition and we're going to use the results of that to decide what to do next for hyper parameter tuning the strategy that we'll recommend here is doing course defined random searches we'll talk about a couple of other strategies you can consider and then finally if you're improving your model and data the the kind of number one strategies that we'll recommend are to simply make your model bigger if you're under fitting and add data or add regularization if you're overfitting but we'll talk about other possible strategies as well okay a few things that we are going to assume that you already have in place before you start going about this process the first is you need to have an initial test set so an initial data set that you're going to use to perform your model evaluation we'll also assume that you have a single metric to improve on so we talked a couple weeks ago about how to choose you know what metric you're going to you are going to work on improving at any given time and then we're also going to assume that you have a baseline right so some target performance that's based on you know maybe human level performance or some paper that you're trying to reproduce or a previous baseline or maybe some requirements and again we talked a couple weeks ago about how to pick a good baseline and so for for the the sake of example today we're going to use this running example so we we're going to use a running example of pedestrian classification so we're like suppose that we're working on building a self-driving car and as one component of that we're training a model that takes as input an image a scene and and tries to classify whether or not there's a pedestrian in that image and let's say for the sake of example that our goal here is going to be 99 classification accuracy okay so some of you are going to go work on self-driving cars after you graduate and i i hope that you're aiming for higher accuracy than this but for the sake of our example let's say that we're targeting 99 so basically the rest of this lecture is
Full Stack Deep Learning - Spring 2021,7,709,Start Simple,we're going to walk through this strategy and we're going to talk through some some more details about how to implement each of these steps so starting with starting simple all right so the steps that we're going to talk about here are first we're going to choose a simple model architecture and then we're going to pick some sensible defaults for that mall architecture then we'll talk about normalizing inputs and then finally simplifying the problem itself so i think like architecture selection is something that causes people a lot of stress and worry when they especially when they start out in deep learning and i think there's good reason for that there's you know there's millions of papers out there and there's a lot of different choices of architectures for different problems and sub-problems and there's many trade-offs between them but the good thing is that when you're starting out on a new machine learning project i think you know if if you're if you're working on something practical then i think that there's some relatively like simple pragmatic choices you can make for version zero you know your first attempt at solving the problem and so you know the way i'll break it down is based on the type of data that you're working with so if you're working with images then i would recommend starting for you know the first implementation that you do with a lynette like architecture and then over time as you you know as you start to improve uh model performance and rule out bugs in different parts of your code base then you can consider moving on to something like like a resnet if you're if you're working on sequences then i think like kind of the the recommendation that that i typically make like at least as of a couple years ago was to start with the lscm with a single hidden layer i think maybe starting with the transformer now could be an equally valid if not more valid recommendation although i think maybe a transformer is a little bit harder to implement the first time and then as your project gets more mature you should consider moving to a transformer or a wavenet like model and then if your data is anything else then for version zero i would recommend starting with just a simple fully connected neural net with a single hidden layer and then as you get more sure then that's kind of when you need to pick something problem dependent depending on exactly what you're working on okay so you might say well this makes sense if i have just a single type of data as input but what if you know in the real world often times we have like multimodal data so how how should you think about dealing with multiple input modalities so say that you have a model that is taking you know two like one or more images as input as well as a sentence and it's trying to classify whether that sentence accurately describes the images so the the model architecture that i'm going to describe here as a baseline is first you map each of the individual inputs down to a lower dimensional feature space using the architectures that we described before so you might map each of the images through a small comp net and you might map the sequence through an lcm then you're going to take the output of each of those models and flatten it concatenate each of those feature vectors and then pass the output of that through one or more fully connected layers until you get to the output that you're trying to that you're trying to predict yeah concatenate good catch and so this you know like very simple choice here but can be a reasonably strong baseline for a lot of tasks where you're dealing with multi-modal input simple to implement and relatively simple to debug okay so we've chosen our simple model architecture and the next thing that we're going to do is we're going to pick some you know sensible defaults for hyper parameters and things like that before we start training so here are some defaults that i recommend using for version 0 of your model i think it's best to start with adam optimizer so some people recommend just starting with like vanilla sgd i think it's i think adam just like solves a lot of problems with you for you so might as well use atom the this is the learning rate i i like to use i think this is also popularized by andre i think it's in popular culture it's known as the the magic learning rate because it's more often than not gets you pretty close to the learning rate that you would have picked if you had done some hyper parameter optimization for activations i would start with value or 10h if you're using lcms these weight initialization schemes which i'll define on the next slide but i won't go into detail on and then importantly i would recommend in the first implementation that you do not starting with any regularization or data normalization and the reason for that is that regularization and data normalization things like batch norm and stuff like that are a very very common source of bugs and so i think you know until you know that you need those things i would leave them out of your implementation so you can so you can make sure that the rest of your implementation is bug-free before you add something like batch normal this slide just has the definitions of the recommended initializers i won't cover this but you can come back and look at it if you're interested okay so we've chosen simple architecture sensible defaults the next thing that is really important to do even in the first implementation is to normalize your inputs what does it mean to normalize your inputs so for most data this is just subtracting the mean and dividing by the variance you're probably doing this already for images it's fine to just scale the values to something between 0 and 1 or negative 0.5 and 0.5 one kind of gotcha here is that sometimes your library will do this for you and so i've had a lot of bugs where i like divide the the values of the input image by 255 but that was also happening somewhere else in my code all of a sudden my input values are tiny model learns really slowly so this is like one step that i i think that you should like absolutely never skip because it can make the difference between your model learning normally and that like not learning anything at all anything at all depending on the scale of your data all right so we've picked simple like simple defaults for our problem but i think one other thing that's really worth considering when you're starting out a new on a new problem is actually trying to simplify the problem itself and then gradually adding back the complexity that you need in order to solve the problem that you really want to solve so a couple of examples of ways that you could do that you can start with a much smaller training set for example so what this will allow you to do is allow you to iterate more quickly and then and it'll allow you to understand whether you know if your model's not performing well in this data set if that's because the dataset is really challenging or if it's because there's a bug right so if your model if you can't get your involved to perform well on a smaller data set then you're not going to be able to get it to perform well on the larger data set that you really care about at least in terms of training error you can also reduce the number of objects or classes that you're considering you can decrease the image size or the size of your inputs in some other way or you could even consider using a simpler data set altogether so you could create a synthetic training set and we'll talk a little bit in labs about how you can think about doing that okay so the way that we might approach this for our pedestrian detection problem is you know we're self-driving car companies so we actually have millions and millions of training images but since we're just starting to work on this pedestrian detection problem we'll start with a subset of 10 000 of those images for training and um it's a little cut off here but a thousand of them let's say for test we'll use a simple lynette architecture since we're doing classification we'll use atom optima optimizer with the magic learning rate and we'll leave all regularization out to start okay and so this is just a quick summary of some of the techniques that you might use to start with the simplest possible version of the problem that you can alright so you picked your simple architecture and defaults
Full Stack Deep Learning - Spring 2021,7,1142,Implement and Debug,and the next thing that you're going to do is you're going to go and implement that model and debug your implementation all right so the steps we're going to cover here are first you need to get your model to run it all which sounds easy but it's not always as easy as it sounds then you're going to over fit a single batch of data and once you can overfit a single batch of data then we're going to compare the results of your model to some known result all right so to just give you a quick preview of some of the stuff that we're going to cover here i think in my experience these are five of the most common bugs that tend to come up when you're implementing a deep learning model so one really common one is you have the wrong shapes for your tensors so you know you you're trying to add something that's like three by four together with something that's like three by five usually you're you're like usually this will fail loudly which makes it relatively easy to debug but it's also possible for it to fail silently particularly in tensorflow where you'll have kind of accidental broadcasting when you have shapes that are undefined and so this can be a silent source of bugs it can be very very painful to to find pre-processing inputs incorrectly so forgetting to normalize your inputs or over normalizing your inputs is pretty common source of bugs one other thing that's like really easy to do by accident is to have the wrong input to your loss function so a lot of times the loss function will have we'll expect the just the raw logits of like from your model but you'll take the softmax because that's you know logically what you want the input to be and then you'll get bugs because of that forgetting to set up train mode for the net correctly this again often comes up in batch norm and then various different sources of numerical instability which you know if you're implementing new types of models you'll inevitably spend a lot of time debugging before we go into specific strategies there's a few pieces of sort of general advice that i have when you're implementing a new machine learning model so the first is to start with as lightweight and implementation as you can so adding the minimum possible new lines of code for for version one and so intuitively like the way to think about this is that every line of code that you write is like could possibly have some bugs in it and so if you have fewer lines of code your debugging process is going to be easier because there's going to be fewer places to look for the error and as a rule of thumb i think trying to get your implementation to something less than 200 lines for the first version is is generally a good goal and oftentimes like i would really try to get things to be less than 100 lines but it's not always possible to do that and one other note here this doesn't count like tested infrastructure components so if you have like some infrastructure that you've been using for a lot of different models in your code base then those are fine like don't count those toward your 200 lines another piece of advice is generally try to use off-the-shelf components when you can so for example keras is great for implementing a first version of your model same with pi torch lightning which we're using in the labs using the definitions of layers that are built into your library instead of writing out the math yourself which you know can be tempting to do because you're trying to make a simple lightweight implementation but the the implementations of layers and things like that in these libraries have a lot of the kind of numerical gotchas already taken care of especially in the loss functions and then lastly and this is maybe actually the most important thing is you know a lot of times we're working with massive data sets and so in order to deal with those massive data sets it's important to build kind of like complicated data pipelines that sort of shuttle all the data from you know wherever it's stored to your gpu in an efficient way but i would highly highly recommend starting with a version of your data set that you can load into memory when you start working on a problem and then building those complicated data pipelines later because data pipelines are almost inevitable source of bugs and and so you want to make sure that your model implementation is working well before you add these in okay now let's talk through um the steps here so the first thing that we're gonna do is we're gonna get the model to run there's a few common issues that'll come up here one is you might have a shape mismatch or you might have a casting issue so you know your your model expects floats and you're giving it hints and recommended resolution here is just step through the model creation in you know in like your python model implementation in a debugger and just like inspect the shape and types of the outputs at each step until you get to the point where something is happening that's wrong out of memory issues are another really common source of bugs at this stage and so what i like to do here is just look for all the the operations that are going to be memory intensive so you know creating large tensors loading large blocks of data into memory and just scale those back one by one like try reducing the batch size try you know making those like the number of channels smaller or something like that until you get down to something that can actually run and then try to and then that will tell you where the original like memory intensive operation might have been and then many other things that can go wrong here and so i recommend kind of a standard software engineering debugging tool kit which is basically like googling stuff until you find a plausible source of the error okay so zooming in on this process of stepping through model creation and inference in a debugger i think you know for for pytorch it's relatively easy to do this in something like ipdb for tensorflow at least like older version non-imperative version of tensorflow can be trickier and so i think like maybe none of you are really using non-imperative tensorflow but if you ever end up using it one trick you can use is you can just step through the creation of the graph in the debugger another trick that you can do is you can actually step into the training loop itself and like evaluate tensors individually by just doing sesh.run in tensorflow and then lastly there's a tensorflow debugging tool that is worth taking a look at here if you ever end up in the unfortunate position of needing to do a lot of debugging of non-imperative tensorflow okay so to summarize you know first thing that we're trying to do is get our model to run so we're dealing with shape mismatches casting issues out of memory issues and then a whole litany of other things that can go wrong that we can try to start by googling all right so i cover a couple of other like kind of more detailed sources of bugs in these slides like diving into a little bit what can cause shape mismatch and casting issues and things like that i'm not going to go through those in detail right now but they're here for you if you want to take a look later all right so we've implemented our model and now it's actually running so we're done right like we're we we can write the paper you know graduate and it's all good no of course not like this when you've got the ball to run that's actually like where your debugging really starts and so the next thing that i recommend doing is overfitting a single batch of data and so what do i mean by this well i mean like literally a single batch so you know two or four or eight or 32 data points and by overfitting i mean driving the loss arbitrarily close to zero so you know if you get lost to go to some number that looks low to you but then it kind of plateaus it can be tempting to say okay yeah we're overfitting a single batch of data but that doesn't actually count so what you really need to be able to do is like pick some number and then make sure that the loss can get arbitrarily lower than that number so arbitrarily close to zero so what are the things that go wrong when you're over fitting a single batch of data well instead of driving the arrow down to zero in some cases the error will actually go up and so if if the if the error goes up the most common thing is that you've flipped a sign somewhere so maybe you've like you're optimizing the negative of your loss function instead of your loss function but there's other possible sources of that problem the error can also explode so it can you know go down um for a while and then all of a sudden shoot up and and then get arbitrarily high and so if the error goes down and then up usually in my experience this is a numerical issue and so you can check all of your like like operations that could be causing a numerical issue but it can also be caused by the learning rate the air can oscillate so it can go down and then come back up and then go down and then come up in my experience this is often caused by corrupted data or labels so you know your learning problem might actually be impossible because for example your labels might be shuffled because that's that's i think the most common cause of error oscillation but it can also be caused by a learning rate that's too high and then lastly your error can plateau so the error goes down down down until you get to 0.1 or 0.01 or something like that and then it doesn't really get much lower after that and and so i think the thing i would try here if you're getting a plateauing error is you can try to turn up the learning rate a little bit and getting rid of regularization if you didn't listen to my advice and you're using regularization in your first implementation and if that doesn't work then i would check you know i would like kind of check very carefully your loss function itself like how you're defining that how you're computing it and your data pipeline because there could be bugs there as well okay and this is just this all this in one slide all right so once we're able to over fit a single batch then the next thing that we should do before we start trying to make our model more complex or adding more data or anything like that is comparing the results that we're getting to some known result and this is the last kind of sense check that we'll use uh to convince ourselves that our model is reasonably bug-free before we move on so not all known results are created equal i'm going to go through some possible known results that you can compare to kind of in order of what i think their usefulness is so i think the most useful thing if you can find it is you know some official implementation of the model that you're implementing evaluated on a data set that's like yours and so the reason why this is so useful is because you can walk through both implementations line by line and ensure that you have the same output and you can also make sure that you're like the performance that you reach in the end is on par with what you should expect your model to do on that data set if you can't evaluate the the official model implementation on um your data set or a data set that's like yours then you can try to like try comparing to the implementation on a benchmark data set like mnist and so this still lets you walk through the code line by line but you know the results like the numbers that you're getting on mnist don't really tell you much about how well your model is actually doing because pretty much everything does well in mnist if there's no official implementation of the model you can instead use an unofficial implementation of the model with the caveat that like i would be very very careful if you go down this path because i found like that like the vast majority of like random modeling implementations that you find on github have pretty serious bugs in them in fact like even some of the official implementations of models have bugs in them so just generally i would be i wouldn't trust other people's deep learning code to too much unless it's kind of like a really well established code base if you if there's no implementations available maybe the paper just came out you can look or maybe you made it up you can look at the results from a paper without looking at the code which will tell you whether your performance is roughly in line with what you'd expect but won't help you figure out what's wrong if it's if it's wrong you can also look at the results of your model on some benchmark data set right so you can look at your model on how well your model performs on mnist and you know pretty safe to say that if it's not getting 99.95 percent or 99.5 let's say on endness then there's probably some bug because you know generally everything that's doing reasonably well should be above 99 on mnist if you don't even have that then you can look at results from a similar model on a similar data set or if nothing else you should be looking at super simple baselines like just the average of all of your outputs or you know the result of running a linear regression on your data set and this you know if you don't have access to anything else this is still useful to do because you will catch bugs this way there there there are examples like i've had examples where you know your model is actually doing worse than just randomly predicting or just like predicting the mean of your of all the labels in your data set and so at the very least this will tell you that your model is learning anything at all all right so to summer in in summary how should you think about implementing and debugging your model once you've chosen your model architecture that you're going to use so the first thing you need to do is get it to run it all and some techniques you can use here are stepping through things in a debugger and watching out for shape casting and out of memory errors once your model runs you need to overfit a single batch and so if you can't over fit a single batch that means you have a bug and likely causes of bugs are corrupted data or over regularization or maybe broadcasting errors silent broadcasting errors and then once your model can overfit a single batch then you want to compare to a known result and keep iterating and looking for bugs until your model performs up to expectations go ahead yeah i wanted to ask so like you talked a lot about like you know creating an ml product from scratch like i'm curious you have any advice for people that are like going in and starting with all and already somewhat like developing our developmental system yeah i think that's more or less what the rest of the lecture will be about like how to iterate once you have some starting point but i would say you know the same principles apply you should like try to gradually increase complexity so if you're you know if you have an idea for a new a new model architecture then you know maybe just change that and see how that how that changes performance rather than like changing the model architecture and also like changing the data set and changing the you know a bunch of other things as well or you can take that even one step further and like if you if you have an idea for a new model architecture then don't make all the changes you want to make all at once you know maybe if like one of the changes you want to make is like switch out you know your your non-linearity for some like fancy new non-linearity from some paper that you read and you also want to like add attention in a couple of places and you know add like one or two other things as well then like maybe just pick one of those to start and then try it and see if that improves model performance before you go and implement like all four of those things thanks so there's a question in the chat uh what is ipdb versus pgb i can answer it yeah can you answer it yeah pdb is the python debugger and then ipdb is the ipython python debugger so it's just a little nicer it has like synthetic highlighting i think i've never used regular pdb before yeah i don't know why you would maybe if you're like running on someone else's machine or something and you can't install stuff yeah yeah ipdb is amazing and how many epochs should you expect to run in order to overfit a single batch oh it could be a lot yeah i wouldn't necessarily like worry too much about the number of number of epochs because your epochs are gonna be really fast with a single batch like they should be very very fast if they're not very very fast then that could be some other source of bug but i mean really i would i would expect this to take like less than a minute probably a quick question and i guess it's kind of like i've had ignorance but i kind of want to push back on the premise that we're fitting a single batch is that useful like i mean i guess for some reasons a like if the batch is too big then you won't be able to over you won't be able to draw the error rate down to zero if your model is small enough and i guess b if there's some problem with the data or like some fundamental noise in the data that you actually can't get then i'm not sure you'll be able to draw it down to zero anyway i'm just not sure what yeah the way i think about it is let's say that you have 32 data points in your batch and you have 32 parameters in your model then you should be able to memorize the batch so if if you're using a really small model then just make your batch smaller and i would say the reason why this is useful is like more more an empirical observation than anything else which is just that like most people i know who who do this consistently frequently catch bugs doing it and most people i and then you know and from personal experience whenever i don't do it there's like a 20 chance that i'll end up regretting it later because i'll spend a day working on something and thinking i have some really complicated bug but then it turns out that i can't even overfit a single batch so it's it's almost it's almost free to do this like should take you like five minutes and if it works then you can just be more confident about the next step do you recommend like visualizing your data at least like like images in particular i guess like do you recommend doing some of that stuff before that was a method of debugging like doing visualization before and after just to see if like something at least make someone sense you mean if you're generating images or something like that generating images or maybe generating labels or bounding boxes you just want to see like what the actual output looks like yeah that's hard to do it yeah absolutely we'll talk more about like kind of a general method for that in a bit but yeah i think you know inspecting your data before you get started is a really good idea like just playing around with it trying to label it yourself and just generally getting a feel for what the distribution of data looks like is not something that you'll regret most likely all right
Full Stack Deep Learning - Spring 2021,7,2192,Evaluate,so moving on to model evaluation so the the tldr of evaluating your model is that we're going to apply the bias variance decomposition and we're going to use that to make decisions about how we prioritize next steps that we that we take in sort of model creation or model like generally the process of making your models better over time so to review the bias variance decomposition so let's say that we have some human level performance or some target performance in general and then we have a loss curve like our training error that's plotted in green here generally speaking your validation curve will be another learning curve that is hopefully above your training error your test error will be yet another one that's above your validation error and so the the bias variance decomposition breaks down your test error into different components so the first is some what you know the term of art is often irreducible error which is you know some error that you don't expect your model to do better than so if you have a really strong human baseline then you could say that that's your irreducible error and then you look at the difference between your irreducible error and your training error and that's a measure of avoidable bias right so how much your model is underfitting to that training set and you can look at the difference between your validation error and your training error and that's a measure of variance so how much you're overfitting to your training set and then finally you can look at the difference between your test error and your validation error you know when when you do infrequently evaluate your model on the test set and that difference is a measure of how much you're overfitting to the validation set so to summarize the bias variance decomposition says that your test error can be broken down into some irreducible error plus bias plus variance plus validation set overfitting so one assumption that's baked into this is that this all assumes that your training validation and test set all come from the same data distribution so what should you do if that's not the case which it's not the case in a lot of real world machine learning problems so for example like if you're working on self-driving cars then you might have a situation where most of your training data looks like the data on the left right so it's it's data from daytime driving scenes but then in the real world you also have to deal with data that looks like on the right so let's say data that's that's at night and the the strategy that i'll recommend here is to actually use two validation sets one that's sampled from your training distribution and then a second that's sampled from your test distribution and i think it's really important to do this even if you have very limited data from your test distribution because what this allows you to do is it allows you to add one more curve to this bias variance decomposition so between your test error and your training error instead of just having one validation error curve instead you're gonna have two validation error curves one that corresponds to the training validation set the one the validation set you sampled from your training distribution and then the other that corresponds to your test validation set and so the way that this affects your bias variance decomposition is that an additional in addition to underfitting overfitting and validation set overfitting you also have this other term which is the difference between your test valve error and your train valve error which is a proxy for how much distribution shift is hurting your model performance so let's let's look at an example of of this strategy so let's say that we you know train our first model our like simple linette model on this pedestrian classification task and these are the numbers that we get so our goal performance was one and our training error is like really bad it's 20 our validation error is even worse and our test error is not too much worse than our validation error so this this difference between our training performance and our goal performance this this massive 19 difference tells us that we're pretty seriously under fitting our training distribution our training set the difference between our validation error and our training error tells us that we're also overfitting at the same time which is very possible to do with neural nets and then the the difference between our test error and our validation error is relatively small so let's say for now that we're happy like we're happy with this difference like we're good here and so the question here is like we are both underfitting and we're overfitting so what do we actually do like how do we decide what to work on next and that'll be what we cover in the next section but for now just to summarize the the what i recommend doing here when you're when you're deciding what to work on next in your model is breaking down your performance into some irreducible error some bias some variance some distribution shift and some validation set overfitting and then you the next thing that we're going to do is we're going to use those numbers to make a decision about what to implement next in your model all right
Full Stack Deep Learning - Spring 2021,7,2479,Improve Model and Data,so next we're going to talk about how to actually make improvements to your model and your data to try to improve model performance and so the way that we're going to apply the bias variance decomposition to prioritize improvements is we're going to follow the the following set of steps so the first thing that we're going to do is we're going to start by addressing under fitting so for both under fitting and overfitting we'll start with under fitting then once we've dealt with under fitting then we're going to move on to overfitting and then distribution shift and then handling overfitting to our validation set by rebalancing data sets if we need to so to start let's talk about how to address underfitting so how how do we reduce bias and here are some strategies that you can use in order that i would recommend thinking about them so the first thing the simplest thing and in many cases the thing that's most likely to be successful is just make your model bigger right so add layers use more units per layer and just increase the capacity of your model you can also consider reducing regularization so if your model is over regularized then that might prevent you from being able to to to get a tight fit to your data set you can do error analysis which is a process that we'll come back to talk about in a few minutes you can choose a different model architecture so you can move from your current architecture to something that's closer to state of the art and re-implement that this can be a very effective strategy the reason why it's so low on this list is because it's it can be very time consuming and also can introduce new bugs so it can cause you to kind of take a few steps back in your process you can also tune hyper parameters which again can can give you big improve improvements in performance if you've tried these other things and then lastly you can try you can consider adding features and so the traditional ml world adding features is maybe the first thing that you try but in the deep learning paradigm generally like philosophically people don't really think about adding features you want to make you want to you want your model to learn the features but the secret and the trick is that in the real world sometimes adding features can really help all right so let's let's walk through an example here so again our training error is much worse than our goal performance so first thing that we might try would be adding more layers to our component maybe that reduces our training error to like let's say seven percent um still not good enough so we might switch from a bigger confidence to a resnet and that might reduce our training error down to three percent and then we might tune the learning rate and let's say that for the sake of discussion here that that gets us down to our goal performance of one percent training error and so now the the gap between our training error and our validation error has gotten even bigger so the next thing that we're going to need to do is address the fact that we're now overfitting so some strategies for addressing overfitting first thing that i would recommend do doing if you can do it or if it's relatively easy to do it is adding more training data so i think this is the most principled way to reduce overfitting in the real world you don't always have access to more training data so there's a lot of other strategies that you can try but you know if for example you're only working with a subset of your data to begin with then before doing anything else that would just make your data set a little bit bigger you can also try adding normalization like batch normal layer norm or data augmentation or regularization i think normalization is usually the first thing that i try it's kind of like more of a modern approach than regularization and normalization often also has the effects of reducing underfitting as well so it can be helpful for both those things you can do the same error analysis process that we'll talk about in a few minutes choosing a better model architecture can also help you reduce overfitting in some cases same with tuning hyper parameters using early stopping which some people really recommend using i i've generally not found it to be all that helpful part part of my toolkit removing features or lastly reducing model size and so i would say i would put these last three things in the like not recommended category and i think like early stopping might be controversial in that category i think the fast ai folks for example recommend using early stopping as part of your regular toolkit but i personally haven't found it to be that useful okay so let's walk through our example again our training error is good our validation error is not good so let's try to reduce overfitting so the next thing we might do is remember we were using a subset of our data to make the problem simpler to start so let's actually increase the amount of data that we're using let's say we'll increase it to 250 000 data points then we might add a regularizer like weight decay we might add some data augmentation and then you know now that we're now that our validation error and our trading error are relatively close to each other let's but you know no longer close to our goal performance then we might go back and tune our hyper parameters again in this case in this this time we might tune more than just our learning rate and this you know could be the thing that gets us down below our goal performance okay so we have you know our validation error is is down at the goal performance that we originally wanted the next thing that we'll need to do is address our any additional error that's caused by distribution shift so if there's a difference between the error on our training validation set and our test validation set then this is the step at which we address that there's fewer things to try here which is both good and bad good because there's you know simpler to pick the one that you want to use bad because it's just a harder problem to deal with the first thing that i would do here is i would i would look at the errors in your test validation set and i would look at those errors manually almost one by one and then i would try to reason about what are the different causes of the errors that are in our test validation set but not our trained validation set now go and actually try to collect more data to try to compensate for that and i'll talk i'll give a more detailed example of this in a second but when you can do this i think this is the most principled way to address distribution shift again not always possible to go out and collect more data so another thing that you can do is instead of collecting more data to deal with errors in your test validation set and send you instead you can just synthesize data to deal with those errors and that can often get you pretty far and then lastly you can try using domain adaptation as as a way of dealing with the difference in the domains so we talked about this idea of error analysis a few times let me give you a more concrete example so suppose that these are some of the errors that we're seeing in our test validation set and in our train validation set and these are errors where you know there is actually a pedestrian in the image but we're not detecting it so let's try to reason about what are possible causes of errors in these data sets so there's one category here that appears in both data sets which is like i would say just really hard to see pedestrians so i i mean i can barely see pedestrians in those images myself at all there's a second category of errors here which seems to be related to there being reflections in the images and that occurs in both data sets so that's maybe something else we could deal with and then there's a third category of errors that really only seems to appear in our test validation set which is images that occur at night right so darker images and so this is maybe like one possible cause of of error due to distribution shift and so one way that you can like be a little bit more systematic about how you do this error analysis process is you can break down the you know you can look at a bunch of data manually and you can break down the types of errors that you see and you can look at how much they contribute to the overall error that you're seeing in your model so in in our case hard to see pedestrians like let's say that those are that's pretty rare both in our our trading validation set and our test validation set and so it's not a huge contributor to our loss or to our error and the potential solution here would really be like how do we deal with these hard to see pedestrians i mean really the main way i can think of is maybe we'd have to get better sensors so this is not a huge contributor to error and it would also be really hard to fix so if i did this error analysis process i would probably make this relatively low priority at least to start out with the second error type that we had identified was reflections and so let's say that this is you know a larger contributor to our training validation error and our test validation error and there's more possible solutions here so we could try to actually go out and collect more data that has reflections in it and train on that data we could add synthetic reflections to our training sets so we could use a graphics engine of some kind to try to add reflections to training data that doesn't have it and train them all to perform well on that data we could try to remove the reflections using some computer vision pre-processing techniques or we could use better sensors so if i look at this category it's relatively large contributor to error both on the train validation and the test validation set and we have some ideas about how we could deal with it without this really expensive process of adding better sensors so i might give this like a medium priority and then lastly we looked at this error type of night time scenes so very small contributor to error in our training validation set but let's say that we found a lot more errors that were caused by this in our test validation set so this is maybe one of our sources of the gap between our training validation error and our test validation error and some ideas that we could try about how to how to deal with this well we could actually go send our cars out at night and collect more data then maybe that's actually really easy for us to do maybe it's maybe it's not maybe that's harder for whatever reason we could try and if it is we could try to create synthetic data to deal with this so we could synthetically darken our training images to make the daytime images look more look more like nighttime images we could try simulating nighttime data from scratch or this would be a reasonable place to try using domain adaptation techniques to deal with that gap and so if i was running this air analysis process i would look at this and say this is a big contributor of error to our test validation score and there's a bunch of things that we could try some of them might be really easy to deal with it so i'll give this a high priority and this is probably the thing that i would work on next okay so let's let's drill in a little bit on this last thing domain adaptation so what is domain adaptation so made adaptation is a class of techniques that there's many there's many techniques that fall in this category but they generally try to train on some source distribution and then generalize to a target distribution that you want to perform well on using only unlabeled data from the target distribution or maybe some limited amount of labeled data from the target distribution and so when should you actually consider using this if access to label data from the test distribution so the distribution that you want to perform well on ultimately is limited but access to relatively similar data is plentiful and maybe access to unlabeled data from the test distribution is also relatively accessible then you can consider using domain adaptation techniques there's a few different types of domain adaptation that you'll see so there's supervised domain adaptation where you have let's say some limited amount of data from your target domain and so some examples here are as simple as just like fine-tuning a pre-trained model or adding the data from your target distribution to your training set but there's more complicated techniques here as well and then there's unsupervised domain adaptation where you have like let's say a lot of unlabeled data from your target distribution but no labels or a very very limited number of labels so here are some techniques that you might see if you if you look into this and practically speaking i would say that these supervised demand adaptation techniques you know fine tuning being the most common actually work really well and so this is the thing that's definitely worth considering if you have plentiful data in one domain and then limited labels in another domain the unsupervised main adaptation techniques i think are kind of more in the research realm than they are actually practical for real world machine learning systems right now but it's a pretty active area of research and so this might no longer be true in a year or two okay so we've addressed our distribution shifts and the last thing that we need to do is consider rebalancing our data sets so if our test validation score is significantly better than our than our performance on our actual holdout test distribution then that means that we at some point we ran too many experiments and we overfit to our validation set and so this happens a lot if you're using a really small validation set or if you're doing a ton of hyperparameter tuning and the only thing really to know here is that you know if this does happen when you period periodically check your scores on your whole held out test set then just recollect your validation data or resample it from your from your test distribution okay so we've talked about techniques for improving model performance all right next thing we're going to talk about is
Full Stack Deep Learning - Spring 2021,7,3252,Tune Hyperparameters,the very fun topic of tuning hyperparameters so so let's talk about like what hyper parameters we actually have available to us in this problem this you know this very very simple toy example that we're using throughout this lecture so our our model architecture at this point is a resnet so there's a few different things that we could tune here we could choose the number of layers we could pick a different weight initialization scheme we could change the kernel size or other kind of model specific parameters we're using atom optimizer we could change the batch size we could change the learning rate we could change the atom specific hyper parameters regularization you know there's many different regularizers we can choose and so i think like kind of one of the core challenges in doing hybrid parameter optimization is just there's so many different things that you could tune how do you even pick which hyper parameters you should be optimizing so a few things to note here one is that like for better and for worse models tend to be more sensitive to some hyper parameters than they are to others so there's more bang for your buck and generally speaking in tuning some of your hyper parameters than than other hyper parameters unfortunately which hyper parameters your model is most sensitive to depends on a lot of things including the choice of model the choice of data set and the other hyper parameters that you're tuning so that's the bad news the good news is that there are certain rules of thumb that like if you're working on a new problem that you can use to think about like okay which hyper parameters should i try to tune first and so these are these are really like only rules of thumb they you know in general like you there's no substitute for building intuition for your problem about which hyper parameters your your your model seems to be sensitive too and then the last thing i'll note here is that when i when i i'm gonna have a prioritized list of hyper parameters on the right and the sensitivity that we talk about here is relative to default values so if you're if you're using sensible defaults already then you know that gets you a large part of the way there for some hyper parameters but if for example you're using like all zero weight initialization right or vanilla sgd like something crazy then changing to the defaults like choosing to this changing to the standard defaults will almost always make a big difference okay so let's talk through some hyper parameters so i think almost always the first thing that's worth tuning is the learning rate generally usually pretty sensitive to learning rate even if you're using something like adam learning rate schedule can also be a good one to tune i think optimizer choice beyond atom can make a difference but in my experience it's usually not really worth playing around with too much until you're trying to push the limits of your model performance you can try tuning other optimizer parameters like beta 1 for adam i think unless unless they're like someone else tuned them in a paper and they told you that it depends on this particular choice of beta 1 then you usually wouldn't really mess around with this too much i haven't found it to be to make that big a difference although some people say that it makes a big difference for some problems batch size i would say generally is not really like a great thing to tune i would just generally pick the biggest batch size that you can while still fitting on your gpu weight initialization can be a way to improve model performance not always and not really the first thing that i would pick but if you're having trouble getting into better performance then playing around with weight initialization is worth doing picking a different loss function can have a big impact on model performance tuning the depth of the model can sometimes have an impact layer size is is you know increasing the width of a layer can be a good one playing around with the kernel size and other parameters of layers sometimes weight regularization sometimes and then i would say choice of non-linearity so like moving from values to some other type of lose lose cell use or anything like that occasionally but usually not okay so this is kind of some rules of thumb that you can use to pick which hyper parameters you want to tune next thing that we'll talk about is techniques you can use for actually tuning them so the first thing that you'll probably end up trying is just manual hyper parameter optimization so this is also maybe better known by its its colloquial term which is graduate student descent so how does manual hyper parameter optimization work you know well the first thing you do is try to like really think you know sit and you stare at the paper and you think really hard about the algorithm that you're implementing so okay so what does it mean if we have a higher learning rate you know generally speaking that'll mean that like our training will be faster but it'll be less stable and for every other hyperparameter that's part of your algorithm you'll think really hard about what effect that should have and then you'll train and evaluate your model and then you'll try to guess right you'll use all this intuition that you built from studying math for many many years and attending great schools and all this stuff and you'll guess better hyper parameter values and retrain and re-evaluate so i'm kind of poo pooing this a little bit but this actually could be a really good technique and in particular it can be a really good technique because it can be easily combined with some of the other methods that we'll talk about so for example you'll often find yourself manually selecting ranges of hyperparameters that you want to run some other optimization algorithm over and really one of the big differences here is that if your goal is like compute efficiency so get to the best possible performance with the fewest number of train runs which you know in in 2021 is generally not really the right goal because computers becoming cheaper and cheaper but in some cases it is like if you're training a massive model for example then this technique will usually have like the least compute required to get to a good result obviously the big disadvantages are that it requires you to actually understand the algorithm pretty deeply to do it well and it's very time consuming so this can be a reasonable starting point but it's generally not recommended the second thing that you'll probably try is doing a grid search and so the way that this works is you can imagine each of the hype parameters that you're tuning plotted against each other on a grid and then you um uniformly sample points on that grid that you know for for each of the the hyper parameters you're optimizing and then you'll run your training for each point on that grid big advantage here is this is just like really simple to implement and it can work reasonably well and the results that you get tend to be pretty interpretable so you know you're picking learning rates and other hyper parameters that are nice round even numbers that you would be happy to report in a paper so that's that can also be like a psychological advantage disadvantage is that this is not very efficient right so in order to make this work you need to train on all of the different cross combinations of the hyper parameters that you're tuning so if you're tuning more than one or two hyper parameters then this tends to be very inefficient and then this often requires prior knowledge about parameters to get good results right so you need to choose the size of your grid like the min and max values for each of the hyper parameters that you want to search over and if you don't do a good job of picking those values then you might not get good results general thing that's slightly better to do than a grid search is to do a random search and so the way that this works is you still choose a range for each of the hyper parameters that you want to optimize but instead of sampling like all of the grid points on that range instead you'll choose like n points that are sampled randomly from that grid and so you might think like okay why like why we do this like why would we expect this to be better than grid and the reason is that empirically this tends to produce better results than grid search for the same number of runs and in fact like sometimes the it's significantly better results than grid search and it's still pretty easy to implement so disadvantage is that it's less interpretable right so you'll get like these like weird looking numbers as the result of your optimization and you know that shouldn't really matter for anything but sometimes it's like nice to be able to say like oh yeah my learning rate is you know 10 to the negative three and that's easy to communicate and easy to remember and then again you know you still need to select these ranges so you still need some prior knowledge about what values of parameters will get you good results one way to get around this this last disadvantage is by doing instead of just a single random search instead doing a course defined random search so the way that this works is that you start with like a really wide range of of parameters maybe they're even log scaled and then you run a bunch of training runs and then you narrow in on the region of best performance so you select like say the the 10 best or the five best performing runs and then you create a new grid around those data points and from inside of them and then you can keep doing this indefinitely and so the advantage of this is that you can get really good performance doing this and you can narrow narrow in on like very high performing hyper parameters and in practice i think that this is a really commonly used method maybe these days like automatic like bayesian hyper parameter optimization techniques are like potentially catching up to this but this is an easy one to implement and do in practice disadvantage is that it can be somewhat manual so oftentimes there's a little bit of judgment that goes into deciding like what's the exact range of of the parameters that i want to sample when i reduce the size of my grid but other than that this is like generally what i would really recommend doing in practice the last thing to be aware of is bayesian hyper parameter optimization methods and so there's a lot of kind of like conceptual surface area here so we won't really be able to do this justice but at a very high level the way that this works is you start with some prior estimate of the parameter distributions and you maintain a probabilistic model of the relationship between the hyper parameter values and model performance and then you alternate between training models with hyper parameter values that are aimed at maximizing some expected improvements given this this this probabilistic model that you're maintaining and then using the results of your training run to update our probabilistic model so at a very high level that's the way that a lot of these algorithms work and yeah here's here's a blog post that i recommend if you want to actually go into more detail about some bayesian hyper primary optimization methods so big advantage of doing this is that generally this is like the most efficient and hands-off way of choosing hyper parameters you can get great results doing this with very little manual effort disadvantages are that these techniques can be sort of notoriously difficult to implement yourself from scratch and it can there are off-the-shelf tools that help you with this but they can be difficult to integrate with so generally speaking if you're working on a new project and you're implementing things yourself i wouldn't recommend starting here but as your project gets more mature this can be a really good thing to consider and i think also like as libraries and like training infrastructure that you use off the shelf gets more mature a lot of these things are starting to have these techniques built in which reduces some of the the disadvantages that we talked about here okay so to summarize like we talked about a lot of different ways of optimizing hyper parameters recommendation here is if you're just working on a project by yourself or like starting a new project from scratch i recommend using course to find random searches great bang for buck you can get really good performance you can inject as much grad student descent into this process as you'd like to and then as your project and your code base get more mature i would consider using some of these bayesian hyper parameter optimization solutions as you kind of narrow in on what exactly your models are going to look like and you want to spend more compute to optimize them okay so wrapping up what did we cover today so we talked about troubleshooting neural nets and debugging deep learning models and the first thing that we observed is that one of the reasons that this is so hard is that for any given degradation and error that you might experience there's many different competing sources like possible causes that could create that that error or that degradation and performance and so that's the fundamental reason why debugging and troubleshooting deep learning models is so difficult so we talked about a technique that we you can use to try to as best you can train models that are as bug-free as possible and the technique that we that we followed is essentially to treat model building as an iterative process where you start with the simplest possible version of your problem and your model in your data set and then you gradually layer on complexity one step at a time informed by your model evaluation process and your judgment about what needs to be done next so that at each step in that process you can reevaluate and re-debug and you can isolate sources of error to the extent that's possible and these are the techniques that we talked about in a little bit more detail for how to go through that process okay last thing i'll leave you with is there's a few resources i would recommend checking out if you want to go deeper on this andrew ing's book uh good book bad name there's a twitter thread and then actually a follow-up blog post by andre carpathi that covers some of his ideas on this and then i also like this this other blog post that's listed here so check these out if you want to learn more and that is it thank you
Full Stack Deep Learning - Spring 2021,8,0,Introduction,so today we're going to talk about data management and here's some motivational tweets one of the biggest failures they see in junior machine learning engineers complete lack of interest in building data sets there's much you know so much to be learned in putting together a data set it's like half the problem here's a poll that someone put together of data scientists uh a couple years ago so like most people's most of their time is spent on cleaning data and moving data as a data scientist and then another kind of twitter data science person matt kelsey said that for the last few projects that they've been involved in most the complexity was in the data flow and not actually the gpu training and you know when we think about what
Full Stack Deep Learning - Spring 2021,8,49,The Common Data Management Path for Deep Learning,data management for deep learning specifically actually entails there might be a lot of different sources of data right you might have images on s3 maybe a bunch of text files just somewhere on the file system log files maybe even spread across machines records in a database but at some point you got to get all that stuff over to a local file system that's next to a gpu right or many gpus and that's specifically because we're doing deep deep learning that's you know one of the constraints we have now the way you're going to get data over to that kind of trainable format is different for you know every single project every single company it's going to be a unique path so for example maybe you're training on imagenet right and all the images are just s3 urls and all you have to do is just download them over to the local file system or maybe you have a bunch of text files that you crawled yourself from some from somewhere and then you want to use you know spark to process them on some cluster that you have and then have a data frame that you can then look at and analyze for some you know for some tasks and then select the subset then get that subset over to a gpu or maybe you're collecting logs and records from your database into a snowflake a data lake or warehouse and then from that you'll process it and get it over into trainable format so there's a lot of you know a lot of different possibilities that we're not going to completely cover in this lecture and in fact we're going to talk a lot about you know the minutia of data management but before we get into that i wanted to cover some key points and the number one key point is we don't want to spend a lot of time with our data set but we should spend you know 10 10 times as much data as much time as we want to on actually just becoming one with the data you know let the data flow through you the second key point is that data is often the best way to improve your overall machine learning project performance so instead of trying a new algorithm or instead of trying a new architecture or instead of kicking off kicking off a hyper parameter search adding more data is is usually the best way to improve performance and in the absence of new data at least coming up with ways to augment your existing data that's often the best kind of bang for your buck [Music] and lastly we're going to talk a lot about complex pipelines and stuff like that and all these terms and it's good to know them but it's also good to keep it simple you know and not over complicate things it's actually not that difficult like at the end of the day we're just trying to get data into a form that we can load onto a gpu it's not rocket science it ends up taking a lot of our time but we should try to keep it as simple as possible still [Music] so from the infrastructure lecture a couple of weeks ago we we focused on the training and evaluation part of the landscape and today we're going to focus on the data part so that includes sources data lake warehouses processing exploration labeling and versioning we'll start with the sources
Full Stack Deep Learning - Spring 2021,8,262,Data Sources,for most deep learning applications we're going to need a lot of proprietary data and there's some exceptions to this for example reinforcement learning gans maybe and then projects where the barrier to entry is not necessarily a proprietary set of data but a proprietary method of computing right so it's either you need you know thousands of gpus and millions of dollars and maybe some know-how and how to how to run it which is what i think the case with gpt3 is right like an open source effort to replicate gpt3 hasn't yet finished and it's been going on for a few months and it'll probably take a few months longer but for most other things we'll need a proprietary data set now publicly available data sets are useful but there's no competitive advantage to them right anyone can get the same public data set and and get to the same level of performance that your model was able to get to but it's useful to you they're still useful to us because they serve as a as a starting point for whatever we're doing [Music] so usually you know company or project will end up spending money and and labeling time on labeling their own data so an example would be satellite imagery right if you're trying to do something for a financial application real estate application agricultural application you're probably going to need different labels than than other people there's i don't even know if there's a public data set there probably is but it's small so most of the time you'll have to purchase the data and then purchase people's time and effort in labeling it [Music] data flywheel is an interesting concept where if you can get your model out there in front of the users but then develop your product in such a way that your users actually contribute good data back to you or clean your data even and and and and fix up your model predictions then that usually is called a data flywheel and it can enable really rapid improvement after you get that v1 model out there semi-supervised learning is a very important idea that is in vogue right now and and the idea is that you don't necessarily need to spend money on labeling your data you can just formulate the problem in such a way that the task is slightly different it might be using a part of the data to predict a different part of the data so for example for text you can predict future words from the past words so like you complete the sentence basically or you can predict the beginning of the sentence from the end of the sentence or you can predict the middle word of a sentence from the word surrounding it right or you can ask do these two sentences occur in the same paragraph in any corpus in my training data and so there's a bunch of ways to formulate the problem that you don't actually need to label anything you just use the data to supervise itself and this can also apply to vision and in fact just a couple of days ago facebook ai research released this this model called seer which was trained on a billion random images so not imagenet but just random images from the internet image that if you remember is a million labeled images this is a billion random unlabeled images and yet training on this data set they were able to achieve state-of-the-art accuracy on imagenet top one prediction task and they released the library that they used for the you know formulating the task and it includes loss functions like contrastive divergence loss so it's worth taking a look at called uh vissel augmenting your training data is something that is really just table stakes you must you must do it at least revision models so on the right here we have an example of you know one way you can or some ways you can augment augment data so the top row are images that are original and then the bottom rows are different augmentations of the images so you can mess with the contrast kind of crop different parts of it invert it like take patches out of the image blank out patches of the image pixelate it rotate it share it you know we did a lot of this in lab also last time and every framework like tensorflow pytorch they provide functions that help you do this and it's done at the same time that the so you you on the cpu you run tasks that are augmenting your data set and then you feed the gpu with the augmented data and you do these two things in parallel so so basically as the gpu trains the cpu is generating training data for tabular data you could simulate missing data by just kind of blanking out some of the cells for text data i don't think they're good techniques that are you know well established but you could replace words of synonyms you could change the order of some things for speech and video like temporal data you could crop out portions of it you could change the speed and kind of you know shrink and grow the the timeline you can inject different types of noise you can mask at different frequencies for video you can do all the same stuff that we do for images [Music] synthetic data is an interesting idea that is often worth starting with but not often actually used in my experience so we linked to a blog post here from dropbox who created an ocr optical character recognition pipeline using a lot of synthetically generated images of words and then they use that for their kind of processing documents that people store in dropbox [Music] and the synthetic data can actually get pretty deep so this is a project i found from andrew moffett on training ocr on receipt images and the receipts are actually rendered in some 3d engine like unreal engine or something and so it's actually a full you know 3d deformation of the surface because people's receipts are often crinkly you can simulate lighting conditions like crazy shadows so you know all the images on the right are are synthetically generated and yet you still have perfect ground truth data right because you know exactly how the deformation was done and so you know exactly what the label still is for every single pixel in the image [Music] and for driving and robotics this is i think more or less table stakes and uh is often called sim to real and it's actually something that josh tobin worked on if you want to chat with them about it so next up let's talk about
Full Stack Deep Learning - Spring 2021,8,683,Data Storage,storage we'll talk about the building blocks file system object storage databases data lake slash data warehouses what should go where and then how we can learn more so the file system is really the foundational layer of of storing data and the unit that we like to think of is a file right which can be a text file or a binary file is not versioned there's no version that's done through the file system can be easily overridden or deleted and this can really be just you plugging in a hard drive and you know formatting and putting the files you need on it could be networked like the nfs system so the the same hard drive can be accessed from different machines can be distributed like the hadoop file system stored and you know accessed from multiple machines and actually stored over multiple machines also and the file system in general right is the fastest option we have when it comes to when it comes to storage [Music] and in fact it's really really fast now so on the here's two plots there's three rows the sata hard drive sata ssd solid state drive and then the nf nvme ssd which is the newest technology and on the left we see throughput so that's like if you were to copy a file at what speed would actually be copied and the hard drive is the slowest right that's the actual spinning platter like magnetic hard drives with a read head and stuff like that so there's they're limited by the rate at which the magnetic disc can spin and it's pretty slow right it's like 200 gigabytes per second or something like that or it's megabytes per second and then the ssd drives are faster much much better technology but it's pushing maybe 500 mega megabytes per second and then the latest iteration of hard drive technology the nvme is much much faster right so it's pushing three gigabytes per second and same with seek time so that's how long would it take you to find to to to go to a file on disk that you're not currently at and this makes sense when you think of like a head reading a magnetic disk right it actually has to seep to the location on disk that contains the start of that file it's very slow right it's like two milliseconds to seek to some place on the disk much slow much faster on ssds and much much faster on the nvme ssds what format should we store data in so for binary data like images audio videos stuff like that just files is usually what you would do so image files in a folder in many folders in tensorflow you have this tf record format which exists to in order to batch binary files so instead of let's say a million individual image files you would have a thousand or maybe a hundred batch files and then each batch file contains you know nicely formatted individual files that makes sense if you're seeking with you know the the old school hard drives it doesn't seem that necessary with with the latest really fast nvme drives for large as in like big data you know tabular data or text data there's some choices so you could have them as also just files and that's perfectly fine just a bunch of text files but if you have loaded files from different sources and you've loaded them into a data frame and you want to store that so that next time you start in the project you can kind of start from not from scratch but from some interim format you have some choices you have hdf5 which is powerful but it's very bloated in terms of its feature set it's like hard it basically re-implements a file system in a lot of ways and it doesn't seem to be an active development anymore meanwhile uh parquet is a widespread format it comes from the hadoop and kind of spark you know big data world and it's what i would recommend right now for storing this kind of data and then feather is the most recent good option it's powered by an open source project called apache arrow and it's kind of up and coming it's getting better you know every year very active development and apache aero aims to be basically like the interchange format for data analytics so it's a columnar data store that's memory mapped for really fast operations on it even for data that doesn't quite fit in memory and then tensorflow pytorch they all provide their own data set interfaces like data loader for pytorch and um tf.data for tensorflow so you you know try to use them as much as you can because that's kind of like the officially sanctioned way to load data and i think the further you stray from that the likelier it is that you'll encounter some cases where it's like all of a sudden really slow next up from the file system is we have object storage and so you can think of this as an api over the file system so so amazon s3 is the canonical example right we can get files we can put files we can delete files and we can think of these operations as basically rest api calls and we don't actually worry about what physical disk the files are on we just know that you know the file's on s3 i'll be able to get it i'll be able to store it and the object can be a text file it can be a binary file and oftentimes it is a binary file and the interesting thing about this is that you can build in versioning and redundancy into the api right so as you store a file the backend can just increment some version number so instead of overwriting your old file it can just store a v2 of that file and it's definitely not as fast as just reading files from local from some local drive but it's fast enough especially within the cloud right so if you're on the cloud compute instance and you're accessing files on s3 that's tends to be pretty fast and in fact this is a slide from i believe databricks recommending that people store your log files or whatever files they're analyzing in in the databricks environment on s3 versus configuring their own hdfs file system to do it and the reasons they give are s3s elastic right you can store like any number of files on it it's pretty cheap it's very highly available it's like incredibly durable right it doesn't lose data it can even have transactional rights and so it's quite nice the database is a word we use to refer to persistent fast and scalable storage and retrieval of structured data that will be accessed repeatedly so what that means is so for example logs are structured data because it's like time stamp you know page uh that was visited the rest verb like get or put or whatever it's so structured data but it's not expected to be accessed repeatedly it's it's a log right you store it just in case you need it but most of the time you don't need it whereas in the database you wouldn't store logs you would store things like the username of of your user or the label of this image right stuff that you expect that might change and you might need to look at repeatedly so oltp is a term you might have heard online transaction processing so this refers to the kind of database we're talking about the mental model to have is that the reason it's really fast is because everything is actually just held in memory so there's a running machine with all the data in its memory and it persists everything to disk and it has guarantees that it will persist to disk and it won't get lost and there's consistency guarantees like it won't let two conflicting rights through and so on but the reason is really fast is because it's actually not on like it's not seeking from disk most the time it's just in ram and this is not for binary data right you should store references to binary data in the database so you store the url of something in s3 and then you store the actual binary image in s3 postgres is the database that we recommend most the time the reason for that is it's a great sql database and it also supports unstructured json and sqlite is perfectly good for small projects and it's in fact used by a lot of things you might have used like mobile apps and stuff like that have a cq light back-end running you might have heard nosql and that was kind of a big thing in 2010s stuff like and there's a bunch of them and this mostly refers to storing unstructured json right so instead of having a schema for your data a schema being something like id is going to be an integer field name is going to be a string field you know created that is going to be a date stamp and so on the nosql approach is to not have a schema but just whatever you want to store we'll just make a json document with that data and then just store it into the no sql database and then we'll be able to fetch it and it obviously isn't going to be as fast at fetching data or analyzing data as an actual relational database and it's obviously also might have some consistency issues where like two separate rights might go through that actually conflict with the interpretation of the state of the world so let's mostly avoid them except redis is really very useful when you just need a simple key value store so for a lot of projects you don't need a database you just need like a persistent dictionary and so redis is great there's the notion of a data warehouse which is some kind of aggregation of different data sources structured in a single place for analysis also known as olap online analytical processing instead of oltp online transaction processing right and another acronym you might have heard is etl so that stands for extract transform load and so the idea is you have all these different data sources like maybe logs in the cloud your transactional processing database which is maybe postgres then maybe you just have a bunch of like flat text files or something and you're going to extract data from all of them transform it into some common schema and then load it up into the data warehouse and then from the warehouse you can then have load the subset of data you need and generate reports run kind of analytical queries the software for this stuff like bigquery from google redshift from amazon snowflake [Music] and most of these solutions use sql as the interface to the data so sql obviously is a very old language and and and some solutions like for example data bricks use data frames instead of sql so sql is the standard interface for structured data but in python pandas is the main data frame solution and is usually what people reach for when they need to kind of analyze data using python so here on the right is a comparison between sql and data frame so let's say you want to select you know three columns from some data table called tips so you want to select total bill tip smoker and time from tips and then the first five results so you say select total build tip smoke time from tips limit five that's what sql looks like but with a data frame language like pandas you have tips as an object and you select columns total build tip smoker time and then you say you know head five give me the first five results and of course this gets more interesting when you start joining different data sources so grouping by different columns counting aggregating things and so sql like you know i prefer the data frame language in the first example where we're just selecting columns but i prefer sql actually in the second example where you now have to do some analytical grouping and stuff like that so if you've never used sql and if you've never used pandas our advice is to try to use both you know try to find a project that gives you a chance to use sql try to find a project that gives you a chance to use pandas because to be a good data engineer data scientist or machine learning engineer or even machine learning scientist i think at some places you should really be fluent in sql data lake is the idea that kind of came out of the data warehouse and the idea is that well we have all these sources like databases logs and stuff like that and and the data warehouse approach is we have to settle on a schema transform all these data sources into that schema and then store them but what if instead we did extract load and then transform so we'll extract data from all these sources load it into the data lake and then later we'll transform it into the format we need for analysis so let's just dump everything in um kind of into this lake and then later we'll be able to settle under transform of the raw data in the lake and maybe load it into a data warehouse or maybe just load into something that can do analysis on it and the trend in the field is to kind of do both in the same suite so the data bricks lake house platform is both a warehouse and a data lake and it's an open source project called delta lake which is quite nice where you can store all data structured data semi-structured data like maybe logs and then unstructured data like just like a bunch of text files that you crawled from the internet or even images right or even videos you store all of it in delta lake and then later it is able to connect to your analytics engines and potentially even to machine learning engines so that's kind of the data bricks vision and the vision of the field as a whole so that was a lot but for now just think about it this way if you have to store data if it's binary data like an image then store it as an object right to store it in s3 if it's metadata about that data right if it's data about that binary data like a label for an image or some user activity like who uploaded this file and stuff like that that goes into the transactional database if you need features from something which is not already in the database so for example logs like how many times did a user log in and like look at this image then set up a data lake and dump everything into it and then set up a process to aggregate all the data and then when you're ready to actually train using deep learning on that data then copy everything you need onto the local file system you know next to the gpu on a drive that's as fast as you can manage and that's that's the guidance right now there's a lot to this story this is kind of like a part of the field that well i think all of the field is really in flux right now and there's no different on the data side as it is on the training and evaluation side so there's a lot going on the reading for this week is this article from from this vc about the emerging architecture for data infrastructure and if you're truly interested in this stuff then you know take some more courses like databases and you can also look at this book called designing data intensive applications so this is a really good book and a common resource for this kind of stuff so we've been alluding to like some you know motivation so let's just
Full Stack Deep Learning - Spring 2021,8,1740,Data Processing,actually make it concrete so let's say we have to train a photo popularity predictor every night because we're operating some website that that would be useful for so for each photo training data should have like when was it posted that what title was it given where was it posted from then maybe oops then maybe something about the user that maybe is like how many times did they log in today right and then we actually have some machine learning models that can identify what's in the image to some extent so like what's the content of the image and then maybe what's the style of the image like is it happy or sad and stuff like that so the metadata is going to be in our database some of the features about the user we probably actually need to compute from logs because they're not in our database and then to see what's in the image we need to actually run some classifiers classifier models so there's a number of tasks that all have to finish before we can train our model that will actually predict the popularity and you know some tasks can be started until other tasks are finished so like we can't start training the popularity predictor until we've finished running all the content style models and also until we finished running our log analysis stuff and so on and um when a task finishes it should like kick off things that depend on it right because we don't want to have to keep managing this process we just want to launch it once and have it complete automatically and like recomputing something should depend on the content of it not on some other stuff like the date ideally the dependencies we're talking about might not be you know simple files but it could be the outputs of programs or like something in the database the work that we're doing is probably not only on one machine but it's over many different machines and then we're not the only ones you know training our predictor maybe other people are training different things at the same time as we're training hours so multiple things are happening all at once and the old kind of big data you know flavor solutions to this are hadoop and spark and so these are kind of like map reduce implementations where you have to process in a bunch of data so you launch a bunch of different tasks that each take a bit of the data and then that's mapping and then reduce it you reduce their outputs into a single output and that's the reduce and it should run on on commodity hardware or like this diverse hardware and there's like things you can do about caching that really speed it up that's kind of like what put spark on the map that's a little bit outdated because it assumes this like monumental or i guess monolithic you know processing environment and in the modern environment like you can't for example run a machine learning model as part of running a spark job right unless that model itself is coded in spark or however that's done but if you just have some like you know a pi torch model and then a tensorflow model for something else and then you need to analyze logs for something else it's all very different so the more modern things uh probably begin with this package called airflow from airbnb and it lets you define a dag a direct direct the cyclical graph of jobs where the jobs can be maybe sql operator operations or maybe actually just python program executions right or maybe just simple files but it lets you define a dag out of like disparate things and then run it and in order to run it you need a a manager the workflow manager this is often called this is often called workflow processing so the workflow manager has a cue of all the tasks that have to be done they know about the dependencies between the tasks they know the workers that are able to do the tasks and then they manage tasks they assign tasks onto workers if things fail they'll restart them or alert you and so on apache beam is like one of these things and tensorflow data sets which so tensorflow the tensorflow team makes a lot of different data sets available through this tensorflow datasets interface and they seem to use apache beam for this kind of processing and it runs on google cloud data flow which is you know a cloud orchestrator so for example the t5 model the transformer model that we discussed a few weeks ago that was trained on something called the colossal clean corpus which is like a web crawl corpus that's seven terabytes in size and before you can actually train on it you need to process the raw you know crawl output into some trainable form i'm not totally sure why they didn't do that for us but they gave us the raw form and then you know gave us like scripts to be able to process it and i think it requires you know hundreds of workers and still like 20 hours or so to to process prefect is something that people use instead of airflow increasingly same idea though it's a python framework that makes it easier to like define tasks the tasks can be code or like sql and stuff like that and then the prefect hosted solution will then orchestrate it for you so you don't need to worry about like launching your own orchestrator you just define the tasks push them over to prefect and then prefect tells your own hardware right what to actually run on or what to what tasks to actually execute dbt is the ability to do stuff like this with sql right so instead of writing python code for a lot of this processing could it could it be done if you just wrote sql so this is for people who are comfortable with sql which a lot of data scientists often are for data engineers this is kind of a nice solution so they call it analytics engineering it's quite a nice idea daxter is another data orchestrator that seems to be quite popular nowadays works with a lot of tools test locally you run anywhere
Full Stack Deep Learning - Spring 2021,8,2162,Feature Stores,okay and then lastly we're going to jump over into the deployment side and talk about this idea called the feature store and so the idea of the feature store was first popularized to my knowledge by uber when they publicized their their uh machine learning platform called michelangelo and they they posted this visualization of you know some of what it has to do and so it's divided into two two halves on top we have the online processing and on the bottom we have offline processing and so the idea there is that to train your model right is an offline task so we're going to take data from our data lake so this is uber so let's say we're trying to predict we're trying to train a model that will predict how to price a ride you know given a bunch of historical data about demand and ride lengths and the availability of drivers and so on so all that stuff is in our data lake and that comes from a number of databases and log files and and god knows what but it ended up in the data lake right so we're going to prep the data using maybe spark or sql and load it up into the feature store which is based on hive and then that feature store can be used to give training data to our training algorithm right and from that we'll put a model in the model repo and then lastly when the when the model is ready now we can actually deploy it in the prediction service but in the prediction service the data that's coming in for the model to predict on is not coming from the same flow right it's not coming from a data lake it's not going through spark it's not landing in hive it's actually going through a separate flow which in this case is the kafka streaming streaming engine and goes into the cassandra which is a different type of database feature store and then from that to the trained model and the reason this is highlighted is because this can lead to a lot of bugs right because these are two different data processing pipelines one is offline one is online but if we can unify as much of their logic as possible into this unified kind of feature store then that would be much better right like we don't have two separate code bases we don't have two separate sources of bugs and so on so a bunch of people from uber i believe started this company called tecton which is the main feature store company and they give you another little visualization right so you have real-time data and batch data all going through the same transform store and serve process from the feature store for an open source alternative take a look at feast which you know implements the same type of ideas okay lastly as we talk about all this stuff and i just mentioned hive and cassandra and all that stuff i think the tendency is like it's overwhelming like there's too much stuff and i think that's true and some of it is over engineered so let's try to keep things simple and and try not to over engineer until you really know why you need to and as like a motivational example like not a serious you know do this example but as a motivational example let's just take a look at the the tools that like every unix installation comes with okay so there's a blog post that was pretty famous at the time called command line tools can be 235 times faster than your hadoop cluster hadoop was like the spark of its day in 2014. so the task was to analyze a bunch of i think log files for the presence of some word and then yeah just count up the number of times this word appeared in like terra and like gigabytes or maybe even terabytes of data and it took 26 minutes on a hadoop installation and with simple unix tools like cat and grep and sword and like unique it took 70 seconds so you just like in this case you catted all the files pipe them over to the grab command pipe the results of that over to sort pipe that to unique now what's interesting about about that that you might not realize is that all these pipes are actually happening in parallel so it's not the case that all of the cat command has to finish and then all the results are aggregated somewhere and then all of the results are sent over to grep it's not the case right as cat streams its results they get straight into grep and grep starts executing at the same time as cat is executing and then as grep results come out they get into sort and that starts executing so these are actually happening like the os manages you know parallelism here for us and we can actually make it even faster by running it even more in parallel and and kind of instead of using crep using awk which is i guess even more efficient for this particular thing but x-args is another built-in unix command that just has built-in parallelism so if you need to do something a thousand times and it's actually distributed over like a thousand cpu cores you can just say dash p 1000 and just launch a thousand of this process with like no extra complexity so it's always worth looking at the tools you already have and just seeing you know how you can make them work for the task instead of it's like oh i heard about cassandra you know for this thing i must that that must mean i have to use it right so let me go download and figure it out but usually that's not the right strategy we should try to keep things simple so to move on let's talk about exploration so i hope
Full Stack Deep Learning - Spring 2021,8,2520,Data Exploration,you guys have all seen pandas right it's really the workhorse of python data science if you have not used it then definitely recommend doing a few projects using pandas because it really is the data science you know tool of choice and r has its own data frame you know spark and and uh scala you know they have their own data frame but the basic idea is that it's like an object-oriented data analysis interface [Music] so i don't talk much about that because i assume that most people know it so i just wanted to share a couple of things that you may not know about so dasc for example is a project that could speed up your pandas processing pretty dramatically if you're trying to process uh data that's too large to fit in your memory right dask is often a drop-in replacement so they have their own data frame implementation that matches panda's interface but parallelizes it you know paralyzes all the operations such that you can actually load very large amounts of data and analyze them similarly a project called rapids has the same approach except scaling out data analysis specifically onto gpus right so pandas is not gpu accelerated but the this rapids project aims to be basically a pandas replacement that can do data analytics on gpu which would be much much faster so it's worth taking a look at i haven't personally used it
Full Stack Deep Learning - Spring 2021,8,2622,Data Labeling,so next time next up let's talk about data labeling we'll talk about user interfaces for it sources of labor and kind of service companies so there's a standard set of features for data labeling you know you have bounding boxes segmentations key points you know cuboids for like 3d annotation there's some some classes that you can assign to the annotations what's crucial is agreeing on what makes a good annotation so you know training the annotators is a large part of of of doing an annotation project so for example here's a motivational example i believe from cs231n where you know reasonable people can can disagree about exactly how to label the fox in the first row like do we draw a little bit of space around the fox or do we focus on the face of the fox and certainly in the second row where like your human mind naturally just imagines the rest of the fox behind the rock and you might annotate it that way and that might be the right way to annotate it or not depending on what kind of project you're actually doing and so it's worth really annotating a lot of your own data yourself first and writing up detailed guidelines with edge cases like this such that your annotations come out reliable quality assurance of the annotations is also key people just aren't you know as conscientious across the board like some will have better annotation quality than other and where do you find people to actually annotate so the choices are you could hire your own annotators and maybe promote like the most conscientious ones to quality control the other people so this is very secure right because you can have them sign whatever you need once you hire them they can work you know 40 hours a week so it's pretty fast and because they keep working on the same task you know less quality control is needed but it's expensive it's hard to scale there's overhead to to the administration you could crowdsource labor so this is a mechanical turk amazon mechanical turf where like people just all over the world go on online and do little tasks for cheap and it is a lot cheaper and it's more scalable because you can find like a thousand people to do your task but you know it's not secure really and then the quality isn't very high partially because it's so cheap or you can hire a data labeling company so it makes a lot of sense to hire a data labeling company for a large annotation project because it really is like a separate software stack to what we're using for training right it requires temporary labor it requires quality assurance so how do you find one to hire still have to you know label a bunch of data yourself as like the gold standard and just so that you understand and are able to write good guidelines then you know try different contenders try to get a work sample on your gold standard sample and compare it to how compare how they did to how you did and then try to see you know what's the price to quality ratio figure 8 is like the probably the largest data labeling company founded by the same people who found that weights and biases funny enough scale.ai is the really dominant you know new company that does data labeling and they're sponsoring some prizes right for the class and there's a lot of others there's a crowded space there's label box supervisedly there's a bunch more right and full service data labeling you know is always pretty pricey and so maybe it's not exactly what you want but you still want to use purpose-built software for annotation so there's there's choices in the market for just the software without the labor that comes with it and there's actually a free option that's really good called label studio which is a open source you can run it yourself there is an enterprise edition for hosting and it's actually what we're going to do in lab is use label studio so you can define your own labeling interface using this config format which looks a lot like html code and then you get the interface and you can start labeling and it's very flexible like you can mix different types of things you can also implement different modules like you can there's a back-end module that serves up data there's a front-end module which we we you can define the interface but it's also a machine learning module that for example can you know you can do active learning so you might have a model running and maybe pre-annotating some of the examples so that what the human annotator sees is not just a blank image but actually is already kind of annotated by a machine learning model and then they just correct it if need be [Music] and there's an interface for looking at your data which is actually a huge part of the task here is just get into grips with like all your all of your data prodigy is a good solution for text data and aquarium is a recent company that was founded by a berkeley alum and it's they they build it as a machine learning data management platform so the idea is that really exploring your data set is a big part of the task and so it's a good tool for that and they call it kind of curating data and then when you find things that are difficult or mislabeled then you can fix that and then kind of plug the data management tool into your machine learning workflow cycle another really interesting vein of work here is in what's called weak supervision and snorkel the snorkel project is the is the dominant tool here it started out as an open source project and you know there's still a version that's open source but recently it also became a commercial platform snorkel.ai the idea is that we can label a bunch of data somewhat automatically with heuristic functions so for example let's say you know you have a bunch of reviews or something of restaurants right and you want to train a classifier of sentiment and there's certain words that you can just kind of you know regex for and if you find them it's probably a good review like amazing and like awesome you know food is awesome stuff like that so you can think of all these heuristics and write a python function that just says like if you know there's text food is awesome then it's a positive review and then via snorkel you can run that which will label a lot of your data automatically but give you like really powerful tools to uh correct that and then it kind of tries to learn a model side by side with your heuristics and the end result is that you're able to go through a lot of data and focus on edge cases very quickly instead of just going through in random order the conclusions that we have are you know if you can afford it just try not to spend time on labeling and just outsource if you can to a full service company if you can't afford that then at least try to use some existing software and if you're hiring then don't try to make crowdsourcing work just try to hire some part-time people on upwork or
Full Stack Deep Learning - Spring 2021,8,3088,Data Versioning,something so lastly let's talk about data versioning and there's four levels which i will go through so the the zeroth level right is that your data just lives on the file system or an s3 and in like a database and so you train on it you have a model you deploy the model what's the problem right well the problem is that your deployment really has to be versioned because you have to be able to like revert it for example and your code is versioned because that's good practice like a software engineer we just know that we have the version code but your machine learning model is partly code but it's partly data and so if your code is version but your data is not versioned then the deployed model is actually not versioned also and you won't be able to get back to some previous level of performance because you just keep overwriting your model and you don't know what it's actually trained on so the first level you might think is well okay fine let's just snapshot everything you know all the data that we're training on just snapshot it archive it and that's the version and it actually would work it can't get you back to past performance if you need to it feels very hacky right because you know it's that's what people used to do to code actually before version control was widespread right it's every release you would just zip up all of your code you know store it somewhere just in case there's a bug in the new release you'd be able to go back and try to figure it out and so it would be a lot better if we could version data just as easily as we conversion code so that would be level two right let's version data as a mix of assets and code so let's store files that are large in s3 in s3 they get a unique id and then as part of your code repository you could store let's say a json file that points to these ids and then maybe has some metadata like the label of of the image so you don't have to store the image in the repo you just store the metadata about it this metadata can still get quite large but we could actually just store it in our git repository using git lfs which stands for large file storage natively supported by github and i believe in gitlab the major git hosts so if you just you know install git lfs and you just track let's say json files in git lfs then automatically as you commit the the the json files will get uploaded to s3 behind the scenes they'll be automatically versioned and you don't have to think or do anything different other than what you're doing already and this can this this totally works because the git signature of that data file is the version of the data set right it fully defines the version of the data set so let's try to go above that though maybe our json file is just a terabyte big now so we can't actually store it anymore so there are specialized solutions for versioning data i think we should avoid them until until we can fully explain how on this specific project they would improve things over get lfs and those solutions are dvc pachyderm and quill here's a helpful little diagram of some of these delta lake is actually worth looking at also for this but dvc is worth looking at a little more detail so it's open source version control system for machine learning project so the idea is if you have a data file you dvc add it just like you get at a code file you dvc add a data file and then when you process the data you say dvc run and then the script that would process the data and then the output name of the processed file and so what that lets you do is that the dvc remembers the provenance of every data file or you know processed or interim data file or even model that came out of training automatically for you right so it knows like what what raw data files was this particular model trained on and it's able to recreate things intelligently it seems like a good solution it says so definitely check it out if if you have that need if you have the need for versioning databases then there's a project called dolt which is quite interesting and so it's basically a git for for sql so it makes it makes it so one thing that git makes easy is merging right so like two different people can be working on the same file in parallel and then if they if there's some kind of conflict they'll both try to commit and then when one of them you know one of them will have to resolve it and then git makes resolving a conflict really easy and so similarly don't makes resolving database conflicts really easy but it also has like a full sql implementation so that lets you kind of explore your data so last thing i want to talk about is concerns about privacy
Full Stack Deep Learning - Spring 2021,8,3423,Data Privacy,so like everything we've talked about so far kind of assume that we have access to you know unfettered access to the data and we can train on all of it but a trend is that people and companies don't necessarily want to share data as freely as they used to and this is particularly important in in some settings even today like healthcare and i think will be more important in a lot more settings in the future but in healthcare for example you have very sensitive you know patient data that hospitals don't want to spend to some third party for training the model on so federated learning is this idea that you can train a global model using data that's on local devices and and the global model never actually has access to all of the data so there's some amount of training that happens on the local device and then things are synced to the federated server there's problems with this that have to be solved and that's why it's a area of research right now like you know it's expensive to send all these things back and forth the different systems that the model has to be trained on now are very heterogeneous and there's still some privacy concerns like are you actually able to de-anonymize certain things just from the model weights right so like yeah you never saw the data but can you still get something about the data from just the model weights that are sent to the federated server similarly a research area called sometimes called differential privacy deals with at a theoretical level with like the notion that like could data be aggregated in such ways that you can't identify individual sources of the data [Music] and another topic that's interesting is maybe learning on encrypted data so could the data be encrypted at the source and then stand for training and and you can actually train but you can't decrypt it these three things are are i would say research areas right now i'm not aware of like really good tools that make it possible to do these things but it's just something to you know keep in mind and maybe and maybe you're interested and for example doing a project in this area and if you know if you know really good resources about it then just let us know and slack about them so that's it on data management for today so thank you
Full Stack Deep Learning - Spring 2021,9,0,Introduction,this week we're going to talk about ethics and before we get going just as a preamble it's a really huge subject spans many disciplines addresses a wide variety of like very real problems and as machine learning practitioners we need to have a student mindset here and not necessarily assume that we have the answers because these are really not easy problems and i personally am not an expert there's excellent resources that we found and we recommend those at the end so when people teach tech ethics of which ml ethics is a subset but an increasingly large subset what do we actually teach so this is a paper from the computer science educators conference from last year and it looked at 115 different university tech ethics courses and they found that there's a large variation in the materials covered right so some a lot of uncovered law and policy privacy surveillance philosophy inequality justice ai social impact and a bunch of other stuff and there was more consensus in outcomes and the main outcome for all the co for most the courses was the ability to critique the ability to spot issues and make arguments and to communicate basically about these issues so that's kind of the guiding light here like this is maybe the best we can hope to get out of just an hour and a half on the subject and as a last preamble there's a little parable in that david foster wallace commencement address that was widely shared a few years ago so there's two young fish swimming along and they happen to meet an older fish who nods at them and says morning boys how's the water and the two young fish swim for a little bit and then one of them looks over at the other and says what the hell is water and that's i think a perspective that we should try to have in a lot of this is trying to see the the invisible like the unspoken assumptions the backdrop behind some of these issues that everyone is just assuming sometimes it's very hard to see the outline of the lecture is going to be just talking about ethics in general some long-term problems that concern ethics and ai some near-term problems best practices recommendations and then some resources to learn more so starting with what is ethics
Full Stack Deep Learning - Spring 2021,9,152,What Is Ethics?,your one answer might be like i can tell if something's ethical because i feel a certain way about it and that's not a great answer ethics are not your feelings because your feelings might mislead you to perhaps you're in a difficult situation where it would be a lot easier to take an option that you actually know to be wrong but you feel like that's just what you should do so feelings can be misguiding ethics are also not laws right we're not just talking about ai laws or legality of machine learning or anything like that because ethics can drift out of sync with the laws they can they can you know supersede laws you can be a person in a society that you consider unlawful and so the basis for you to consider that would be your own ethics or some other understanding of ethics and ethics are also not the societal beliefs this is related right if you are in an immoral society in order to recognize that there's some other source of ethics that you must have so then what are ethics deep questions that have had a number of theories through the years and one of the most common one ones on earth probably for a lot of people is the divine command of ethics so basically a behavior is moral if it's commanded by the divine and that might be perfectly accurate but there's not much that philosophy can say about it because it's just a fate a complete so philosophy doesn't really engage with that view the ancient greeks had this notion of virtue ethics which is that a behavior is moral if it upholds a person's character good character which could also be called virtues so stuff like bravery generosity and you know love so it's kind of morality through the lens of doing and not necessarily believing anything it's actually apparently surprisingly robust to philosophical inquiry like it really holds up from a lot of angles but there is a lot of evidence now that what you would call virtues or character traits actually just aren't persistent across a person's life and and are somewhat illusory so it seems weird to base your whole ethics on a notion of something that might not actually exist and then deontology or like duty-based ethics sometimes called duty ethics are the view is that moral behaviors are those that satisfy the categorical imperative right called that by con for example don't lie might be a categorical imperative or don't kill and the criticism is that it leads to really counterintuitive moral decisions in a lot of in a lot of situations and so it has like unacceptable inflexibility to a lot of people and then maybe most recently or maybe not but another view is utilitarianism which is that a behavior is moral if it brings the most good to the most people but of course how do you measure you know good and how do you do calculus on good times the number of people so how do you measure utility in another way that doesn't seem to be a clear winner like among either at least among professional philosophers so this is a survey of some almost a thousand you know practicing philosophy professors and it was pretty evenly split among deontology consequentialism and virtue ethics trolley problems which you've probably seen are often used to gain intuition about different about a person's ethics just from presenting them with a moral dilemma in the classic dilemma is you're seeing a trolley it's about to run over five people but you could divert it and it would only run over one person so it's quite contrived but it's designed to like elicit some intuition about would you rather kill one person or five people basically or perhaps the duty-based ethics like you have a duty not to kill no matter what so if you pull the lever then you're responsible but if you just stay away then maybe you're not so there's all kinds of views on it and in fact it's a good meme so here's one the trolley's rolling along you can stop at any time but it would disrupt the trolley service causing the company to lose profits or this is known as the boomer trolley problem would it be fair to the people the trolley is already killed did they hurt it now or my favorite is if you pull the lever one person dies and your liability exposure is one wrongful death suit if you do not pull the lever five people die and you have no liability exposure what do you do then there's some metal ones which i just included for fun and then there's this one which is in the trolley problem it's always assumed that you're the person pulling the lever but what if you thought about it a different way what if you actually didn't know which person you were and this is called the veil of ignorance and this actually leads to another ethical theory which i think is maybe the most dominant now with which is john rall's theory of justice or veil of ignorance and the thought experiment is look you know let's say you were reborn into the society you're in but you didn't know but it wouldn't be into your life it would be just into a random life in the society do you believe that your society is fair if you like would you choose to be born into this society if you didn't know which member of the society you were going to be born as right and so the intuition there is we should try to improve society such that from the veil of ignorance we would feel safe about living in the society like we would be happy to be a random person in the society do as you would be done by when applied to technology i think it's important to understand that ethics are not static right they actually change with what technology allows us to do and for a concrete example of this we can think of the industrial revolution which just radically changed the calculus of of human labor whereas before the industrial revolution all work on earth was done by you know human or animal muscle after the industrial revolution we had machines doing work in the physics term and that just leads to different ethical problems and it leads to different ethical decisions or for example the internet is a recent invention but it seems so like fundamental to how we live today that people talk about internet access being a human right is a new thing or if you look at the way reproduction has been happening for all of history there's certain ethics associated with it and a lot of them have radically changed in just the 20th century and and the 21st century first with the invention of birth control that's reliable and cheap then in order to have a child no one needs you personally don't need to be pregnant anymore there's surrogate pregnancy which would be hard to explain to your great-grandparents for example now there's embryo selection so you can actually do genetic testing on a number of embryos select the one that is best under some metric and then implant that one and then in the future in the near future we might have artificial wombs so you can in fact have children without ever going through pregnancy and of course genetic engineering and stuff like that or lab grown meat i think the ethics of being vegetarian or not will change if there's uh really abundant and cheap lob-grown meat instead of farm-raised meat there's a good book about this uh called right or wrong which is a fun read if you're into this kind of stuff i can move on and we can think about long-term problems that are ethical in
Full Stack Deep Learning - Spring 2021,9,644,Long Term Ethical Problems in AI,a.i so the first i think a lot of people's minds go to autonomous weapons and maybe they go into a place that is a little easy to dismiss as maybe far-fetched not realistic we don't have to worry about it it's just a movie but of course as the saying goes the future is already here it's just not evenly distributed so israel apparently has autonomous robo snipers on their borders today and just i think last weekend or something there was an article about the new york city police deploying the boston dynamics spot robot which actually anyone can buy now i think it's only like 60 000 or something but they were deploying it in like a situation of a crime in progress so i think this will be something our generation is going to grapple with for sure replacing human labor is another concern right that seems to be on the horizon still but is also creeping up on us and the traditional view is humans have been working certain jobs that now ai is taking or robotic labor might be taking in the near future and particularly with the pandemic you see a lot of articles where millions of people have lost jobs and it's dawning on a lot of people that maybe they'll never get them back because they'll actually be replaced by ai or robots in just 2020 and 2021. now this could be good and bad could be bad if there's no social safety net or no other job for you to have and you're no longer able to work the only job you're qualified for because now a robot has it and that's just a couple of hours ago i saw on reddit a shower thought the problem isn't that robots are taking over our jobs the problem is that we've created a world where that's somehow a bad thing it really should be a good thing that we don't have to do a lot of these jobs because robots can do them now the problem is in our society it's not like a robot is replacing you then you get the pay that you would have got it's a robot's replacing you some company saves money and then you don't get an income at all but it's also good and we just have to figure it out because there is a mega trend of the demographic conversion and so what that means is the world population is basically topping out right at around 10 to 12 billion and there was a huge births spike and it's different in different countries right the united states famously had the baby boom but for example in africa the population is very young even today like their baby boom was basically like right now or maybe 10 years ago and in china they instituted the one child policy which obviously has the effect that for every um single person of working age there are two people of retired age and that's not quite true just yet but it will be true in 20 to 30 years and if that's true then basically the economy can't function as as currently designed right like our economy really depends on growth and it depends on a certain ratio of economically productive people to children and retired people and if you invert that ratio then you basically need to make up the labor from somewhere and rodney brooks is a roboticist i believe from mit also the founder of irobot pretty brilliant person and this article which i recommend you click on talks about how we basically need robots in order to have a functioning economy in the next few decades an interesting spin on this worry though is ai not necessarily replacing human labor but controlling human labor and so if you think of like the amazon fulfillment warehouse it's worked by people but the efficiency comes from basically machine learning allocating labor in such a way that a person really has no agency right in this environment so this is an article from the verge how hard will the robots make us work in warehouses call centuries in other sectors intelligent machines are managing humans and they're making work more stressful grueling and dangerous and that led me to remember the short story which i recommend you guys read called mana two views of humanity's future and the first sentence is depending on how you want to think about it it was funny or inevitable or symbolic that the robotic takeover did not start at mit nasa at microsoft or ford it started at a burger g restaurant in cary north carolina may 17th and it's very much in this vein of ai not necessarily having to have a physical body in order to i guess subvert human life and the final worry is well the ai's maybe if it's super intelligent and it has robotic labor it actually just doesn't need humans at all it can replace humans entirely and that's a screenshot from the matrix if you guys haven't seen that so what's common in all of these long-term problems what's common is the problem of alignment so this notion of alignment is often expressed through this other parable of the paperclip maximizer which i think is is from the philosopher nick bostrom who works on ai safety and related concerns but basically the story goes is that assume there's a artificial general intelligence so some an entity that's as smart as a typical person not necessarily any smarter and it's given the goal of producing paper clips and that's its goal so it's able to act in ways that an intelligent entity is able to act in which includes making itself more intelligent potentially by developing you know its own ai and so eventually develop super intelligent ai and because its goal is only producing paper clips it starts just tearing up the streets and like metal buildings and stuff and eventually turns every atom on earth and then every atom in space into just paper clips so obviously something went very wrong by the way universal paper clips if you click on that in the slides it takes you to a game where you play as an ai producing paper clips fun this is actually an old lesson if you think about it like you rub a lamp and a genie comes out and then they tell you three wishes but then you ward the wish in such a way that it actually becomes a curse for you and then you use the second wish to revert that and then the third wish is that there's some other thing um that's problematic about how exactly do you communicate your goals and values to a technology basically or you can think of frankenstein and his monster where you create something and you think it's under your control but it in fact is not if it's given enough power so the guiding principle for basically everything i think in this lecture is that the ai systems we build including the current limited machine learning systems we build today need to be aligned with our goals and values so this problem of alignment is a very deep topic and it's actually an active area of research at a number of places including at birklane at the center of human-compatible artificial intelligence of which peter abeel is one of the faculty so that would be useful to hear from him as well at some point but i think this alignment lens is useful for near-term problems as well not just long-term problems and so let's get to those now so let's talk about hiring
Full Stack Deep Learning - Spring 2021,9,1131,Hiring,here's a headline you might have seen or something like it amazon scraps secret ai recruiting tool that showed bias against women and there's been a number of headlines about using machine learning systems for recruiting or in hiring in some way but let's say we're trying to work on something like that so we're trying to develop a machine learning model to and specifically this is given a resume we want to predict the hiring decision the eventual hiring decision so to train the machine learning model we need data right now we have a choice what should the data contain should it contain hiring decisions which is what we're trying to so like obviously it's gonna contain resumes but then what's the label on the resume is it the hiring decision that was made or is it the eventual job performance given the person was hired you might want to predict job performance instead of the hiring because what if they were hired and then they were immediately you know fired two months later or if it just didn't work out but what you really want to hire you know who you really want to hire are people who get hired and then get promoted quickly and our leaders at the company okay so the data comes from the world and the world is known to be biased in many ways right there's bias in the hiring pipeline so potentially maybe if we're hiring for software engineers maybe not enough women are getting educated for the job so in the pipeline there just aren't that many women candidates and there's reasons for that that come from sources of bias in the world there could be bias in the hiring decision itself so by the people doing the hiring intentionally unintentionally they are selecting people that match some prejudice that they have and there's bias in the performance ratings which is if that's the signal we're trying to predict then then maybe people are getting promoted not because they're actually real good at the job but because they're good at something else or they match the expectation of the promoter in certain biased ways so because we know the world to be biased then basically no matter what decision about how to structure the data we make the data is also biased and therefore any model we train on that data is also going to be biased okay and then the model is trained in order to help or take some kind of action so what is that action are we scouring the internet for resumes predicting whether it's a good candidate and then feeding the candidates into some pipeline so that we can reach out to them or is it that we are just running the machine learning model alongside human decision makers and we're just double checking how the decision the human decision agrees with the machine learning decision or are we actually using the machine learning prediction about whether a human would hire this person to actually do the hiring and let's say we are doing that in that case the action that the machine learning model suggests directly impacts the world because basically whatever we predict we actually we predict that this person would be hired so then we actually do hire them and so that adds to the state of the world which then changes the data set which then if we retrain the model changes the model and we're basically amplifying existing biases that we knew from the beginning were present in the world and amplifying existing biases in the world is usually not aligned with our goals and values and for that reason amazon scrapped the machine learning model in their hiring pipeline so the next
Full Stack Deep Learning - Spring 2021,9,1357,Fairness,thing we can talk about is uh this notion of fairness and in order to look at that in more detail and just really dig into it we're going to use a case study about compass which is the correctional offender management profiling for alternative sanctions system that was in the news a couple of years ago and the goal of this system is to predict recidivism which is basically committing another crime such that judges can consult the compass score which is between one and ten in their pre-trial sentencing decisions which is basically do i let this person go free on bail or do i actually hold them in detention until their trial which can actually be several years in some cases the motivation behind developing the system was to do what kevin said and actually be less biased than human decision makers because the criminal justice system is notoriously biased right against certain races and the goal of using a machine learning system is to try to be more fair when it comes to these kind of decisions so the solution that the company came up with the companies called northpointe was to gather data right so there are certain features about the person like age whether the crime was violent the number of crimes in their past like number of years in jail like a bunch of things that you might think are relevant but notably exclude protected class attributes such as race so the model does not have direct access to these protected attributes and then train the model and really make sure that statistically the score given between one and ten corresponds to the probability of recidivism in an accurate way and also in an accurate way that's accurate the same way across the different demographic groups that we care about so we'll talk more about this in a second but basically the solution is to gather data and do a good job and exclude protected class attributes right and yet there's a famous pro-publica report machine bias their software used across the country to predict future criminals and biased against blacks so they give this figure of the risk scores that the compass system um gave and this is in broad county in florida they got the information through a freedom of information act request and like the first thing they noticed is it's pretty even distribution of scores 1 through 10 for the black defendants but for the white defendants it's definitely skewed to the low risk scores and the other thing they noticed they actually had to gather the data themselves took a couple years but they for every person in the foia data dump that they got they actually didn't know whether they re recidivated or not so they um had to look up arrest records for the next few years and and they figured out like which person actually recidivated and was arrested and went back to jail which did not they got that data which was a huge project in itself and one thing they noticed is that there's kind of two sets of people that they cared about here one was labeled as higher risk but actually did not reoffend and so among white people that's 23 percent among african-american people is 45 percent and then there's another set which is labeled lower risk but yet it they did reoffend and so among whites that was 48 among african-americans 28 percent so there's that clear disparity in those numbers there's a big article and in the slides that follow i'm going to borrow heavily from aravind narayanan and uh excellent youtube tutorial called 21 definitions of fairness so there's a bunch of definitions that we can talk about and one of the first ones we encounter is bias and in machine learning we often mean statistical bias and so specifically that's the difference between the estimators or you know the machine learning model's expected value and the true value and so in the sense of statistical bias the compass scores are not biased with respect to re-arrest okay which is an important caveat because we only have data for arrests not crimes committed okay and there and there may well be there's a lot of reason to believe there is bias in which crimes by which race of people result in arrests so there's bias and that process but but that's also the data that the pro-public people gathered and they found something that they perceived as unfair that was also rearrest data but it's also just important to point this out because that's that's the water here as we got to see there's always that it might be biased in the data but there's also bias in the data generating process aka the world and so we always have to look at both in any case the compass scores actually predict the chance of recidivism like the the risk score maps to the probability of recidivism at the same rate across these two demographic groups of african americans and whites so in that sense it is free of statistical bias it is well calibrated across these groups but is this definition of fairness an adequate fairness criterion right is this definition aligned with our values and that's a deeper question and to get at it let's actually just take a step back and just look at binary classification okay and this will apply to the problem we're talking about now but it also applies to a lot of other things like whether to give someone a loan whether to hire someone like we talked about insurance all kinds of things so in binary classification you have the this is basically matrix of true negatives false positives false negatives true positives so if someone was labeled as high risk and they indeed recidivated then that's a true positive right but if they were labeled as high risk and they did not recidivate and that's a false positive and so on so you've seen this in your machine learning classes but the interesting question here is what do the different stakeholders want from the classifier what would they perceive as fair so the decision maker aka the judge or the prosecutor what they care about is they don't want to make mistakes right so those that they label high risk how many actually recidivated so that's the predictive value of the estimator and true positives over the true positives and false positives which are all the people predicted as high risk the defendant cares about something different though they care about the probability that they will be incorrectly classified as high risk because they want to go free before trial and if they're not likely to recidivate then they should be labeled as low risk and so they're really afraid of being labeled as high risk and that would be the false positive rate which is false positives over false positives and true negatives so that's like the people who were labeled high risk but actually did not recidivate or would not have recidivated and then the society at large might care about the demographic balance of the selected sets and so that could be a demographic parity so you want some you want let's say the accuracy to be equivalent across demographic groups and that gets at the notion of group fairness right so do outcomes differ between different groups for example demographic groups but you could also define groups in a number of ways which we have no reason to believe are actually different and so that's the motivation of the pro-public article which is that they did observe that outcomes differed by demographic in this way by the way just to put it back into the language we just had the top row is false positives so they're labeled high risk but didn't reoffend and then the bottom is false negative so they were labeled lower risk and yet they did reoffend so the false positive rate and the false negative rate does not have demographic parity here there's a paper published after the propublica article that proved that if an instrument satisfies predictive parity meaning basically if the plot looks like this and the top in the top right so it predicts the same across the different demographic groups if the instrument satisfies predictive parity but the actual prevalence of the thing that is trying to predict differs between the groups then you actually cannot achieve equal false positive and equal false negative rates across these groups so it's impossible to both satisfy how compass would like to define fairness which is that they predict the same score will predict the same chance of recidivism no matter what the race of the person is and the pro-public definition of fairness which is they want to see equal false positive and false negative rates across these two groups it's actually impossible in this case because the prevalence of recidivism actually differed between the groups and a lot of these group fairness metrics have natural motivations and there's no correct there's no like correct fairness definition because it really just depends on the politics of the situation and which stakeholder you think is most important and so on and in fact it's there's nothing special about the false positive rate false negative rate and predictive parity as the three metrics it's actually any three metrics that you might pick you will be able to prove that you can satisfy all of them and it gets even worse because you might say okay well forget about predictive parity we only want false positive rate and false negative rate to be equal and furthermore we'll actually even allow the model to use protected class attributes like race to make its prediction but then we would fail we would fail another definition of fairness would be individual fairness which is basically if you want there to be a single threshold that's used for the sentencing decision or the pre-sentencing release decision if you don't want that threshold to be different for whites and blacks then you may not be able to achieve this and if you do want the threshold to be different then pretty naturally that fails individual fairness because just because of the color of your skin you're subject to a different standard for pre for pre-sentencing release so then okay let's just pick one right we just want to include we just want equal false positive rate across the groups we care about that's all we want in that case we will still sacrifice some utility which in this case would be the public safety like we want to not release people likely to recidivate or on the other end of the spectrum we might be releasing too few now the defendants because we've set the threshold in the point which yeah it ensures equal metric that we care about here but then it's it's not achieving optimal metric that we should also care about which is utility in this case and there's a fun interactive example i don't know how fun it is but there is an interactive example about it where in a slightly different setting so this is giving loans and this is a companion piece to a paper by waddenberg viegas and hard and mort's heart is a professor at burklane and i'll have more to say about about about his work later on in the lecture but yeah you can basically drag the thresholds there's two populations or blue and orange and then you can say okay i want equal opportunity so i want to maximize true positive rate or i want equal true positive rate i want to set different thresholds so that the true positive rate is equivalent for the two populations or maybe i definitely don't want to be making decisions based on this group so i have to have the same threshold what's the best i can do then or i want to just maximize profit i don't care about anything else or i want there's all kinds of different things it's kind of interesting to play with [Music] so by the way i said that compass removed the protected attributes from the data that the model was able to see and that's i think often would you think might just solve the problem once okay we can't we don't want any differences based on for example race so let's just remove rays from the feature set and then refine so that does not work and you can quantify exactly you know how it does not work and more it's hard also has done that in a paper but just a couple of words about why it doesn't work machine learning can be very good at finding patterns that maybe humans can't find so for example your zip code and your age together might be highly correlated with your race so you can remove race but the machine learning model will learn how to basically construct it from your zip code and age or something like that so you can always pick up on a protected class attribute from other attributes okay so there's trade-offs right there's trade-offs between the different measures of group fairness that we looked at there's a trade-off between maximizing for group fairness versus individual fairness and there's trade-offs between the notion of fairness and the notion of utility and it's in fact not specific to machine learning it applies to human decision making too but i think it's maybe good that machine learning has brought this to the fore because now a lot of people are analyzing it whereas in the past the same kind of decisions were being made but they weren't really as out in the open as as now and it's interesting to think of how a human decision might be seen as a prediction so like the police can search your vehicle if they can reasonably believe that you have some contraband but that's a prediction do you or do you not have contraband so that's a prediction so we can gather this data set and take a look at how that predictor what what features does it has is it biased in certain ways and so on we don't tend to do that and then there's a tension between disparate treatment and disparate impact and this gets into supreme court cases and it's a very deep subject but the basic thing to say is like in that example is it allowed to have two different thresholds based on race that can end up in front of the supreme court and or in front of some court and what happens in court is that you basically have case-by-case decisions where the judge considers you know the full evidence and the full context of the situation and then makes a decision and that's the decision society settles on but in machine learning that just doesn't the whole point of applying machine learning to stuff like this is to scale it and so you lose that ability to make case by case workarounds which is why it's really pushing this to the fore and what's the water here right we've been just like getting deep into this but is there something maybe even more fundamental so here's a tweet from moritz heart and it says here's an example i found helpful in understanding why opting for a prediction as a solution concept of its own regardless of all the stuff which is talked about is already a consequential political act that de-prioritized alternatives so failure to appear in court okay so one approach you can predict the failure to appear in court and then jail the defendant if the risk is high the alternative is to ask why people fail to appear in court and so maybe if you ask that question you'd recognize that a lot of people fail to appear in court because maybe they have a child and there's no way for them to take care of their child or maybe they don't have a car or money for the bus and they just can't get to it or they got a job and they can't take hours off maybe they have multiple jobs maybe they already have to appear in a different court for something else so just having that empathy and asking that question you might recognize that we shouldn't even be trying to predict if people are failing to appear in court we should just change the system so that people maybe don't have to appear in court and i thought that was very valuable and this goes to this diagram that's often shown in a lot of places i don't know where it's from so i wasn't able to really i maybe it's from this blog but maybe not but it's the difference between equality equity and justice so in in the notion of equality there's a situation right there's a fence in front of a soccer game that's the situation and we give everyone the same support which is this little box to stand on and that works for some people it does not work for others and it's equal treatment because everyone got the same thing but it's not really equitable because to make the situation equitable you want to actually give more support to the people who need it rather than equally to everyone and and that's an idea behind affirmative action or other policies and interventions that are designed to produce equity and then the notion of justice is maybe let's see what the situation actually is and instead of trying to stack the deck in the current situation can we actually change the situation so that we address the cause of the inequity and i thought that's a very valuable perspective to have because as computer scientists i think we have very literal minds right and we get into this one track thing like oh it's fair in this way but not fair in this other way whatever and then you argue formulas and false positives but then taking a step back and just seeing the whole situation like maybe that's not even the right thing to be doing at all we can talk about so we talked about fairness let's talk
Full Stack Deep Learning - Spring 2021,9,2462,Representation,about another near-term problem that of representation so if you have ever had a problem grasping the importance of diversity in tech and its impact on society watch this video let's watch the video and then in the comments to this video yeah people posted another another video which i thought was also good [Music] so yeah obviously this doesn't align with our goals and values and it's sadly not a new problem because for uh photographic film the way that it was designed is to you know elicit really good skin tones in people but the problem is they only focused on white people so they had these what are called shirley cards which is like a card of a white lady and then it's for like making sure the color temperature and so on is good on photographic film obviously a problem but there were no other skin tones now they have shirley cards with multi-racial people on them it's also not a problem in just our field so it's i think it's well known and there's many publications about how a lot of medical testing like drug testing and all kinds of stuff is mostly done on men and the lack of females and drug those trials leads to over-medicated women a lot of the what is found in the drug trial is like the safe dose and the effect of those and so on that might be ineffective for a typical man and then when it's applied to a typical woman who weighs less it might be an over medication or the same kind of thing can exist or you can't you can probably observe this in medical trials based on age so maybe most medical testing is done on maybe mostly healthy people of middle age but then applied to increasingly unhealthy people of advanced age and that can lead to problems there's recent improvements though like in the actual actually in the vaccine development in 2020 the operation of warp speed which is the united states government effort to really try to do the phase one two three trials at a breakneck pace they actually had to slow down the moderna trial in order to find a diverse enough set of participants in their phase 3 trial which is you know bad in the sense that the vaccine wasn't approved until a little bit later but really good in the sense that the phase 3 trial established that the vaccine is effective in people of you know all ages races and so on so what's a how do we solve that as it's obviously not aligned with what we want but how can they ship a soap dispenser that just doesn't work for a large part of the population i think a large part of the solution is actually in this new york times article just from like yesterday what is it the 16th yes from yesterday an article called who is making sure that am machines aren't racist largely about team nit gabru and just to read an excerpt so she was an ai researcher at stanford at the time and she went to a conference in about five years ago in spain hundreds of people gathered for the first lecture at the world's most important conference on ai some were east asian a few were indian a few were women but the vast majority were white men more than fifty five hundred people attended the meeting timnit gebruth undergraduate student at stanford remembers counting only six black people other than herself all of whom she knew all of whom were men i'm not worried about machines taking over the world i'm worried about groupthink insularity and arrogance in the ai community especially with the current hype and demand for people in the field the people create this is what you wrote after getting back from the conference the people creating the technology are a big part of the system if many are actively excluded from its creation this technology will benefit a few while harming a great many and so a large part of the solution to these problems of representation are including people who weren't previously part of the community in the community of developers it's great organizations that are fighting for this specifically in machine learning so black and ai is the logo here founded by timnit women in machine learning latinx and ai these are three organizations that are you know probably at the forefront of bringing previously underrepresented uh people into the ai community so this is very important another example that is commonly given is when you used to be when you search for ceo on google you used to get a page of results that looks like this all white middle-aged men and that doesn't really align with our values of bringing all kinds of people into positions of leadership recently i just did this search yesterday it's much a much more diverse set of people now so somehow or other it's improving in the google search another example that is often given is gender bias in language so here's the example in google translate they someone typed she's a doctor he's a nurse translated it to a language which is turkish in this case that doesn't have gendered pronouns and then translated back to english now magically it became he is a doctrine and she is a nurse and that can be shown to like work for a lot of different occupations so that's obviously a problem the recent improvement i just tried this just yesterday so translating the sentence that in turkish has no gender now doesn't give you a single gender in google it gives you a choice so that's a clear improvement and it educates the user that exactly lets them select what they want this notion of gender bias in language can be seen in word embeddings so we looked at some word to vac examples in previous lectures and we showed it's basically just to remind you it's a training this embedding which is something that converts a word in the vocabulary to a dense vector of real numbers so word2vec was trained on a large corpus and the weights were published so that people started using them for all kinds of things basically as the first layer in their nlp application and they are useful like it's interesting to see that that the embedding picks up on stuff like verb towns and the different relationships between like man woman king queen whatever country capital so these are on slides from rachel thomas's lecture on the subject but they also reveal harmful biases encoded in the language so you can say father is the doctor as an analogy and then ask for the analogy with the word mother that like completes it and will complete it to to nurse although there's a little bit more to say about that because the word doctor is actually removed from the set of results so it's actually father doctor as mother is the doctor but then if you remove the word doctor then the next word the next best match is nurse but if it was like father doctor as man is to it it would probably have something different it wouldn't say nurse but there is this hidden sexism in the language embeddings so one potential solution is to try to de-bias at training time such that the model never learns this kind of potentially harmful biases and at the very least make the user aware like in this example that there might be bias in like the thing they're seeing of course that's more difficult if you're just using this as a pre-trained first layer for some nlp tasks and so like the nlp task might end up biased in certain ways that you can't actually get at very easily anymore because it was embedded encoded in the embedding nowadays we don't use word to fact we use large language models like the gpts and the birds and the t5s and what have you and as we talked about in the transformers lecture gpt3 part of the reason the weights weren't released by open ai is because of societal concerns partially from things like this where gbt3 can say pretty unacceptable stuff if you just get it started with some words and the thing about gpt3 is that it learns it's a very large model has a large number of parameters and it learns in a large number of data the amount of data is so large that you actually can't be sure what's in it so for example like all of reddit is in it and we know there's very harmful pieces of language in reddit but the data set is so large that it's not really feasible to remove like everything that that seems objectionable so then you get things like this and it's like an open problem as to how to deal with it and to tim it was in the new york times just yesterday and that article talks about how she and actually a fellow scientist margaret mitchell were fired by google ostensibly because of a paper they co-wrote with some university of washington researchers named on the dangerous stochastic parrots can language models be too big so and this is an excerpt from the abstract they talk about environmental concerns of training these large models but also concerns about bias from training on such a large amount of data that you don't know what's in it so we provide recommendations including weighing the environmental and financial cost first investing resources into curating and carefully documenting data sets rather than ingesting everything on the web carrying out pre-development exercises evaluating how the planned approach fits into r d goals and support stakeholder values and encouraging research directions beyond ever larger language models not a very objectionable paper by any means and good advice seeing the water here so part of the argument is should the language models that are trained reflect the world as it presents itself in the data or as we believe it should be i think that depends on what we're trying to apply the language model to so if we're trying to apply it in this kind of conversational ai setting where we actually want to speak to gpt 3 and use it to write our emails and so on the correct answer is probably to the language model should learn to reflect the world as we believe it should be not the world as it actually is but if we're applying the language model for the purpose of let's say analyzing posts on facebook in order to find hateful posts in order to remove them well then if you remove hateful speech from your training set then the model you train will not be able to recognize hateful speech when it sees it in production so i don't think there's a one-size-fits-all answer to this like not all language models should be trained on device data but i believe that some should and then the second question presents itself which is if we want to train on data that reflects the world as it should be do we actually agree on the way that the world should be and who gets to decide you know who gets to impose their vision of the way the world should be on the rest of society i don't think there are good answers to this it's just worth posing as a question and just to close the section out you might have seen this article about face recognition it's about a company called clear view which might end privacy as we know it because it ingested a lot of photos from the internet and now it's available to police departments around the country to basically find people right in all kinds of photography by matching them to faces from the internet and i think this is an example of a clash of this is how like technology changes ethics like the old context of this kind of ethical problem is that if you're out on the street and you're wanted for a crime and a police officer sees you they have every right to arrest you and you don't have expectation of privacy in public right if you're going out in public and you're wanted that's the risk you're taking on the new context though is we changed what in public means in public now means basically anywhere in the world like on any street in any city because there's always those cameras around in the corners or on any website on the internet so basically anywhere means in public now whereas it didn't used to in in the past now is it ethical to work on technology like this from one perspective ostensibly it's used for to catch criminals right to reduce crime on another perspective it seems to violate a lot of people's notion of what privacy should entail and then as a little bit of a provocative statement what if you found out this particular face recognition technology doesn't work as well as on some ethnicities as it does on some other ethnicities is that an ethical problem in the same way as the other ones we've discussed or is that potentially actually good because now you don't think the application of this technology is ethical in the first place so here's a quote from fair ml book at the same time debating the merits of these technologies on the basis of their likely accuracy for different groups may distract from a more fundamental question should we ever deploy such systems even if they perform equally well for everyone we may want to regulate the police's access to such tools even if the tools are perfectly accurate our civil rights freedom or movement and association are equally threatened by these technologies when they fail and when they work what can we do about
Full Stack Deep Learning - Spring 2021,9,3326,Best Practices,these problems right now there was a paper in 2019 called improving fairness and ml systems would what do practitioners need talk to people in is a survey nlp predictive analytics like what ankit was talking about computer vision yeah search recommender systems most people are data scientists or researchers or software engineers support in fairness aware data collections curation overcoming blind spots implementing more proactive fairness auditing auditing complex ml systems deciding how to address particular instances of unfairness addressing biases in the humans embedded throughout the ml development pipeline that would be developers but also the data annotators the pms yeah so that's a lot some suggestions from rachel thomas [Music] do regular ethical risk sweeping so like a cyber security penetration testing where the red team is trying to get into your website or app or whatever in order to make sure that they can't try to find ethical problems in your machine learning system hopefully to find that you can't right there are no ethical problems but do it regularly just like you do uh pentastic regularly expanding the ethical circle whose interests or experiences or values or desires have we just assumed as we were thinking about this machine learning system instead of actually consulting them and getting their perspective [Music] think about the worst case scenario could someone abuse the system or maybe steal the system or the data or weaponize it in some way or alternatively what incentives are being created by deploying the system and how is that going to lead people to behave closing the loop so this kind of stuff is not something you just do once and then you're done with it and then you don't have to do it again it's something that should be a loop where you keep improving so how do you keep improving you have to set up a process you have to identify someone is responsible for improving you have to set up feedback channels and yeah make sure it keeps getting done one really good practice that i think basically everyone should start doing is this idea of model cards which came from a line of work actually by timnit and this particular table paper is called model cards for model reporting from 2018 margaret mitchell a bunch of people at google mostly but the idea is for whatever machine learning model or system that you have deployed there should be a page that describes the typical input output the model architecture the performance the performance should be broken down by whatever is interesting like maybe demographic group if that's a relevant factor or data set or geography like whatever it is but try to break it down in a way that's informative and it's just the chance to talk about the what the model does what the limit what the known limitations of the model are what trade-offs were made in developing the model what performance it has should you expect a given level of performance on some input the ability to maybe upload your own data to test the predictions and the ability to provide feedback so it's something that we're adopting in our company and i think it just makes a lot of sense for like a lot of machine learning applications and the goal is not to be perfect and unassailable the goal is to just put it out there with known limitations and then everyone knows that you have some ways to go and you're working on them and you're aware of it there's a project called aquitas i'm not sure if i'm pronouncing that correctly from the center for data science and public policy that allows you to perform a bias audit on your machine learning model so you can literally upload data in the predictions and then it'll parse it out in a number of ways you can set certain class attributes as like the reference and then it'll show you if it's biased on around that reference and it also includes this fairness tree which i think might address a question i heard earlier in the lecture but it basically guides you through like how should you be thinking about what metric to optimize for what definition of fairness is like most applicable in your problem so just not going to read all of it but yeah i'm actually not going to read a lot of it because it's very dense but one of the interesting questions are your interventions punitive or assistive the punitive would be jailing people before sentencing but assistive could be something different that takes you to different parts of the tree [Music] one way to think about it from my colleague eric i turn it in is as we deploy machine learning systems we recognize that we're building on basically a crooked foundation right there's deep-rooted inequities in our society there's furthermore maybe unequal same playing or representation in our data in our training data and then we risk deploying an ai system that amplifies these underlying biases and then furthermore when the user sees the ai output they might further reinforce the biases right now what we could do is try to like un uncrook the pyramid as much as we can and we can't do we can't get at the deep rooted social inequities we like we just can't but what we can do is we can build an ai team that is diverse enough to bring a variety of experiences to the work so that we have fewer blind spots we can make ai fairness and transparency a key component of what we do and d bias ai to the extent possible which is not 100 but also display insights about the bias that still remains that we're aware of in the product that the user interacts with so just like google does right with the the different translations such that they have the context and they aren't just guided blindly by this ai system i think an interesting you know question is should we have a professional code of ethics and we we obviously don't but some professions do so i think medicine is the most well known for this right it's the what's it called hippocratic oath which at least historically doctors had to take when they were graduating from med school and before becoming doctors you might have heard first do no harm supposedly the first line of the of that oath but yeah it's basically just saying you're going to use medicine in in these ethical ways and not harm people and to say when you don't know how to do something instead of just trying to do it anyway and respect that to be ethical or to be empathetic with with your patients and so on and it's very ancient right it goes back to ancient greece the armed forces have an ethics regulation but i don't think it's actually used it's like 102 pages and i just i i don't know if people internalize it there's the geneva convention and stuff so i think the armed forces do have some code of ethics that's different from society at large that is internalized to some degree i'm just not sure how written down it is but software engineers and machine learning scientists don't and i think it's an interesting question whether we should
Full Stack Deep Learning - Spring 2021,9,3786,Where To Learn More,so where can we learn more oops rachel thomas is a co-creator of fast.ai and she actually has a course called practical data ethics it has six lectures it covers everything in pretty good depth or you can just sample the single lecture from the most recent iteration of fast.ai ethics for data science highly recommended they there was a cs294 just like this one from morris hart professor at berkeley called fairness and machine learning and and fairness and machine learning is also the name of a textbook that's being written by him sullen barokas and also arvin marianan and it's a work in progress but it's very good as is and i'm sure it'll get even better there's a very good workshop from some people at cmu called dealing with bias and fairness and building data science mlai systems a great set of slides from a presentation at kdd 2020 i think it's like a whole day workshop so there's a lot of material a great book is called the alignment problem machine learning and human values by brian cruz yeah it's a really well written book it covers a lot of terrain here including the near-term problems and not just not just the paperclip maximizer type of stuff things that are problems today and cathy o'neil has a book called weapons of math destruction which is about talks a lot about stuff i didn't even mention such as like the facebook news feed how does it influence our politics and and that's a very rich subject i just didn't really have time to get into that so i thank you
Full Stack Deep Learning - Spring 2021,10,0,What's Wrong With Black Box Predictions,welcome back everyone hope that you all had a good spring break and didn't just work on your research the entire time so today we're going to talk about what happens next after the troubleshooting lecture right so let's say that you've got a model you've trained it accuracy or whatever metric that you're optimizing is below your threshold and so now you're starting the process of thinking about how should we actually start putting this model into production so the first question that you need to answer the first thing that you need to do before you actually deploy the model which is what we'll talk about next week is you need to figure out how you're going to test that thing how are you going to know whether the model is really performing well enough in order to go into production so that's what we'll talk about today so the way i want to start this out is just by contrasting a little bit the way that you'll feel once your test score gets below your target performance between when you're in school doing research and when you're like out building a machine learning system in the real world so in school you finally run that experiment and your your validation score looks really good you beat state of the art you've beat previous performance and so next step is one go party two write the paper three graduate four go work at google and five go buy a lamborghini so you're done right you beat soda you're gonna get that job everything is good now let's contrast that with what happens when your test scores blow your target performance in the real world so the first thing is you're gonna have a boss and that boss is gonna say how do we know we can actually trust this model you also have potentially some users hopefully you'll have some users and those users are going to say be saying the same thing and maybe if you're in a regulated industry that the government joe biden will be will be saying that as well and eventually i think once you start to get an understanding of like what all can go wrong once your test scores blow your target performance hopefully your your conscience will be telling you the same thing so what's wrong with the black box right so maybe the black box doesn't get such a bad rap maybe maybe mystical holistic prophecies are actually not so bad so to answer that question let's dive in a little bit and understand what it means when we have a good test at performance right so what are the assumptions that are underlying and so if your test set performance is good then what that means is that if your test data and your production data the data that your model is actually going to see in the real world come from the same distribution then in expectation the performance of your model on all of the evaluation metrics that you ran it on offline will be the same in production as it is in your testing environment so let's unpack some of these assumptions a little bit so in the real world the distribution that you train your model on doesn't always match the offline distribution or the the production distribution and the offline distribution don't always match right so you could have data drifts you could have some sort of data shift you could have or you could even have malicious users that are trying to attack your model and make the distribution of data go into that model intentionally look different than what the model was evaluated on expected performance so saying that the performance of your model in expectation is going to be as good in the real world doesn't always tell the full story so for example if you're working in a long tail data distribution then the sample of data that you took in order to evaluate the model offline might not actually tell you that much about the tails and so if you care about the tails of that distribution your test set score could still be misleading and on top of that if you are evaluating your model just with a single metric across your entire data set then that doesn't mean that your model is actually performing well against all the subsets of data that might be important so all the slices of data where you might care about model performance this only actually tells you about the performance of your machine learning model but in the real world you're not actually just building a machine learning model you're building an entire machine learning system and so there's other things that can go wrong with the machine learning system that don't have anything to do with the model and then finally this only tells you about performance on the metrics that you're evaluating on and in the real world you're probably not actually optimizing the thing that you really care about deep down and so there might be some mismatch between the metrics that you're evaluating let's say accuracy and the ones that you really care about like maybe user engagement or uh revenue growth or something like that how about is this really i i picked this quote from a conversation that i had with a machine learning engineer from autonomous vehicle company who i don't want to name because i don't know if i don't know how much they want this quote to be public but i've heard the sentiment from a few folks that the single biggest thing holding back the entire autonomous vehicle industry according to this person is that even if we had an autonomous vehicle that would work on the roads no one would know that and the reason is that no one like no one actually knows exactly how we would evaluate that model properly in order to decide that this model that this car is ready to go into production so evaluation is is one thing that might be holding that entire field back and i think that there's a similar sentiment to lesser degrees in other fields of machine learning so what are we going to talk about today we're going to introduce some some concepts and methods so we won't be able to go to a ton of detail about a lot of these things but hopefully they'll give you a high-level understanding of what's out there and how you might think about applying it and the methods that we'll introduce will help you really try to understand at a deeper level than beyond just one metric how well your model is really performing become more confident in your ability your model's ability to perform well in a production setting and then really understand the performance envelope of your model right so where you should expect it to perform well what type of data you should expect it to perform well on and where you shouldn't expect it to perform well so those are really the goals of the things that we're talking about in this lecture okay so quick outline of the topics that we're going to cover first we're going to since we're talking about testing in the context of machine learning the first thing that we're going to do is we're going to talk about testing in the context of software engineering more generally then we'll talk about specific practices for testing machine learning systems and how those differ from testing other software systems and then we'll talk about explainable and interpretable ai and so these are we'll define these terms but these are ways of trying to understand why models are making predictions that they're making and we'll talk about the relationship between these techniques and testing and and then finally there's time we'll have a little discussion on what actually happens in the real world like what are common things to go wrong so starting with software testing let's say let's talk about a few things in software testing so first we'll talk about different types of tests then we'll talk about some best practices and some some controversial breath best practices um then we'll talk about one practice in particular called testing and production that is i think gaining in popularity and is important to understand in the context of machine learning and lastly we'll talk about continuous integration and continuous deployment systems which are you can view those as a way to operationalize tests that you write so what types of tests should we
Full Stack Deep Learning - Spring 2021,10,407,Types of Software Tests,consider when we're building software i would define three sort of basic types of software tests so you have unit tests and the goal of unit tests is to test the functionality of a single piece of code so it could be an assertion or a claim that you make on a single line of code or a single function or a single class but really aimed at testing that unit of code in isolation then there's integration tests the goal of integration tests is if you're only testing units of code individually then there might be things that break when you combine those together so the goal of integration tests is to put one or more units together and test how those components work together and then finally there's end-to-end tests and so the point of end-to-end tests is that you really want to see how your system works when it's all put together and in a perfect world you test that on realistic inputs from a real user and and make sure that the entire system works together when it's tested in that way so these are the three core types of software tests there's many other types of tests that you might see and i won't really get into all of these here but testing is like a pretty pretty broad field and so it's there are other types of tests that you'll encounter as you build software
Full Stack Deep Learning - Spring 2021,10,485,Software Testing Best Practices,okay so we've talked about kind of three basic types of tests next thing that i want to talk about is what are some best practices for writing tests and testing software effectively testing best practice is that generally speaking you want to try to automate your tests so i think in a long time ago when people thought about testing software the way that they thought about it was like user testing right you actually you actually build your software system you deploy it and then you like try to have a user click through it and try different things and those users can catch bugs that way and that's how you test your software system a lot of companies actually still do that they employ like professional testers whose job it is to do essentially just that but the best practice these days is to aim for more automated test suite and so what that means is you have tests where you or it's possible to run them by themselves without a user like logging and clicking on a bunch of stuff and so these are typically run from a ci cd system which we'll talk about in a little bit but the goal really of automated testing is that you should your testing system should spit out like a single answer as to whether your whether your system passes the tests or whether it doesn't pass the test there should be no ambiguity or very little ambiguity as to whether your system performed up to standards on the tests that are being run second best practice in testing software systems so it's it becomes as your code base gets bigger and bigger you tend to add more and more tests for that code base right so if you're unit testing all or most of the functions or classes that you're writing then the number of tests that you write is going to grow in proportion to the size of your code base and what that means is that as your project gets bigger and bigger you're going to be running more and more tests and so if those tests are unreliable so if they let's say randomly fail one out of every time 10 times that you run them or one out of every 100 times that you run them or if they're slow right so if testing starts to get up from seconds to minutes to hours or if the tests themselves are untested then that's as your code base grows and as your project grows that's going to start to create problems right so if your tests are unreliable for example 10 of the time that you run this test it randomly fails then people will start ignoring them and so that that test can lose its value if the tests are slow then that means that you won't want to run them frequently during development and so testing will become this add-on process to your development process that you like only do at the end when it's like absolutely necessary in a perfect world your tests can run fast and they can actually help you develop faster because as you add functionality you can run rerun your tests and then just be more confident that you didn't break anything and then if you're not code reviewing your tests themselves then like tests are just code so the tests are going to end up having bugs in them and then you're going to have buggy tests and that can be worse than having no tests at all [Music] another best practice that i think is like pretty unambiguously a good thing to implement in your code base if you're working on teams especially teams of more than just a couple people is having a team norm that when you that you actually have to run your tests and those tests have to pass as like part of your github continuous integration suite or whatever ci that you're doing before you merge new code into the main branch so this is just a forcing function a team norm that says we trust you to run tests on your own but we're also gonna have a record that all tests pass before this new code was merged and so this is just a good way of making sure that everyone's always doing this and also can be helpful for kind of regulatory concerns if you're if you're running your own company another good practice to do when you're testing your software is in terms of thinking about when you should add tests so when you're writing new code like building new features that's one good time to add tests like you want to test this new feature to make sure that it works the way that you expect it to work and also really to make sure that like when one of your co-workers is is adding some other feature they don't actually break the feature that you broke but another good time to add tests is when you find production bugs so if you if you notice that oh our users like when they click on this link they're getting to a bad state on our website then you know don't just go in and fix that bug but also create a test out of that bug and that can help make sure that for trickier bugs that you fix someone doesn't go in and accidentally reintroduce those bugs later another testing best practice that is maybe somewhat more controversial but i think is still like pretty pretty reasonable like universal recommendation is to follow what google calls the testing pyramid and so what the testing pyramid says is that the relationship between the number of tests you write in each of our three main categories so unit tests that test single unit of functionality of the code integration tests that test how two different components of the code are integrated to end tests which tests the entire functionality of the system you should be writing a lot more unit tests than you are integration tests and a lot more integration tests than you are intent so in some ways this is actually counter-intuitive right because end-to-end tests are the things that test like the full functionality of your system so why wouldn't you want a lot of end-to-end tests right why wouldn't you want a lot of tests that like run real user behavior through your entire system and then spin out a result of whether the system is doing the right thing or not it's not that you don't want end-to-end tests but compared to end end tests you the advantages that unit tests have are that they can be run a lot faster right so you don't have to necessarily like compile and build your entire application in order to run the unit tests they're more reliable so there's a lot of stuff that can go wrong in your entire application that could cause an end-to-end test to fail other than just the new feature or the new code that you changed like for example the data that you're inputting could get corrupted and i think probably to me the most key reason is that they're better at isolating failures so if your end-to-end test fails if your application doesn't do the right thing when user clicks on this button then how do you know how do you diagnose that how do you actually know what to do about that how do you know what the cause of that failure was then tests are written so that they point you exactly at the cause of failure when that test fails so it'll point you at exactly the class or the function or even the line of code that caused that failure and so you can have a much more targeted debugging process so it's not to say that you shouldn't write end-to-end tests and tests can still be very valuable but the the rule of thumb the like rough split that that google recommends at least is a 70 2010 split so 70 of the tests that you write being unit tests twenty percent integration tests and ten percent end-to-end tests okay so that's a quick overview of some of the kind of uncontroversial best practices about software testing next i'm going to talk about a few things that i think are pretty good general recommendations but are somewhat controversial so one controversial best practice for testing for unit testing software systems is to do solitary tests so the difference between so the distinction here is between a solitary unit test and a sociable unit test and a sociable unit test is where that unit test relies on some other unit in order to fulfill its behavior so you're still testing the functionality of one unit but you rely on another unit doing the correct thing in order to test it so for example if you're unit testing some of your data loading code a sociable unit test for that data loading code might rely on the database so it might rely on some data already being populated in that database or if you're if you're doing a sociable test for like part of your pre-processing pipeline you like let's say that there's multiple steps in the pre-processing pipeline you might rely on the output of the first step being correct in order to test the second step on the other hand solitary unit tests are tests where you're like very principled about making sure that this component is being tested completely in isolation of all the other components in the system and so these are often like called called mocks or like mocking tests because you often create fake data to go into that to go to that unit in order to test the like basic functionality of that unit and so a lot of people prefer writing solitaire unit tests because they really isolate like what it is that you're testing on the other hand there's some stuff that's really difficult to test with solitary unit tests so there's no one universal recommendation here i think it depends a lot on preference of how you like to write tests and how the team that you're working with likes to write tests sergey do you have a preference between these styles of unit testing i think it kind of depends on the project i don't think there's like a one-sixth fits all but in general i prefer integration testing and test okay another controversial best practice that i think is has some big problems with it but is still pretty commonly used in practice is the concept of test coverage and so the idea of test coverage is you you get a test coverage score for your code base and what the test coverage score tells you is what percentage of lines of code in your code base are being called by at least one test and so the reason why why test coverage is useful is because it gives you a single metric that can give you some sense of the quality of your testing suite the issue with test coverage is that it doesn't really measure what you care about just because you have all it's like very easy to write a testing suite that calls all of your lines of code but doesn't actually like test them in any meaningful way it doesn't test the right things about them so it so code coverage doesn't tell you anything about the quality of the test so i think code coverage is something that you'll see a lot in like open source code bases as you poke around them potentially companies that you work at i think it's not necessarily a bad idea to look at code coverage when you're thinking about the testing strategy for for your code base but it's if you do want to look at it it's important to think about what test coverage is actually measuring right which is it's helping you find parts of your code that aren't tested right now and that's really what it's useful for but the failure mode here is like mistaking a lot of like open source libraries for example will have a proud sort of badge on their github repo that says 100 code coverage and so i think that like sometimes tar like chasing code coverage can be can be an excuse for not actually like thinking about what things you really need to test in your code base okay one more controversial best practice that's i think is maybe more useful as an idea than a thing that like people actually do that much in practice although some people do actually follow this in practice is the idea of test driven development so the way that test driven development works in principle is that you want to create your tests before you write your code and so what that allows you to do is it allows you as the author of the code or maybe even not the author of the code but someone who works with the author of the code the manager or someone else on the team or the user of the code before you actually sit down and start figuring out how to implement the features you write tests as a specification for what you want the functionality of that code to be so if you want the code to do to be able to like average numbers or something then you would write some tests that where like you would feel like okay if the code passes these tests no matter what's in the code then i believe that it can average numbers and so you write those tests before you actually start writing the code itself and then the what the test driven methodology tells you you should do next is once you have the test then you um should go back and run all you run your entire test suite right and so in a perfect world what happens is all of your old tests still pass but your new test fails because you actually haven't written the code to satisfy that test and then what you do is you start writing the code and so you you iterate on like adding minimal new sets of code to your code base to get closer to passing the test and as you do that you iterate on on running that test that the one that you added the one that's failing until you get to the point where that test passes and then in order to complete the project what you do is you then would rerun the rest of your tests to make sure that you didn't break anything in the process of adding that new code and then you're done like refactor your code in any way that you want to and that's the code that you're going to use so i put this under the category of controversial best practices because i think i i don't think there's like too many people that really religiously stick to this methodology of developing tests but it is a useful tool to have in your toolkit if if you're figuring if you're having trouble figuring out like where to get started in implementing something or if you know that the change you're going to make is being is going to be really complicated and so you want to take things more slowly and so this is for me something that i'll like i'll use every once in a while if if i feel like it or if the situation seems to call for it all right the next thing i want to talk about is a another kind of idea in testing
Full Stack Deep Learning - Spring 2021,10,1262,Sofware Testing In Production,software systems that is distinct from the types of tests that we've been talking about so far which is offline tests but is is gaining momentum and is particularly relevant for machine learning systems and that's testing and production so i think when a lot of people hear testing in production this is what they have in mind it's like this this yellow attitude of whatever we don't need to really worry about how our system works we're just going to figure it out when it's in prod and so i think testing and production has a bad reputation but i want to try to defend testing and production a little bit here the traditional view right so the this view of testing and production i would summarize as like the whole point of testing is to prevent shipping software that has bugs in it into production okay so therefore like by definition you have to do your testing online before your system goes into production that's where you test however there's a couple of caveats to that so one is that when people do like meta surveys of software engineering practices one thing that they often find is that automated offline testing the type of testing that we've been that we've been talking about so far is pretty ineffective and on top of that right like the types of systems that we that that like modern software engineering organizations often deploy to production these days which are these kind of like larger service oriented or micro service oriented distributed systems are particularly hard to test because the interactions between the components can get pretty tricky and because the system itself can be difficult to reason about so the case for testing and production is that no matter what you do no matter how well you test your stuff offline bugs are inevitable and so the philosophy of testing production is that you might as well set up your system in such a way that your users can help you find those bugs and so this is a twitter like quote from a twitter thread that i liked on this which is that if you're if you have enough scale so if you have enough scale that the bugs actually come up and you have sufficiently advanced monitoring so you can actually find the bugs and detect them when they do happen then a realistic strategy might be to like write your code push into production and and just take a look at the error rates like no maybe even no other testing other than that and if something in another part of your app breaks then it'll really quickly become apparent in those error rates and so you can just roll that right back and just bring your old version of your code back into production while you work on a fix and so essentially what this is doing is you're letting your monitoring system or your observability system play the role that like a testing suite or a continuous integration suite play in the traditional view of software testing so i my personal take on this is that you probably don't want to take this to the extreme right like you should probably doing be doing some testing of your code offline to catch obvious bugs to catch simple mistakes catch whatever bugs you really can catch offline but as systems become more and more complicated testing and production might become an inevitability and i think that's particularly true for machine learning systems which we'll get into in a few minutes so how to actually test in production there's a few strategies here you can do a canary deployment which just means don't roll out the new version of your software to all of your users right away because if there's maybe there's some really obvious bug and you don't want every single one of your users to have a bad experience because of that but instead just roll it out to a small percentage of your users one percent of your users and and then just monitor those users separately and see what see if their behavior or their error rate is any different than the rest of your users you can make this more formal you can run a like more principled statistical test if you have particular metrics that you know that you care about by running an a b test between the different versions of the code the old version that's currently running and the new version that you're trying to test one strategy that can be really effective is to look at the actual behavior of your real users so to rather than just looking at things in aggregate like only looking at click-through rates for a certain link or something like that try to follow the journey that an actual user takes through your app and look at a couple of those in order to build a sense of how users are experiencing the changed version of this app and that kind of points to a philosophy that is a little bit different here between testing between how most people recommend testing in production and how the typical view of offline software testing works and that's that testing and production often ends up becoming not something that you really try to fully automate but instead something that involves a little bit of exploration so you might you might be gathering a bunch of data about how your users experience your application about how your application is performing against certain metrics and rather than just setting thresholds for all those things and having an automatic criteria for whether you're at your test pass or not you might actually do a little bit more exploration you might look at individual users you might dive down in some metrics and try to see what's going on so there's a bit of a different philosophy there that i wanted to point out because there's some carryover between that and how i think you should approach testing machine learning systems so conclusion on testing in production is maybe it sounds like a crazy idea when you first hear it but as systems become more complex and as your infrastructure becomes more mature it becomes a realistic option to have in your bag and potentially an inevitability okay last thing i want to talk about on traditional software testing is ci cd and so cicd is
Full Stack Deep Learning - Spring 2021,10,1602,Continuous Integration and Continuous Delivery,like really one way to think about it is this is how you you're doing automated software testing this is this is one way to automate tests that you run so there's some software as a service providers that do ci cd and so what is cic really quickly cicd systems are essentially systems that hook into your code repo let's say your github and then when you trigger some action to be to take place so for example when you push new code or when you merge new code into a branch or when you submit a pull request then ci cd platforms take care of taking off a job that is responsible for building like packaging up your code and building it into a package from scratch running all of your tests producing a report that tells you how well your model how well your code did on your tests and then gatekeeping whether your tests can actually whether your new code can actually make it to the next stage so whether it can get merged into your main branch for example based on the performance on the tests so there's a bunch of software as service providers that help with this they integrate with your code repository typically you'll kick off a job on push or on on pull request and tactically often times the way that you'll describe these jobs is there'll be commands like just shell commands that you run inside of a docker container and these tools provide like nice convenience features for storing results for later review caching results that you know that so that you can run things more quickly next time and things like that most of the software as a service providers maybe all of them maybe all the like fully managed ones don't have gpus available so that's one caveat to be aware for machine learning but some of them do have free plans and so i think the kind of default recommendation here is if you're just getting started with this then if you're using github out already then github actions is just really easy to integrate and so i would maybe start there and then explore the others later there's another option to be aware of jenkins and build kite so these are more more manual options you can run them on your own hardware you can run them on your own cloud or something in between you can there's a lot more flexibility about the types of jobs that you can run through these systems if you have gpus you can use them and they're very flexible the trade-off is that they are quite a lot harder to set up and they don't take care of as much of this stuff for you so typically like when i would see a team switching over to a jenkins or a bulk kite is when you've gotten frustrated enough with github actions or circle ci or one of the sas providers with the limitations of those that it's worth putting like some serious engineering effort into rolling your own cicd pipeline so we've covered we've given kind of an overview of software testing in general and now the
Full Stack Deep Learning - Spring 2021,10,1765,Testing Machine Learning Systems,the part that's more relevant to this course which is how do these things apply when you're the software system that you are testing is in fact a machine learning system so one thing you might be tempted to believe is a machine learning systems are just software right they're programs that run on a computer so maybe we can just test them exactly like we test software and i think it's a that's a productive philosophy in the sense that many of the lessons from software testing have analogous counterparts in the machine learning testing world but there are some core differences between traditional software systems and machine learning systems that add some complexity that applies in particular to testing machine learning systems so some examples typically your software system consists of a bunch of code and so there's you can test the code but machine learning systems are really a way of combining code and data into a compiled program and so there's this addition of data so that begs the question of like how do we test the data software is typically written by humans to solve a particular problem so that means that the humans that are writing the software have are going to have good intuitions about how the software works what types of tests that they might want to write for it and and what are the likely edge cases that it might face machine learning systems um are only specified by humans in the sense that humans specify like some proxy metric that they want an optimizer to compile the data down into and so they're the the people who are building the models are not likely to have necessarily good intuitions about what needs to be tested in those models software is prone to loud failures so typically when your approach when your program fails you'll get an error a bug a log statement somewhere when your machine learning system fails it can fail silently so you can just have a degradation in performance that doesn't trigger any errors doesn't trigger any any bugs and that degradation and performance could be caused by lots of different things and then lastly software systems in the abstract tend to be relatively static so in principle is not true at all in practice but in principle like you could just write your code once and then it should just work forever again in practice it doesn't really work that way because your dependencies change and the web changes and all the stuff changes from under you but in principle maybe you could do that but machine learning systems that's almost always impossible because the inputs that are going to that system the data that is going that is feeding into that system constantly changes for most real-world data distributions so it's another layer of complexity that we have to contend with and because of those differences i think there's some common mistakes that teams make when they test machine learning one is thinking of your machine learning system as just a model and and just testing that model rather than testing the entire machine learning system itself and i'll come back to this in a couple minutes [Music] another mistake is not testing the data so in traditional software we test code maybe not entirely clear how we should test data at all and so a lot of teams don't do it in reality it's equally important to test the data as it is to test the code not real like another mistake that i see a lot is not trying to build a granular understanding of the performance of the model before deploying so looking at your model's performance and saying yeah our validation score is below our threshold so this model seems good enough let's ship it in reality it's like building confidence in a model is an exercise of trying to build a granular understanding of where the model performs well and where it doesn't perform well so that you can put guardrails around it in production another common mistake is not is like not really closing the loop and thinking about what is what the relationship is between the metrics that we're optimizing in this model and the metrics that we're actually trying to drive as a business so a lot of team a lot of times machine learning teams will get siloed and they'll think oh my job is to make this model as accurate as possible and they won't really go back and test okay the hypothesis was that an accurate model would do x right drive click through rate or do some other thing and so not closing the loop and understanding does improving our accuracy actually improve the metric that we care about is another state that i see a lot relying too much on automated testing so again our intuition for traditional software is that most of your testing should be automated if not all of it and in machine learning i think that the reality is if you have a system that where you need to spend a lot of time testing it it's not always clear how to write good automated tests for it and so a lot of times concepts like exploring the data exploring the model's performance and testing in production become particularly important for machine learning systems and then lastly testing and production i think is like particularly important in machine learning and is is one thing that i think you almost certainly should be doing and the core reason for that is that data and production can change okay so let's let's focus in on this first one that i was going to come back to so what do we mean when we say that you need to test the entire machine learning system not just the model so here's the system diagram of how you can think of your entire production machine learning system right so in the middle somewhere here is your machine learning model this is like the artifact that's created by your training process it's the thing that you know takes in inputs and produces predictions but that machine learning model exists in the context of a larger system that is the thing that you're actually deploying into production and what that larger system does is it consists of a few different systems one is a training system and the job of the trading system is to take code and data as an input produce a trained model's output the machine learning model is then passed into a prediction system the goal of the prediction system is to it basically wraps the model and it takes data in it preprocesses that data it loads the model loads the weights constructs the model calls like model.predict on the data post processes the output and returns it that's the prediction system this wrapper around the kind of the model itself once you have a prediction system that's actually deployed into production as a serving system and the job of the serving system is to take in requests from users scale up and down to meet the traffic demands and produce predictions back to those users but then the system closes the loop by collecting data from production so both the inputs that the model is seeing predictions that it's producing and any additional feedback from users business metrics or potentially labelers before saving those back to the offline world then there's a labeling system and what the labeling system does is it takes your your raw data that you've seen in production helps you get input from labelers provide labels for that data then that data is stored somewhere in a storage and pre-processing system that preprocesses the data and passes it back into the trading system so this is one way to think about the entire the entire system the entire machine learning system that you're building as as opposed to just the model that you're building and so one way to think about how to test machine learning systems the right way is to think about the tests that you can run for each of the components of this system and across the borders of those
Full Stack Deep Learning - Spring 2021,10,2199,Infrastructure Tests,components first let's talk about unit tests for your training system so we can call these infrastructure tests so the goal of infrastructure tests is really just like you want to try to avoid bugs in your training pipeline so it's really easy when you're you're implementing a new model architecture or you're changing the way that your training loop works or you're working on some other part of your code base to introduce bugs to training pipelines potentially not the one that you're even working on and so how do we do this how do we prevent these bugs one thing that you can do is just unit test or training code like you would any other code just make sure that it make sure that functions make sure you can construct those classes do the right thing another common practice that's i think a really good idea to do here is to add single batch or single epoch tests and what these do is they make sure that for all the training jobs that you care about so all the models that you might want to train as part of your system it's part of your machine learning code base let's just make sure that we can that we can run even like a single gradient step of training on a small data set or maybe a single epoch of training on a small data set and what this will help you catch is just obvious regressions to your training code just like overfitting a single batch would help when you're in the kind of model building and model evaluation station and then how should you actually use these tests the nice thing is that these are unit tests right they're designed to run quickly they're designed to test individual units of functionality of your code base so one way that you can use these is just running them frequently during development so if you're making a lot of changes to your training code just run these periodically and this will help make sure that you're not introducing bugs anywhere that you're not intending to so the next category of tests i want to talk about is
Full Stack Deep Learning - Spring 2021,10,2293,Training Tests,what i would consider like integration tests between your data system and your training system so this is an integration test right so it's meant to test how well your data system and your training system function together so what are we actually aiming to do here i think one way to think about the goal of these tests is really what you're trying to do is you're trying to make sure that your training jobs are reproducible common cause of bugs and machine learning systems and one that we actually faced a lot at openai which is when we started getting serious about testing to begin with is you're you're working on some tasks you're building a lot of models you're able to solve one of those tasks that's great and then you come back a month later and you've made some progress on another task but all of a sudden you can't train your model on that previous task anymore like it just doesn't work and then you have a problem right because if you've made a lot of changes to your code base it's going to go it's going to take you a long time to figure out like what the change was that actually caused that problem and to go back and fix it and that's time that you could have spent building more models shipping more features so we want to make sure that we're able to reproduce our training runs so that we're not preventing ourselves from making progress in the future so how do we do this easiest way to make sure that your training runs are reproducible is just to reproduce them you can pull a fixed data set and just train your model on it or if it takes too long to train your full model it takes days to train your model you can do an abbreviated training run like just train it as for as long as you can train in four or six hours or overnight and that'll often times get you pretty close to proxy for how well the full training run would work and then once you've run that training run then you want to just check to make sure that your model performance on that new train model is consistent with the like the reference right like the model the performance that you had the first time you trained it another thing you can consider doing here is instead of pulling a fixed data set which is typically what i would do so every time you run this test you grab the same data you could also consider doing this with a sliding window of data so if you have new data coming in every week or every month maybe you always pull data from seven days ago to run this training run so this will test something slightly different because you're also because the output of that test also depends on the quality of the data that's going in so maybe you can think of that more as an integration test rather or more as an n10 test rather than an integration test so that's another thing that is worth experimenting with and then tactically like how should you actually run these um i think these don't need to be run frequently they don't need to be run um every single time you make a change or even every single time you merge new code into your code base because they're slow and so you don't really want to slow down iteration speed so i think the best practice here is run these types of tests periodically and i think the ideal thing is just to run them nightly so if something if you wake up in the morning and one of your models in your code base didn't reach the performance that you expected then you knew that then you know that like one of the commits that you made the previous day is the thing that broke your ability to train that model now you could also run it more infrequently if for example if you're not touching the code very often
Full Stack Deep Learning - Spring 2021,10,2484,Functionality Tests,okay so that's that's tests of the training system and tests of how this training system works together with the data system so the next thing that we're going to test is the prediction system itself and so remember the prediction system is a the code that wraps um this trained model artifact that knows how to load that model knows how to pre-process data call the model to make predictions and um post process the predictions to whatever form they're gonna end up taking and so unit tests of the prediction system are called functionality tests so functionality tests unit code unit tests for your prediction code and the goal here is to like really avoid regressions in the code itself that makes up your prediction infrastructure right so you're not trying to catch like issues that are caused by let's say you trained a new model and that new model is worse in some way we'll have different tests for that what these tests are trying to help you do is for a fixed model let's make sure that there's no changes in the code around the model that would cause worse performance how do we do that one thing that we can do is we can just unit test our prediction code like we would any other code but a specific thing that we can do for machine learning code is we can we can actually call these this prediction code on a few key examples where we feel like we we know what the predictions of the model should be and you can just have this can be a test that can run really quickly and you can run it in your development loop just like you do your infrastructure unit tests
Full Stack Deep Learning - Spring 2021,10,2571,Evaluation Tests,all right next category of tests that we're going to talk about are integration tests between your training system and your prediction system so these are tests that are meant to to evaluate how good a job your training system did at producing a new model that performs well when it's placed in the context of your prediction system so really what these tests are trying to do is they're trying to make sure that the new model the train is like ready to go in production and so this is going to be the bulk of i think what is unique about testing machine learning systems how do you go about doing this we'll go into more details about some of these things but at high level you want to evaluate your model on all of the metrics all the data sets and all the slices of data that you care about and you want to compare that performance that evaluation between your old model and your baseline models and then you want to try to understand the performance envelope of your model which at a high level means you want to understand where your model is going to perform well and where it's not going to perform well what types of data might cause your model not perform well and operationally when do you want to run these tests you want to run these tests whenever you have a new candidate model like a new model that you're thinking about okay is this should i put this model into production or not so the important thing to know about evaluation tests is that this is more than just your validation score right so in particular you want to look at not only just your your kind of your loss or whatever the main metric that you're optimizing is but you want to look at all the metrics that you care about and so where might those metrics come from there could be a bunch of different metrics that are associated with your model so instead of just accuracy you might also look at precision and recall and other model metrics there's behavioral tests robustness tests privacy and fairness tests and simulation tests those are some of the other categories and i'll talk a little bit more about each of these so first what are behavioral tests the goal of behavior behavioral tests is that when you test your model in a fixed data set that tells you like okay an aggregate how well is the model doing but what it doesn't tell you is does the model have the behavior that you would expect it to have does it in particular does it treat perturbations of the data in the way that we intuitively expect that the model should treat those perturbations so there's different types of behavioral tests one is called an invariance test and so what you test in an invariance test is you assert that when you change an input that shouldn't affect the output so for example like if you're if you're doing nlp then the sentiment so the example here on the right under category b is testing sentiment and the the invariance that you're trying to test is that if you change the city or the like the location that of a tweet that's like about an airline that shouldn't change the sentiment so like someone's saying oh i had this terrible experience in chicago should have the same sentiment as someone saying i had this terrible experience in barcelona that's an invariance test there's directional tests so category c on the right and what directional tests do is they say okay when i change this input i expect it to change the output of the model and i expect it to change the output of the model in this particular way so for example if you take a really negative word and you make it really positive then you might expect that the sentiment classifier that you're using would flip its assessment of that sentiment from negative to positive [Music] and then third category is called minimum functionality tests and so what these do are they allow you to test whether certain inputs and outputs that you know what result that they should have when they're combined together actually have that result the example they use here is if you write a sentence like i negation followed by a negation like can't or didn't or won't followed by a positive verb recommend or like something like that followed by the thing then there are certain rules that you should expect those sentences to follow about whether the sentiment is positive or negative so you can generate data that follows those rules and tests whether and test whether the the model follows them or not so behavioral testing metrics this is actually a relatively new idea it's mostly used in nlp right now and it was proposed in this this paper that's linked below and so this is not something that you'll see a ton in practice but is a pretty promising idea in nlp in particular that is worth knowing that if you're working in that domain all right next talk about robustness metrics so the goal here is to understand the performance envelope of your model and so we mentioned that what we mean by understanding the performance envelope of the model is that you're trying to build some sense of where you expect the model to do well and where you expect it to fail so one thing that you might study in this category is things like feature importance feature importance doesn't necessarily tell you how good your model is a good model could place a lot of weight on one feature but it does tell you that if the value of that feature is outside of what you expect then you probably shouldn't expect your model to perform well sensitivity to staleness is another like commonly recommended test to run in this category so the way that you test this is you would train a model on data from a long time ago and then you would test it on windows of data moving forward in time and you would plot the curve of how the performance of the model degrades as the data gets more and more stale so you have a you have an understanding of on average for this particular type of data how long does it take this model to become stale and that can be a useful input to deciding whether your model's stale when it's in production [Music] another category of things that i think you'd really want to measure in principle if you could is sensitivity to data drift data datadrift means like that your the data that you're seeing in production comes from a different distribution than what you saw in training and in a perfect world i think what you'd be able to do is you'd be able to just measure the sensitivity to different types of drifts and then when you detect those drifts if you detect them in production then that will give you a good sense of how well you expect your model to perform in the presence of those drifts in the real world this is a tricky thing to measure right now but i think is like underrated and i think more tests will start to take this on this characteristic in the future and then one more category of tests that you can run here is sensitivity between the performance of your model say on an old version of the model and business metrics you can build them up you can build a mapping between like different model performance numbers like how accurate was my model and then how much did this impact the the business metric that you really care about so that in in production when your like accuracy dips by a certain amount how scary should that be in the prediction of the business metric in the future [Music] okay and so this is this is i think not something that you'll see a ton in practice feature importance is pretty widely used in a lot of types of models sensitivity to staleness is a common recommendation from like the systems papers from google and other places that describe how they do testing but i think is not really actually that commonly implemented but i think that this is a form of testing that's going to become more important as we build better techniques to do it another category of metrics that you can test is privacy and fairness metrics a couple of resources i'll point you to here one is the fairness indicators work from google another is this sort of definition of different fairness definitions paper that i link on the right and one thing that you should probably do if you're if you're worried about your model biasing against different subsets of your users or different classes different genders is make sure that you're picking some metrics that will allow you to try to distinguish whether your model is in fact biased against those classes [Music] and then the last category of metrics that i'll point out is simulation tests and so the goal here is if your model affects the world in some way right so if you're building a self-driving car the the action that your self-driving car takes affects the behavior of the other cars and the other people around it recommender systems the recommendation that you make affects the behavior of your users and the types of movies that they'll like in the future if you have a system like that where you're not in a static world and the predictions that your model makes changes the data that it'll get in the future then simulating the the world and using that as a form of test is a good idea if you can do it the main usage of this in practice is in autonomous vehicles and robotics so this is a pretty important category test that's run on autonomous vehicle systems when you push a new version of your computer vision system for instance you'll run a full simulation test that will have some scenarios it'll render those scenarios run your computer vision model on the rendered scenarios take the output of that computer vision model pass it to your control system and drive the car in the simulation and just see what happens the caveat with simulation tests is that these are really hard to do well because you need a model of the world and how the world works which generally we don't have and you also need a data set of what scenarios you're gonna you're gonna evaluate these simulations on and things like that so i think this is an important category of tasks of tests to know about if you're working in robotics in particular i think there's like in my mind the other maybe potential use case of this is recommender systems but it's a lot less clear how you might apply it there but maybe it's maybe maybe you all will come up with a way to do that okay coming back to this concept of evaluation tests so we talked about metrics that you'll use to evaluate the next thing that we're going to talk about is that you shouldn't one thing that you should try to do is instead of just evaluating those metrics on your entire data set in aggregate so just saying we have one score for accuracy one score for f1 whatever you should also try to evaluate those metrics on multiple slices of that data so what is a slice of of your data in the abstract a slice of your data is any kind of mapping of an individual data point to a category so a few examples one example of a slice could be the value of the country feature right so united states mexico canada england so a slice of your data would be all the data points that have that particular value for that category feature another example of a slice could be if you have a more continuous feature a numerical feature bucketing that numerical value into different intervals so ages between 0 and 18 between 18 and 30 et cetera but in general this can also be like arbitrary other functions and it could even be another machine learning model so for example like you might actually train a classifier that tries to detect something that you think is like an interesting category to break out your model performance along and run that a lot and then run that to to slicer data and evaluate the performance of your model on all the slices um yeah so the way that you use slices is basically like when you run your model evaluation for any slices that you have you'll also you'll also break out that that metric along the slices so you'll instead of just knowing accuracy you also know accuracy for for country equals usa you'll know accuracy for country equals canada accuracy for country equals mexico etc so this is a way of like understanding when we look at an aggregate number for accuracy what does that actually mean where is it which subsets of the data is it performing well on which substance of the data is it not performing one really natural question i think to ask about slices is like how should we actually like how do we know what slices to use what slices should we actually even pick slices can be pretty arbitrary there's a couple of tools that are out there that can help you identify slices so if you have a model you evaluated on some data set they can help you find slices of your data that have different performance so one is one is the what-if tool which is a google ml tool and basically this is just like a visual exploration tool that allows you to filter the results of your model slice by different metrics look at the results in nice charts and just play around with the data and try to find places where the performance looks weird there's also some research in the direction of automatically surfacing slices that have poor performance so there's this paper that proposes a system called slice finder which essentially has a clever way of computing your metrics across all pop all possible slices for certain types of data and certain definitions of slices and then surfacing to you the ones where the performance is the worst so in some applications it could be helpful to try to use a tool like that caveat always being with these types of tools that is you have a multiple hypothesis testing problem right so if you're if you are evaluating tons and tons of different slices then the likelihood is you're eventually going to find one that has worse performance okay so we've talked about not just looking at a single metric but looking at multiple metrics we've talked about slicing your data in different ways to try to have a more granular understanding of the performance of those metrics and how they're distributed across your data and then the the last thing that i want one of the last things i want to mention here is that is the data sets themselves in when you're when you're working on a system when you're like prototyping a system building it out over time or when you're working on a system in academia you typically have a single evaluate like a single validation set and a single test set and those are the source of truth for the performance of your model in in like in this evaluation phase i recommend like potentially adding more data sets to your evaluation when it makes sense to so the reason for that is that i think as a matter of principle your main validation set or your main test set should mirror your test distribution like your production distribution as closely as possible and that's because you need some way of getting like a proxy for how well you're actually going to expect your model to perform on the data that sees in the real world now the challenge with that is that oftentimes in real world machine learning systems we're not just using data that's sampled from our test to test distribution you might have some domain shifts or you might have other reasons why you're training or evaluating your model on some other data other than just the production data that you've seen so for example let's say that you are facing a specific edge case in your machine learning system so you find out that your the example i always use is your self-driving car just isn't really doing that well on left-hand turns in the rain at night so one thing that you might do is you might like actually go out and collect a specific data set for that particular education that you can use to train the model on and evaluate it once it's trained one reason to add a new data set to your evaluation suite would be if you've collected a specific data set that doesn't come from your real distribution but measures a particular edge case or a particular category that you really care about another reason to add a new data set is if your like data your production data actually just comes from a bunch of different data modalities so if you're if you're evaluating your system on two geographies san francisco and miami english and french and you expect there to be some difference in how well the model might perform or the type the distribution of data that comes from those geographies or those languages then it could be a good idea to have different evaluation sets for each of those so you can break out the performance and then finally if you when you augment your training data set with data that's not found in production so if you're building a synthetic data set you might want to have a separate evaluation set that is just synthetic data or if arriva you buy a rival self-driving car company and you integrate all of their data into your training pipeline but that data doesn't actually look exactly like the production data that your cars see it still might be helpful training data but you should create a separate evaluation set for it rather than grouping those two things together all right so what does this actually look like when you run it so what is it about what is a report that's produced by an evaluation system look like so what the report looks like is you have reports for each of the data sets that you care about and those reports are broken out break out each of your metrics along the in this case along the y ac along the kind of horizontal axis against each of the slices that you that you have specified for that data set in this case we're measuring accuracy precision and recall we have two data sets our main evaluation set in our nyc user's evaluation set and we're evaluating those metrics on different slices for for each of the two data sets which is also okay so so one challenge here is that you just have a bunch of numbers right there's many metrics here and it's it could be really common for some of these metrics to go up and others to go down and so how should you actually decide whether like when you get a report like this whether this is a pass or a fail and this can actually be really tricky but at a very high level there's two comparisons that you want to make or at least two comparisons that you want to make one is that you'll want to compare the new model to the previous model so to see if this new model is in fact better than the previous model and then the second thing is that you'll want to compare it to a fixed older version of the model so a model from some point in the past that just stays static for a long time even if you roll out a bunch of new versions of of models so the reason why you want to do that is that you when you're deciding whether your new evaluation passes for the new model you're like it's generally it's not always going to be reasonable to say that you have to be you have to have an improvement across all metrics and all slices of your data because there's going to be some noise sometimes some of those subsets will go down sometimes will go up especially slices that have small amount of data the there can be a lot of variance in the value of your metrics on those data sets and so you'll want to set thresholds for each of the metrics and slices that you care about but many of those metrics aside from maybe a few core ones that you care about the threshold might look something like i don't want to degrade my i don't want my model's performance to degrade by more than a certain amount the risk that that exposes you to is that if if you have a threshold on a particular metric and a particular slice that says okay never degrade more than two percent then if you trade a bunch of models and every single time you train a new model and you degrade by two percent then even though you're passing your tests the model is actually getting much worse over time so the way to alleviate that is through this mechanism of having a fixed older model that you always compare performance to and having some threshold there as well another thing another comparison that you might draw is setting thresholds on the difference be in performance between slices so if you have if you have a slice that's users below the age of 18 and a slice that's at users above the age of 18 depending on your problem you might care about there being like some equity or some fairness between the the performance of your model in those two slices and so one thing you might do is set a threshold on the difference in performance of a single model between the two slices all right
Full Stack Deep Learning - Spring 2021,10,3687,Shadow Tests,so next thing we're going to talk about is integration tests between your prediction system and your serving system and i'll call these shadow tests the goal of shadow tests is you you really the high level goal is you want to try to catch production bugs before they hit users so in machine learning systems you know in all systems but in machine learning systems in particular in practice there are like some categories of bugs that are really hard to catch until until you are actually using the real production system so some of the types of things that you might want to try to catch are detecting bugs in the production deployment maybe the code path that you're using to actually build the production model maybe there's some bug there you want to make sure that you catch that before users see that bug you might want to detect inconsistencies between the offline model and the online model so really common source of bugs in production machine learning systems is for in many companies the model itself the model that you're building using tensorflow or pytorch scikit-learn or whatever needs to get deployed in a production environment that relies on using some other language so in many companies there's a translation step between your training pipeline and between your like offline trained model and your online production model could be in the model itself but maybe the more common thing is in the preprocessing pipeline where you might like the the way that the actual code that's used to find those features might might be different code and so inconsistencies can crop up as a result of that and so one really good health check to run is just is my is like actual production model producing the same predictions on a fixed set of data as the model that i have running on my laptop and then lastly another goal here is you can detect issues that don't appear on the data that you have offline for some reason but do appear when you start to look at like real production data and so how can you actually approach doing this this requires some like pretty significant infrastructure to do but you but like typically the way companies will approach this is they'll run the new model in the production system but alongside the previous version of the model and don't return the predictions of the new model to the users just collect them somewhere where they can be analyzed and then once you've collected that data to be analyzed then you just you take a look at it and you look for any consistencies between how the current production models is predicting and how the new candidate production model is predicting or between the offline version of the model and the production version of the model okay so that's tests that you'll run as you're getting ready to take a model that you've tested offline and deployed into production and so the next category of tests that we'll talk about is
Full Stack Deep Learning - Spring 2021,10,3838,A/B Tests,tests that are meant to evaluate the serving system itself so i'll broadly group these into the category of a b tests the goal of a b tests is to test you've already tested like when you ran your shadow test how what is the distribution of predictions that the model is making in the production system like how well is the model doing in the abstract in the production system but what a b tests are meant to do is to test how your users and how the rest of your system will actually react to the predictions that the model is making and so the goal is really to better understand how well the predictions that the new model is making how those affect users and business metrics how do you do this one starting point is you can what's called canary the model which means that instead of instead of sending all the traffic to the new model right away instead you can send like tiny fraction of the traffic like let's say one percent or 10 to the new model and unlike in shadow testing you do actually return the predictions to the users but you analyze those two cohorts separately and see if there's anything weird going on with the the new production model you can also do something a little bit more statistically principled and like actually run a real a b test and that's something that is pretty common to do for machine learning systems as well so again this is this is like a pretty powerful way to test your systems and catch bugs that you might have missed but it does require like pretty significant infrastructure and so it's typically not what you do in the earliest stages of a project one other thing to to note on testing and production is that another way that you should also continue to test your model in production over time as opposed to when you first deploy it is by monitoring that model and seeing how the data that's going into it and the performance that it's producing change and we'll cover that in the next lecture
Full Stack Deep Learning - Spring 2021,10,3940,Labeling Tests,[Music] but next we're going to talk about tests of the labeling system itself so how do we unit test our labeling system so the goal of these labeling tests is we want to machine learning models are garbage and garbage out right so if you have bad labels you're gonna end up producing bad models and so the goal here is to really try to catch poor quality labels before they craft your model how can we think about doing this one thing we can do is like actually put some process around making sure that people who are the people themselves doing the labeling are meeting some standard of training and certification so those people have documentation that they're reading and they have a consistent way that they're actually doing the labeling so that's in some sense one test that we can run to make sure that our labeling system doesn't degrade another common thing to do is instead of just trusting the labels that each labeler produces you can aggregate labels so if you have you can have three people label the same data point and if if if not all three of them agree or if at least two of them don't agree you can throw that data point out or send it back to be relabeled if you're aggregating labels you can use that to assign labelers a score like a trust score so if they're frequently like the third labeler that is not actually the one that got the label then maybe you assign less weight to their labels going forward but i think like maybe the most common thing to really do in practice is to just spot check the labels pretty frequently so when new batches of labels come in just look at a hundred or a thousand data points and just make sure that you would have labeled things the same way that the labelers did no real way to avoid this one one thing you could do to make it more efficient is you can target which which data points you spot check so a simple way to do that is you run a previous version of your model on the new labels and then you look you the ones that you inspect are the ones where the model has the biggest disagreement with what the labeler said the label should be and so if your model is already fairly accurate then that can point out places where you have mistaken labels
Full Stack Deep Learning - Spring 2021,10,4056,Expectation Tests,all right and the last component of the system that we'll talk about testing is the data storage and pre-processing system itself and so the type of tests that we'll cover here are called expectation tests so the tagline of expectation tests is unit tests for data and the goal is you want to catch data quality issues and catch bad data before it makes its way into your training pipeline because once you've trained your model on bad data then it's unlikely that model is going to perform well in the real world how do you actually go about doing this the typical way that people define expectation tests is that you define rules and thresholds and those rules and thresholds describe properties of your data tables and you define those rules at each stage of your data cleaning and pre-processing pipeline so at each sort of pre-processing step the output of that step is like a new version of the data table and then you'll have some rules and some thresholds that apply to the output of that step you'll run those rules and thresholds and you'll you'll fail if if the thresholds are not met and typically you'll run these like every single time you run your batch processing your batch data pipeline processing jobs there's an open source library that is gaining a lot of popularity for doing these types of data expectation tests it's called great expectations and yeah pretty apt name and the the types of things that great expectations allows you to define so the types of assertions are more or less like kind of hard rules on things like that your data should satisfy so for example for a particular data table you might know that okay i really i really know that i shouldn't have any null values in this column so that's an assertion that you can write let's fail if there's any no columns they can be a little bit more fuzzy than that right so you could say okay i think that all the values in this column should be unique so this is a unique id if we have duplicate values here then that indicates a data quality problem it could be in addition to properties of each individual data point it could be properties of the whole table it's the whole table you might expect that if let's say that you're the table is an input to a machine learning training job then maybe you should fail out if the data set is too small because that means that your training job wouldn't have produced the right thing anyway but it could also be statistics about the values of the columns so for example you can test that the mean of the call a numerical column is within a certain range you can test that the range of values is not outside of a certain range or the median is between two values things like that so these are kinds of expectations that you can set so the types of checks that you can run next question you might ask is like how do we actually set the thresholds for these checks so like how do we know if if we really can't tolerate any null values or how do we know what's the maximum mean of this column that we should expect this column to have so typically the way this is done right now is pretty manually right so users will go in and specify things that they know should be true about the the data set and you can define these things iteratively over time add more rules as you go [Music] another approach you could take is to profile the data set and so what that does is it computes a bunch of statistics about the data set what how many values of each type are in each column if the column is numerical what are the statistics of the data in that column things like that and then you can set thresholds based on that profile so you might for example say that okay i know i know that the range of data in this table is like between zero and one and so i'm just going to set that as my expectation going forward anytime that data comes in if that data is not between zero and one then i'm going to throw an alert and so that can that can work okay but oftentimes that produces like unnecessary alerts because the statistical properties of these data sets just vary and better approach here is to do both right so maybe you generate a profile from a data set that you know is good and then you go in and manually tweak some of those thresholds that you don't really trust from the statistical profile and then over time as new data comes in and you get more alerts you go in and tweak the thresholds even more
Full Stack Deep Learning - Spring 2021,10,4303,Challenges and Solutions Operationalizing ML Tests,okay so that's expectation tests and so now we've covered unit testing each of the components of the machine learning deployment pipeline and integration testing many of them as well so next thing i want to talk about is actually operationalizing these tests so running them in your production environment what are the challenges that are going to come up so one is organizational i think in the software engineering world testing is like pretty established methodology and most software engineers that are not super contrarian all share the belief that like testing is good and we should be doing testing and we should have organizational practices that like help make sure that we're writing tests and writing good tests data science teams are not always that way so because of how the field evolved this is not something that a lot of machine learning and data science teams are used to doing and so there can be some organizational friction to adopting tests of your machine learning system there's some infrastructure challenges as well so a lot of the ci cd platforms especially the sas platforms that we talked about are not particularly well tailored to these types of jobs these machine learning model evaluation jobs that we talked about so for one most of the sas platforms don't support gpus two there isn't really there aren't really like great integrations um or primitives for data i o so if you wanna load a big data set into your continuous integration pipeline not always like a super easy way to do that but some of the more manual ones like jenkins you can if you put enough effort into it you can configure it to do all this stuff just fine it just ends up being a bigger infrastructure lift tooling i think is still a big challenge here so particularly in like the finer um details of evaluating comparing model performance finding slices that have different performance telling at a very granular level how the performance of two models differs what down all the way down to the individual data point level there's a couple of tools for this like slice finder and the what-if tool that we mentioned but i think better tools are still coming here and then lastly and we've alluded to this a little bit is decision-making aspect of it right so it can be really hard to tell when your test performance across all these metrics and slices and data sets that you're evaluating is really good enough to go into production and so this is just something that like all machine learning teams will have to grapple with and hopefully as your infrastructure gets more mature and you can test more in production then this will become less of a concern so to conclude what are like how do we boil this down to recommendations what is the right way to test machine learning systems so one kind of takeaway here is that you really like when you think about in principle how you should test your machine learning system you should really be thinking about testing each part of the broader machine learning system not just the model itself right the the thing that you're building is a machine learning team is the machine that builds the model right not just the model and so you need to test all the components of that machine in addition to testing code you should also be thinking about testing data potentially using expectation tests and testing model performance using evaluation tests so it's not enough to just test the code another takeaway here i think is which is maybe an unsatisfying one is that i think testing model performance right so doing evaluation testing at this point is still more of an art and science so there's unless you know exactly what you're looking for in model performance you're gonna have to spend some time like poking around the results of your model evaluation and building an intuitive sense of whether you think the model performs as well as you would expect a production model to perform and i think like maybe a a parallel to that is that when you the goal of testing model performance really should be to build that understanding to build that understanding of the performance envelope of your model right what parts of the data space do you expect that model to perform well on and where do you expect it not to perform well because the reason why that becomes important is because you can make that use that to make decisions about promoting your model into production but also because it can help you prioritize things to improve about the model and potentially help you put guardrails around that model in production then lastly like maybe the one like another practical recommendation here is this i i give you a lot of ideas of different ways that you can think about testing your machine learning system that doesn't mean that you need to like try to implement all these from the beginning or use or even ever use all of them potentially and so the approach that i would recommend to take here as you're building out the testing for your machine learning system is similar to the approach of building the machine learning system to begin with which is to build up gradually and i think the most sensible starting points here are doing infrastructure tests so testing the the code um that is used to train your models testing to make sure that short versions of your training runs are able to overfit a single batch this is really easy to implement and will catch a lot of bugs even before you start deploying your model into production so that's i think why it's maybe one of the first things to do evaluation tests this is a little bit harder to do because it involves this tricky question of what metrics and slices do we want to evaluate on but is really important because aggregate performance does not tell the whole story so this is this is another thing that i would start doing relatively early in the life cycle of your production machine learning system and then finally expectation tests i think now there's good libraries out there to help you with this and expectation tests can catch a lot of bugs so i think it's it's another like relatively low hanging fruit when you're building out the tests for your machine learning system so i'm gonna next thing i'm talking about is explainability sergey are you doing a lab today i was going to but it's getting late i think i'll just it's live on github i think i'll record a video maybe tomorrow morning okay cool maybe i'll just power through them next thing i'm going to talk about is explainable and interpretable ai okay so
Full Stack Deep Learning - Spring 2021,10,4649,Overview of Explainable and Interpretable AI,this is maybe a buzzword that many of you have heard so one thing we're going to talk about is what does this even really mean so i'm going to propose some definitions caveat here is that these are not really like industry standard definitions as far as i can tell the terms explainability and interpretability are often used very loosely and are often used interchangeably that being said i think that there's like actually a real distinction that can be made between different types of interpretability and explainability and some of the other concepts that we've been talking about and so i'm going to use some definitions to help like make that conversation a little bit clearer the first definition which is actually which is very much not industry standard this is just one that i came up with when i was making this lecture is idea of domain predictability and so what i mean by domain predictability is the degree to which it's possible to tell when you get a new data point or when you get some new data points whether that data is outside of the circle of competence of your model how knowable is it what data your model will perform well on or where it might where there's very good chances it won't perform well next definition is interpretability and so when i say interpretability what i mean is the degree to which we as a human can predict what result the model will have on this data so it's about how well can i end like how good a model do i have in my head of what this model is doing can i predict whether like what it's going to do on this data and then the third definition is explainability and so when i say explainability what i'm going to mean is the degree to which a human can understand not only predict what the model would do on this data but also understand the cause of the decision that the model is going to make okay so next thing i'm going to talk about is i'm going to give a very high level overview of some of the different categories of techniques that that you can use to try to make or that have been proposed to try to make models explainable or interpretable in the literature and we'll talk a little bit for each of them as which of those definitions they correspond to and so the categories that we're going to talk about are using interpretable families of models so using families that are inherently interpretable distilling complex models down into simpler ones that are interpretable understanding the contribution that individual features or feature values make to the prediction understanding the contribution and then understanding the contribution that individual data points make to the prediction
Full Stack Deep Learning - Spring 2021,10,4800,Use An Interpretable Family of Models,okay so using interpretable families and models so some interpretable families of models are linear regression logistic regression more generalized linear models and decision trees right so these are interpretable families of models because if you understand the math of how these models work then it's pretty easy to understand like why they made a certain decision for a linear regression for a logistic regression model you're multiplying a weight matrix by vector and so you can understand the contribution to the to the output of the different feature values and weights and then that's your explanation that's literally how the model made the decision so the big benefit of these types of models is that they're both interpretable and they're in some sense like actually truly explainable with the caveat that they're really only explainable to the right user so if you have studied linear regression and you have a regression model and that model makes prediction you can understand the cause of why the prediction was like a certain thing if you're giving if you're giving that as an explanation to to a user that is non-technical for example that might not actually be a good explanation big disadvantage of these categories and models is that they tend not to be very powerful so this is a deep learning class you'll notice that there's no deep learning models in this category one thing that you might ask is what about attention isn't attention interpretable because one thing that you can do is you can you can visualize the attention map so you can look at like for each output that the model is producing whether it's text or or something else let's actually just visualize the attention weights on the input data and maybe that's our explanation for why the model made the prediction so the nice thing about this is i think this is actually like pretty interpretable in some sense right like this i think this is this can be a very helpful tool for building intuition about why a model makes like what kinds of predictions a model is going to make on certain types of data big drawback here is that i think using this as uh as a way to get explainability so understanding of why certain predictions are made is a trap that a lot of people will fall into so the reason why attention maps do not produce explanations is because so a couple reasons one is they're not complete explanations right so in the example we looked at before let's look at the the kind of top middle image example where we have a dog that is standing on a hardwood floor and you look at the attention map that that corresponds to it's the dog first of all the dog's not actually standing so there's something going on there it's not quite right and then second of all it's the attention map is like pointing mostly to the dog so i i think i get why it says a dog but it doesn't really and then i see how there's a little bit of like a lighter area on the hardwood floor so i could see why there's a hardwood floor but i have no idea why it says it's standing and i have no idea why you know why there's like some weights to the attention map that are on the like cloth fabric of the couch that the dog is lying under so one issue with using attention for explainability is that it does not produce complete explanations another problem with using attention for explainability is that the explanations that it does produce are not reliable there's a a paper that explored among other things like whether these types of attention maps change under like different types of evidence that you're trying to ask them all to explain and so they use class conditional models and so for example like in this image of a siberian husky they asked the model to explain what is the evidence for this animal being a husky or what is this animal what is the evidence for this being a flute and the model had almost exactly the same attention maps right so attention maps just show you what the model is looking at that is not the same thing as why it's making the decision it's making them so i think a lot of people really complete these like in popular literature about deep learning models and so this is a pet peeve of mine attention maps are not explanations so we talked about interpretable families and models next thing that next category of
Full Stack Deep Learning - Spring 2021,10,5029,Distill A Complex To An Interpretable One,techniques that is worth looking at is instead of just starting with a simple model and training that and using the interpretation that comes alongside that let's start with a complicated model that solves the task that we want to solve and then let's distill it down to an interpretable one that we can use to create interpretations and so the main category of techniques here is centers around surrogate models so the idea behind surrogate models is that you have once you train the model that is actually going to do your prediction task then you take the data it was trained on and you train a another model a simpler more simpler interpretable model on that data typically what you'll do is instead of training the model to predict the actual labels which would just be the same as like training an interpretable model from scratch instead you'll train the interpretable model to predict the predictions of the model that you actually care about so you'll train a model that maps inputs to predictions of another model and then when it comes time to interpret the model's predictions you will use the surrogate's interpretation as a proxy for trying to understand the underlying model nice thing about this technique maybe the main nice thing i can find to say about it is that it's very easy to use it's very general works for any model you can just fit a surrogate model to it big question mark here in my mind is if your circuit is really good like why don't you just use that right why don't you just use that as your model and if it's not very good right if the surrogate doesn't do a very good job of accurately predicting the predictions that your underlying model made then why should you trust its interpretations at all and then on top of that like maybe in a more existential way if your aim is for explainability not interpretability then how do we know you know if our goal is to understand why a decision was made how do we know that the simpler model makes the decision in the same way that the more complicated one does another category of surrogate models that is really commonly used in practice is local circuit models and the most common technique here is called lime the way that these work is that instead of trying to explain the model globally instead what you do is you pick a data point and you say i want to create a local explanation around this data point and the way that you create that local explanation is you you take that data point and you perturb it around in the input space you you sample values from around the value of that input data point and then you run your prediction model on all those perturbed data points and then you fit a surrogate model a simple model linear model or a decision tree to the mapping between those perturbed data points and the prediction that your original model produced so this kind of makes more sense conceptually to me than than trying to train a global surrogate model because it's at least reasonable to believe that you're around in the neighborhood around some data point that your surrogate model should do something like should be able to do as well as your global model does right because your global model is trying to solve a much harder task than the surrogate model is trying to solve this technique i think is like pretty widely used in practice from what i've seen like for folks that are doing interpretability it seems to be one of the most common ones and one of the other nice things is that it works for almost every data type so there's extensions of this for text data and image data as well and that'll get you nice maps that show you like what parts of the data like that explain the neighborhoods of data the challenge with lime is that it can be really hard to define these perturbations so how do you sample data points from around your original data point like what is the right neighborhood what's the right distribution and then in practice there's some research showing that the explanations that are produced by these these circuit models are pretty unstable so if you make small changes to the input conditions it can produce big changes to the explanations and as a result of that they can also be pretty easily manipulated so if someone knows that you're using the explanations to make a decision it's not that hard for them to choose the input conditions to produce the explanation that they want but that's local circuit model slime okay so we talked about interpretable families and models we talked about distilling models down to interpretable models next kind of category of techniques and there's a lot in this category is trying to understand the contribution
Full Stack Deep Learning - Spring 2021,10,5272,Understand The Contribution of Features To The Prediction,that your input features make to the production of the to producing the prediction so one way that you can do this is through data visualization so there's a couple of types of plots that people use here one is a partial dependence plot where you you look at the values of a particular feature and then you kind of correlate those against the output values of your prediction function and so you can see okay where how much does the prediction function change as i change this one input feature where does it change what is the what does that function look like and then there's another visualization that is some will commonly use called individual conditional expectation which is doing a similar thing which is like looking at the how the value of a particular feature corresponds to the probability or like the result of the model's prediction but instead it's doing this kind of like messy looking visualization where it produces one line for each data point individually so you can start to get a sense of the distribution of how these things evolve in terms of more like numerical ways to evaluate the importance of features one common one is permutation future importance the way that this works it's pretty simple you pick a feature that you want to measure the importance of and then you basically apply a permutation to that in your data set so you randomize the order of that feature in your data set and then you evaluate your model on the original data set with the order of that particular feature randomized and you see how much that degrades the performance this is another technique that's really easy to use and relatively widely used in practice it can give you decent proxies for like how important different features are challenges with it are it doesn't really work very well for high dimensional data so if you're working with images you probably don't really care too much about the individual feature importance of a particular pixel in your image and it also doesn't really capture the dependence between features but it is a useful tool to have in the toolkit there's a more principled approach to to trying to explain the importance of an individual feature to the model's predictions called shop and so i'm not going to cover this in detail today but the the very very high level explanation for what shap does is that you're trying to measure how much the presence of this feature so think about this in the case of binary classification like whether this feature is one or zero how much does that affect the value of the classifier independent of what other features are present so you're controlling for the values of all the other features and then you're testing how much a change in this feature affects the performance of the model the nice things about this method is that it also works for a variety of data there's extensions for image data text data it's pretty mathematically principled you get you get feature importances that are like normalized and add up to one which is nice the disadvantage is that it's pretty tricky to implement and still doesn't it's still really more of an interpretability tool than anything else but this is another like pretty widely used interpretability technique in practice saliency maps are another common way of evaluating feature importance one of the most common tech uh classes of saliency maps is gradient based saliency maps the way that these work at a very high level is you pick some input like for particularly particular input value say you pick a particular image you perform a forward pass of your model and then you take the gradient of that output with respect to the pixels and visualize that gradient so what you're measuring is is how much does a unit change in the value of each of my pixels affect the prediction that the model makes and that's where you're computing and then visualizing nice thing about this is it's easy to use there's many variants that produce like more interpretable results more principled results a lot of techniques in this category pretty widely used at least in the research world the challenge here is i think similar to the challenge with using attention as a as an explanation right which is that how do you actually know whether this is the correct explanation what is the correspondence between this explanation and the actual way that my model is making predictions and on top of that it's pretty fragile very sensitive to small changes and it can be unreliable okay so that's a very quick kind of lightning overview of some of the main categories of interpretability and explainability techniques there's a good book on this online that i will link to at the end of the slides so zooming out a little bit we talked a little bit about different definitions of interpretability or explainability or just reliability of models then we talked about some of the techniques just give you a very quick flavor of what are some of the techniques out there to make models interpretable or explainable oh sorry one more class of techniques that's worth mentioning which is
Full Stack Deep Learning - Spring 2021,10,5556,Understand The Contribution of Training Data Points To The Prediction,trying to understand the contribution instead of features to the prediction instead understand the contribution of individual data points to the prediction so what are the important data points to to this model or to this even to this prediction itself two classes of techniques here that i'm not going to go into detail on because they're not commonly used yet as far as i can tell in deep learning but are more common in traditional ml one is prototypes and criticisms where you essentially try to cluster your data points into kind of clusters or prototypes which are at a very high level like clusters that explain a lot of the variants of the data set and then criticisms which are data points that are not very well explained by the prototypes so that's one category of techniques that is used in machine learning maybe not a ton of it yet in deep learning and then influential instances so essentially like looking for data points where if they were removed from the data set it would cause a big change in the classifier that ends up getting created those are other just like categories explainability techniques that are worth knowing about in the abstract if they end up being useful for your problem
Full Stack Deep Learning - Spring 2021,10,5619,"Do You Need ""Explainability""?",okay but getting back to the point i was making i think the high level question that i always ask myself with these techniques is is explainability really the right goal when people say explainable ai we're trying to do we're trying to build models that are interpretable or explainable why is that and like how does that map to the actual techniques that we should choose so there's a few different reasons why you might want your model to be explainable right so one is that in some industries uh the regulators the regulators just say your model needs to be explainable and so i think the the rational response to that is what do you mean by that what does it really mean for a model to be explainable um not sure anyone really has a great answer to this but i think maybe the pragmatic response is you should probably do some explainability to make sure that you're in compliance with the regulators another reason why people give for wanting explainability is because they say my users want want explainability right like they want to trust them all as predictions and so they need us to be able to explain why the model is making the predictions that it's making so a couple of things to think about here one is is it the the model doesn't really exist in isolation when it's interacting with users good product design can alleviate some need for explainability by helping helping have guardrails around the predictions the model is making so that users are more likely to trust the ones that they do see even something as simple as if you're if you're like if you're working with doctors let's say like just allowing them to override the predictions a design choice like that in your product could make it a lot neces less necessary to have really good explanations of the decision the model is making [Music] another consideration here is coming back to the discussion that we had earlier so how often are your users going to be interacting with the model so i think when people like think about the need for explainability so we really need to be able to understand why the model made the prediction that it made i think the place where users actually really need that is when like they're interacting with the model very infrequently so if you're like if there's a model that is like making a big decision like whether you get approved for a loan or not or for a mortgage or not and that mod that decision decisions being made by a machine learning model that decision has a big impact on the user and the user doesn't doesn't get to play around with that model and understand why it makes certain predictions then being able to have a good explanation a reliable explanation for why that prediction was made could be really important on the other hand if you're in a situation where your users are going to be interacting with your models a lot so you know if it's a recommender system or if it's something along those lines then i would just ask yourself the question of maybe what we really should be going here for here is like good interpretability techniques so like good techniques to help our users build intuition about what types of things the model does well and doesn't do well and at the extreme if like your model is really only going to be used or primarily going to be interacted with by like machine learning developers who are going to interact with the model a ton then maybe you don't need it at all another reason that people give for needing explainability is that i don't know if i should trust my model like how do i decide whether i'm really confident enough in this model to deploy into production and here i think is like where i take issue with the use of the term explainability or like the goal of having of having models that are explainable are predictable because i think what you really want here is domain predictability right if you're an engineer building a system deciding whether to deploy into production the question that you really want an answer to is am i confident that i have a good performance envelope around this model right where if something goes wrong in the inputs to the model or to the data that the model is consuming then i want to know i don't want there to be i want there to be as few like unknown unknowns there as possible so i think in my mind that's a separate goal then having been really trying to explain the model's predictions or have interpretable understanding of the model's performance that being said some of the visualizations that are created in the interpretability literature could be helpful for building for creating domain predictability or even just for debugging when things go wrong so that doesn't mean that this is not a useful field if you're in the category of trying to create domain predictability one other question i want to touch on is is it possible to explain deep learning models and the conclusion that i've come to here as a non-expert in this field but someone who has been trying to pay attention to it for the past couple years is not really right none of the known methods that we have for explaining or even interpreting the predictions that models make are really that faithful to the original model performance or at least not faithful to the original model performance in a reliable way like it's very easy to use these techniques to cherry pick examples of oh i see saliency maps that show me that the neural net is focusing on the ball when it predicts that this is a ball in a field but that doesn't necessarily mean that those saliency maps are going to be useful for every single prediction that the model is making and on top of that i guess related to that is that these explanation methods tend to be pretty unreliable right so they tend to be a little bit fragile sensitive to inputs and things like that and they really don't as i alluded to before they don't in general like fully explain the model's decisions so even if you have a faithful interpretation that is reliable oftentimes that visualization like that saliency map or that attention map only tells you part of the story of why the model made the decision that it made so the conclusion that i have reached and you all should read some of the links that we have in the literature here and make your own decision is that explainability is not really a realistic goal for deep learning systems right now in you know mid-2021 i think there's a lot that we can do to make deep learning systems more interpretable and that might be a worthy goal if that is something that contributes to the project that you're trying to create and so the in conclusion like what i would say about explainability and interpretability is if you really need if you genuinely need
Full Stack Deep Learning - Spring 2021,10,5972,Caveats For Explainable and Interpretable AI,to explain your model's predictions if you genuinely need to understand why model predicted things in a certain way then really the only answer is right now is to use families of models that are interpreted to begin with so linear models decision trees things like that and you can try to explain deep learning models you can even train a model on the weights of a deep learning model to output an explanation sentence there's like all kinds of things that you can try here and those can produce really cool results and has a really interesting research direction but it's quite unreliable and not ready for production if the goal is true explainability in my opinion interpretability methods like shap and lime in particular seem to be the ones that are used most in production one thing that those i think those are really useful for and where i would strongly consider using them in a production system is in helping the users of your models make quicker get to their threshold of interpretability that they need to trust them all as predictions faster those visualizations can help them build intuition about model performance and on top of that like that's none of this i'm somewhat skeptical of the the value of this field as applied to deep learning in particular but none of that is to say that that these visualizations are not useful in fact i think many of the visualizations that are created here can be useful for things like debugging and like prioritizing where to improve model performance i'm just not sure that they're really ready to produce like accurate explanations of model performance all right and that's all that i have for today
Full Stack Deep Learning - Spring 2021,12,0,Introduction,good afternoon or whatever time it is for wherever you are um excited to uh head up this lecture today so most of this class has been focused on bringing deep learning into practice what does it take to build real applications today we'll actually do something quite different we'll look at research now first thing you might wonder is in a class it's about practice why would you even care about research um i think of all disciplines deep learning is probably the one where research and practice are the closest together where very often something new gets invented in research and within less than a year uh it might already be put into practice and so um it's it's good to be aware that things are always changing and new things come in through research that you might want to incorporate in whatever projects you're working on now when we look at research this is a graph showing the number of papers posted on archive which is a repository where people post their machine learning and ai papers as well as other topic papers but it's just listening to machine learning and ai papers posts on archive per month and so the top dot there that blue dot is roughly 4 500 in one month which is a lot and the curve is going up uh still so there's kind of no way you can read every paper that gets put up at least i don't think anybody can do that so you need other methods to keep up with research so what i'd like to do today is give a bit of a sampling of research directions this is not comprehensive in any way but it can give you a sense of the kinds of things that have been happening in the last couple of years that might be of interest to you and a representative of other things that could be happening um step back a bit and reflect on the overall research theme that runs across all these samples and then give you a little bit of advice on on how to keep up with the well relentless flood of new research so there are many many exciting directions in ai that people are doing research on i'm listing a bunch of them here there's even more what i thought i'd do is give you kind of a quick taste of a few of them the ones listed in the left column but feel free to also ask questions about the others at the end of the session i'm happy to talk about any of them but i only have slides prepared for the ones on the left so the first one is unsupervised
Full Stack Deep Learning - Spring 2021,12,160,Unsupervised Learning,learning and i think this is in many ways the the most intriguing one because up to maybe three years ago unsupervised learning was i would say a pure research thing and nobody would really be putting it into practice and then as of about two and a half years ago things started working so well that it pretty much immediately became practice so deep supervised learning the kind of more default way of doing machine learning it works uh but it requires a lot of annotated data you need data where you know what the input is and the corresponding output and so the question is can we get around it can we learn from data that's not all labeled or even you know random other data and the answer is yes and there are two main approaches deep semi-supervised learning and deep unsupervised learning which are already being put into practice today so what is semi-supervised learning the semi kind of stands for it's half supervised half unsupervised and so we'll still have a classification problem and in this toy example each data point will belong to one of two possible classes the yellow class or the blue class and you get to see all the data points and if you look at the shape of how the data points are laid out you might say hey even if i don't know for most of them what their label is because i'm only seeing for five yellows and five blues where they are the others were not given you might still say hey i see two half moon shapes there are two banana shapes um probably the top banana is um is all yellow and the bottom banana is all blue and that's the most consistent way to complete this labeling and now the question is what intuition did you use when you came up with that idea of how to complete the labeling one way to formalize is to say hey if anything is close to a labeled example so for example here we have yellow one and then this empty one is very close then likely that one's also yellow and that way you can propagate the labels out from where they're given to the neighbors to the neighbors of the neighbors and what you'll see is that it will not propagate across this empty space here but it'll propagate through that densely filled shape on the top and yellow and then densely shaped shape at the bottom in blue now this is easy to do with an example like this where you can just look at it and annotate by hand if you want the question is how do you generalize this to let's say image classification it's an approach and there's many approaches there's one approach that i want to highlight here that has gotten some of the best results called the noisy student approach you train your teacher model with labeled data so you start with labeled data train a teacher model then there's unlabeled data on the unlabeled data you infer what you call pseudo labels they're not real labels it's labels that you get from using the neural network that you trained the teacher model you use that to now label the unlabeled data now those labels will not be perfect because you train on a small amount of labeled data but you could essentially see where you are more confident about those pseudo labels and wherever you are more confident you could inject that into your training set as additional label data and that is essentially the same thing as what would happen here in the two yellow blue shaped data set you would be very confident about the neighbors because they're close to the labeled ones but less common about the others and so you go you grow out from the labeled ones to its neighbors and from there keep going that's exactly the idea here there's another idea that's at play here that when you when you then re retrain you train with dropout data augmentation and stochastic depth effectively you're injecting noise um and you try to match those pseudo labels while training a model that has a lot of noise in the training process to make it more robust and generalized better which you might often do anyway but that's just part of what they did here this is just from 2020 by the way so this pretty recent result let's look at the performance curve so the horizontal axis here is number of parameters in the neural network and the vertical axis is top one accuracy on imagenet so classification accuracy and the blue curve is um efficientnet fully supervised and the red curve is if you use the semi-supervised approach and you can see that actually you can do substantially better and so it's a simple idea to propagate out your labels and then retrain and repeat but it can actually significantly improve your performance now this makes a little bit of an assumption though which is that the labels that you used in the supervised data set are still valid for your unsupervised data but most images on the internet that you download might be in other categories maybe you're trying to categorize animals but you download a lot of images of also people and objects and maybe street scenes from self-driving car data sets and so forth and so you if you then go label it might not be that meaningful so there's a limit to the applicability here it assumes that the unlabeled data is roughly from the same distribution as the labeled data so the recent results are unsupervised so not semi surprised unsurprised where you don't make that assumption anymore the idea here is well let's train a neural network again and we'll have two tasks so we'll give the neural network two heads ahead for task one ahead for task two and the idea is that the main body the trunk here of the network is most of the network so most parameters live in the shared trunk and so when you train for task one and task two most of the learnings are shared and only a little bit gets specialized to task one versus task two you can do this for any task let's say two supervised tasks uh maybe you have image segmentation and image classification or image segmentation and depth prediction those could be two different tasks or per pixel you do something different but in the setting we're looking at now it's going to be one task going to be unsupervised which you don't have to provide labels and the other task is going to be the task you care about that's supervised so the key hypothesis behind this is that if for task 1 which is unsupervised if your neural network is smart enough to predict let's say the next frame in a video or the next word in a sentence or generate realistic images by let's say denoising an image or translating image from one type of image to number for example grayscale to color if it can do that then whatever it learns to solve that task whatever is learned in the trunk will also help it to then be ready to do supervised learning from a very small label data set for the task you actually care about now you might wonder if there are two heads to the network why is one head called unsupervised the other one called supervised it's i mean from a neural network training perspective when you run it there's a loss function and the loss function will look pretty much the same whether it's unsurprised or supervised it's more of a kind of pragmatic thing on the unsupervised head you supervise which is of course kind of funny on the unsupervised head you supervise with signal that does not require effort so next frame in a video once you have the video you don't need to have somebody annotate the video the next frame is readily available next word in a sentence if you have a document the next word in a sentence or in a document is readily available nobody has to go in and annotate this will be the next word same for images if you have color images you turn them into grayscale you still have the color available so going back to color is something for which you have it readily available no human annotation effort required and that's really the kind of core of the unsupervised learning paradigm is you want to take out the human annotation effort as much as possible so task 1 will be unsupervised task two will be the real task for example task one could be predict the next word and task two could be predicted sentiment this is a positive or a negative sentiment article maybe you're a stock trader you wanna know when whenever company gets mentioned anywhere on the internet whether it's a positive or negative sentiment mentioned because that might affect what you want to do with your uh positions in the stock market so the unadvised task would be next word prediction but in itself is actually quite interesting even independent of the fact that later will be used with supervised learning here's an example from openias gpd2 which was the the first such model that was really impressive of course since then has been gpt3 but i think gpd2 is kind of the landmark where it started to look like this really really works once you have trained for next token prediction you can then give it a prompt and this case was trained on a lot of documents downloaded from the internet to predict the next token in the document from seeing everything that comes before and now it's asked a train carriage containing controlled nuclear materials was stolen in cincinnati today its whereabouts are unknown what comes next and here it is the incident occurred on the downtown train line which runs from covington and ashland stations in an email to ohio news outlets the us department of energy said it is working with the federal railroad administration to find the thief then a quote from the u.s energy secretary and then something about you know top priority and so forth this is exactly the kind of story that you would expect if it starts out with that first paragraph is it grounded in reality in this case not so it's it's fake it's a fake story but it reads like a real story and that's what the neural net learn to do is how to complete articles in a way that matches what's seen on the internet here's another one recycling is good for the world no you could not be more wrong that's the prompt and it starts completing and i'll do this one token at a time but i'm showing them all in one go recycling is not good for the world it capitalizes the knot it's bad for the environment bad for our health it's bad for our economy i'm not kidding recycling is not good for the environment so it goes on and on it's a bit of a rant it's a bit weird um but that's somewhat of a natural completion given it started with this weird prompt and so very realistic in fact so realistic that there were a lot of news articles at the time saying hey wow this is so realistic and then open the eye said hey we cannot release this engine because maybe people will abuse it and so the text this model is so disruptive it's deemed too dangerous to be released in the wild for people to use um it i can tell you from from this experience here that we saw play out that um you know there's no better way to get a lot of press coverage than to say your ai that you build is too dangerous to to release um never seen as much press coverage before but actually um while maybe it seems to get sidetracked by the press coverage something really powerful happened under the hood with these kind of models you can do sentiment classes together so you can for example classify a review here and as it goes through it from having just seen a few examples it can decide whether or not this is a positive or negative or human the beginning was positive the later part was negative it can solve other problems here is a common sense reasoning scheme winograd schema challenge the trophy doesn't fit into the brown suitcase because it is too large okay that's our sentence the it here what does it refer to the correct answer is the trophy because the trophy is too large to fit into the suitcase but now one word is changed large becomes small it is too small well now what is it is the suitcase because the suitcase is too small for the trophy to fit into it and that the fact that the gbd2 system can figure this out shows that it's doing some pretty sophisticated reasoning about this sentence and has some notion of understanding of what it really means because changing just one adjective will make it change its response to the question what does it refer to there are other questions they can answer who wrote the book the origin of of species charles darwin that's correct what's the largest state in the us by land mass the correct answer is alaska it said california so not correct it's not perfect but it can do things pretty well now here's a table of a bunch of benchmarks it was evaluated on the hour result is not my result i'm not involved in this is just taken from from the paper where the authors called it our result but what it shows here is in bold phase this the best result which is the result achieved with gpd2 better than the previous record on all tasks the details of the tasks don't really matter what's so interesting about this result is that it was kind of the first time that a model was trained unsupervised on a lot of text to predict the next token and then fine-tuned on specific supervised tasks and then beat prior methods that might have been more specialized to each one of these supervised tasks so one model very generically trained then fine-tuned and is able to beat more specialized models and so that's really really interesting to have such a generality of the model that's trained another very interesting uh thing that was shared uh in the paper was that as you grow the number of parameters of the neural network horizontal axis in all these plots here vertical axis is performance on various tasks and higher is better and you see that as you grow the number of parameters of the model you use performance goes up consistently this is very interesting for supervised tasks at the time i would say it was a bit more of a saturation it was like you had enough parameters maybe what this showed is that because with unsurprised learning you can incorporate so much more data it's actually really helpful as your data set becomes larger to have larger models and in fact the end is not in sight and as i understand it this observation here this research funding inspired openid to fundraise a billion dollars for future projects to have essentially more compute available to train larger models because it seems like doing that will lead to better results and so far that's been true gpt3 is even better than gpg2 now gbd2 does next token or next word prediction bert which is a very similar model came out around the same time out of google does filling in so you have text that you download you mask out a word or multiple words in a sentence and the new network on that head first head that it's trained on needs to fill it back in very similar task you might wonder why might you care about this versus the next token the nice thing about this task here is that the new network looks at the entire sentence as it's filling things back in which can often help in later tasks because the neural network's already been unsupervised trained on always looking at entire sentences or entire documents at times so and then you can do supervised tasks later sentence a the man went to the store sentence b he bought a gallon of milk label this is the next sentence that makes sense on the right the man went to the store penguins are flightless label says not a natural next sentence and so those are supervised tasks which you can learn to do well there's some more tasks here the takeaway message is not so much the details of these supervised tasks but the fact that these supervised stacks have relatively small amount of labeled data compared to the unsupervised training that happens ahead of time and that by doing this on every one of those supervised tasks the birth model the large model in particular outperforms all prior state of the art and so what we had here just about two years ago is a major revolution and how natural language processing was done used to be done more specialized on tasks now every single nlp application you see will be pre-trained on a large body of text often the pre-trained model is just downloaded you might download birth large or you had a larger model that's more recent and then fine tune on your task especially inside google search when you search you don't want to just string match you want it to understand what's in your sentence that's effectively what the neural nets do as they learn to fill things in they're effectively recovering the meaning of sentences in a way that allows them to fill in the missing word and that also means that they can now find better matches for your searches you don't need to be uh perfectly string matched with whatever is on the documents on the internet that you're hoping to retrieve so whole revolution there if you look at the nlp leaderboards everything is some kind of birth model uh these days some of them are called birds something called ernie um some are called t5 but under the hood it's the same idea so how about other domains how about vision can you do the same thing just fill in a missing patch in principle you can't people have trained models to do this and for example alyosha efros at berkeley professor berkeley and his students have done some of the you know leading work there and so it's fun because you're all at berkeley to know that you know some of the work happens at berkeley now the tricky thing with filling in a patch is that it's very high dimensional so let's say you're just filling in i don't know a 16 by 16 patch that's 256 pixels three colors so that's times three now that's something like i don't know 786 maybe or something something there things that you have to fill in each of them could take on value from 0 to 255 and so the number of possibilities in that patch is really really high it's 256 to the power 7 700 something and so 256 to the power 700 something is a very large number it's much larger than the number of words uh in english let's say much much more and probably larger number of words in all languages combined so i'm not exactly sure how many languages we have in the world but it's very very large and so the problem with that is that it's hard to predict that precisely and make that work as well as it's worked in language in english maybe there is 200 000 words when it's tokenized the way it's often done there might only be 30 000 different things you have to predict and that's much lower than 256 to the power 700 something so envisioned people have iterated over other prediction tasks for example jigsaw puzzle solving so you have an image and you just split into nine pieces and then you move it around and you say now the neural network has to reconfigure it in the original configuration if it can do that it understands something about the world about images of the world and so that trunk of that neural network should hopefully be reusable and indeed it tends to be reusable one thing that works surprisingly well at least very surprising to me and i imagine surprise to many people is this very very simple task you collect images and all you do is predicting what has been rotated zero degrees 90 180 or 270. so you collect an image you randomly pick one of those but of course you know what you did and then the prediction is what what did you do without getting to look at your label of course it turns out that somehow puts a lot of interesting information inside the trunk of that neural network that you can reuse for supervised tasks in fact this is a system that worked the best until the method came around that is most widely used today which is um contrastive learning uh often uh known under the two kind of versions that are most popular simclear and moco seem clear from jeff hinton and collaborators mocha from kamingha and collaborators and the idea is effectively the same imagine you download images from the internet and you don't want to label them you don't have labels yet so you just download let's say an image of a dog image of a cat but you don't know it's a dog in a cat it's just two images if you now for one of the images let's say the dog image but your computer doesn't know that you duplicate it and make two versions of it you make a grayscale version and you keep the color version but crop it and then the other image you just keep then you can say for the one that you duplicated made two variants off these two the neural networks should think of as the same thing as in those images but it should be different from what's in that other image and so you want to bring the two dog images but you don't know there are two diagonals you just know they come from the same root image that was duplicated in two different ways close together and the other one should be far apart from it when you do that that's how you train a neural network and then you fine tune with just a linear model so just a linear classifier on top of training completely unsupervised so not even full fine tuning you can match a fully supervised performance meaning that you really get the right features here extracted from these images this is when trained on on imagenet in this case and again we see the trend with more parameters you can do better which is not as much the case did not to the same extent true for supervised learning uh it's much more true in unsupervised learning okay so that's the end of unsupervised learning which is the thing i wanted to
Full Stack Deep Learning - Spring 2021,12,1520,Reinforcement Learning,spend the most time on of all the topics okay so as far as learning is probably thing that's transitioned the fastest from research to real world practice nlp uses all the time vision starts using it now and it's only invented two three years ago for vision only one year ago now let's switch gears for a moment to a different kind of um machine learning that is not as practical yet i would say in in the same general sense that supervised learning and unsurprised learning are practical but that nevertheless has shown great results and is becoming practical in some domains which is reinforcement learning in reinforcement learning your ai is an agent more so than just a pattern recognizer the agent acts in the world in the environment and changes the world and then this process repeats over and over and the ai system the agent is typically goal oriented wants to achieve something in the process it's not a one-shot thing it's okay you act for a while hopefully at the end you achieve something like maybe you baked a great pie if you're a robot or maybe you clean the kitchen or maybe you you know autonomously drove a car safely to a destination the way this tends to be expressed is by reward so you would say you know a tasty pie is a higher reward than a brand pie or uh maybe you know getting to destination fast is better than getting there slow but getting their slow is still a lot better than getting there while you know getting in some accidents and then worse is not getting there at all and having an accident and so the reward function would be where you encode that you assign for each situation a score what are new challenges in reinforcement and compared to supervised learning in supervised learning when your neural network sees something it's also told what the output should be in reinforced learning that's not how the supervision happens when it sees something it has to take an action but it's not told whether that was a good or a bad action right away it'll be at the end the robot might be busy in the kitchen baking your pie and that's by the way not not super realistic yet but imagine the robot's busy baking your pie only once you taste it might you say this was good or bad and so it's been busy for a long time and now you might not know when you say it's bad what did it do wrong in the whole hour it took to make it you might have to make you know a thousand pies and then compare notes when do you say it's good about what did it do differently in those different times teasing that apart that's the credit assignment problem and it's a very hard problem that reinforcement algorithms have to resolve another challenge is stability because if it updates itself because it's learning by trial and error it could destabilize and could make big mistakes so it has to kind of be clever about how how it's going to update itself to not destroy uh things along the way and it has to try things it's never done before that's called exploration despite these challenges some great successes have happened for example deepmind showed that neural networks can learn to play atari games back in 2013 so the reward is a score in the game and it just learns to play from looking at the screen and handling the joystick now since then a lot of progress has been made um many many places have contributed to this but um yeah deepmind was first back in 2013. under the hood it's a neural network just like the ones you're used to from computer vision image classification type networks and now the softmax on the output is effectively over joystick actions rather than classification labels but what's so different is how it's trained it's trained from its own trial and error and looking at the score in the game to internalize what might have been good or bad and the score in the game doesn't change very often so that's again the challenge there the credit assignment problem game of go was cracked uh by alphago deepmind showing that the computer can now play go better than the best human players alphago zero was the version that was trained by only playing against itself pure reinforcement lighting was even better than alphago and alpha zero was a more general purpose version that could learn to play go but also learn to play chess and learn to play some other games so deep reinforcement definitely works it also works for robotics so we see here some work that was um done at berkeley and this robot is learning to run and initially was falling over but through its own trial and error and based on how it scored and scored higher when it makes more forward progress it figures out how to i should do really well and run at pretty high pace now one of the nice things about reinforcement learning compared to a more classical approach to robotics is that you don't have to design the controller yourself you just implement the reinforcement learning algorithm and of course you need them the robot or the simulator but then once you have the reinforcement learning algorithm if you want a new robot to run you don't have to think about oh four-legged robot what do i have to do differently to design a controller you just say oh just let it run it'll train itself and actually the algorithm that was used to train these robots to learn to run that exact same algorithm was used to learn to play atari games so very general approach to have ai systems acquire new skills fact can acquire a very wide range of skills this is the deep mimic work that was led by berkeley psg student jason pang and you can also do this for non-human-like characters and this this is already really um at this point this is changing how you could design video games or animated movies instead of designing the key frames for every step along the way in your video or your game you can now just say hey lion go from point a to point b and run this fast and then it's been trained to do that and now i can run or it can do all these things to deep mimic uh humanoid was doing to go from point a to point b maybe while doing a cartwheel and so forth so it can really bring down the effort required to design games and animated movies it's also been shown on real robots so this is brett the berkeley robot for the elimination of tedious task and it's learned to put the block into the matching opening it can learn that in about one hour starting from scratch the neural network is randomly initialized and it learns a vision system and control system all together in one neural network to put the block into the matching opening the reward here is based on how close the block is to being inserted into that opening and so over time it figures how to get it closer and then from there it figures out um to consist consistently get it in we actually then did a project with nasa on this robot when you look at this you're probably thinking who would design a robot this way well turns out nasa designs robots this way and there are some good reasons for it what are we even looking at there are some metal rods and then there are some elastic cables but i mean not super elastic uh pretty pretty tight still um these cables connect these rods and the tension in those cables that's what the integrity name comes from uh keeps keeps it uh hold that shape um the motors at the end of the rods can effectively shorten or lengthen those cables and by doing so change the shape of the robot shift the center of mass and make it move some properties very lightweight for the size this robot is it's low cost and it can withstand pretty significant impact so a lot of advantages there you can also pack it very small inside a rocket when you want to transport it volume comes out of price and so just like you have a tent to go camping you don't carry the tent around in this full volume you carry it on a small package same thing here small package and then it can expand when you need the robot to start doing things major challenge is how do you control this thing not very intuitive but that's okay reinforcement learning figured out how to control this robot and is here rolling this robot in this case purely training simulation uh many versions of the simulator which i'll talk more about later and then um from there able to control the system quite reliably similar ideas were then applied to robotic manipulation uh solving rubik's cube this was done at openai this one what you see here is a robot hand that has figured out how to solve this rubik's cube i'm scooting forward in the video but it is really solving it over time you see that ashley gets everything in place you might wonder what's hard about it you can run a star search and related algorithms to know what moves to do at a high level but the contact forces the in-hand manipulation is a really difficult robotic control problem that was mastered with reinforcement learning the fact that it's starting to work so well also in robotics uh the different ai approaches actually inspired me together with some former students uh to start a new company covariant um you see the co-founders there ken hao jang rocky dwan peter chen going left to right we started kavanaugh about three and a half years ago to bring a lot of these advances that we made in the lab into real world and actually we're building a lot of advances secretly at covariant that we don't share with the world to take things to the next level and so an example of what we'll look at there is helping with order fulfillments anytime you go online you order something well that means something that's stored in a warehouse has to be retrieved it might come down a conveyor belt as the blue bin here is doing coming down a conveyor belt and then a robot here is picking it out of the blue bin which is the storage bin and putting it into the cardboard boxes that are used for shipment in this case here this might not be you ordering this is in berlin for electrical supplies but this is used this kind of system is used in many many places and for now most of them are you know manually picked but there's a lot of demand to start automating this picking process and that's one of the examples of things that is becoming possible with these recent advances in ai so let's go to the next topic and
Full Stack Deep Learning - Spring 2021,12,2160,Unsupervised Reinforcement Learning,i'll revisit the slack later reinforcement algorithms achieve mastery on many many at least definitely simulated domains often doing better than humans but you could also ask the question how fast is the learning well some people study that and these graphs the details don't really matter but it shows that a human can learn in about 15 minutes to do better than what double dqn one of the state-of-the-art approaches at the time of this study how well that approach could do after 115 hours so a major gap there in learning speed even in the end double dqm can do very well it takes a long time to learn to do well how we're going to bridge this gap well one observation here is that when you do learning for control and you have these visual inputs learning seems to be a lot slower than when you do it with access to the underlying state so when you work with a simulator you can take the images it generates or you can take the coordinates of the robot the coordinates of all the links the joint angles and so forth and if you do that that's the blue curve learning curve goes up very quickly but if you do it just from image inputs that's the green curve and so it's about 100x different and so the first way to get to faster learning would be to see if i can bridge this gap and intuitively it seems it should be possible because when you look at these images the information is there it's just that we're not presenting it right and we've just been talking about this maybe if we bring in unsupervised learning we can somehow turn these pixel level representations which are not that informative into a new representation that is much more informative that's actually very similar to the underlying state and then we can get much faster learning we started investigating this about a year ago inspired by the results with contrasting predictive coding and simclear that we already talked about in the concepts of unsupervised learning and we looked at in the context of reinforcement learning so typically there's a replay buffer where you store your past experiences you load some observations from there then you would feed that into an encoder neural network which then has two heads an actor and a critic those are the reinforcement learning heads actor estimates the best action to take next and critic tries to estimate how good that action would be now if that's all you do the green path we know it's pretty slow so we add an extra head at the bottom here which includes augmented observations and thus contrastive learning on that bringing augmented observations coming from the same original close together and augmented observations from other originals far apart so it's not dogs and cats here it's just different configurations of the robot some get brought closer together other further apart there are some things you have to think about exactly how we generate the the query and the key to do this but we effectively found that random crop is the best way to generate variance so that's what we did you can also recolor do other things but random crop was the most important then there's a way to measure similarity there's a notion of bilinear similarity and cosine similarity bilinear work better for us which means you have that w weighting matrix between the query embedding queue and the key embedding k so yeah that's just an empirical thing and then moco which is similar to simclear but has this additional thing about a momentum encoder to not change one of the encodings too quickly um it turns out that that helps for the training also again empirically we check this and it does better the main takeaway if you look at the learning curves here in red is when you have the contrastive unsupervised reinforcement learning and gray is if you directly do reinforcement on state and you see in almost all of those learning curves the red is as good as the gray meaning that learning from image input can match almost every time learning from direct access to state meaning the unsurprised learning in this case likely recovers something very similar to state such that the learning is very effective very efficient then instead of just comparing with learning from state we also compare with past methods that learn from pixels the red curve is what i presented you can see it consistently outperforms the past methods that i learned from pixel inputs and the gray and the black is learning from state input so it's matching that almost all the time same thing is true in atari games it does really really well outperforming past methods such as simple and rainbow in most or all of the games there are also some environments where it doesn't do as well as learning from state and so as part of research is to kind of dig in when something doesn't work in some situations what might be under the hood here our hypothesis here was that maybe the images don't have enough information maybe the state has more information things like contact forces that are hard to extract from images or velocities that might be hard to extract from images and so we did is we on the ones where it didn't do well we then set up a supervised learning task well actually before i say that all past method the bottom plot is showing that all past methods struggle in the same environment so there's nothing about our reinforcement method it's something about learning from pixels in those environments we then set up a supervised learning task we said let's go from image input to state and see if we can predict that and look at the error rate and we saw that the larger the error rate when trying to go from image to state the larger the error rate of a very well tuned train neural network so the best we can possibly do the larger the error rate there the worse it does at training from images suggesting that the lack of information the images is really the issue and why in some of these environments it did not match the performance of state-based learning okay let's dive into meta reinforcement learning but please keep the
Full Stack Deep Learning - Spring 2021,12,2555,Meta Reinforcement Learning,questions coming trpo dqm a3c dwg ppo rainbow the most popular rl algorithms are really fully general rl algorithms that have been designed um by humans to to hopefully have an agent be an efficient rl agent and the peters they can work anywhere no matter what your environment is you can run them and in the limit of collecting enough experience they will do something interesting but if we think about a bit more deeply the environments encountered in the real world maybe including uh simulated worlds that humans are willing to build it's just a tiny tiny subset of all the environments that could be defined mathematically so we have these algorithms that are so general that can work for any mathematically definable environment that are only used for a small subset of those environments and so maybe the algorithms are too general and that's why learning takes a long time if they could be a bit more specialized to the things they're going to encounter maybe they could do better and so question is can we build better algorithms that take advantage of this so with the environment we have the agent agent takes an action environment changes state might emit a reward the agent sees that and process repeats inside the agent there's two parts there's an rl algorithm which is still typically classically designed by human and there's a policy or a q function that is a neural network and that has all the latest wells and bells and whistles from deep learning but the algorithm is still human design and so when an agent goes environment a it learns a positive environment a in environment b same algorithm can be used which is nice no new programming but a new policy has to be learned and so forth but you can also question the notion that these algorithms are human designed because we have not been able to design algorithms are nearly as good as human learners so can we learn an rl algorithm that is better than a human design algorithm so what would be the the way to do this well there's no final final answer to this yet but here's something we tried and that has shown some good signs of life imagine you have many environments environment a b and so forth and then you have some meta algorithm which is an algorithm that learns an algorithm so in this case it's an algorithm that learns a reinforcement learning algorithm that outputs this faster learning reinforced learning agent from having interacted with these environments that's the hope and the reason we have that hope is that the way you can be fast is because in the future you're going to be an environment f let's say that is yes different from a and b but still related there will still be gravity there will still be maybe electromagnetic forces there will still be maybe people in your environment and so forth so the notion that what you learn in a and b is likely useful for environment f suggest if that's true that we should be able to learn faster in environment f or g and h and so forth so here's a way to formulate this we want to train an agent a full agent not just a policy but a full agent that is such that on expectation when dropped in some environment sample some mvp then trajectories rollouts are collected it achieves high reward pictorially shown at the bottom it gets maybe episode one episode two and mdp one then it goes to mdp2 same thing happens and it goes from mdp to mdp to mdp and when it gets to the next one we hope because of having been in the previous ones they can learn more quickly so we want to optimize expected reward we'll do that on training mdps but the hope is that it'll then also generalize to testing mdps you need to decide how to represent your agent an rnn is a generic competition architecture it's quite popular to do this with different weights in the rtn would mean a different rl algorithm and prior over worlds effectively different activations in the rnn would mean a different current policy or strategy of the agent the mediterranean objective can be optimized with an existing rl algorithm this is an rl objective it's just a reinforced learning problem where the mdp changes a lot but other than that it's still a type of mdp and so we can train with ppo dqn and so forth but the result will be a recurrent neural network that is ready to be dropped in a new environment we tested this on slot machines the banded problems very classical problem it doesn't matter too much but it's a problem that you can study in detail i actually have a analytical solution for that tells you what to do when faced with a new banded problem so we can compare with these analytical solutions gittum's ucb thompson sampling what we see is that the rl squared learned agent outperforms these asymptotically optimal approaches in many banded environments and as the band has become a very large number of bandits and arms and number of runs you get then rnn struggles a little bit might need more training in this bigger environment but for the smaller ones it actually does even better then you can look at something more interesting like a maze can the agent learn to navigate mazes so at training time you put the agent in many many mazes and it doesn't know the map of the maze the agent just gets to see and then has to somehow explore it find the target which is shown in red and then gets dropped in the same maze has to run to the target as quickly as possible just to remember things from the past be able to go to that target quickly again that's actually what it does if you don't do what we are doing in this specific work just run standard um exploration methods you kind of get stuck then you run along a wall and so forth it doesn't really work very well if you have trained in many mazes before then go to a new maze you have learned to explore so now knows that running down hallways is the key thing to do looking for things around the corner seeing the target going to it and then it remembers enough to go there again and so that's an example of a learned reinforced learning algorithm it's learn to explore its strategies a little specific to mazes that it's been trained in um it's really good at exploring mazes it's not going to explore well in an atari game all of a sudden but you could imagine if you scale up your compute power scale up your neural networks you might be able to train this on a large set of games a large set of robotic environments and then it could quickly learn something new in a new environment that's related the work we did it didn't always learn sometimes it kind of gets stuck because it's a hard learning problem because the outer loop is still reinforcement learning it's becoming a fast learner by using an existing hand-designed reinforcement algorithm so um yeah some sometimes it just doesn't learn that well but you know about 70 of the time it learns pretty well so a lot more work going on in this direction um so listed a few papers here that you might be excited about
Full Stack Deep Learning - Spring 2021,12,3015,Few Shot Imitation Learning,um now instead of just doing rl often people will complement it or even just do imitation learning which is supervised learning where the output is an action for an agent and that of course gives you a lot more signal because now for every input you have the corresponding output again so you have many demonstrations the imitation learning algorithm is effectively a supervised learning algorithm that trains a policy that observes the environment outputs the right action actually works quite well in many situations bottleneck of course is collecting demonstrations which is often even more time consuming than just labeling data and supervised learning and for every task you need new demonstrations so you might think can we circumvent this can we do some kind of multi-task meta learning and indeed that can be done so you can have many demonstrations but for different tasks then metal learn a one-shot imitator which can now from one demonstration know what to do this is done with the architectures that you're kind of used to these days so you would take in a demo of a task you see a demo of another task you see a second demo of the same task for the first demo you see the entire demo so full video sequence for the second demo you just see one frame and your your task as a neural network is to predict the next action in demo two which direction should the robot move something like that up down left right um rotate and so process demo one look at demo two current frame and output in action that's a very heavy duty visual processing task and under the hood the best methods these days we'll have often per frame confident processing followed by over the whole video transformer processing then attention modules that go from demo2 into demo one to figure out which frames in demo one should be paid attention to right now and then from there goes back to generating an action so those kind of things tend to be under the hood a great example for people to test this is block stacking where different stacking configurations are different tasks and what you see here is actually visualizations of the attention heads the tension over the demonstration is shown on the left horizontal axis is time you can see at which time most attention is being paid in the demonstration and then on the right you see which blocks it's actually paying attention it's effectively doing relational reasoning over the different blocks in the scene before it decides on what action to take but it also gets information from the demo on the left so a lot of attention happening here to to make this all work so why would we care for simulation
Full Stack Deep Learning - Spring 2021,12,3193,Domain Randomization,well compared to the real world simulated data collection is less expensive it's faster more scalable less dangerous can't break an actual robot easier to label because the simulator has the labels built in but the question is how can you learn something useful for the real world by just training in a simulator well there's a few approaches one approach is make your simulator as realistic as possible if you can do that that's great um often it's hard and when you succeed at getting closer and closer often your simulator becomes slower and slower and slower and so there are some downsides there very often but if you can do it i mean try to do it and see how far you can get while keeping your simulator fast then another thing people have looked at is something called domain confusion or domain adaptation the idea there is that you might train a network on effectively simulation and real-world data at the same time and some slice of the network at some layer you make another network look at it a discriminator network like in gans and that discriminated network tries to see if that layer is representing something about the real world or representing something about the simulator a simulator and so if you can fool that discriminator that means that in that layer the neural network doesn't know anymore whether the original image input was sim or real and so everything that comes after that layer can be fully trained in simulation so that's really nice you of course get the complicated dynamics of this fighting this discriminator but you know when you can get it to work that can be a really nice way to go about it one of the most surprising things to me that's happened in the last five years in robotics is that actually something much simpler at least to execute on once it's understood can work really really well in fact often better and the idea is that if you have a simulator instead of just trying to make it realistic you actually try to make it have many many variations so on the left there you see tabletop scenes sometimes the background is pink sometimes it's purple sometimes it's green sometimes the robot is brighter sometimes it's darker the objects have different colors different shapes and if you have enough variation it turns out that what you learn can actually generalize to the real world even though nothing on the left looks like real world training on enough variations on the shown on the left gives you a neural network that understands the situation on the right in fact early work was done at berkeley prestigious at degree and sergey 11 collaborate on it for drone flight and show that you can train in simulation to understand how to navigate um well buildings really and then put it in the real world and it actually works surprisingly well it'll avoid collisions josh's question at the time was okay that's great avoiding collisions but can we make this work in a more general way where we need to be very precise let's say we want to localize an object and pick it up what can be done and he showed that actually this works so the average error shown on the vertical axis is going down as there are more training samples but none of these training samples come from the real world they are all training samples from the simulator but the error is measured evaluated on real world images and so this is this is amazing purely training simulation can actually localize objects in the real world images um then to kind of dig into what matters the most the number of unique textures that's used to texture things in the simulator is really critical you need to have a high number of textures how about pre-training on imagenet when you're going to train a visual recognition neural network why not pre-train an imagenet well you might always do that but it's still interesting to ask the question is it important or is it enough to train purely in simulation and never see a real image including no pre-training on imagenet and you can see here red and blue end up in the same spot sure pre-training buys you a little bit early on that's to be expected but then it becomes equal so no need for pre-training josh then applied similar ideas to grasping and kind of the question there is how do you generate enough random objects well there's not that many objects in the simulator actually if you try to download meshes of objects you'll see that after a couple hundred maybe a couple thousand you cannot run out of meshes to download and if you're gonna apply this idea of enough variation in the simulator to be useful in the real world a hundred or a thousand not going to cut it so josh's idea here was that actually you can just slice any objects that are downloaded into pieces and randomly recombine these pieces into new objects these new objects will not look very realistic because in reality you know you won't have half a coke can combined with half a water pitcher or half a screwdriver combined with half a computer screen but um you can generate those in simulation and train on it and again the surprising thing here in my mind is that the training of some very unrealistic things but nevertheless it learns meaningful things that are carrying over to the real world and it showcases again the importance of large amounts of data even if the data is not perfectly matched very large amounts of data is very helpful and so here's an example of the system in action hopefully for josh this is a a good memory and not a nightmare flashback to two long nights uh in the lab i have a funny story about this uh these particular two long nights in the lab that i'll tell some other time okay sure you don't want to share right now no no no not while we're recording okay yeah looking forward to that story some other day what you see here is the robots picking up objects reliably even though never having been trained in the real world it's only trained to predict grass points on simulated objects and then now it sees a real world object predicts the grass points and then uses the you know robot controller to move the gripper to those grass points doesn't always work i mean grasping actually as a whole is not a solved problem it's something that still is a very active research domain to get to very high reliability but it does really really well comparable with other approaches out there that have been trained in the real world then i showed this video earlier but actually this video used domain randomization also same idea was used to train in simulation many variants of the simulator what exactly is this robot hand what are the dimensions how do the tendons exactly dynamically you know uh function in the real world what kind of friction is there what kind of hysteresis uh what kind of wear and tear um what where exactly is contact being made with what kind of friction um very hard to know and so instead build many many many many versions of the hand and the rubik's cube in simulation train that way and then deploy in the real world and we watched this earlier it's actually um deployed in the real world it succeeds even though it's trained purely in simulation so for the last research
Full Stack Deep Learning - Spring 2021,12,3693,Deep Learning For Science and Engineering,topic deep learning for science and engineering this is a very interesting topic that in some ways is very different from all the other topics if you look at the other topics and the classical areas of artificial intelligence which are computer vision natural language processing and robotics those are things that humans are really good at or most of the time humans are pretty good at those things and if you're successful you might build a system that is as good as a human or maybe a little better but if you start using deep learning or ai for science and engineering there's a possibility in the future it'll do things that isn't just like oh a person could have done this but it's nice that now an ai is doing it um but it could be doing something that maybe we we couldn't have done and i think that that's really really exciting and we're seeing a bunch of progress happening there most famous result of i would i would say is um alpha fold and alpha volt 2 out of deep mine so a couple months ago deepmind made this headline here uh it will change everything which i believe was a quote from a nobel prize winning um biologist or chemist saying that you know protein falling is such an important problem being able to predict how a protein will fall and deep mind's ai system really deep neural networks were able to when given a sequence predict how it's likely to fall better than any previous methods um and so here are some examples of this so yeah in green is the experimental result meaning this is sciences we have to measure there's no actual ground truth but there is a you know measurement process with crystallography that allows you to measure the shape of a protein experimentally when it's folded and that's what's shown in green but that's expensive and time consuming and then the hope is of course that you don't have to go through that process but you can just sequence it which is very cheap sequencing proteins and if you sequence it and then can predict how it'll fold shown in blue and it matches then you can save a lot of time and dollars so these are very close and give an idea of what's becoming possible the competition is called the casp competition it's held every two years higher is better here you see the past works when people wrote some kind of programs or other machine learning approaches to predicting the folding of proteins on test proteins that you don't have access to the fold you have to predict it that without having seen the answer actually not a lot of progress between 2006 and 2016 and roughly at the same level then in 2018 was the first deep learning submission alpha fall out of deep mind and it won and it did quite a bit better than the previous approach but then overfall 2 took it to a whole other level and some people call it solved i mean is it solved not solved i'm not an expert to to make claims about that but levels of precision are achieved that on some families of proteins where there's enough data already about those proteins it seems that it achieves the precision that's needed to um then do other things with the 3d structure that you might want to do predict where the two proteins might bind and so forth what's under the hood a pretty complicated model even by today's standards i would say it takes in the protein sequence it then does a search for related sequences okay they might say why does it search for related sequences there's an intuition there if you find similar sequences evolutionarily they probably come from the the same ancestor which means that even though they have mutated they might still serve the same function which might mean they still have roughly the same 3d structure so finding these related ones gives you a set of sequences for which if you have any 3d structure for them then that's great you can inform your prediction based on that if these others don't have 3d structure associated with it there's still something very interesting because you can then look at which sides have mutated and which ones don't have any mutation and as site has mutated it probably is not affecting structure much the structure will affect function but if a site is not mutated that is probably important for the structure of how it folds and so this gives a lot of information about how it's likely to fold which amino acids in the sequence are likely to make contact with each other in the fold because they are preserved over time across all evolutionary offspring so that's done with some kind of multi-headed attention models and they are doing reasoning over these different sequences um then it's also generating embeddings for residues to reason about how replaceables each well residues and amino acids are the same thing here um so reasoning about which amino acid or which residue is okay to replace which other residue because they have somewhat similar properties from there then a structure module neural network tries to turn it into a 3d structure there's another output auxiliary output which is the pairwise distances if between every amino acid and sequence you predict the pairwise distance that is also very informative about the structure now some of these predictions are not realizable because when you have three points and you have their pairwise distances the triangle is fully defined if you now have a fourth point and you have arbitrary other pairwise distances it's not going to dramatically work out in 3d anymore all right well in 2d 3d four points will work out but then more than four points you need to have some consistency these pairwise distances that the outputs are not always consistent so it also has to output this other thing that outputs a consistent 3d structure okay so now you can predict protein properties including structure with neural networks this by the way deep minded supervised learning for this there's a there's a data set with 3d structures you can train and then learn to output 3d structures and then do pretty well on new i mean uh or new proteins um there's another thing you can do is you can speed up design with deep learning the idea here is that often when you have a simulator it's very slow you might build a new network that replaces a simulator or that even replaces the prediction of how well your thing will do in the simulator you don't even simulate just say this design will do this well and then you can run an evolutionary algorithm on that to speed up designs we've done this for circuit design people have done this in other domains um then other thing people do is they actually augment their data sets physics data sets with generative models and people have done math integrals we're solving differential equations with effectively language transformer models and doing pretty well with them and a few pointers here what's a big trend right now compute is
Full Stack Deep Learning - Spring 2021,12,4154,Overarching Research Trends,increasing a lot of companies are building better ai chips um let's calibrate that a bit a fly hundred thousand neurons alex net from 2012 650 000 neurons miles hundred million neurons human hundred billion neurons more importantly about 10 to the 15 synapses connections between neurons that's where the compute happens so each synapse can fire or not fire once per second so effectively have about one petaflop of compute well that's interesting because the djx2 provides two pedaflops in one server rack for 400 thousand dollars that new djs that came out yesterday i believe provides five pedaflops at 000 but uh i couldn't find an official um announcement on that but that's i think what i remember from the talk so these days for you know order of 100 000 or even less you can get a petaflop and so equivalent of human brain um in terms of compute power uh according to this back of the envelope calculation in a box um you can also rent the equivalent of human brain of compute power in in the cloud if you do it preemptable it's about five dollars an hour probably even less by now so cheaper than most uh human workers uh so very interesting that you know if just we had a better computer program it would be cheaper to run these things on a computer than to run them inside the human brain the trend in research is that amount of compute used for the most kind of prominent experiments keeps going up this is a logarithmic scale so it's exponential growth in the number uh the number of petaflop days used to get to those results uh what does it mean for our research agenda um well i think we go back to what are the key enablers for ai progress data compute and human ingenuity and then you can say well i can work on problems like this where my human ingenuity seems to dominate or i can work on problems where human ingenuity is a small piece of the puzzle but mostly data and compute enable the progress and maybe different people want to be in different places i want to be on the right and what i mean with that is if you work on the left and human ingenuity is kind of everything then you're kind of competing with what everybody's been doing for hundred years 200 years um just thinking but if you work in prom territory too you work on problems where you use an amount of data and nobody had available last year an amount of compute nobody had available last year well you're not compu competing as anybody's ideas from previous years because they could never test them you can work now on things that nobody could work on before and that often leads to very exciting lower hanging fruit than the things on the left so learning to learn is a good example but all the unsurprised learning is another good example um and so that's where i think a lot of the breakthroughs will happen because it's easier you're not competing with a lot of past attempts you're in a new territory that wasn't available before now it's a saying that says give someone
Full Stack Deep Learning - Spring 2021,12,4345,How To Keep Up,a fish and you can feed them for a day teach them to fish you feed them for a lifetime so i've been giving you so many fish in some way about research um i haven't told you about phishing so how can you keep up with research and i want to hit on two things here how can you mostly keep up with mostly not bothering reading any papers because we know so many papers come out it's a bit much and what to do when you do decide to read some papers so how can we keep up without reading all the papers because there are too many of them to read all of them here are a few pointers tutorials at conferences whenever there's an important new trend in machine learning almost invariably within one year somebody will put together a really nice tutorial on that topic at one of the main conferences that tutorial will capture the essence the most important takeaways going forward of maybe you know 50 100 hundreds of papers that happen that direction will be condensed into that tutorial into the things that will matter the most for the future so i highly recommend checking out tutorials as a very high bandwidth way of learning rather than just you know always try to read papers graduate courses and seminars are very similar if you want to learn more instead of just go read papers you could look at graduate courses what's covered there especially if there are lectures it can be much more informative much more formatted for consumption fast consumption than reading papers yannick culture has a youtube channel where he explains papers so you just go there he kind of does two things he picks a paper to explain so you don't have to pick out of all these papers coming out every month like 4 000 papers and counting every month you don't have to pick you might just watch the 5 or 10 or 20 videos that he puts up and learn from those there's another youtube channel two minute papers again they do two things for you they pick the paper so you don't have to find it and they give an explanation in two minutes so very very fast to learn something there's a newsletter by andering the batch um great newsletter comes out every week um i think it's actually recommended on the course website and then there is um import ai newsletter by jack clark which i think we might also be recommending on the course website and interestingly in this week's newsletter uh sergey kerry of sergey's recent work was featured as one of the handful of things that are featured every week um sergey's work on character optical character recognition parsing homeworks tests and so forth was highlighted as one of the key things to make people aware of so yeah congratulations again sergey that was uh very exciting to see yeah yeah so that's the thing to learn about this week apparently now when you do decide to read papers um definitely don't try to read all 4 000 papers a month um which papers to read the things i just mentioned are giving you the pointers if you are watching a tutorial at a conference and you find things very interesting you can then see what papers they reference go read those same for the graduate courses the youtube channels and so forth the newsletters what else can you do i love this thing called archive sanity by uh andre carpathi this is a snapshot from today so i went in there and i said okay top recent from last year and it says end to end object detection with transformers this is the paper that is saved into the users libraries the most often of all papers in the last year so if you read only one paper maybe that's the one you should read because that's the most saved paper for the past 12 months you can also do last month last week last three days top hype when there's the most likes and tweets and you want to read those papers you don't even need to go to twitter you can just go to our cache sanity and select the top hype over there you could go to twitter there's a bunch of ai people who like to post actually these days sergey and josh post a lot too i should have added them they've been very active uh posting things on twitter latest progress in ai so yeah then there's a facebook group where you can interact with other people there's a ml subreddit i mean these groups and reddits tend to have high variance uh discussions but you know if you're totally alone in your effort it's still nice to be to be connected even if the quality is you know varying from from discussion to discussion and contributor to contributor how to read a paper not going to step through this but there's an r to it you don't want to read it start to finish word by word that's very inefficient because most papers actually in a minute you should realize you shouldn't be reading it or maybe in two minutes you've got the main takeaway and say okay i can read it later if i need to know more details you might want to form a reading group if you have some friends who want to learn more about machine learning stay up to date saves you a lot of time in selecting papers and you can listen to them explaining a paper instead of having to read every one of them at the very end of as a very last slide here i wanted to mention one thing which is why do a phd or not do a phd i think it's a really good time to not do a phd actually it's one of the first times ever it is the first time ever i would say that you could not do a phd and keep working in ai ten years ago you want to do ai work you had to do a phd these days ai is becoming so practical and exactly the kind of things we tend to cover in the course that there's so many applications to be built in the world so many problems to be solved that you can work in ai without doing a phd you can go build really cool solutions to whatever problem you like to solve that said there could still be a reason you might want to do a phdn that would be if you really want to become one of the world's experts in a specific technical topic that you care about something technically deep and demanding that you want to spend a lot of time on diving deep thinking very hard and it'll be more narrow than you're used to for my undergrad education we'll pick a specific topic try to become the world leading expert on that topic for example the topic could be fu shot imitation or it could be maybe model based reinforcement learning or it could be you know contrastive contrastive learning i mean probably not the future phd topic that would have been a good topic uh three years ago so you would be working on a topic where you become one of the world leading experts and you would be developing new tools and techniques rather than using existing tools and techniques so the analogy i like to use is as an engineer you use a hammer you use a screwdriver as a phd researcher you invent the next hammer you invent the next screwdriver that's that's a little bit the difference it's not the black and white thing there's definitely a gray zone in between but that's probably the high level distinction between the two all right thank you
Full Stack Deep Learning - Spring 2021,13,0,Introduction,this week we're going to talk about machine learning teams so why why do we talk about machine learning teams as part of this course like what does this have to do with uh with building working machine learning systems um well i think one of the challenges with machine learning is that you know running any technical team is hard um it's hard to hire good people it's hard to manage teams of people and develop them into better versions of themselves it's hard to manage the output of your team and make sure that all of your your vectors are pointing in the same direction and you're producing the output that you want the team to produce it's hard to make good long-term technical choices it's hard to manage technical debt and it can also often also be difficult to manage expectations from leadership and this is true for any technical team that you might work on but machine learning adds quite a bit of complexity to this machine learning talent tends to be expensive and scarce there's a diverse set of teams of roles rather that need to be present in order to make machine learning work the projects that you work on are often going to have pretty unclear timelines and a high degree of uncertainty to them so managing output can be even more difficult the machine learning field itself moves really fast and is uh to quote the famous the now famous google paper the high interest credit card of technical debt so the the process of making sure that you are making good long-term technical choices and avoiding debt is even more challenging and then when you're managing expectations from leadership in many organizations leadership doesn't actually really understand ai and how it's different from regular software and so this can make your your job as a manager even more challenging um and so for the for those of you that are uh looking at this and thinking well i'm not managing people so how is this relevant to me well hopefully um this will give you some insight into how you know maybe your manager is thinking about building and managing machine learning teams and also a lot of the advice here is tailored towards kind of helping you get a job in the machine learning world and so that's kind of what i hope that you take away from this what are we going to talk about so first we'll cover the different roles that exist in machine learning organizations and what are the different skills that are required for each of those roles then we'll talk about machine learning as in the context of the broader organization then we'll talk about some management best practices for how managing machine learning teams looks different from managing regular software teams and then we'll talk about hiring so how a lot of machine learning teams think about or maybe you should think about hiring ml engineers and how to also get hired if that's your goal so starting with roles
Full Stack Deep Learning - Spring 2021,13,164,ML Roles,here is a list of some of the common machine learning roles that we see out there machine learning product manager devops data engineer ml engineer i'm a researcher data scientist right so there's there's a lot of different roles that are involved in the process of building machine learning models so one question you might ask is what's the difference between all these different things how do they work together to build a machine learning-enabled product so starting with ml product manager this person is sort of responsible for working with the machine learning team to help prioritize and execute on on uh projects and one way you can think about the the work that these different roles do is you know what are what is the output that these roles are typically measured by and so for product managers it's things like design docs and wireframes and work plans devops and engineers are the engineers that are responsible for deploying and monitoring production systems and so their work product like that the thing that they're measured that they're measured on is the final deployed machine learning system data engineers are responsible for building the data pipelines the aggregation the storing and monitoring of the data that goes into creating your machine learning systems they're building distributed systems essentially i'm getting more into the ml specific roles ml engineers are you know the way we define it the folks that are typically responsible for training and deploying the prediction models themselves and so their work product is the prediction system itself running on real data in production and so these are these folks are like working with tensorflow but they're also working with tools like docker to actually productionize the machine learning systems that they're creating there's another role that exists in some organizations that i would call a machine learning researcher and so these are folks that are also training prediction models but those prediction models are either kind of more forward-looking more speculative less production critical or they're working really closely with an ml engineer or another engineer to productionize them and um and so these folks are producing a model and sort of a report that is used to describe that model how well is this model do what is it useful for and things like that and so the distinction is that ml researchers typically are not deploying models themselves and then lastly there's this role called data scientist which is kind of a catch-all term used in this field so it's it's used to descrip i've seen it being used to describe any of the roles listed above and in some orgs this is actually something totally different from the machine learning process so in some organizations data scientists are more really more like business analysts they're they're you know running sql queries to produce dashboards to help answer business critical questions and so when you see the term data scientist it's important to dive in a little bit and understand in a little bit more detail what that actually means in the organization that you're you're talking to here's a um here's a breakdown of kind of how we think about some of the different skills that might be needed for all of these different roles so on the left access is kind of how much software engineering skill you need um on the right on the the bottom axis is how much kind of skill and experience you need in machine learning and the size of the bubble is the the bar for um technical writing and communication so how well you're able to communicate the ideas that you come up with with other people and there's a spectrum of the different degrees of these two skills that are needed in these roles so starting from um starting from the top left with really really high software engineering skill and actually not necessarily any knowledge of machine learning at all maybe beyond the basics are these ml devops folks and so these um people who are in these ml devops roles typically come from um traditional software engineering pipelines traditional software engineering roles and this role really is more software engineering with a little bit of knowledge of ml baked in because those are the the customers that you'll be working with data engineers also require a lot of software engineering skill but they they also are starting to need some knowledge of of the basics of machine learning as well because the machine learning team is kind of a very active customer of the pipelines that you're building machine learning engineers are kind of right in the middle requiring actually a ton of skill and experience in machine learning but also really solid software engineering fundamentals and so this is this is a pretty rare mix and these folks can come from different types of backgrounds it's often software engineers people who worked in software engineering for some number of years who have done quite a bit of self teaching um in you know uh in the field of ml but it's also sometimes people who are you know science or engineering phd dropouts or have some more uh machine learning focused background who then went into a software engineering role and trained in that discipline for a few years and these are kind of the the unicorns of the machine learning hiring landscape these folks are very hard to come by and typically command significant premiums on the job market these days machine learning researchers these are your ml experts right so these are the folks that you know if anyone on this list tend to have some higher degree of a master's or phd in cs or stats or in some cases they did one of these industrial fellowship programs like the google brain residency or or something like that um not all ml researchers have that characteristic some folks you know just did undergrad and went right into the field but i would say those are the more common types of background for that role data scientists again since this is kind of a catch-all role catch all term these folks come from a really wide range of different backgrounds you see folks that are in data science who you know maybe did an undergrad program that specializes in data science and then you also on the other side of the spectrum you see folks who have you know hard science phds uh physics phds and things like that who have transitioned to this kind of role and then lastly the this mlpm role this is kind of an emerging role so there's not really a typical kind of path to this role right now but it's often people who come from a traditional pm experience but you know who for whatever reason have gotten a lot of exposure to the ml process um you also see a lot of times folks who came from the ml world realized that they didn't want to actually stay in um in like kind of core technological development and transition into pm later in their careers any questions on the the different roles in the machine learning typical machine learning development life cycle startups that i've seen um be really successful this kind of thing uh tend to index more on these roles in the middle that do multiple things so ml engineers um you know folks that can that know enough ml to build machine learning models but also our solid engineers those that tend this tends to be like over represented in startups from what i've seen um because it's kind of a generalist role um i don't see many startups that have mlpms or um and then ml researchers are present in startups but mostly in startups that are really doing like pretty hardcore you know machine learning first type of companies so like in self-driving car startups or other robotic startups um devops i think is a pretty uncommon role in startups because you know devops tends to be a role that um becomes more critical as the complexity of your product and your engineering team increase so i think that one's still pretty uncommon and then data engineering i think is uh also something that at least startups should invest in i'm not sure how many of them actually do but maybe that's that's uh that's if you know if i were building a startup that was building a product that has machine learning in it data engineering is one of the first things i think is worth investing in
Full Stack Deep Learning - Spring 2021,13,675,ML Organizations,all right so we talked about the different roles that go into machine learning organizations next thing that we're going to talk about is how machine learning teams themselves are situated in the context of the larger organization that they're part of so um i went out and had kind of a bunch of conversations with folks about this to understand where where their machine learning teams are situated in their org and how they kind of interact with the rest of the organization the overall lessons learned here are that there isn't really a consensus yet as to what is the right way to structure a machine learning team um different organizations have different practices and different kind of structures seem to work well for different types of organizations and so the goal of this section is really to help is really just like provide kind of a taxonomy of best practices for at different like maturity levels of organizations based on what we've seen so the metaphor that we're going to use is scaling the machine learning organization mountain so starting starting from the bottom the bottom of the mountain we have organizations that have kind of nascent machine learning or maybe they're doing machine learning in some ad hoc way so what does this look like for your organization really this means like no one's doing ml or there's a couple people in the in the organization that are doing it ml and some ad hoc basis you probably have very little machine learning expertise in-house um what kind of organizations fit this bill right now well if you look outside of like silicon valley tech companies and fortune 500 companies most companies are in this category like most kind of small to medium businesses um even ones that do have software teams in-house and especially ones in less technology forward industries tend to be doing very very little machine learning right now maybe there's a couple people in the org that are experimenting with it um you know hard to find advantages of a model like this if you're trying to do ml but one of the big advantages if you're looking to join an organization at this stage is that there's probably low hanging fruit there's probably things that you can go in and do and just have a big impact the big disadvantage of joining an organization at this stage if you want to do ml in the context of the organization is that there's often very little support for machine learning projects the organization itself might not really believe in machine learning they might not feel like there's a mandate to be doing complicated things like machine learning um and so you might be fighting an uphill battle in terms of like convincing people that what you're doing is worth the time it's taking worth the money that it's taking and um actually a good idea and then if your goal is to kind of build out a function in this organization to use machine learning to do good things in this organization it can be really difficult to hire and retain good talent because um in machine learning like in many other technical fields a lot of you know many really talented people want to work with other really talented people and so you can go and work in an organization like this and be the talent magnet but um it's it's going to be you're going to you're going to be fighting against the um the kind of natural tendency of people to want to go where other machine learning people are climbing up the machine learning organization mountain to the next stage um kind of the next stage of uh that many organizations adopt machine learning at is what i would call like ml r d archetype so what does this look like um this is kind of the stage at which the company is you know decided that it's curious about machine learning it's starting to do some research and development um around it it has some pocs and um you know it's starting to invest in it in maybe a more exploratory way to figure out okay is this does this really make sense for our business and in many of these organizations the machine learning effort is centralized into a smaller team that's maybe off somewhere in the research and development arm of the organization and in a lot of cases what this actually corresponds to in terms of the outputs of these teams are since they're not really closely embedded with product teams they're really operating more like researchers they're kind of getting data sets from other parts of the organization they're running experiments on them and they're producing internal reports or maybe external papers that are essentially you know proof of concept that maybe one of these models could be useful in the company so i've seen this a lot in some of the larger industries that are slower tech adopters so oil and gas companies manufacturing companies telecom companies that maybe have an ml research group somewhere but are not actually doing ml at scale yet there's a couple advantages to this model one is that you can often hire pretty experienced researchers into an organization like this because since the the mandate of this like mlr d team is to do research and not to necessarily build actual products that tends to appeal to researchers who want to keep doing the kind of thing that they've been doing and the other big advantage of this i think is pretty underrated is that you can you you know since these teams are not really being held accountable for near-term output let's say they often have the mandate to work on longer-term business priorities and things that could be bigger wins if they actually work out but the disadvantage is that people that work in organizations or machine learning teams that have the structure site are a big one tends to be that it can be really difficult to get data from the rest of the organization right so if you're if you have to go out um into other parts of the organization and ask them for data um that you need in order to solve the problems that you're solving in many cases those there's not a lot of buy-in from those other parts of the organization that they really need to help you and so getting data even internally can be difficult for these teams and then another big disadvantage is that you know if if the goal if your goal is to produce you know products or business innovations that have machine learning in them then this model just doesn't really work very well so these efforts from what i've seen rarely actually translate into into real business value and so usually what that means is that the the amount of investment in these organizations remains pretty small um unless through some heroic effort of the team they manage to push something out into the world that um that causes the the organization to believe that this is really something that can make their products better now moving up the the mountain from the ml r d archetype the next um archetype that is pretty common is having having machine learning folks that are dedicated to doing machine learning but not actually having a centralized machine learning team so this what this looks like is you'll have machine learning oriented um individuals who are embedded into different parts of the business or different product lines um and maybe these are only certain product teams or maybe it's most of the product teams and the um and this model really the thing that excels about this model is that you put your ml expertise alongside the software um and analytics talent in the organization um the ml folks on the team typically will report up to the same engineering lead or tech lead that the rest of the the software team and the organization reports up to a lot of smaller or medium sized or like even high growth like software technology startups fit this you know choose this model when they're first starting to adopt machine learning and it's pretty common in particular in financial services companies and fintech companies there's a couple big advantages to this model so i think the biggest one and the reason why a lot of startups in particular choose this is that the when the machine learning folks that you have in the organization make improvements it's very likely that those improvements are going to lead to real business value right because those folks are sitting with a team that's building a real product and so they're the problems that they're choosing are inspired by the needs of that team and they have the the support of the engineering organization around them to actually get those improvements into production um and there's in particular there's a tight feedback cycle between an idea that the machine learning practitioner has and actual product improvements and so you can really see the impact of your work if you take a role in an organization that's structured like this but there's a few disadvantages to this model as well which is why i think it's not where um most companies end up in the long term if they invest really heavily in machine learning so one thing is that um kind of just like in the nascent machine learning organizations it can be hard to hire and develop really really top-tier talent in an organization like this and i think the core reason to that is that um you as a as a like data scientist or a machine learning person in an organization like this you're a little bit off in an island so you're you're kind of um you're not really working with people that are are practicing the same craft as you at least not super directly um and so that can be that can be challenging for folks that want to surround themselves with people who are really good at their um their their craft and to learn try to learn quickly from them access to resources can lag so in a lot of organizations if you're a data scientist reporting up to the product team it might be hard to get tons of compute resources or things like that because your budget is going to be coming out of the budget for that product team and um i think maybe the the the um largest disadvantages to working to working as a machine learning person in an organization like this is that machine learning project cycles tend to operate a little bit differently than software pro uh product cycles and we'll talk a little bit more about this in a bit but you know the core difference is that when you try something in machine learning you know most of the time it doesn't work right so machine learning projects tend to come with a lot of risk and a lot of uncertainty and that can be difficult that can be difficult to fit into engineering sprints and engineering planning cycles and in particular longer term machine learning projects things that might take that might be very uncertain and take six months or a year can be very difficult to justify in a structure like this so that's i think one one thing to be aware of if you're if you are considering jumping into a role like this the next higher stage on the machine learning organization mountain is the an independent machine learning function and so what does this look like this is a centralized machine learning organization just like exists in the ml r d archetype but it's um where it's situated in the organization is very different so the the ml division of the company might report up to senior leadership in some cases maybe even the ceo of the company this these types of organizations might have mlpms they might have ml researchers and ml engineers that are working with internal customers to build out machine learning-enabled products to help those internal customers make you know either make their processes better or maybe the products that they're producing better these teams often are also engaged in longer term research whether it's internal facing or actually writing papers and um oh you know the types of organizations that tend to have this type of structure are like large financial services companies big banks for example um and also like many of the larger uh larger tech companies outside of you know google and facebook and uber and places that are really well known for machine learning like that like a salesforce type organization the big advantages of this model and the reason why many organizations choose it is because you're creating this sort of um the center of excellence around machine learning so you have a really high talent density which means that you can hire really good people who want to work with other machine learning folks and train them because you're surrounded by other people who are doing the same thing the advantage of structuring the organization like this as opposed to having it be part of the r d function is that you know typically it reports up to a pretty high level of senior leadership and so that means that a lot of the problems of data availability you can you can kind of bust through because you know if you're if your machine learning organization reports to the cto or something like that then the cto can help you um get data if that's what you're having trouble with and then lastly in in the kind of machine learning embedded in product functions archetype um one thing that often lags is um is like centralized tooling and centralized infrastructure for the ml team but organizations that fit this archetype can invest a lot in making sure that they have a great machine learning platform and culture around deploying machine learning models the disadvantages of an archetype like this are that um you since the the folks that are doing the machine learning are not embedded into the product teams themselves they have to hand off the models at some point to the to the folks that are actually going to be using them in their products and that can be challenging so you need buy-in from the users and you also need them to have some level of kind of baseline knowledge about what is this thing that they're using so that they can make good decisions about when to you know when it's working and when it's not working and because of that um feedback cycles can be pretty slow all right so we've reached the top of the machine learning organization mountain and uh the i i think kind of the the standard that most organizations should eventually try to build to as their machine learning function gets more mature which is the the archetype of machine learning first organization so what does this look like um it's like really strong buy-in that machine learning is important across the organization and you have both a machine learning division of the company that is working on challenging long-term projects like infrastructure research and higher risk projects within the context of the business but then you also have ml expertise within every line of the business every products team that are focusing on quick wins and are working with that central ml division to make sure that there's machine learning being deployed into the products that they're working on so there's a few examples of companies that are um fit this archetype or are close to this archetype and they tend to be you know your large tech companies your google facebook uber type companies um and then also some of some really machine learning oriented startups the big advantages of this are you have great access to data um because you have folks that are thinking about things from a sort of a data machine learning first perspective scattered throughout the organization but you also have central resources to collect that data and push through organizational silos it's really great for recruiting because you have this centralized ml team that works on the hardest machine learning problems in the organization you have lots of resources and lots of real business problems to solve and deployment is easiest in this model so your product teams have some understanding of machine learning baked into them but you also have a team that's able to invest in the infrastructure that it takes to deploy stuff well disadvantages of this model um really the disadvantages are practical disadvantages it's just hard to do this so it's hard to implement this structure well it's hard to recruit enough talent and it's culturally difficult to make sure that everyone in the organization um has the base level of understanding that they of machine learning that they need to make this work so a few kind of design choices that are uh that um that machine learning organizations need to make as they're figuring out how to set up their team structure so one is balancing software engineering and research so you know to what extent is the machine learning team itself responsible for building and integrating with software versus like you know shipping models off to some other team um and related to that like when you're when these teams are hiring people how important is it for them to get software engineering skills on the team versus focusing on getting people that are good at machine learning and good at data data ownership um how much control does the machine learning team have over data collection um data warehousing data labeling and data pipelining um versus being you know customer of some other team that's solving those problems model ownership um is the machine learning team the ones that are deploying the models into production or do they hand them off to some other team to do that and who maintains the deployed models when they're in production um so different organizations um make different choices uh for these different design choices depending on like which stage of uh what where they are in the mountain um and so i'm not gonna go through the details here but um kind of as you'd imagine um as you get further and further to the right on the on on the mountain things get more and more specialized and the machine learning team gets um gets more and more control over the data and the models that they're producing
Full Stack Deep Learning - Spring 2021,13,1730,Managing ML Teams,all right i'm going to move on and talk about managing machine learning teams so there's i think as i alluded to earlier there's a core challenge in managing machine learning teams that makes it uh more difficult in some ways than managing traditional software teams and that core challenge is that it's often really hard to tell in advance how easy or hard something in machine learning is going to be um so this is a set of charts from a blog post by lucas b wald the weights and biases founder and this is from a kaggle competition that he ran where they were um and this is plotting the the accuracy of the best um the best submission over the course of the competition so in the in the first couple weeks this is what the accuracy over time looked like so things are improving really really fast right and so it's like this is great you know we've hit we've gone from 35 to 70 accuracy in like a week so um you know we're going to crush this we're going to crush this benchmark we're going to hit like 99 accuracy and solve this problem right um if you extend this graph throughout the entire competition this is what it looks like right so essentially you know there was a huge improvement in that first week and then very very marginal improvement um thereafter and that's not due to lack of effort this is if you can plot that against the number of teams that are participating in the competition which is going up really steadily over the course of the competition and so the the upshot of this is that you know um you might read this chart on the left and infer like okay based on past improvements we think we can improve our model by this over the course of the competition but making those types of judgments in ml is really really challenging it's very very difficult to know how difficult or hard or something is and whether you know early signs of progress are really indicative that the problem is going to be easy or whether you know the easy gains were easy and the hard gains are going to be hard um and so in addition to being difficult to tell how hard something is the progress the like kind of pace of progress for machine learning teams tends to be very non-linear um so it's very common in in my experience for machine learning projects to entirely stall for weeks or for weeks or even longer right where there's no kind of measurable improvement in performance over over that time period and um in the early stages of the project it can be really difficult to plan because it's unclear what's going to work right you might have a dozen ideas of different models to try different ways of augmenting your data different ways of collecting more data new architectures that you're dreaming up but it's very very hard to know in advance whether any of those or which subset of them will be successful and so as a result of these two things planning um project timelines is in particular it's extremely difficult so one way to think of this is that you know even production machine learning is still somewhere between what you know how most of us think of research and what we would think of as like a true engineering discipline on top of that there's um you know in in the real world machine learning teams at some level if you're going to build production systems need to interface with engineering teams software sharing teams but there's often called big cultural gaps between these two fields so they have you know different values backgrounds goals and norms um and in more toxic cultures what this can lead to is these two sides really not valuing one another right like you'll have um a setup where you know software engineers think of machine learning researchers as these like divas who don't even know how to code and machine learning researchers think of software engineers as these like um you know these like plumbers who don't have you know who can't have a creative idea to save their save their lives and in reality neither of those like for in healthy organizations these teams collaborate really closely with each other and have a high degree of respect for one another um but the the differ the different cultural norms in the fields can be a um be a blocker to that another big challenging a challenging thing for managing machine learning teams is that in many organizations leaders just don't really understand machine learning um so they you know they they may not know like what's actually feasible they may not know how and they may not understand this like the fact that things are just going to take longer or at the very least the timelines are going to be more uncertain around machine learning projects so the next thing i want to talk about is you know i don't have an answer to how to get rid of all these problems i think managing machine learning teams is a genuinely difficult and unsolved problem but i want to give a couple of insights that i've learned from folks that are really good at this about how to manage machine learning teams better the first is to do um instead of doing kind of traditional waterfall based project planning to instead do project planning for your machine learning projects probabilistically so this is what your um kind of like task chart might look like for a traditional software project you have all these tasks and they maybe feed into one final task and you can plan out how long each of them take and how they um pipeline into one another in the machine learning world um like conceptually one way to think about project planning is that you should also be assigning success probabilities to each of the tasks um so you know task a might you might you might say like uh that's only 50 likely and then in order to do task d which is maybe the thing that we care about it depends on these two tasks each of which is maybe only um or maybe let's say it depends on at least one of task b or task c um but each of them are only you know 25 or 50 percent likely to succeed and so we're going to do both in parallel um and then what this might look like over the course of the project is well we work on task b and task c for a week we realize hey task c is really not working at all like let's call this failed um and task c actually seems pretty promising like this this model architecture that we picked seems like it's going to work but it's going to take longer than we thought so we'll spend the second week on task um task a or task b rather and then we'll extend our timeline for cassidy um and then over time you know as you get a sense for which projects are going to be successful or not as your beliefs about the likelihood of success of each projects of each project evolve you can then think in the future about which other projects depend on those projects and adjust your plan based on that and so um really what this means is that you have what you want to have is a portfolio of approaches so um the the corollary of uh of of doing machine learning project planning probabilistically is that you shouldn't have any um path critical research projects right so if you if you need to um if you need to like have an answer to this one question in order to move forward then you shouldn't you know in in a perfect world you shouldn't just have one idea about how to do that you should be working on a couple of different things in parallel um you don't need to do them in parallel but um you could try them sequentially like let's say if you only have one person that's working on them but um many good organizations do they have kind of a norm where hey we have we have to like we have to improve this model by x percent and so we're gonna have um two researchers try five different ideas and the one that looks the best after two weeks is the one that we're gonna go with um so kind of like a a friendly competition of ideas um and you know coming back to the cultural norms around managing machine learning teams well this can be a very difficult thing to get right culturally because when you have you know different people in the team having their ideas compete with one another that can um that can lead to negative cultural consequences if you don't build the culture of the team the right way another another upshot of this is that it's when you're measuring the success of the team it's really important to do this uh based on inputs rather than based on success um so what that means is that you know when you're doing performance management when you're deciding like who on the team is doing well who on the team is not doing well it's really important not to get hung up on whose ideas worked and whose ideas didn't work in the long term it is really important for people to do things that work um but on any given project the success measure is how well you executed on the things that you tried not necessarily whether the things that you tried ended up being successful or not all right since we're taking this portfolio of projects approach um another kind of thing that i've seen be really important for organizations that do this well is having researchers and engineers working pretty closely together um one common failure mode for a lot of organizations is thinking that either engineering is more important than research which often leads to kind of getting stuck on the machine learning side or the opposite right thinking research is more important than engineering which tends to lead to like really elegant solutions to problems maybe that no one has or solutions that can't be productionized another best practice for making this work well is trying to get something end to end working relatively quickly um this does a couple things for you first it makes it just makes the task more likely to be successful since you already have like a version of it that um does almost what you want but it also allows you to communicate your progress to leadership better right because if you have um if you have a basic prototype of your thing working that's maybe only like 50 accurate when you need to be 80 accurate then you can go report to leadership of like oh this week we went from 50 to 60 this week we went from you know and next week we're going to try to go from 60 to 65 um and it's just it's a it's a clear way to communicate the progress because you can actually um have clear metrics and have something clear to show people um and then the last kind of best practice i would cite here is trying to educate the leadership of your organization on this phenomenon right this phenomenon of machine learning timeline uncertainty um unfortunately there's you know as as like machine learning engineers in many cases there's not a whole lot we can really do here to to change people's minds about this um a lot of the onus here is on leadership like to actually build their own realistic understanding of the way that these things work but i think that there's like machine learning teams themselves bear some responsibility for this as well um so uh you know just to give you an example what is a what is a bad like kind of weekly status update to leadership sound like um so a example of a bad weekly status update to leadership might be like hey leadership um you know this week we did great we improved our cat detection model from 60 to 80 accuracy you know we have lots of ideas for how to continue to improve from here next week we're going to try making our model bigger and we hope to improve accuracy even further right so it's kind of this kind of like um over optimistic like hype building style of communication that focuses on the things that the machine learning team cares about and is working on not what's actually leading to making the system the overall system that you're trying to create better it doesn't communicate the risks and it doesn't communicate the uncertainty that you have around timelines going forward all right um oh yeah next thing i want to point to is um if you if you are in this position where you know someone higher up in the organization doesn't understand the way that machine learning works and um and the way that it's different from building software there's a couple of resources that you can point them to um there's this this kind of older blog post from andreessen horowitz or i think it's like actually a talk that's a very high level overview of ai um which is pretty out of date at this point but still one of the best intro level like executive level materials that i've seen and then um peter taught an ai strategy class uh in the business school at berkeley what i guess maybe a year ago at this point um and so that's also worth pointing people to all right last topic that we want to cover is hiring so both we're going to look at
Full Stack Deep Learning - Spring 2021,13,2477,Hiring ML Engineers (Or Getting Hired),this from both perspectives right from the perspective of someone maybe who's trying to hire ml engineers maybe um you know in a couple of years you have a your own ml startup and you're looking to hire ml engineers but also from the perspective of someone who's trying to get hired as an animal engineer we're gonna talk about a few things here um first is the ai talent gap and so this is just important for setting the scene about what hiring in the machine learning world is like right now um so you might ask yourself like you know what are the supply and demand dynamics of this market right how many people are there out there that know how to build ai systems um different ways of estimating this there's a couple from element ai you know 5000 actively publishing research that's probably too narrow um 10 000 with the right skill set um bloomberg estimated there's 22 000 phd educated ai researchers um element ai gave an upper bound of 90 000 uh people based on the methodology that they used um and tencent also had a number that they thought was around 200 or 300 000 which is the number of like ai researchers and practitioners um and so you might think like oh this is actually kind of a lot but if you compare this to the number of software developers even just in the us which is 3.6 million roughly or around 18 million in the whole world there's really not that many folks that are that are out there in the world doing machine learning right now um and so the the sources here are all listed in this blog post i think these numbers are probably a couple years out of date at this point but hopefully still give a rough indication of of um the talent gap and so what this is produced is a fierce competition for ai talent um this is a quote from a uh from bloomberg you know everyone agrees that the competition to hire people is intense academic conferences are becoming frenzied meat markets seven-figure salaries for top researchers you know it's crazy out there um this is a quote from a computer vision engineer at a later stage startup that we interviewed hiring is crazy right now this is a young field it got popular very quickly there's a ton of demand not a lot of supply um uh from another startup founder it's really really challenging to hire from ml takes way more time and effort than we expected um we have someone working on it full time and we're still only able to get a few people per quarter so that's just setting the stage for you know the difficulty that that companies are having hiring ml engineers right now um so the next thing i want to talk about is how um how folks should think about sourcing talent for machine learning teams folks that are in a hiring position again we we looked at a number of different common machine learning roles um uh i think for some of these roles like for ml product manager devops and data engineer it requires maybe a slightly different mindset um than hiring for you know the more software like traditional software version of these roles but not really that different like you might want to look for some interest in ml some study of ml but fundamentally not too different um and so for the purposes of this lecture we're going to focus on the more core ml roles ml engineer and ml researcher type roles so there's a there's a um there's maybe not like a right way to do this but there's certainly a wrong way to do it um so this is a this is like maybe a caricature of a job description but uh maybe not too far off from like what i whether what's an actual machine learning job descriptions feel like um which is you know duties of this unicorn ml engineer um keep up with state of the art you have to you have to be able to implement models from scratch you need to have you know deep understanding of mathematics you need to be able to come up with new models on your own you need to be able to build all the tooling and infrastructure for the machine learning team yourself you need to build data pipelines as well you need to be able to also you know deploy and monitor all the models that you're creating in production and you know so what you really need in order to be successful in this role is of course you need a phd um definitely need at least you know four years of tensorflow experience um uh and like at least four years as a software engineer as well right oh and by the way like if you don't have at least a couple of publications in nurps or icml then probably not fit for this role either um so this is obviously like this is this is obviously taking this idea of needing people who are good software engineers and good ml um and have good ml skill sets too far um but this is i think like how a lot of companies are really trying to hire ml engineers right now um so what's the right way to do this well again i'm not sure there is a single right way but um i think one thing that more companies should think about is hiring for um a couple of different paths right so one is primarily hire for software engineering skills and maybe some experience in ml and a desire to learn and train folks to do ml um another thing that you that companies should do more of is go more junior right like these days most most folks are graduating from berkeley with some ml experience so more junior folks are more likely to have this kind of background um and then the core thing that i recommend teams to do is to be more specific about what you really need right like not every ml engineer that you hire needs to have all the skill sets on the previous page um if you're hiring for ml research positions there's a couple of specific recommendations here one is to look for more for quality of publications rather than quantity and this requires like having some taste for what you consider a high quality publication to be but it's things like originality of the ideas the quality of the execution of the paper other thing i recommend is look for researchers who work on problems that you think are actually important right a lot of researchers tend to focus on problems that are trendy at the moment that they're doing their research without really thinking about why the problems that they're working on matter and that is the kind of research that won't translate super well to a to a company environment oftentimes folks that are that excel outside of academia are researchers that have worked before outside of academia so that's another thing to look for and um a couple other like alternative paths to consider are folks who are really talented in adjacent fields like physics or statistics or people you know without phds right from non-traditional academic backgrounds people are really talented undergrad or master students with some research experience or folks who have been in one of the industrial fellowship programs how should um how should folks think about like sourcing ml engineer and ml research candidates you have your standard sources you can also keep an eye on top conferences and archive and flag first author papers that you like at open ai some of the hiring committees would basically just go through all the icml papers and flag the ones that they thought were interesting and reach out to the first authors another source other than just papers is looking for good re-implementations of papers um so if you if there's a paper that you're interested in and you find a re-implementation that is good that you know whoever re-implemented that paper could be a good hire um a lot of this recruiting you know in the when in-person conferences happen again a lot of this recruiting happens at ml research conferences um so that that's another good place to go look for people to hire or go look for a job so you know next question you might have is like if you're if you know if when you're running your startup and you're um maybe you find some really some really talented machine learning research machine learning engineering candidates how do you actually convince them to join like what are folks um in those types of positions looking for um so i think like there's no real generalization here different people want to join companies for different reasons but there's a few common things that i've seen from folks that fit this profile so working with cutting edge tools and techniques you know latest papers the latest infrastructure systems building skills and knowledge in an exciting field so really being able to learn a lot about about a fast-growing area of machine learning working with excellent people um working on interesting data sets so one of the unique things you can offer as a company is is the data that the folks will be working with and then lastly and maybe most importantly doing work that actually matters and so how do you translate this to you know to the pitch that you that you give to people that are thinking about joining well um for folks that want to work with cutting edge tools and techniques you as a company um can work on research oriented projects and when you do that really highlight those things so publicize them maybe write papers about them or blog posts invest in the tooling that your team is using empower people to try new tools and really create a create the kind of environment where people are actually working on cutting-edge stuff for for candidates that are excited about building skills and knowledge in an exciting field there's one thing that some organizations do really well is building a team culture especially on the machine learning team that is oriented around learning so peter talked about reading groups in his lecture last week another other ways to do this are learning days so specific days that you have set out for focus to for people to focus on learning new things having a professional development budget a conference budget just really investing in people to make sure that they're staying up to date working with excellent people one way to make your company stand out on this access is just to have you know at least one or two people working there that are relatively high profile now that's maybe easier said than done because you need to convince the high profile people to join but the other way to kind of bootstrap that if you don't have that is to help your best people build their profile and the way to do that is to help them publish blog posts and papers so that they get their name out there um for if you have an interesting data set to work with something that's unique something that is um maybe has interesting technical properties then you can sell the the properties of the data set in the recruiting process and then lastly you know just like recruiting for any position selling the mission of the company and the potential in particular for impact of machine learning on the company's mission can be really helpful in in closing folks that uh to work on this kind of stuff all right um next let's talk about interviewing um so uh i think like some of the things that you'll see folks test in machine learning interviews um if you're going through some of these interviews are you know trying to help assess whether you can think creatively about new machine learning problems testing generalist software engineering skills um and i think in many organizations that's true both for researchers and for software engineers uh who are slotting into ml roles um and so the the degree to which those things will be tested will differ depending on the role but i would expect to be tested on both your knowledge of ml and your ability to do software engineering if you're going into an industrial role what actually happens in ml interviews so the ml interviewing process is much less well defined than the software engineering interview process so there isn't really a book that you can go and buy that will help you prepare for machine learning interviews in the same way that there is for software in engineering interviews but there's a few kind of types of assessments that i've seen to be pretty common there's you know your traditional background and culture fit there are there's you know analogies of typical software engineering interviews like whiteboard coding or pair coding one thing that's i've seen be unique to machine learning interview processes is pair debugging so looking at some machine learning code that has a bug in it and working with your interviewer to find that bug um math puzzles like say involving linear algebra are pretty common take-home projects are pretty common um what i would call like an applied ml assessment so explaining you know interviewer articulates a problem that you need to solve you explain how you'd use machine learning to solve it um probing into past projects that you've worked on so if you have a past machine learning project that um that you can talk to maybe it's the project for this course being able to kind of go deep on what are the different choices that you made what worked and what didn't work and then machine learning theory questions so explaining things like the bias variance trade-off to your interviewer last thing i want to cover is finding a machine learning job so if you're if you want to get a job as a machine learning engineer where should you look so again there's your standard sources your linkedin your recruiters your on-campus recruiting but if you want to go beyond that then looking at the machine learning research conferences can be a really good way to do it there's often tons of recruiting that happens in those places and then this is maybe bad general advice for getting jobs but in the machine learning world in particular you can often just apply directly for companies you know because there's a talent shortage a lot of companies are really just trying to look wherever they can find for talented machine learning folks and so they're more open to direct reach out in many cases than um in the traditional software engineering world um how to stand out so if you are applying for these jobs what are things that you can do to make your background more impressive um so having basic software engineering skills you know working at a stripe or a google like a well-known software engineering company is is really good um having some interest in machine learning is also really important um if you can if you can demonstrate that knowledge so writing blog posts that synthesize a research area or writing blog posts that you know um explain a new research topic that's emerging or a new category of models that is emerging can demonstrate that you have like a good grasp of the overall field and that can be really impressive to people that are doing hiring but even better than that is demonstrating the ability to get machine learning projects done so if you have side projects or paper re-implementations that you could point to that's often a way to get over the hurdle of this person doesn't have that much experience so how do we know that they're going to be able to get stuff done and then especially if you tend more to the research oriented side than proving you proving that you can think creatively in machine learning so you know winning toggle competitions publishing papers and things like that can really make you stand out um in terms of how to prepare for machine learning interviews again maybe this will be relatively obvious given the you know what we talked about being in these interviews but one thing i would emphasize here is that you should in addition to preparing like machine learning specific stuff like reviewing ml theory um thinking about how basic ml algorithms work like rev reviewing your 189 materials i would also recommend preparing for a general software engineering interview because many companies do test machine learning engineers for basic software engineering skills as well
Full Stack Deep Learning - Spring 2021,13,3414,Conclusion,all right so to wrap up here since we're going to move over to the to the panel um we talked about roles organizations managing machine learning teams and hiring there's a couple of takeaways from each of these here uh in in terms of roles there's many many different skills that are involved in making production machine learning work and so the thing i would want you to take away there is there's a lot of different ways to contribute and so you should think about that that chart of different roles and how they plot skills on the on different axes and you know just know that there's like you can you can max out on software engineering you can max out it on ml or you can have some some balance of the two and still find a lot of ways to contribute to machine learning projects um and uh and and lastly i would say you know in in terms of in terms of hiring um it can be difficult to break into the machine learning world as an outsider um and so that the maybe the main kind of takeaway that i would give you there is like one of the best ways to do it is to use projects as as your way in so um hopefully the project you create as part of this course you know if you don't have a portfolio of projects already can be the first step toward doing that if you do want to go get a job in this field all right and that's all we have for today um thanks a lot everybody
Full Stack Deep Learning - Spring 2021,11A,0,Introduction,this week we're going to talk about deploying and monitoring models for most of the class so far we've been talking about basically getting up to the point where you have a model that you're confident enough to deploy into production and so this week we're going to talk about how do you actually deploy it into production and what do you do with it once it's there like how do you maintain it and make sure it stays healthy little motivation i think machine learning models in production are great you can scale up and down to meet the demands of users they can deliver thousands of predictions or millions of predictions per second unlike these models that we train in notebooks which only work if you actually run the cells in the right order but i think most data scientists most machine learning engineers don't really know how to build production machine learning systems and so part of the goal of this lecture is to give you a flavor of the types of ways that you might actually go about deploying your model so two topics today really essentially two separate lectures the first one is deployment and then the second is on monitoring so what do you do with the model once it's in production but first we'll talk about deployment one way to conceptualize different sort of approaches to deploying your machine learning model is to think about where in the overall architecture of your application the machine learning model should be deployed this is a cartoon of what your web application might look like there might be some client that the user has and that is running locally for them it could be a web browser it could be some other device and it connects to a server where your code is running and that server interacts with the database in order to pull pull some data out and render it in a certain way and show it to the user and the first kind of
Full Stack Deep Learning - Spring 2021,11A,85,Batch Prediction,paradigm for deploying a machine learning model that you might think of is why don't we just run them all offline dump the results in a database and then run the rest of our web application normally and this is called batch prediction so the way that this works is you'll periodically run your model on new data that's coming in cache the results in a database and so you might think that this is over simplification but this can actually work and is used relatively commonly in production when the universe of inputs is relatively small so let's say that you have one prediction per user per day if you're if you're making a recommendation or something like that that's a case where you can just run it offline and cache it and return it to the user if they query it so some really nice things about batch prediction it's super simple to implement doesn't really require you to change your application code very much if at all and it's relatively low latency to the user because even if your model is slow the predictions are cached and so it's as fast as a database lookup but batch prediction is pretty limiting so it doesn't scale to more complex input types right if you don't if you can't just pre-compute all of the possible outputs and users are not getting the most up-to-date predictions so their predictions are always stale by the amount of time that it takes you to recache the outputs of the model and when those predictions become stale right when they don't get when there's a bug or something that causes them not to get updated frequently enough it can be really hard to detect this and it's very easy for users to start getting old predictions so the next
Full Stack Deep Learning - Spring 2021,11A,168,Model In Service,thing that you might think about is okay if we're not going to just cache the predictions of the model to store them in a database we might just deploy our model as part of our existing web service um so we'll put the model in the web service the way this works is you'll package up your model using whatever packaging format that your deep learning library or your machine learning library typically takes and then you'll include that in the deployed web server so you'll either literally copy the weights into the web server or you'll store the weights in some file storage like s3 and then download download them on the web server when it gets spun up and then on that web server it'll actually load that model and call the model to make predictions this is a pretty simple thing to do it's nice because it reuses your existing infrastructure so you don't have to think about okay how do i spin up my own services for my model or anything like that but there's some pretty heavy drawbacks to doing it this way which is why it's not the most common pattern for most machine learning systems the first is that oftentimes like the web server itself is written by maybe a different part of your team in a different language so maybe you built your model in python tensorflow let's say and the web server might be built in like node or something like that and so you then there's a translation step that's needed in a lot of machine learning applications the server code and the model code or the model itself don't really change at the same rate so maybe for example let's say you don't really update your your web server all that often but maybe you need to retrain your model every hour or every day or something like that would actually require you to redeploy your entire web server using this model if you have big models this can really start into eat into the resources that you allocate to your web server machine learning models are often very expensive to run and in many cases the hardware that we choose for our web servers is not optimized for your model so most concrete example is if you want to run your model on gpus you're probably not already using gpus on your web server and but i think the most fundamental limitation of this method is that the you when you put the model inside the web server you have to scale the web server and the model the same way so if as your traffic scales up you can your only option is to create new copies of the web server and new copies of the model in order to scale horizontally to meet that demand but in reality your web service and your model might have very different scaling properties and so more common pattern here is to is to do what we'll talk about next so the next thing that you might think of is
Full Stack Deep Learning - Spring 2021,11A,304,Model as Service,instead of actually packaging up the model and putting it inside of our existing web service let's deploy the model separately as its own service and and then our web server or even the client can talk to that model directly and this i think is more or less the most common pattern for deploying machine learning models and is most of what we'll spend time on today and in the labs but the way this works is you run your model as its own web server and the back end or the client itself interacts with it by making requests to the model service and receiving responses back so it's almost as if you have your own little app that just is responsible for taking care of the model nice things about this are it's more dependable than the model in-service pattern because if you have a bug in your model if your model nands out or something it's less likely to actually crash your web app or like memory leaks or things like that that can go wrong with machine learning models it's easier to scale or it's more scalable rather because you can pick hardware and horizontal scaling which we'll talk about a little bit that is tailored towards your model rather than tailored towards your entire web application and it's more flexible in some sense because if you have one model that's used in multiple different applications or multiple different parts of your application they can all interact with the same service you can update that service once and all of the other parts of your app that are interacting with it will automatically have the latest version drawbacks of this pattern so it can add latency right you have an extra an extra kind of call sometimes across the network in order to access your model so really low latency is what you're going for this is probably not the best pattern it adds some infrastructure complexity because there's now a new service that you have to deploy and maintain and manage the interactions with but i think the biggest thing is especially for a lot of machine learning folks is the consequence of doing this is now you're on the hook for running your own model service and so let's talk about some ways that you can actually approach doing that all right so a few things we're going to cover in in this model service topic first we'll talk about rest apis just what does that mean what are those i will talk about dependency management for your web for your model service we'll talk about performance optimization so how to make this run fast and and with high throughput we'll talk about horizontal scaling so how to scale up and down to meet demand we'll talk about deployment and then we'll also cover a few of the managed options that are out there if you look at all this and you're like this is too much for me to deal with
Full Stack Deep Learning - Spring 2021,11A,449,REST APIs,starting with rest apis so what is a rest api at a very high level it's a way of serving predictions in response to http requests that are formatted in a canonical way rest is not the only sort of standard for this for request response protocols but it's probably the most commonly used one in practice and it's the one that we mostly recommend using and we'll use in the labs there's alternatives out there like grpc which is used pretty heavily in tensorflow and within google which you'll see as well and then graphql is the other thing that you'll see is the new hotness in web services but is i think like maybe less relevant for the ml use case so this is what it could look like to call your model as a rest api curl is a command line utility that allows you to just post some data to this url and you might have a url api.fullstackdeeplearning.com predict it's on a it's not a real url if in case anyone's trying it it's just uh what it could look like and then you'll send it some data and then the response that you'll get back is some json that contains the prediction of the model so one thing you might be wondering is like why did i format the data this way is there a standard way of formatting data that's going to go into a machine learning model the answer is sadly not yet there's no real standard these are screenshots from the rest apis of google cloud azure and aws's machine learning offerings and they all accept different formats so i think one thing that i would love to see in the field is a standard to emerge around like what does a standard sort of rest api request and response look like to a model service but unfortunately we're not there yet so the the recommendation i would make here is pick something that makes sense to you or maybe just pick one of these three formats and go with that but there's no real standard to fall back on yet next thing
Full Stack Deep Learning - Spring 2021,11A,553,Dependency Management,we're going to talk about is dependency management so how do you actually get stuff into your model servers so model predictions depend on a few different things so you need the actual code that contains the logic for for serving up the prediction you need the weights of the model and then you also need any code dependencies that are required in order to run the code that you wrote locally and all those need to be present on your web server so for model weights and dependencies this is a an easier problem like you can or sorry for code and model weights this is an easier problem like in principle you can just copy these things on your onto your web server in whatever way that you're normally deploying code onto your web server for model weights since they're very large oftentimes you'll write a script that just downloads the model weights when the web server gets spun up so that they're present on the local machine but dependency management can be trickier so dependencies can cause a lot of trouble because when they become inconsistent that can actually affect the behavior of your model and dependencies can be notoriously hard to update even if the version of tensorflow that you're running changes that can change the way that your model behaves and and if you're if you're trying to manage dependencies yourself on your server then you're going to run into all kinds of problems where let's say that you want to deploy a new version of the model it requires you to bump your tensorflow version and then all of a sudden that new version of the model doesn't work and you want to roll it back how do you actually do that without without actually breaking any of those dependencies so dependency management on servers can be pretty hard and there's kind of two high-level strategies that you can follow to make this a little bit easier one is you can destroy constrain the dependencies for your model so you can just make a decision up front that says here are the decision the dependencies that we're going to allow to be part of our web service and we're rarely if i were going to change them and then the second is you can use containers so we'll talk a little bit about each of these strategies so if you want to constrain the dependencies that your model has one way that you can do that is you can use a standard format for your neural net so the closest thing to an open standard for for what a saved neural net looks like is this thing called onyx and so onyx is basically like a way of representing your neural net the premise is that you should be able to define your network in any language and then run it consistently anywhere so the idea is that like you you should be able to write your model in keras or tensorflow and then if your version of tensorflow changes that's okay because you're going to compile this model down to to this standardized onyx format and then your web service will know how to doesn't need to know about tensorflow it just needs to know how to take it on a service or an onyx like compressed file and then run that file the reality is that this kind of open exchange format is like a little bit tricky to get to work right now like current state in 2021 and i think the core reason for that is because all of these libraries that it's depending on or it's converting from change really quickly themselves so there's often bugs in the translation layer and then the other tricky thing here is that in a lot of cases in your machine learning use case you'll have some um code that's important to defining the prediction that is not part of your actual neural net definition so it's not part of your like pi torture tensorflow code so it could be like a feature transformation for example so that doesn't really get captured by something like onyx and so you still run into this dependency version issue unless you use like kind of a very constrained subset of pi torch or tensorflow or whatever other machine learning library that you're using so a more kind of robust way of dealing with the dependency issue is to use a container service like docker and so we'll give like a very high level overview of what docker is and why it's useful so the things we'll cover are we'll cover how does docker differ from a typical like virtual machine that you might have seen in some intro cs class and then we'll talk about docker files and layer based images which are the nice syntax that docker has for defining these lightweight virtual machines we'll talk about the ecosystem and docker hub and then we'll touch quickly on or like container orchestration and what that means this chart shows the difference in architecture between a typical virtual machine and a container like a docker container and so the core difference is that the docker con containers themselves don't contain a copy of the operating system instead the copy of the operating system is contained within the docker engine itself and so that means that these containers are much more lightweight like they're just like smaller easier to pass around easy to spin up etc and so because of that the kind of like the kind of standard pattern for using docker containers is that when companies start adopting docker containers they tend to just wrap like everything in a docker container you might have a separate container for every single discrete task that you're working on as part of your code base so if you have a machine learning model that's going to become a model service that might have its own docker container and then if you have a database that might be in its own docker container if you have a web app that might be it's in its own docker container and so the pattern is to give like every component of your other product its own docker container and then have those docker containers talk to one another and so again like docker being lightweight is what facilitates that being possible so in an example web app there might be four containers the web server the database you might also have a separate container for a job queue where you're gonna enqueue all the like tasks that need to happen and then a separate container for the worker that executes those tasks so that's conceptually what a docker container is it's a very lightweight virtual machine and in practice the way it's used is that you'll have docker containers that you spin up for every component of your infrastructure in many cases and those things will all be running on your infrastructure talking to each other they'll each kind of manage their own dependencies and so if one part of your app has one version of tensorflow and another part of your app has another version of tensorflow then that's totally okay because the code that's contained in the containers doesn't need to know about each other it just needs to know about its own its own dependencies and then the protocol that it uses for talking to the other containers so how do you actually define what goes in one of these containers like how do you actually do dependency management in a container so in docker there's a thing called a docker file and a docker file is basically like docker's specific format for how you define these containers docker file looks something like this so in this one you're you're building on top of an official python runtime so the python the this first line is from python 2.7 dash slim you're setting a working directory your you're pip installing some stuff you're exposing a port you're setting some environment variables and then you're running a command inside of that container environment that's what this docker file says at a very high level and so the way that you might use this is you might interact with a thing called a docker registry so what a docker registry is it's kind of like the equivalence of github forget so instead of storing all of your containers locally you might store those in those those images inside of a inside of your docker registry and then like when you want to actually use them you can just pull them down and then run them and so when you pull them down and run them you have this you're guaranteed to have this like reproducible environment that has all the same dependencies that you left it with and no changes that you made in other parts of your environment can break it the other thing that people the other thing that people like a lot about docker is that there's a really strong ecosystem associated with it it's relatively easy to find images for kind of more or less different like whatever different tasks that you want to work with so there's like pre-built images specifically for data science or for machine learning for tensorflow for pi torch and then since you have this this system of docker files that allows you to extend existing images to import from existing images and add additional dependencies and stuff to them it's pretty easy to extend those things so if you find like a base tensorflow image and you want to add some other python package you can just import from that base tensorflow image and install that python package and then save it back to the docker hub and it's pretty easy to do this with private images as well this is just a chart of a number of polls of docker images over time and so dockers for the past few years has been on this exponential growth path of adoption in the machine learning or in the software world more generally and it's become a pretty ubiquitous tool next thing i want to talk about is container orchestration so what is container orchestration again if our like web application is built of these of all these different containers each of which has its own dependencies and its own like application logic and then we said they're all going to talk to each other to make up our application then what container orchestration is it's basically like the logic that tells you how to take these containers and distribute them onto your actual machines or virtual machines and then have them all talk to each other and coordinate to solve the tasks that they're aiming to solve together the the kind of like emerging standard here or maybe at this point it's like fair to call it v standard is called kubernetes and so this is the the like most popular container orchestration tool but there's also offerings from the cloud providers and then there's an easier to use container orchestration tool from docker itself called docker compose which is a good thing to get started with if you're just learning about this stuff but kubernetes will come up again later in the lecture because it's been adopted pretty fervently by some corners of the machine learning ecosystem so a lot of the tools that we'll talk about are built on top of this container orchestration framework called kubernetes so we talked about dependency
Full Stack Deep Learning - Spring 2021,11A,1110,Performance Optimization (Single Machine),management and rest apis next thing we're going to talk about is once you have your model service let's say running in a docker container as a web server how are you actually going to make this thing run faster first thing that we'll talk about is like how to make it or mostly what we'll talk about here is how to make it run faster on even just a single machine so there's some kind of core questions to ask here or like some core techniques that you can follow one is do you want to run inference on a gpu or not so usually in deep learning at least we're training our models and gpus but that doesn't necessarily mean that we're going to do inference on gpus then we'll talk about concurrency how to run multiple copies of your model at the same time on the same host we'll talk about model distillation talk about quantization caching which is another performance technique batching sharing the gpu and then we'll talk about some libraries that might be able to help you out with this all right so the first question is you trained your model on gpu does that mean that you should run inference on gpu so there's some pros to running inference on gpu one thing is it's probably the same hardware you trained on that can be nice because it can avoid trickiness of potentially translating your model to different hardware which can arise sometimes and then as your model gets really big and if you're willing to tune the batch size that goes into the gpu then usually this is like the way you can get the highest possible throughput and in some cases the highest cost the highest possible like throughput per dollar but that's not always the case big cons of using gpu for inference is that it's more complex to set up and in practice it's often more expensive i think using gpus for inference is still not really the norm in the industry i would say and in the labs we're going to do inference on cpus next thing we'll talk about is concurrency so if you're not going to run your your model on gpu where you're taking advantage of like batching things up and making them parallel then you still want to take advantage of the parallelism that's inherent on the machine that you're running your model on so you like maybe you have many cpus or maybe many cpu cores that you can actually run the model on so what this means is instead of just running a single copy of your model on the machine you'll instead run many copies of the model on the machine each of which takes advantage of its own cpu or cpu core or like group of cpus or cpu cores and the way this will work is that like you'll as requests come into your service and they'll get picked up off a cue and then one of the one of the copies of the model will grab it make the prediction on it and then return it back to the user and so how do you actually make this work in practice for machine learning in particular one kind of thing to be careful about is thread tuning so this is also a great blog post by roblox about how they scaled up bert to serve like an enormous amount of traffic on their platform and one of the kind of gotchas that they pointed out was make sure that each copy of your model is using really like the minimum number of threads that it really needs to use so that it's not hogging those threads for other copies of the model so that's concurrency next thing we'll talk about is model distillation what is model distillation it's basically a category of techniques that revolves around taking your larger machine learning model the one that you trained the one that is the one that you think is performing really well and training a smaller model not on your original task but instead to imitate the behavior of the larger model and in practice this can be a way to get relatively similar performance out of a much smaller model there's several techniques for this this blog post below is a pretty good overview of the different techniques you can try i would say the reputation of these techniques is that they're pretty finicky to do consistently and like robustly in practice so you might be able to get work to work once but if you're training your model a lot of different times and you need to be able to have it work consistently then adding this model distillation step can add some risk to that and so it's from what i've seen relatively infrequently used in practice with the exception of pre-trained distilled models so for a lot of the categories of models that you might want to grab a pre-trained model for there are there's research that's gone into how do you distill these models without losing too much performance so distilbert for as a distilled version of the bert family of models is a good example of this and so these are a great option if you want if you care about getting more like higher performance out of your system and you're willing to trade off a little bit of accuracy or a little bit of like model performance another way that you can reduce the size and like increase the speed of your models and throughput of your models is to use quantization what is quantization normally when you're running a forward pass on your neural network you have all of the weights encoded as let's say 32-bit floats and you basically do all this matrix multiplication and you produce this thing as an output so the observation that's made in quantization is that maybe we actually don't need all that precision for those for those model weights and those model activations like maybe we can get away for some of those weights if not all of them using a like a more compact numerical representation and so typically you'll um compress a lot of the weights if not all the weights down to 8-bit integers instead of 32-pit floating point numbers so there the again just like in distillation there are some trade-offs here with accuracy generally when you distill a model it makes it or when you quantize a model will make it a little bit less accurate or a little bit less performant along the metrics that you care about most of the like the main sort of libraries that you might consider using like pytorch and tensorflow lite which we'll talk about a little bit have ways of doing quantization that are built in and one thing to be aware of if you're going to rely a lot on quantization is that there is a class of techniques called quantization aware training where you take the fact that you're going to quantize the weights into account when you run your training procedure and these can be ways of reducing the amount of accuracy or the amount of model performance that you're going to lose when you quantize the weights after the trend so that's quantization the next performance optimization technique to be aware of is caching so what is caching for a lot of machine learning models the the distribution of inputs that you get into that model is not uniform right there's some inputs that you see more frequently than others and so caching takes advantage of this fact and instead of always calling the model on every single input no matter what instead it'll cache the frequently used inputs to the model and before calling the model it'll first check the cache and so if you're if it's one of the frequently used or maybe the recently used inputs to the model then instead of actually needing to run your expensive neural net you'll instead just pull the result from the cache and it'll be faster for your users in those cases [Music] how does this caching technique can get like really fancy this is a rabbit hole that you can go down into at some point if you're ever interested but there's a basic way to do this in python using funk tools and so there's a there's some like built-in methods in the like python func tools library for caching that can allow you to build a cache and that's that's one way you can do it is just build cache in memory using python functools and just rely on that for your most frequently used predictions another technique that can increase the make your models more efficient and higher throughput to run is batching the inputs before they go into the model typically with a lot of machine learning models you'll get higher throughput when you do prediction in parallel right like neural nets are built to run in parallel and this is especially true if you're doing inference on a gpu in fact batching is like an essential thing to do if you're doing inference on a gpu so how do you actually make this work let's say that you have a you don't necessarily have a batch of requests that comes into your model and you want a batch of outputs that are coming back what you the maybe a more realistic scenario is that you have a stream of inputs coming in that are coming from requests from users and so you need to take your library needs to take care of caching on the back end and then breaking up those batch predictions and sending them back to the right users so at a very high level the way this works is you'll collect predictions that come in until you have whatever constitutes a batch for your system and then you'll run your model on that batch and return each of those predictions to the users that requested them there's quite a bit of tuning that needs to happen in order to make this work well so you need to tune the batch size itself to trade off between throughput and latency for the users right if your batch size is too large then you know the first user that makes requests in the new batch will have to wait until all of the other until enough other users have have put in their requests before they actually get their prediction back so it can introduce a lot of latency and to get around this a lot of times what you need to do is you'll need to have a shortcut where in addition to like building up to a fixed size batch or like a desired batch size so if your latency gets too long you'll just return the prediction you'll you know run the prediction and return to the users anyway so there's quite a bit of tuning that's required to make this work well but if you're doing gpu inference this is pretty essential to get like kind of the benefits of doing that and yeah the last caveat here is this is maybe as it's clear from the description this is pretty tricky to get right and so you probably don't want to have to implement this yourself and we'll talk in a second about libraries that implement batching and some of the other performance optimization techniques that you can use if you're hosting your model especially on a gpu another category of performance optimizations that i think is interesting to consider not really very widely used in practice yet but hopefully will be widely used in practice soon is sharing a jeep a single gpu between multiple models if you're tuning your batch size and it turns out that like in order to get reasonable latency for your users you need to use a batch size that doesn't use up the entire gpu then one thing that you might wonder is are we just going to under utilize the gpu or is it possible to actually maybe share that gpu between multiple models or as while you're waiting for requests to accumulate and you're not actually using the gpu to make a prediction maybe that gpu can be used to run predictions on another model how does this work yeah this is something that you unless you're like really focused on this as a project this is probably not something that you'll implement on your own but it's worth being aware of as a technique because it's something that some of the model serving libraries are starting to support out of the box and i think will be like hopefully part of next generation of model serving tools so talked about out of the box model serving libraries a couple of times there's canonical ones for both pi torch and tensorflow there's tensorflow serving and torch serve and so these both take care of a lot of the kind of more complex batching and and things like that that we talked about in so far in this section and they're open source and so if you're if you do want to do gpu inference with your models if that's important for you to do then would recommend like building off of one of these libraries rather than trying to implement it all yourself couple others to be aware of uh ray which is a project of berkeley actually has a new ish model serving library which seems pretty promising and then nvidia has one as well that is like more tailored to nvidia gpus but has some of the kind of fancier primitives like gpu sharing that we talked about so that's performance optimization if
Full Stack Deep Learning - Spring 2021,11A,1771,Horizontal Scaling,you're running like a single host to serve requests for your model next thing we'll talk about is what it what happens if you have too much traffic in order for like your single host to be able to return predictions fast enough so the next thing that we'll talk about is horizontal scaling which means if you have too much traffic for a single machine let's make multiple copies of our model server and let's split traffic among those multiple machines how do we do this at a very high level you'll just duplicate your s your service your your prediction service and then you'll use something like a load balancer to split traffic and send traffic to the appropriate copy of your service in practice there's i would say two commonly used methods for how to do this one is so you can do this like just in your aws account or whatever with using primitives that your that your cloud provider provides but in the machine learning world it seems like it's more common to if you're going to manage this yourself to use a container orchestration toolkit like kubernetes as we talked about a little bit before to to do this or there's another option which is instead of managing these servers yourself to use a serverless option like aws lambda so i'll talk a little bit about each of these all right so if you're if you're doing the if you're going the container orchestration route one way to think about this is that if you look at the the third column here this is what it might look like to deploy a single container you get you somehow get your container onto your onto your web app or onto your sorry onto your hardware and then that container takes care of and fielding the requests and returning the responses to the user what this looks like in a container orchestration paradigm is that you have your docker container your docker containers work together and are coordinated by this tool called kubernetes and kubernetes is going to take care of things like providing a single service for you to send requests to and then dividing up traffic that gets sent to that service to the different actual like physical or like virtual copies of the container that are running on your infrastructure this can be you can build a system like this yourself on top of kubernetes if you want to and many companies do that but there's also some frameworks that are emerging that kind of take care of handling a lot of this infrastructure out of the box if you already have a kubernetes cluster running so the two i would point to here are there's one called kf serving which is part of the the kubeflow package which is a popular kubernetes native machine learning like infrastructure in a box solution and then the other is called the other kind of like commonly referred to one is called selden and so if you're if you find yourself in a position trying to implement a model serving stack on top of kubernetes then it's worth taking a look at these open source options all right the other approach that is relatively common to scaling your model service horizontally is like just not dealing with the the scaling at all by using a serverless function so the way this works is that you take your application code and all the dependencies and then you can pat and you package that up and traditionally for for a serverless function like aws lambda you always had to do that via zip files which is like really annoying but recently they started supporting docker containers so you can just write your docker container like you would anyways and have the entry point like a web service that gets run inside of that container and then a service provided by your cloud provider so aws lambda or google cloud functions or azure functions manages everything else for you so they they take care of like actually running that container on some physical hardware somewhere scaling it up and down to meet demand load balancing etc and one of the really nice things about this is that you actually only pay for compute time so instead of paying for all the all the instead of paying for your server no matter what no matter like whether it's getting any requests or whether it's totally saturated instead you're just paying per request and so for workloads where you have like spiky traffic like you have more traffic in the morning or like occasionally you'll get a huge burst of traffic this can be a much more cost-effective way of running infrastructure and so the like the galaxy brain idea here is your servers your servers can't actually go down if you don't have any servers so think about that next time your your servers go down and and you're wondering what you did wrong of course there are actually servers running under the hood here so it is still possible for servers to go down generally speaking these types of managed services are pretty reliable so some drawbacks to this approach the size of the overall package that you can deploy tends to be limited it's uh it's pretty big now i can't remember what the actual numbers are but like for most like smaller model or like reasonable size models you'll be fine here but if you're trying to run like the latest massive gpt3 model or something then you might run into issues with the size of your deployment package all of these services right now are cpu only and have limited execution time so you can't have single function call run for an arbitrarily long time it'll time out eventually which is fine typically for doing like machine learning model inference but can be or model prediction but it can be limiting for other things it can be challenging to like build these things into pipelines of models and there's like little or no state management that happens here if you want to do something like caching it can be very difficult to do that in a paradigm like lambda or another serverless function paradigm and there's some research coming out of berkeley actually that is focused on building a serverless platform that has some state management capabilities so this probably won't be true forever but for now like if you want to store state in your lambdas there's only limited ways to do that and then lastly i think there's like limited deployment tooling so the deployment tooling here is getting better but when we talk about other things that you might want to have in your model deployment process just know that a lot of those aren't present in lambda or are like relatively hard to do in lambda so situ to situate us we talked about what is this thing what is a rest api we talked about like how to manage
Full Stack Deep Learning - Spring 2021,11A,2122,Model Deployment,dependencies on your server we talked about how to make machine learning models run fast on your web server we talked about how to a couple of different strategies for scaling them horizontally next thing we're going to talk about is the process of deploying models so like the operational side of deployment so model deployment you know if if model serving so what we've been talking about so far is how you actually turn a model into something that can respond to requests deployment i would define as the process of how to roll out manage and update these services right let's say that you do train a new version of your model and you want to you think that new version of the model is ready to go into production how do you actually move from the existing service that's in production to the new service that's in production how do you what are some best practices for actually doing this things that you probably want to have in your deployment process are you want to have the ability to roll things out gradually so rather than just saying at this moment in time all of the traffic in my production system is going to go to the new model ideally you want to be able to say okay first i'm just going to send one percent of the traffic to the new model and i'm going to send 10 and then 50 and if things still look good then i'll eventually send all the traffic over to the new model you also probably want to have the ability to roll back instantly so if you do detect some change in the new version of the model that you've deployed you want to basically just be able to have a flag that says okay let's switch right back to the old version of our model and then on top of that you probably want to be able to deploy pipelines and models so rather than just having a single model that exists in a vacuum has to do all of its own pre-processing and stuff by itself in a perfect world you want to have you want to be able to deploy two models that are or the input of one is the output of the other and we're not going to talk about ways to actually implement this because it's a pretty there's a lot of challenging infrastructure considerations that go into this but the hope here is that if you're using one of these existing deployment libraries like the kubernetes based ones that we talked about or to some degree something like lambda then hopefully your deployment library will take care of this for you so this is probably isn't something that you'll find yourself really implementing from scratch but it's something to to look for if you are choosing an option for deploying your models
Full Stack Deep Learning - Spring 2021,11A,2251,Managed Options,all right and then the last thing that we're going to talk about is if you don't want to deal with any of this by yours like yourself at all like you don't want to deal with containers um you don't want to deal with horizontal scaling performance optimization any of these things then what should you do luckily there's some managed options there for you so the cloud providers all have one there's google has one amazon has one i believe microsoft has one too that basically allow you to package up your model in a predefined way turn it into an api and then there's a couple of startups that are offering this too one is called algorithmia which is the more established option and then there's a newer one called cortex which is actually founded by berkeley alums which is also worth checking out and the big drawback here is i don't know about pricing for the startup options but for the cloud provider options these tend to be very expensive and so you're really paying a premium for convenience if you go down this route [Music] okay we did we did a dive into this particular pattern for deploying models which is building a model service what should we take away from this the first takeaway is if you decide to do inference on a cpu then you can one thing you can get away with especially early on in your project is like when you don't have a huge amount of volume is just scaling that service by launching you can either scale that service by launching more servers or you can go serverless and i think like a the baseline recommendation is serverless makes a lot of things easy for you if you want to pick the same default then it's probably to use something like lambda or some other serverless option and in slightly more detail if you can get away with cpu inference you don't need inference for on gpus and if you have not a huge volume of traffic to your model or the traffic that you have to your model is high volume but it's very spiky then serverless probably makes more sense if you're doing gpu inference then the takeaway is that you really should be using one of these like serving tools like tensorflow serving or nvidia triton or torch serve it's going to save you a lot of time and take care of a lot of the like tricky implementation details like batching and potentially gpu sharing for you [Music] and then lastly i think it's it's also worth keeping an eye on startups in this space particularly if you're interested in doing gpu inference so like requests for startups actually is making gpu inference as easy as as like doing cpu inference on lambda
Full Stack Deep Learning - Spring 2021,11A,2390,Edge Prediction,all right last deployment pattern that we'll talk about today is putting the model inside of the client itself so this is called edge prediction and this is a pretty deep topic we don't we're not really going to do it justice in this class but just want to point you to some resources that you can go to to look to learn more about this if you're interested so the way this works is you send the weights of the model itself to the edge device so to the browser or to the to the user's phone or to like your robot or whatever your edge device your client device is and then that client loads the model and interacts with it directly there's some really good things about this pattern it's the lowest latency way that you can deploy models so if you're a very latency sensitive application this is basically the way to go it doesn't require an internet connection so you can run this on like a gpu on a robot in the middle of the desert with no interconnect internet connection and then there's a big data privacy data security benefit to this as well because the user's data doesn't actually need to leave their device in order to have predictions called on it but there's some pretty strong drawbacks to this method as well often on the client there are very limited hardware resources available particularly for like true edge devices but also even in the browser the frameworks that are used for deploying models this way are less full-featured than your typical your sort of full version of tensorflow and pytorch and so you can be constrained in the model choices that you make and translating from one to the other can be a little bit painful it's really difficult to update models so if you you know ship some model weights over to a client device and um you need to change them then that can be a tricky thing to figure out how to do and then it can be really hard to tell to debug and tell when things go wrong right because you're putting this model on someone else's device and you don't necessarily have the ability to get data back as to how well the model is doing next thing we'll talk about is just a very high level cover some of the tools that you can use for edge deployment just so you're aware of them i would say maybe the most commonly used framework for this in practice is uh tensor rt and well actually i don't know if that's fair but this is uh this is nvidia's like the nvidia kind of oriented framework and so this is meant to help you optimize your models for to do inference on nvidia devices like both they're like bigger gpus but also they're more like edge-oriented gpus so this is used pretty commonly in like robotics and stuff where you actually have a gpu or something gpu-like on the client device apache tvm is a pretty cool library and the promise of tvm is that like you're supposed to be able to take models that are written in any different framework and then compile them down into something that's meant to be run in any run time this is a pretty powerful tool if you have if you're writing models in lots of different libraries and you need them to be able to run in different environments like maybe both on a phone and on a gpu or like on different types of embedded hardware if you're deploying tensorflow models to mobile devices and also some edge devices the go-to way to do this is called tensorflow lite and so this will help you compile your tensorflow model down to something that can say run on your android device there's also a kind of alternative of this for pytorch called pytorch mobile which is oriented on toward getting your pytorch model toward your ios or android device [Music] and then lastly if you're not using an nvidia device and you you know don't want to use tvm for some reason and you're not using an iphone or an android device then like javascript is a pretty portable way a pretty portable way of running code on different devices for example you can run javascript directly in the browser and so there's also i think like the sort of most ubiquitous tool for this is uh tensorflow.js which is a way of running tensorflow code in javascript in your browser it's actually relatively full featured like you can also train models in the browser using tensorflow.js but i think it's like probably the more common use case is for um converting your model into something that you can just like ship out as part of your web service and have it run locally in the user's browser oh i'll quickly mention the the different like frameworks for running machine learning models on different mobile devices so there's core ml which is apple oriented only works for inference there's ml kit which is google kind of android oriented and you can you can upload tensorflow late models to this and then there's also this tool called fritz which is supposed to work for both like gonna be like a universal input for both ml kit and core ml i haven't actually tried this but the promise is to build something more general so the other thing that you might need to do if you're deploying if you're doing like client deployment or edge deployment is make your models themselves more efficient so one way to do this is to use the same quantization and distillation techniques that we talked about before but i think like a pretty essential part of the toolkit if you're going to be doing this is to pick mobile-friendly or edge-friendly model architectures there's a couple of like prominent examples of this one is called like maybe the original or the first one of the first really successful ones is called mobilenet and the way it works is it replaces a lot of like full convolutions with depth-wise convolutions and one deconvolutions to reduce the overall number of parameters in the model and this is a chart that shows on the x-axis the total number of operations that are present in the model on the y-axis the like top one accuracy for imagenet and uh green is mobile net is like all the different variants of mobile net with different like compressions and so you can see it's like pretty good trade-off between having a relatively small number of operations like quite a bit smaller than alex net much much smaller than something like vgg but still achieving like comparable if not better performance and so i think that's like probably the core of why this has been such a popular model architecture because it's it you know falls in the right quadrant of that trade-off in the nlp world there's also distilbert is like a commonly used smaller model there which i think i mentioned before and it's worth that's worth checking out this case study if you want to know a little bit more about how some of these model distillation things can work in practice but yeah they were able to get it down to half the number of parameters with 95 performance which is a pretty good result okay so that was like a lightning overview of some of the tools to be aware of if you find yourself deploying models on edge next thing i want to talk about is just what are some mindsets that you can adopt or like that maybe you need to change if this is the way that you're going to be deploying your models in the end so i want to talk to a few folks that i know who i've i haven't done edge deployment myself so i went in and talked to a few folks i know in the self-driving car world and the like mobile deep learning world and ask them um some questions about what they have found to be effective in practice for actually making these projects work and so some takeaways to share from those conversations i think like the typical recommendation that we tend to make when you're training deep learning models is consider model performance first and then like speed and inference speed and latency and things like that later because generally speaking like if you can get a model that has your target performance then if you are not hardware constrained in the way that you deploy that model there's different ways that you can go about actually making that model as efficient as it needs to be to run in your environment if you're going to be deploying your model on edge on a phone or on in the browser or on a like a robot that has access to only a small gpu let's say then one mindset that like practitioners have found essential is to choose the architecture like the search space that you use for your model architecture with the target hardware in mind so rule of thumb that i heard from folks is that you can make up a factor of conservatively like a factor of two and in some cases maybe up to a factor of 10 through some of these like optimization techniques that we talked about like distalization quantization or distillation quantization other tricks but you're generally not going to get much more than that so if you're not already in the ballpark of the size of the model that you need in order to run it on your device then it's good then getting it there can be really painful one thing that you can do and so like how do you actually find this like a couple of folks i talk to describe a process for choosing your initial architecture that involves basically doing a literature review and and essentially building like a two by two like a version of this two by two matrix for your particular problem where on one access you plot performance and on the other axis you plot the size of of the model or maybe like the latency or some other performance characteristic that you need to that you need to meet for your for your for the model that's going to run on the client device and then trying to choose models that are like that are good along both axes so once you have that initial target architecture in mind when you're iterating like when you're trying to come up with ways to improve your model architecture or improve the data or anything like that then generally speaking what folks will do is they'll just treat the the initial model architecture and the performance of that model architecture the latency throughput and things like that as baselines locally so you can when you run when you evaluate your model you can also evaluate the latency of your model just on whatever gpu you're using or even on your laptop and then compare the relative performance of the new version of the model from your experimentation to the baseline that works well on your client device or your edge device and if you follow a technique like that then there's some risk that there will be performance degradations that are present on the actual hardware that you're going to be evaluating on but are not present on your laptop or your your cloud provider but generally from what i've heard from folks the risk of that is relatively small a reasonable iteration pattern is like start with a model that i know works well iterate on it evaluate it locally and make sure that my latency throughput and model size aren't too far off from that model that i know i can get to work on my edge device another kind of mindset that i think is pretty key if you're gonna make this work well is to treat tuning for that model for that edge device as an additional risk in your deployment cycle and test accordingly so once you're out of that experimentation mindset and you're moving into testing it's really important to make sure that you are adequately testing your new model on all of the hardware that it might run on or like as good a sample as you can get of the hardware it might run on because there are bugs that can get introduced in the model compilation process that both affect like the the sort of performance characteristics of the model or like the system performance characteristics of the model but also ones that affect like the accuracy of the model and the model performance characteristics so it's important to re-run your tests after your model has been compiled down to whatever device or whatever form it's going to run on yeah and like a particularly important example of this is like always test your model models on production hardware before you deploy them and then the last piece of advice that would share here which is similar to a question that we had earlier is since these models can be since machine learning models in general can be finicky it's a good idea to try to build fallback mechanisms into the application in case the model fails or it's running too slowly so having some sort of like baseline thing that you know is gonna work and produce like a somewhat reasonable prediction good practice in general for building machine learning systems but can be particularly important for edge systems right because if you let's say you're building a robot or something and all of a sudden you get you hit some weird bug and it's taking your model like 10 times longer to make a prediction then you know that could crash the car which would be really bad but if you have a fallback option which says like oh prompt the user to take over and keep the steering wheel straight and apply brakes hopefully that's not actually the fallback option that self-driving folks are using something along those lines then that can greatly reduce the risk of like things going wrong with your machine learning model on the edge device all right so to to wrap up this kind of last deployment paradigm that we're going to talk about i'd say my overall recommendation here is web deployment is just easier so i would really only think about edge deployment if you're sure you really need to at this point second recommendation is in terms of thinking about the tooling landscape there's a lot of stuff out there but i think like the high level framework for a high level decision framework for choosing which software framework to use could be but just try to match the like hardware that you're going to be deploying on and the corresponding mobile framework so if you're going to be deploying on nvidia devices tensor rt can work well if you're going to be deploying an apple use something that's compatible with core ml and one thing you could try is instead of following that advice just use apache tvm and i think still early like relatively early days for that project it's pretty much for project but not like super ubiquitous yet in production from what i can tell but sometimes there's a performance trade-off where it's not quite as fast as using the library that's optimized for whatever specific hardware you are really targeting but it just generally does a pretty good job and especially if you're targeting multiple different types of hardware this can make your potentially make your life a lot easier and then lastly last piece of advice is unlike when you're doing like web deployment targeted machine learning you should probably think really seriously about your hardware constraints at the beginning of your project and constrain the space of model iteration that you do accordingly so you're never veering too far away from architectures that you can get to run on your client device
Full Stack Deep Learning - Spring 2021,11B,0,Introduction,next topic that we're going to talk about is monitoring machine learning models so once you've deployed the model how do you make sure it stays healthy when it's deployed there's a lot of things that can go wrong with your model once you've trained it for example maybe the model trend wasn't good at all like validation loss wasn't actually below your target performance or maybe your test losses was actually worse than your validation for because you over fit your validation but as you start to get more into the process of building your model you'll get to the point where you have a model that performs well not only on your validation set but also on all the critical slices and metrics that you really care about you'll have looked at the predictions qualitatively and they'll make sense you'll have to verify that the production model and the model original model you trained have the same performance characteristics so there's no like changes in the production model that caused the performance of the grade and you'll have maybe even run an a b tests and verify that the production model is indeed actually better than the model that it's replacing we covered some of this in the training and troubleshooting lecture a couple weeks ago and we covered the rest of it in the testing lecture last week so you're done right nothing else can go wrong with the model you can just deploy it and forget about it not quite so an unfortunate fact about
Full Stack Deep Learning - Spring 2021,11B,70,Model Performance Degrades Post Deployment,machine learning systems is that model performance tends to degrade after you've deployed the model and so one way to kind of reason about this is if you think about what what are your guarantees or whatever what is the problem that you're solving when you train like supervise the machine learning model so you're approximating some like p of y given x and you're doing that by learning a parameterized function on data sampled from the joint distribution of x and y and theoretically what are the things that can go wrong one of the things that can go wrong is that p of x can change so the distribution of inputs into your model that probability distribution can change and you'll see this referred to as data drift some examples of this are let's say that you depend on some feature that's being created by um somewhere else in your pipeline maybe even by some other team that team changes the way that they define that feature and potentially they even introduce a bug where all of a sudden all the values of that feature are like negative one or something so that's one way that your that p of x can change but it's not only bugs so malicious users can also change p of x they can give you intentionally weird inputs you could for example launch your service in a new region and that might change the distribution of inputs or you could start onboarding new users and those users might have different demographics than the users that were present at training time another thing that can go wrong is that p of y given x can change and so you'll see this referred to as model drift or concept drift in the literature one kind of concrete example of this is like maybe your user's behavior changes like potentially even in response to your model so for example if you're building a recommender system and you make some recommendations to your users their preferences are going to change after that because they might actually like click on one of those recommendations and watch that movie and then there they that might change the types of movies that they want to see in the future and then the last category of things that can go wrong is that we're we're approximating this parameterized model we're approximating this probability function using data a parameterized model that's trained on data that's sampled from the underlying distribution and so there can be artifacts that are introduced by the sampling process itself and so some names for this are like domain shift where there's like some difference in the domain between training distribution and target distribution or another term that describes this is the long tail so if you have if your data distribution has long tails then randomly sampling that data might not give you all like data points that represent all the parts of the distribution that you care about so some examples are tasks where outliers really matter so if you have if there's some something in the probability distribution that only happens one out of a million times but if you get the prediction wrong that's really bad that's an example of where you might not have adequately sampled the distribution bugs in the trading data pipeline can cause this and this is often also frequently actually very frequently caused by bias in the sampling process so let's talk about different types of data drift
Full Stack Deep Learning - Spring 2021,11B,240,Data Drift,so if this is like our the time series of our data points like uh value of that data point is on the y-axis and time is on the x-axis then there's a few different types of changes that this data distribution can take they're worth being aware of at a high level so one is instantaneous drift and you also see this called distribution shift it's where like all of a sudden the distribution changes some examples of this are you deploy your model for the first time in a new domain like maybe you deploy your self-driving car in a new city or bugs getting introduced in the pre-processing pipeline can cause this type of shift but also like big external events like covet happening can cause this type of shift as well something happens all of a sudden the world is different the data that comes from the world is different as well you could also have gradual drift so the the distribution like the the values of the feature change gradually over time for example like users preferences might change over time as a result of like your users getting older or like pop culture changing around them or like new concepts can get introduced to your corpus over time as well [Music] you can also have periodic drifts look like these directional drifts but then they change directions and repeat over and over again so you know some types of user preferences might be seasonal or for example maybe like people in different time zones use your model differently potentially let's say because they speak different languages and then the last category of drift that we'll talk about and the um most probably the most difficult one to actually detect is temporary drift where like also there's a big change in the data distribution but then it goes back to normal so for example maybe this change in your distribution is a malicious user attacking your model or or it could be like let's say some new user tries your model their data doesn't really look like the data the model is trained on so they get bad results and they churn that could also produce a drift like this or just someone using your product for the first time in a way that is not intended to be used so these are the different ways that data drift can manifest themselves in your production machine learning system and one question you might ask is okay is this i get why this is a theoretical problem but is this actually a real problem that affects real world machine learning systems and the answer is that this can actually have a huge impact in production there's this article from can't remember the publication i think it was maybe like forbes or something that was like describing what was happening with all the machine learning models during the pandemic because all of a sudden like everyone's models drifted and so some really weird stuff started to happen another story that i heard from someone who was building a machine learning system at a kind of e-commerce type company is that they had a bug in their retraining pipeline that caused them to make the same recommendations for their users over and over again and so that the new users that came in particular were like just saying oh this service is just telling me to do the same thing over and over again this is not useful most of them churned and they estimated you know it took them a month or a couple months to catch this and they lost like millions of dollars in revenue as a result of this bug so this is a real problem that affects like a lot of a lot of companies and i think is only just now starting to get like the attention paid to it that really needs to be if you want to build reliable machine learning systems most of the rest of the lecture we'll talk about different strategies that you can use to detect these types of changes that might affect your model quick outline first we'll talk about what you should be monitoring then we'll talk about like how you measure if that thing is actually changed then we'll talk about how you can tell if that change is a bad change or not we'll talk about tools and then finally we'll talk about like where monitoring fits into the broader context of your machine learning system firstly what should you monitor i think
Full Stack Deep Learning - Spring 2021,11B,445,What To Monitor?,there's four signals that you should probably consider monitoring that trade off with each other in terms of like how useful they are if you have them and how hard it is to get them so the most informative thing to monitor about in order to tell whether your model's performance is changing is to look directly at the performance of the model and so this is the easiest thing to do there's still some technical challenges associated with it even if you can even if you have labels but this is the hardest of all these to do because for many of the problems that we're working on machine learning models are difficult to get labels for especially in a timely fashion and in a cost effective fashion so if you don't have access to like infinite free label data that comes in right away as soon as your model makes the prediction then what are the other signals that you can look at so another really informative signal to look at is business metrics so if you're building a model is i'll come back to the recommendation example there's some metrics that you might really care about that driving like user engagement or user click-through rate or things like that that don't actually tell you how accurate the model is because there's some conflating factors that that are also that also help determine those business metrics and so that's why it's not like quite as useful as signal in terms of like really telling whether something went wrong with your model because there's other things that can cause those signals to go awry as well in many cases on the other hand they're often easier to measure because you often have access to these signals already you're already measuring these things as part of part of your like analytics team another signal that is worth looking at is the inputs to the model and the predictions that are coming out of the model and the distributions of those inputs and predictions this is this is also can be quite informative like you can detect drifts this way just by looking at the raw data even if you don't know what the label should be but it is there's still a bit of an art to how you actually measure things when you're looking at them at this level and we'll talk a little bit more about why it's such an art but this is also something that's worth doing if you have access if you have limited access to like labels and business metrics and then finally the the baseline thing to do is just monitor system performance what is your gpu utilization what is the average latency of your requests and things like that and so this will only catch like very coarse bugs like your servers crashing or something like that or memory leak maybe it's really important to measure and it's very easy to measure because there's really good tools for doing it already but it doesn't really tell you about a more machine learning specific stuff like your model performance different ways of like categorizing these things that like some terms that i'll use model metrics and in some cases business metrics are what i would call like ground truth performance metrics so these are things that like actually tell you directly how well your model's performing model inputs and predictions as well as business metric as well as like some other business metrics that are like have more confounding factors or what i would call like approximate performance metrics and so these are ways of if you don't have ground truth performance metrics ways of approximating how well you think your model should be performing on the data that's seeing before you have access to labels as to how well the model's actually doing and then lastly system performance and some types of like model inputs and predictions or what we'll call like system health metrics which are just which are helpful for telling if your system is is working as intended but aren't really helpful for telling you whether your model is working as intended so those are the signals that you might want to measure okay so we talked about the different signals that you can look at next thing
Full Stack Deep Learning - Spring 2021,11B,642,How To Measure When Things Change,that we cover is like how do you actually measure if there's been change to those signals so the overall strategy here for measuring distribution change is we're going to select a window of data that we consider good like this is what the data should look like to serve as a reference so in in this example this is the reference on the left how do you pick this reference how do you pick the data points you consider good you can one way you can do it is you can just pick a fixed window of production data that you believe to be healthy some papers that you'll see will advocate for using a sliding window so like always using a window of data that's one day behind or 10 hours behind or whatever the window that you're actually trying to tell whether it's healthy but i think the pragmatic thing to do here is most of the time you should probably just use your training or evaluation data as the reference because that's ultimately like what the model was trained and evaluated on and so if you're if data that's coming in in production looks different from that doesn't really matter if it isn't too different than the data that you saw in production a day ago it's still different from the data that you really care about which is the training data next thing that you'll do is once you have this reference window of data you'll select you know your new window of data to measure a distance on how do you select that measurement window very problem dependent i don't i'm not aware at least like a principled approach for saying okay should i monitor like an hour of data or a day of data or like a minute of data i think pragmatically one thing that you can do is just pick one window size or a few window sizes and just you know slide them over the data so every day monitor on the last day every hour monitor on the last hour something like that there's a special case here which is like where your window size is one and there's a whole separate class of techniques here called outlier detection which is i think present in some like model monitoring libraries less practical for for a number of reasons but it's worth being aware of as well a couple of footnotes here if you want to dive more into this stuff and so the last thing that we'll do is we have our like reference distribution and the distribution we want to measure and we'll compare these two windows using some distance metric so what distance metric should we use first let's consider the case where the data is like continuous and one-dimensional so there's a couple categories here one is rule-based metrics you also see this referred to as like data quality metrics and then there's statistical distance metrics and so these are things like kl divergence your ks statistic something called a distance and like plenty of others so let's talk about rule-based distance metrics or data quality metrics so the kinds of things that you'll measure here are like okay for this window of data is our is like the minimum value and the maximum value and the mean value within some acceptable range that's determined by the reference window maybe plus some some allowable access beyond that you might also check like are there enough data points in this window or are there too many missing values in this window or there's is there a bunch of nand out data in this window you can also measure more complex things like are all the values in this column bigger than all the values in this other column and there's a whole host of other different like kind of rule-based metrics that you can use there's an open source library for this that is gaining a lot of popularity recently called great expectations which helps you like define these tests it's not machine learning specific it's not like super well suited out of the box to being used for like continuously streaming data but it's it's worked pretty nicely for like batches of data so you might need to set up a little bit of infrastructure on your own but then you could run these tests on that data when you do that recommendation here is this is very much worth doing when you there's two i would say like landmark kind of industry standard papers on monitoring machine learning models which i'll i'll link to at some point later in these slides one is from google that describes their their tfx system which is they have an open source version of it but also is a thing that they use internally and then the other is from amazon which is also used internally and is also part of an open source library called dq and what both of those papers concluded was that like these rule-based metrics for detecting changes in data distributions will are much much easier to use than the statistical metrics that we'll talk about later and already catch like a huge percentage of bugs and like the pragmatic thing to do is just use these rule-based things and like maybe try layering on some statistical stuff as well but this is you'll be in in good company if you use this as a starting point all right next let's talk about some statistical distance measures and so we'll start with every machine learning person's favorite statistical distance measure the kl divergence this is kl divergence it's an expectation of a ratio of logs between the two different distributions it's not actually a distance metric it's not symmetric and it's a very bad distance metric for detecting data distribution shifts there's a few reasons for that one is it's really sensitive to what happens in the tails of the distribution so for example since you're looking at a ratio like a log of the ratio of probability values if you have in some tail the distribution for one of the probability distributions you have a value of 10 to the negative four and in the other you have a value of 10 to the negative three then that ratio is going to be 10 which is going to be a really big contribution to to your kl divergence when you take the log of it so it's like mathematically not really the right thing when you're sampling your data and so like the tails are gonna be pretty noisy it's not very interpretable so it's like not super clear how you explain to someone on chart what this distribution distance is measuring and it's not very clear what to do if p and q contain data that doesn't fall in exactly the same range so you're gonna have you're gonna end up with a zero like a negative like a zero in a log or a zero in a denominator somewhere and so that's another reason why this is tricky to use so recommendation here don't do it don't monitor the kl it's very tempting because it's a familiar distance metric in machine learning but there's there's better things to monitor for this particular use case and yeah i posted about this on like this machine learning ops slack this morning and someone made this someone made this meme which i kind of liked yeah monitoring kale divergence it's worthless next like commonly referred to metric is the is the ks statistic which is like the statistic that goes into the ks test ins in stats that you might be familiar with this is how it's defined mathematically but like the way to interpret this is that this is like the maximum distance between the cdfs of the two district different distributions i just described the interpretation to you so you know that it's like quite easy to interpret you can like plot on a chart what this refers to and it's used quite widely in practice so this is something that i would i would recommend using so just say yes to ks and we'll and caveat here is this is referring to the ks statistic itself not necessarily the k the statistical tests where you actually get a p value back from it and i'll come back to that in a second it's the it's like the statistic that goes into the the test that produces the p value next distance metric is one that you might not be familiar with because it's not too widely used outside of this particular use case it's d1 distance and this is the way it's described it's the sum of differences between the values and the pdfs yeah that should be sum of distances between the pdfs not the cdfs and the reason why this is worth bringing up is because this is the distance metric that like after spending much time thinking about this at google is the one that they use in their systems my caveat with this is always isn't this a little bit hacky are you just like summing up the differences between the probability density functions like why is that a meaningful distance what's the justification for this i don't really know but it's really easy to interpret this is what the interpretation looks like and if google does it maybe it's not all that bad so i think this is another good distance metric to look at for monitoring distribution change there's many other distance metrics that you might be familiar with or you might see if you read about the subject earth mover distance is like a pretty cool one talk about the earth mover distance it's pretty cool it's population stability index lots of other stuff i think like many of these distance metrics could be really good for your use case or even just for monitoring in general and so i'd say it's like worthy of some research worthy of trying out other distance metrics that you're excited about and i think that there's like some missing research here in fact like a request for research here is i would love to see a study that looks at different types of drift from the perspective of how those effects the performance of trained machine learning models so if you have if you have drifts that are measured by these different metrics do those have different impacts on the machine learning models that are trained on the bad data all right so that's the one dimensional case we have some distance metrics that we can use there next let's talk about what you do when you're working with higher dimensional data first caveat here is this is very much an open problem i don't think there is a good solution to this yet some things you can try are you can use an explicit like multi-dimensional test like the maximum mean discrepancy the way to understand this at a high level is you're looking at the distance between the means of the distributions in some embedded space so that's that's one thing that's worth trying another thing that's really common to do in practice is just do a bunch of one-dimensional comparisons and look at all the distance metrics for all the different features and take let's say the max of those so there's some things that are not very nice about that it doesn't capture cross-correlation between the features and you run into the multiple hypothesis testing problem but this is a thing that people often do rather than looking at do the comparisons for all of your features you might also like prioritize some of the features so you might use some of like maybe the interpretability methods that we talked about last week like feature importance and just say okay we're only going to measure drift along the like 10 most important features and then the last like kind of category of techniques that i'll mention here is called projections so projections are basically what they sound like i think as far as i can tell they were proposed for machine learning model monitoring in this paper called failing loudly which is one of the only like strangely like one of the only main track machine learning conference papers on data data data set drift that that i've been able to find and the approach that they advocate for is you first take your raw data let's say your image you pass that through some black box dimensionality reduction technique it can be a random projection it can be an um auto encoder that's pre-trained an auto encoder that's randomly initialized or some other options that projects the thing down to a lower dimensional space so instead of an image maybe you have a 32 dimensional vector or something like that and then you run a two sample statistical test on that lower dimensional data so you can run you can aggregate any of the one-dimensional tests that we talked about before or you can run a maximum mean discrepancy test and in their experiments i think they found it didn't really matter too much which one you did so there's different ways you can project the data in in their paper they used things like like like random projections randomly initialize auto encoders pre-trained auto encoders stuff like that but you could also like one one thing that i think is like a good idea to do in practice if you have high dimensional data is to use projections that come from your domain specific knowledge for example like if you're monitoring images you might want to like project that image down to the mean pixel value and monitor that and if that pixel value gets too crazy you might know that there's something weird going on with your distribution in nlp this might be like the length of the sentence or it might be like some vocabulary like one of the top five words something like that but you could also imagine defining any other sort of analytical projection that depends a lot on your domain and what you think could be important like features of that domain to monitor and you can define that any way you want to another category of things here is random projections so you can just like linearly project the data this can work really well or statistical projections training an auto encoder or another kind of density model right like you might you might fit like a likelihood based model to your data and then take the the output of that model like the the likelihood of that data point and monitor that or you might use like pca or t-sne or something like that to reject the data and monitor the result of that yeah and so i think the last thing i would say on like measuring the distance between these distributions is that like when you think about it from up from a higher level what do we really want to be measuring here the thing that you want to measure is you the reason why we care about measuring drifts is because we don't necessarily have access to the ground truth performance of the model if you had access to the ground performance of the model that would tell you what you need to know about drift which is is my model getting worse and so really i think what in a perfect world the way that these techniques would work is that rather than giving you like some kind of abstract drift score which can be hard to interpret and you don't really know like whether a drift of a certain amount really affects your model performance or if it affects your model performance whether that performance changes is material i think the direction that techniques in the space should go is toward techniques that give you actually some approximate performance of the model on that data so techniques that combine both some of these statistical ways of telling whether the data looks different and then also some sensitivity that is specific to your model that tells you like whether drift of a certain amount like how much you might expect that to alter your model performance and so i only mentioned that because i think that this is like the direction that the the field should go and so if any of you are like interested in doing some research i think there's like some interesting research to be done there but you won't i don't think that you'll find too much in the literature about that and it's typically not how people do it in practice now all right moving
Full Stack Deep Learning - Spring 2021,11B,1447,How To Tell If A Change Is Bad,on so we talked about what things you should be monitoring how you should be measuring changes and then the next question which i alluded to earlier is like how do you actually tell if that change is bad like how do you know should do something about the change fortunately again this is another place where there's not a very satisfying answer so the i think the there's one approach that you can take here to setting um thresholds on test values which is using statistical tests so the kind of like canonical example here would be using the ks test as opposed to the ks statistic so the ks test is a way of getting like a p value for whether these two distributions are the same like what are these two data whether these two samples of data came from the same distribution or not and unfortunately this is like the wrong thing to measure for detecting data drift because when you have a lot of data then like any kind of minor change in the distribution even if it's one that's like completely insignificant is going to give you tiny p-values right because even if the mean of your distribution is just shifted by a tiny amount like if you have enough data then the statistical test will tell you oh yeah p equals zero these are different distributions and that's not really what we care about in machine learning monitoring because generally your models will be robust to small shifts like that so those p values don't end up being very useful in practice there's a number of other ways of detecting of setting these thresholds like these from this blog post from this like data quality company called anomalo so you can set fixed rules never have any null values you can have ranges okay this test should always be between these values and set that manually you can look at the time series of the values of the test and like actually run that through an outlier detection model and fire an alert whenever it looks like an outlier or you can even imagine like training an unsupervised learning model on the values of that test and then using that to determine whether something is is like an acceptable value or not in practice you'll almost always see these two techniques used unfortunately at this point it still generally comes down to a human being looking at this and setting the rule themselves for what they think an acceptable amount of change in the distribution might be but i think the latter categories might be interesting areas for research although i hope it gets combined with model performance like i described earlier oh yeah i forgot i had a slide on this but yeah just i'll reiterate my request for research for telling approximating how model performance changes given a change in data distribution all right next quickly
Full Stack Deep Learning - Spring 2021,11B,1579,Tools For Monitoring,cover just give you an overview of some of the tools that are out there to help with this so there's system monitoring tools these are useful for like setting alerts for when things go wrong and cloud providers have like decent monitoring solutions where you can just type your data and your metrics into them they'll help you set alerts for them anything that you can log in there can be monitored and we may take a look at this in lab but we may actually change up the lab as well so some examples are you have your cloud provider tools like amazon cloudwatch and then there's like specific tools that have evolved just to solve this problem new relic data dog honeycomb are some of the more prominent examples next category of tools that's worth knowing about is data quality tools and so these tools are really aimed at like answering some of these data quality questions so for a given for a fixed window of data can you tell me if this violates any of these handwritten or like machine learned rules um about whether the data looks really different relative to the rules i've set so great expectations i mentioned earlier is a pretty popular up and coming open source library for this and then there's a few vendors that provide solutions for this monte carlo and anomalous being two of the ones that i hear discuss the most and then lastly there's a category of tools that's emerging around machine learning monitoring arize arthur and fiddler sort of being the best known examples i think too early to say to have a recommendation here yet but keep an eye on this space i think there's going to be a lot of interesting stuff that comes out around it so that was like the very lightning overview of tools last kind of topic on monitoring on autocover is putting monitoring the
Full Stack Deep Learning - Spring 2021,11B,1666,Monitoring And Your Broader ML System,context of your broader machine learning system so i think when people think of monitoring in a traditional software system they see it as this thing world you it's like useful for helping you catch bugs and catching when things go wrong in production but those bugs are usually like pretty loud failures they're like exceptions or events like that that are relatively easy to detect on their own and the data that you put into a monitoring system in a traditional machine learning system is things like system metrics cpu utilization request latency things like that and i think in the traditional software world you think of these things as like data that's primarily useful just to like detect and diagnose problems with your system so if your system is performing well fundamentally you don't really care that much about what the request latency was a month ago or what the distribution of request latencies was it's like data that you only really need when something's going wrong in machine learning i think like monitoring is in some sense much more central to the entire process of building and iterating on machine learning models than it is in traditional software two main reasons for this one is that unlike in traditional software and machine learning normally when you have a bug what that bug leads to is just a silent degradation and performance and so these bugs can be like extremely difficult to catch because they have very little signature and on top of that the data that we're talking about monitoring here not system metrics like things like model inputs is like quite literally the code that you're going to use to produce the next version of your model right like the data that goes into the model the predictions that come out the labels those are that's your training data for when you retrain your model so in some sense it feels like this system has to be more important to the model um building and maintenance process people typically think of monitoring i think where you see it fitting into like your system diagram is you let's say in a machine learning system you train a model you evaluate it you put into production and then monitoring is this like module that you slap on top of that and your monitoring thing is like watching your production system and helping you find out when things when things go awry in that production system i think the reality of monitoring and machine learning is that your monitoring system really like actually needs to be pretty tightly integrated with the rest of your production stack i'll talk about i'll talk about like ways in which i think it should be integrated in a second but because of this like i've started using a different name to describe like what this system actually is which i've been calling an evaluation store just as a way of distinguishing this in people's minds from okay this is just like a module that we slap on top this is something that like really needs to talk to the rest of your machine learning system and in particular like i think the one of the roles that a monitoring system could play in your machine learning infrastructure is helping you close the data flywheel right so the data flywheel is a concept that we've talked to about before where as you get more and more data you use that data to train better models makes your product better better product gets you more users getting more data and more data allows you to close the loop and make the model better over time and so let's talk about like how this monitoring system or this evaluation store could help close the date of flywheel so a training time and so like the one distinction i'll make here is that i think i mentioned this i think in the testing lecture too but i think when you think about like your monitoring system if my kind of hypothesis is right which is that like in the longer term the best way to monitor your models is going to be to have some way of using the data that's coming in to approximate how performance might have changed on your model on that data then your if that's the case then your monitoring system needs to be integrated with your your training and evaluation system as well because training and evaluation is like where you'll build those models of the mapping between data change and model performance and so i've started thinking of your kind of monitoring system or your evaluation store as like a sort of cohesive evaluation system that is used not only for monitoring per se but also for offline evaluation and so the ways that this would interact with the rest of your stack are like when you're training your model you want to register your data distribution and your model performance into the system so that you have something to compare to when you're in production but evaluation store or like your monitoring system could also help you catch performance bugs or like implementation bugs during training for example if you made a bug in how you're training your model and all of a sudden your training data looks nothing like your production data that might be something you want to catch earlier rather than later when you're testing your model you might want to actually like record that data here as well in deployment like we've talked a little bit about running things like avtech the if you have like a centralized system for keeping track of model performance metrics then that system is like what you'll use to run an a b test because you'll just ask the system like okay as the performance or maybe the approximate performance of my model of these two versions of my model differ on this slice of production data monitoring there's a relatively straightforward application of this like you'll just you'll ask your monitoring system how well you think the model is performing in expectation but i think what starts to get interesting is when you talk about closing the loop one challenge for a lot of uh companies when they're just like deciding what data to collect from their machine learning system is that with really high data volumes like on a robot or on a web scale application or something you can't necessarily store and label and train all the data that's going through your model so you need to pick a subset of that data to store label and train on and so how do you pick that subset the simplest thing that most people do is they'll just like randomly sub sample that data but that can be like i think probably not the most efficient thing because if you have like tails of your distribution or like areas where your model is not performing well you might want to over sample data from those areas and so another function that your monitoring system or your evaluation store could play is you might want to over sample data from regions that have low approximate performance so if if if it looks like maybe your model is not doing too well on this data point or this window of data points you might want to sample more data from that window cleaning and labeling i think is a similar story right like you might want to label more data that has low proximal performance and then retraining right like another kind of challenging problem to figure out is like how when should i retrain my model how often should i retrain my model conceptually like the way to make that decision is the cost benefit analysis right there's some cost that's associated with retraining your model compute costs data costs and so you want to be able to measure like what's the approximate benefit of retraining my model like how much better could my model get by retraining it and so one way one like proxy for that might be knowing how much you think your model has degraded in performance relative to the last time you trained to train that and so this system this monitoring or evaluation store system could help you answer this question of when you're retraining so takeaways on monitoring i think if you're going to
Full Stack Deep Learning - Spring 2021,11B,2066,Takeaways,take one thing away from this it's just do it monitor your models it's a good idea i think there's a culture in the machine learning field for a long time of machine learning people like machine learning focused people like not necessarily caring too much what happens to their models after they get shipped like thinking like oh it's my job is to produce the model not to maintain it i think one mindset that i hope that you all adopt as you go on to implement machine learning models in industry is that what happens to the model after you you deploy it is um still at least impart your responsibility right because you're the one who built the model you know what the data that's trained on looks like you know how to fix it when things go wrong and so monitoring is something that you should be thinking about because something will go wrong like models it i think it would be very unusual to have a model that you can just put in production and leave there and have it never break and so you should at least have systems in place to know if something is going wrong if you're getting started on monitoring i think the kind of like pragmatic starting point is by looking at these data quality metrics so these rules about okay are all my features within some allowable range are all of the means of my features within an allowable range as well and looking at system metrics and main reason that those are the recommendations are because they're the easiest and they'll already start to catch a lot of bugs for you in a perfect world i think the role that your monitoring system should come to play in your machine learning system is that it should be fused with your testing system and it like the bigger like the sort of higher order calling should have is not just helping you tell when things go wrong in a sort of abstract way but helping you like close the data flywheel and fix your system and train on better data over time because these same approximate performance metrics that you're using your monitoring system to estimate are the same metrics that you should be using to decide what data to collect label and retrain on and then another note here is like the tooling here is not really that mature yet but this is a pretty active space for like startups and other companies so it's one to keep an eye on and i think that in spite of all this there's a number of i think this is like a very underresearched area it's a super important problem for making machine learning systems work in the real world and there's like a painful lack of research on how to make this work well and so there's big open research questions here and if you're like looking for stuff to do research on i would highly encourage checking it out all right that's all i have for today thank you
Full Stack Deep Learning - Spring 2021,2A,0,Introduction,um but so to review um what we're going to cover today is first we're going to talk about the convolution operation so kind of the basic mathematical operation behind convolutional neural networks and then we're going to talk about some of the other important operations that make up components and then finally we're going to touch on quickly a classic combat architecture and then in the next part of the lecture sergey is going to talk about computer vision applications and we'll go into more detail about other neural net architectures so to review um first question we'll cover is what's convolutional filter and then we'll talk about stacking convolutional filters and um using these stacks to make up convenience and then we'll talk about strides and padding and we'll touch a little bit on filter math and then finally we'll um give a couple of um of brief notes on how these things are implemented in the libraries that you'll be using so what's a convolutional filter um i
Full Stack Deep Learning - Spring 2021,2A,68,Convolutional Filters,think the first thing to start with here is just sort of the inspiration right so you know question you might ask is what's wrong with these fully connected neural networks that we've been talking about so far like why are these not the right things to use for computer vision um and so to give you a little motivation for this um how would you think about applying a fully connected neural network to an image right so let's say that you have this grayscale image of a dog maybe it's 32 pixels by 32 pixels um so one thing you could do is you could just flatten that image and that'll give you a vector of 1024 length and then if you're doing some classification tasks that has 10 possible output classes you can multiply that vector by matrix so say a 10 24 by 10 matrix and produce 10 outputs and that that's kind of one way that you could do classification with of an image with a neural network okay so what's wrong with this um yeah and then so the in this case the matrix multiplication those are the weights that you would learn um well one thing to notice here is that this is a really small image right this is 32 by 32 and we already have a 1024 dimension vector so what happens if you start increasing the size of that image so increase it to 64 by 64. you'll have you know four times as many um uh dimensions in that tensor which means you'll have four times as many weights to learn in your matrix multiplication if you increase it um to 128 by 128 it'll go up by a factor of 16. and so the dimensionality of the number of weights that you need to learn scales really poorly with the size of the image um and we want neural networks that we use for computer vision to be able to work well with larger images as well so this is one issue with using fully connected networks for computer vision another issue is just you know i would ask the question of like whether you really need this many weights at all so um if you think about what this matrix multiplication is doing for each possible output class you're learning a weight um a separate weight for each input pixel in the input image right and so you're saying like each of those input pixels contributes you know possibly equally as much to the value of the classification right and so in many images there's maybe a few regions of of that image that are particularly important to telling what's inside the image um so maybe this is really overkill and then lastly you know one other way you can see why convolutions are helpful is um is you know like what happens if you slide this image a little bit to the right right so to us it's pretty easy to tell that these are um images of the same class right they're the same animal um but to a fully connected network the the like let's say the last pixel um or the last row in this matrix multiplication always looks at the same pixel in that image and so when you scale the image it's looking at a different pixel or when you uh when you translate the image rather it's looking at a different pixel right and so um just naively without doing any form of data augmentation or anything like that these networks that you form like this are not necessarily going to be invariant to translate translations of the input image and in many computer vision applications um like image classification for example translation in variants is a is a property that we want our networks to have so that's that's the motivation for convolutional filters so the way a convolutional filter works is instead of taking the entire image flattening it into a vector and then multiplying it by a matrix instead you extract just a single patch of the image so maybe you maybe you say let's take a five by five patch of the image so five pixels by five pixels you flatten that and instead of producing you know this this massive vector instead it produces a 25 dimensional vector and then you take the dot product between that vector and another vector of the same length and that gives you a one-dimensional output um so this is only looking at one part of the image so how do you make it look at the entire image well you can take this 5x5 patch and slide it across the image so using the same weights you can look at each different part of the image and so you can slide it across the row and then across each of the rows until you get to the bottom right corner and each of those operations is extracting a five by five patch flattening it multi multiplying it by a vector and producing a one dimensional output and so if you combine all those together what you end up with is um this output that's uh that has one corresponding output for each five by five patch in the input okay so one question you might ask is okay that's like i understand that's an operation we can do but what you know what reason to do we have to believe that that operation can do anything useful um and so convolutions have been used in computer vision um for a long time and were popular even before deep learning and so if you if you uh carefully select the weights that go into that convolution operation so the values of each of those weights then you can produce effects like blurring an image so this is a a um a convolution that applies a uh a blur operation and there's a link here to um to other types of image kernels that you can take a look at if you want to build some intuition about what types of things these convolution operations can do and the intuition behind covnets is that instead of like kind of carefully selecting each of the weights in this convolution instead we're gonna learn them um so one extension here is um that in general your image you know
Full Stack Deep Learning - Spring 2021,2A,430,Filter Stacks and ConvNets,we were looking at a grayscale image before so it's 32 by 32 just one sort of value at each of those 32 by 32 pixels but you know for example in rgb images you actually have three values at each of those spatial positions so you have the r channel the g channel and the b channel so your input image is actually not 32 by 32 it's 32 by 32 by 3. and so again you know if you wanted to if you wanted to classify an image like this using a fully connected network you would need to flatten it and you'd have a vector that's three times as large as it is for the grayscale 32x32 image and a similar thing happens with your convolutional filters so if you have if you're extracting a 5x5 patch of the image to apply a convolutional filter to instead of just having 25 weights that's going to instead have 75 weights because you extract all each of the channels for each of those filters but the operation is going to be the same you slide this this five by five by three window horizontally across the image and then along each of the rows and you create an output the input is not the only thing that can have multiple channels the output can have multiple channels too um and so the way to think about that is that when we describe the the basic convolution operator uh it's we basically described it as a dot product right so you have this five by five by three patch of the image that is turned into a 75 dimensional vector and then you take the dot product between that and another 75 75-dimensional vector to produce a single channel in the output but instead of taking a dot product you can instead have um some number of of convolutional filters that you learn independently of one another and so instead of a dot product you can have a matrix multiplication and so in this case you'll multiply this 75 dimensional patch from the image by a 75 by 10 matrix um and then you're going to produce an output tensor that is still 28 by 28 but instead of only having a single value on the third dimension which was created by this dot product instead you're going to have 10 values on this third dimension that's created by this matrix multiplication one thing to notice here is that the kind of the overall like format of the inputs and outputs to this convolution convolution operation are the same so the input to this model or to this operation was this tensor um which is like kind of this stack of matrices that's 32 by 32 by three right so it's this three dimensional tensor and then after you applied these ten five by five convolutional filters to this image you're left with something that's 28 by 28 by 10. so not exactly the same shape but the same overall dimensions it's still this three-dimensional tensor and so that obs and so because of that observation this the output of this filter is equally valid as the input to another convolutional filter right so we can just take another convolutional filter and we can apply it to the output of this filter so we can stack these convolutional filters or these convolutional layers on top of one another and we can um and we can produce these like separate layers just like you would have in a in a fully connected network with multiple layers and so in practice usually you'll you won't just apply these layers one after another because each of the layers that we described so far is just a linear operation what you'll do instead is you'll apply a non-linearity after each of the layers just like you would after each of the layers in a fully connected network
Full Stack Deep Learning - Spring 2021,2A,685,Strides and Padding,so next we're going to talk about strides and padding and so these are different ways of changing the how the convolutional filter gets slid over the dimensions of the image so convolutions can sub-sample the image and one way that they can do that is by jumping across some locations right so in the examples that we showed before if you have a five by five filter uh five by five filter then the way that you apply that is you first apply it to the first five by five like upper left corner of the image and then you move over one um one pixel to the right and you apply it again and then you move one more pixel to the right you apply it again um what you want what you can do instead if you want if you want to sub sample your image is you can skip some locations right so if you have a stride of one which is kind of the example we've been talking about so far this is what it looks like to apply the concept to the image so you move one pixel at a time and the corresponding output moves one pixel at a time as well so we do this across the whole image [Music] if we add a stride to this convolution operation what we're going to do is and we're going to skip the second position that the convolution operates on so we skip over that second position and we do that in both dimensions because this is a two by two stride and usually you'll see strides that are the same on on both dimensions so question i would raise here is what happens if you want a bigger stride so what happens if say you want a three by three stride and so what that would look like is you would move the filter three pixels to the right instead of two and now all of a sudden the filter is kind of falling off the edge of the image and so one way that we can solve this problem is by adding padding to the image so padding is some default value that you um that you add to the borders of the image so that when you slide a convolution operator over those default values um over the edges of the image you can still apply the operation it just takes for the for those pixels that are off the edge of the image it takes that default value and so this is kind of what this looks like um you know you're uh you're you have a stride and you're looking at three different positions on the image but the the edges are are taken from a default value and um usually usually the default value that you pick is just zero and so there's different strategies that you can use to pick how you apply padding this is what's illustrated here is called same padding and this is a this is a pretty common form of padding that basically says i want the output of this operation to be the same size as the input
Full Stack Deep Learning - Spring 2021,2A,875,Filter Math,all right so this this raises a lot of questions of like okay if um if i have an image that's this size and a convolution operation that you know a filter that's the size and a stride of this size um how do i actually know what size my output is going to be and you know one reason you might care about that is because if you're designing a neural net architecture how you pick the next layer depends on what the size of the previous the output of the previous layer is so there's some basic math that you can use to calculate the size of the output of a convolution operation and i'm not going to go through this in detail but um it's worth kind of just playing around with and getting familiar with if you're going to be using comnets a lot um a couple of notes here there's a few parameters that you have so one is the number of filters um so again you're stacking filters how many filters do you want and typically this is set to a power of two um so 32 64 128 but you can set it to whatever you want um there's uh different strides that you can pick and so usually the way that strides are picked is that they're almost always the same for both dimensions and they're usually five three two or one but again there's other choices here if you want to play around with your architecture and then for padding um the two types that you'll usually see are same which we discussed briefly and then valid which just doesn't add any padding um and yeah below this are are the formulas that you can use to calculate the size of the output tensor um which is pretty intuitive but it's it's kind of worth playing around with a few examples to get a sense for how these convolution operations actually down sample the tensors that they're operating on and one last thing i'll mention on this there's this great guide to uh the arithmetic arithmetic behind convolution operations which um sergey always recommends and i've since checked it out and enjoyed as well and so this is kind of like a very intuitive guide and has lots of cool um nice visualizations and equations that will will bring you joy if you want to understand the stuff so recommend checking this out um i have a super quick question sure um stuff like well so i'm assuming the answer is yes but stuff like stride and like the specific uh convolutional operation like can it be learned um and i'm curious whether like the state-of-the-art con confidence or image models actually have that stuff set by hand or whether it's all just lettered by at this point yeah it's a it's a good question um i think like one way to think about it is for any for any um sort of properties of the algorithm like this that typically you consider hyper parameters um one challenge with learning them is that uh it's the the operation of like chain of like how your how the stride for example affects the output of the model is not it's generally not differentiable um so it's difficult to learn like in the inner loop of your learning process it's difficult to learn just by like standard gradient descent methods um but a pretty sort of promising research direction that i think peter will talk about later in the course is um uh is is like meta learning or um neural architecture search and the way that those techniques work is kind of you have this outer loop of your optimization process so the inner loop is um you know learning a model with a specific set of hyper parameters on a specific task and then the outer loop is selecting which hyper parameters allow you to perform best on this task um so there's a whole research field behind this and um that's kind of one way that people um are learning things like are learning hyper parameters for these types of algorithms and i think that the um state of the art on like quite a few tasks that are where peop that are like very well studied um you know depending on when you look tend to be like taken up by um by architectures that were learned via neural architecture search but the other other thing to note is that you know typically those approaches are extremely computationally inefficient and so in most cases in the real world it's not very common to use them there's a couple more questions maybe it's a good time yeah we can pause here [Music] so uh someone asks are con filters always square they don't need to be mathematically but i don't think i've ever seen an example where they're not have you sergey no uh yeah they tend to be square and then is there some property that guides us on how to choose filter size like one would you use a five by five or a three by three or one by one yeah so we'll talk a little bit about this when we talk about receptive fields in a few slides um but i would say like there's there's so there's intuition that you can build about what effect different filter sizes have on um what the model can see and what types of outputs it can compute which is what we'll talk about in a few slides but just as a general philosophy i would say that um in deep learning um a lot of decisions like this are kind of guided by intuition about the way that these things work but um uh like the empirical results of of um other people who have studied these algorithms tend to like rule out over anything else and so a lot of it is also just being familiar with what other people have tried and what works and then why would you ever want to use a stride that would cause the filter to move past the boundary of the image um well so strides are useful for sub-sampling the image um which is so different reasons you might want to do that for example um if you're doing classification your input image is really large and uh so you want to you know it's like 128 by 128 and the thing that you want to produce at the end is 10 dimensional um you need to get rid of a lot of those dimensions somehow and so one way to do that is through strides and then the question is um the question of like why would you ever want to do something that goes over the boundary um one simple reason is just that um it's like kind of an it's like you can't always make the math work to have a stride the stride that you want without going over the boundary of the image so that's kind of maybe one one practical reason why you might want to that's it for now okay a couple notes on how these things
Full Stack Deep Learning - Spring 2021,2A,1304,Convolution Implementation Notes,tend to be implemented in some of the libraries that you'll be using so again let's think about the the operation that we do when we're performing a convolution right so we're um if and for the sake of example let's say that the input is five by five by three so like a five by five rgb image and we're gonna use um 32 filters of size three by three with a stride of one right so remember that basically what we're doing is we're extracting each of the patches in this image and we're turning them into columns and each of those columns in this case are going to be three by three by three or 27 dimensional and we're going to do that for each position on the image and what we're left with in this case is this 27 by nine dimensional matrix right so 27 is the extracted patch and then nine is each of the positions in the input tensor and this operation in a lot of libraries is called into call so image to column and then if we're um if if we're if we're taking a three by three filter um let's say we're taking a single three by three filter um with three channels of input um so if you remember from the previous slides that's the the number of weights that you have to learn for that single channel output is 3 times 3 times 3 or 27 and then we have 32 filters so we're essentially learning a matrix that is 32 by 27. and then what we're going to do is we're going to take this 27 by 9 matrix which is all of the extracted patches we're going to multiply it by this 32 by um by 9 matrix um is this right or this should this be uh oh yeah sorry so we're multiplying it by this 32 by nine matrix um and then the result that we get from that um is 27 by 32 and so we're just going to reshape that so it's 3x3 by 32. next we'll talk about a couple of other important operations for compnets
Full Stack Deep Learning - Spring 2021,2A,1444,Increasing the Receptive Field with Dilated Convolutions,and so the first that we'll cover are um operation an operation that's used to increase the receptive field which is dilated convolutions first what is it what is a receptive field so if you think about what the convolution operation is doing um you're sliding this window over an image and you're producing something that is lower generally lower dimensional and so you can ask the question of for a particular pixel a particular position in the output tensor which pixels or positions in the input tensor contributed to the weight of that output so in this case if we're looking at the center pixel in the output since this is a three by three convolution um these these this three by three patch in the center of the image contributed to the value at that output and so we say that the receptive field um is three by three so each pixel in the output sees a three by three patch of pixels in the input so one way we can increase the receptive field is by stacking convolutional operations so in this case if we have a five by five input and uh and then we apply two three by three convolutions the last output sees all nine pixels in the intermediate and each of those sees nine inputs in the original um the original image and so the receptive field of this single output in the input is all five by five pixels and so um just to review um stacking two convolutions one after the other increases the receptive field and so one thing you can notice is that if you stack two by two three by three convolutions you get the same receptive field as a single five by five convolution and a couple of nice properties are that in practice this actually tends to perform a little bit better um maybe because you have an extra non-linearity um even even though there's actually fewer parameters right so there's uh two times three by three um for the two three by three filters which is 18 and then there's 25 in the single 5x5 filter so fewer parameters and empirically better performance one other way to increase the receptive field is through dilated convolutions so dilated convolutions are operations that kind of look like this um and so dilated convolutions um increase the receptive field without increasing the number of parameters and the way that they do that is by skipping pixels in the input and so here you have a three by three convolution so it still only has um nine weights but it has uh but it's dilated and it's dilated by one pixel so it's one dilated and here it has a 5x5 receptive field and so if you if you want to get like a really large receptive field for your task for whatever reason then you can stack a bunch of dilated convolutions and the receptive field numbers will add up very quickly okay so in addition to increasing the receptive field one thing that you might want to do is uh to decrease the size of the output
Full Stack Deep Learning - Spring 2021,2A,1650,Decreasing the Tensor Size with Pooling and 1x1-Convolutions,tensor right so why might you want to do that right so like let's say that you have um a large image as input and you want to produce a small vector as output like if you're doing classification well then somehow you need to take this you know let's say 1024 by 1024 image and reduce it into something that's 10 dimensional and so you need to um throughout your convolution operations you need to try to decrease the size of the tensor and so we'll talk about a couple of ways in um in addition to striding that we can that we can do that so one is pooling the idea behind pooling is that you subsample an image by looking at a small rectangular region of the image and then sort of sub-sampling it by applying an operation to that region that reduces its size and so typically this will either be you you take the average of the pixels in that region or you take the max of the pixels in that region and in principle there's other schemes that you can apply here but these are the the two most common the most um of all those the by far the most common um to see in computer vision applications is two by two max pooling right and so what that looks like is you take each two by two patch and you replace that with a single pixel that has the max value of all the pixels in that patch one thing to note is that this is kind of less common to see now with state-of-the-art computer vision architectures um it was more common in sort of 2015-2016 when deep learning was kind of exploding in popularity but has recently fallen out of favor and sergey will talk in the next section about some of the other architectures that don't use max pooling one of the ways to reduce the dimensionality of the tensor that we're operating on is instead of taking patches of an image and replacing them with a single value instead another thing that we might want to do is decrease the number of channels in the image or in the in the tensor rather and so let's say that we have something that's 28 by 28 by 192 right so there's there's a lot of a lot of channels in that tensor and we want to decrease it to something that's 28 by 28 by something smaller like 32. so the way that this works is you ex is you basically apply a one by one convolutional filter to the image so it's only looking at a single channel um a single pixel but all of the values like for all the channels of that pixel and it's um it's still you're still learning the weights um but the receptive field of each of these convolutions is just a single pixel and so like if you want to think about conceptually what this corresponds to it's like taking a single layer neural network um or like a single linear layer with uh with some non-linearity after it and applying it to every pixel in the convolutional output independently and this operation turns out to be pretty important in a lot of architectures that have been developed over the last seven or eight years last
Full Stack Deep Learning - Spring 2021,2A,1854,LeNet Architecture,thing i'm going to cover before the break is just really basic kind of classical combined architecture that you'll still see a lot and we still actually recommend using as a baseline when you're starting new tasks um we'll talk about in several lectures um and then in the next section after the break sergey is going to talk about more modern confident architectures but the most standard content architecture like the simplest like viable combined architecture is the um linette architecture or the lynette like family of architectures so the way these architectures work is you take your image then you pass that through a a convolution operator then you apply a non-linearity in most cases a relu and then you repeat that option n times so you apply n convolutions followed by the corresponding values once you've repeated that option n times then you apply pooling usually two by two max pulling and then you'll repeat this entire process so several convolutions followed by values followed by pooling m times and then once you've done enough uh convolutions and pooling to get your image down to a more reasonable size then you'll apply some number of fully connected layers so k fully connected layers followed by values and then take the softmax of the results and use that as your output and so the most common like kind of iteration of this that you'll see is um a uh is the actual lynette architecture right so um in this case this is used for mnist your input is 32 by 32 you use 5x5 convolutions stride of one six filters very small number of filters you apply a 10h non-linearity because those were more in fashion when this architecture was developed you apply average pooling because average pooling was more in fashion when this architecture was developed then you apply a 5x5 convolution again this time with 16 filters another 10 h another average pulling and then you pass through two fully connected layers to get to the output but in general you can apply this type of architecture with any number of kind of columns cons and values followed by pooling operations um and then pass through fully connected layers to get like a reasonable baseline architecture for many image related tasks wait i'm sorry i i i got a little got a little lost in your notation of like applying con the convolution end times and the pooling empty like what additional benefit does doing it so many times give you so um we talked about a couple of different ways of um of achieving like the same like receptive field let's say right so one thing you can do is you can apply a three by three convolution twice another thing you can do is you can apply a five by five convolution once um and we observe that like in practice oftentimes applying two three by three convolutions um gives you better results than a single five by five convolution and we sort of hypothesize that one reason why is that there's an additional place that you um that you get a non-linearity right and so basically when you're adding layers here you're increasing the receptive field you're adding additional non-linearity and you're adding additional learned weights and so you're basically making the neural net um more expressive right so it has more open learn larger receptive field and more non-linearity that is able to be captured so this is implying end times concept consecutively yes yes so so to clarify um what this subscribes is you take the image um and you have this operation which is like con plus value you apply that end times consecutively right so each time applying that to the output of the previous one and then you pull and then for that entire operation so n times con by value followed by pool you can do that thing over and over again consecutively again each time applying um that entire stack of operations to the output of the previous stack of operations um and so you have this one tall stack of like many columns and values and pools and then you use that to produce your output okay and after the break sergey we'll talk about many more modern combined architectures but before we get to that i'll pause and see if there's any additional questions is there an activation function between the last two fully connected layers uh so usually you won't have an activation function um so uh so there's usually an activation function between the fully connected layers um but you need to be careful about activation function after the last fully connected layer and the reason that you need to be careful is that is a pragmatic reason which is that in a lot of neural net libraries the loss functions that you use expect the output of a linear layer not one that has a nonlinear non-linearity applied to it so yes i would generally put nonlinearity nonlinearities be between the fully connected layers but then in practice when you're implementing this i would be careful about what you put after the last fully connected layer and i would make that decision based on the loss function you're using and how that loss function is implemented in the neural net library that you're working with and then is the main point of convnets to limit the hypothesis class um what do you mean by the hypothesis class i guess the space of like possible networks i don't know in a sense yeah right like in a sense you're saying you're sort of injecting some domain knowledge about the task um of image classification let's say right and that domain knowledge is that you know when you're determining the value of a particular sort of pixel in an intermediate layer or in an output tensor then uh that depends more heavily on things that are close to it right so things that are immediately surrounding it and a little bit further out than that so in some sense you can think of this as like injecting your prior about what the space of functions that might solve this problem could look like um and that could also explain why these things are so much more efficient to learn in practice and does pooling consecutively have benefits similar to applying convoys consecutively so pooling consecutively i think would just be kind of like applying a larger pooling operator um which empirically i don't really see people do that like usually you usually pull you do a small pool as possible and then you like apply some more layers so you add some more um you have ability of the neural network to like compute more stuff about each of the um things in that tensor and then maybe you'll pull again yeah okay well i think that's it
Full Stack Deep Learning - Spring 2021,2B,0,Introduction,this week we'll be talking about computer vision applications so just want to follow up to josh's lynette and just review some more notable applications to deep learning and computer vision and really just build the toolbox of architectures and tricks that were used in them so that we can apply them to future problems so first we'll tour some confident architectures like lynette then we'll talk about localization detection and segmentation problems and then we'll talk about some other stuff that that vision that deep learning has applied to envision so if you go to torch vision which is the kind of official pytorch package for computer vision models you'll see some pre-trained models and there's a a bunch of them right there's alexnet vgg of resnet squeeznet dancenet googlenet and so you might think well what are all of these and the answer is that we've really been every year since 2012 really improving accuracy on this one data set called imagenet and each year essentially there was a new architecture or an iteration a new spin on on a on a comp.net architecture that won and people started using that one so that's been going on for some number of years and that's why we have all these different models um alex matt well okay so imagine that a little bit about this technically called the image in that large scale visual recognition challenge in 2010 was the first time they launched it and it's uh it's a number of classes a thousand classes some of them very fine grains like donation some of them pretty pretty uh broad like container ship right there's a bunch of different types but we just care about this big category so there's a thousand object categories and there's over a million images in the training set and the tasks are classification which is given an image you predict the class of the thing that's in it localization and detection but really when people talk about imagenet they usually talk about classification so before deep learning right in 2010 2011 the winners were not deep networks they were shallow approaches like svms or something like that and the the error rate of the winner was around you know 25 at best
Full Stack Deep Learning - Spring 2021,2B,171,AlexNet,in 2012 alex not published the uh paper that kind of kicked off the deep learning revolution and that was a deep enough work with eight layers and that got 16 percent so that was really a huge jump compared to the previous uh state of the art and so that got everyone's attention basically and so what alex knight is is and that worked very very similar to lynnette except instead of the um the 10h activation it introduced the value activation it added a new type of layer called dropout in which some set of weights are just set to zero randomly in training it introduced heavy data augmentation so that means the input image would be flipped horizontally would be maybe rotate a little bit scaled crops of it would be taking and uh you know the reason it's usually drawn as two parts you can kind of see it here there's like one patch goes to one network one patch goes to another is because the biggest gpu at the time only had three gigabytes of memory and uh this network couldn't fit even though it's not that large but it couldn't fit on just one gpu so a lot of the engineering that that alex krazzewski had to do actually had to do with model distributed training where the model lives on two different gpus and the parameters have to be uh kind of kept in check because they're actually shared between gpus so this is a schematic input goes into an 11 11 11 by 11 con and then a five by five conf max pool three by three cons max pool three by three con three by three con max pool and then a few fully connected layers so it's very similar to lynnette um and this is kind of a breakdown of the input and output sizes of each of these layers and the total number of neurons which unfortunately my face is in front of but several million parameters and next year there was another five percent improvement in the air rate
Full Stack Deep Learning - Spring 2021,2B,309,ZFNet,with really just essentially also an alex net but it had a um it it just had some tweaks as to how the con filter size is versus number of channels worked but basically you can think of alex net as just like the beginning of a hyper optimus of a hyper parameter optimization process and then everyone implemented their own alex nat and started tweaking parameters and there was like five more percent of error rate to squeeze out just by tweaking hyper parameters that's what happened but this paper is more famous for introducing these deconvolutional visualizations which basically um as the neural network trains each conf filter can be thought of as detecting a certain type of image patch right and then in the early layers that are close to the to the pixels these convolutions learn basically edge detection and then maybe texture detection like color detection but in the later layers like layer five on the right hand side of the slide the con filters because they're now seeing a larger receptive field into the image and because they themselves have input from the lower level con filters are able to basically detect parts of objects so maybe ears or eyes or wheels stuff like that and so these visualizations are pretty cool and one of the readings for this week actually talks more about them so the next year got another four
Full Stack Deep Learning - Spring 2021,2B,414,VGGNet,percent drop in accuracy and this one came with just a lot more layers and so it's called vgg network and it's a pretty simple architecture it's deeper than what i've been trying before it only used three by three convolutions so none of that nine by nine or five by five or anything and it introduced uh or and it also used two by two pool max pools and that an increase the channel dimension with each layer so like early on close to the pixels it would be a three by three convolution with let's say 64 channels and then later it would be a three by three convolution with like 512 channels so one observation that the authors made here is that if you stack three by three convolutions you get the same receptive field as a larger convolution but there's fewer parameters total so four stack three by three columns is the same as a 9x9 con but there's less parameters so you can train faster so vgg has 138 million parameters and you can kind of break it down by memory use versus computations but the most memory used is in the early convlayers because remember that im2call operation so a con operation is really just a matrix multiply but if the input to the con operation is large then the matrix that has to get multiplied is actually very large and so that uses a lot of memory but most the parameters are in the late stages with the fully connected layers even though the matrix the matrices aren't as large anymore but there's a lot more weights to learn and that really shows you how you know part of the reason to use cons is because they use a lot fewer parameters they're able to do a lot of computation with very few parameters because you constrain parameters in different sizes different regions of the image basically so next up is google net
Full Stack Deep Learning - Spring 2021,2B,546,GoogLeNet,sometimes called inception net and it's just as cheap as vgg but only had three percent of the parameters and it had no fully connected layers and it was really a stack of inception modules and these inception module is you know if you noticed there was a basically a con value con value pool kind of pattern that would get replicated many times right and so you can call that a module basically like a con pool module and an inception module is something a little more advanced where basically the input instead of going through a single con of some size goes through four different channels and then gets concatenated so it goes through a one by one conf a three by three conf and a three by three by you know stacked a couple of times conf in an average pool and it's always reduced with a one by one con first which like reduces the number of channels and so this is the um i guess the inception hypothesis is that the cross-channel correlations so the input to a confidence has depth which is the number of channels second image has three channels and then later on in the content you might have like 64 channels whatever your comps are outputting and then it has spatial dimensions so it has channels and it has spatial dimensions and the inception hypothesis was that the cross channel correlations so in the depth dimension and the spatial correlations in the spatial dimensions those two things are kind of decoupled and you can actually treat them separately and so that's what a one by one column can let you do because it doesn't see anything it only sees like the one pixel that's coming in but it sees all of its channels so it's able to for example reduce the number of channels or expand the number of channels but it doesn't have anything uh to do with like spatial correlations like edges or anything like that and another cool trick that googlenet did is they injected basically they had classifier outputs not only at the end as all the other networks do but also kind of in the middle and so what that does is it lets the network get gradient from the loss function at more spot than just at the end of the network um so that's kind of interesting so that was five million parameters which is actually a lot fewer parameters than the previous years
Full Stack Deep Learning - Spring 2021,2B,717,ResNet,and then we got a really deep network called resnet with 152 layers and that improved by another three percent and so resnet is probably like the most commonly used network now um and it's actually resulted in a top five error rate that was lower than the human performance of like five percent which i think andre carpathi just measured himself by doing imagenet by himself so the problem that resnet author is observed is that you should be able to make a model deeper and deeper and it should perform at least as well as a shallower network of the same architecture but sometimes it doesn't right and the reason that they that they pointed to is that while the gradient can vanish because each layer is an opportunity for the gradient to basically disappear and if the network's very deep then the gradient can vanish but what you can do is you can add an option to basically skip around each layer and so if that layer makes the gradient vanish then you know it actually won't vanish in backdrop because you can just follow that identity path back so we're basically adding shortcuts from the input around the processing of the input and adding it to the result of the processing and the reason it's called a residual network is because basically you can think of h of x as like the old plane conv operation and um f of x is the is the new one which is h of x plus x and so we're kind of trying to fit h of x minus x so it's like that x is the residual i don't know um and the other thing it i guess started doing differently is it stopped using the max pool operation and it just started down same playing convolution or down sampling spatially using strides instead of max pool and uh resnet is kind of like what people use nowadays there's variants on it so for example densenet adds even more skip connections so instead of just adding the one skip connection around a conf block you add a skip connection to every other part of the network from every other part of it res next yeah i kind of think of it as you know it's almost like combining the inception that idea and the resnet idea it's the inception that idea is to send the input through multiple processing channels at once and then concat them and then the resonant idea is to do this residual kind of shortcut connection resnext seems to do both so the resnet was 60 ml parameters 152 layers and a couple years later the architecture of squeeze squeeze and
Full Stack Deep Learning - Spring 2021,2B,915,SqueezeNet,excitation that is worth looking at because it's basically adding a module of global pooling and a fully connected layer to basically adaptively re-weight the feature output maps which is kind of like attention so it's it's basically saying you know like the the input comes in it goes to the comp block it comes out is every channel that comes out equally important or can we wait some channels heavier than others and that's basically the idea and that's what it's trying to do so it's like sends the input through the traditional kind of conf channel but then it also sends the input through a special fully connected network um by basically taking the input global pulling it so that just averages everything into kind of one vector and then you apply some fully connected layers to it and then you use the output of that to weight the channels of the conv output and lastly squeeze net is uh just really focused on trying to reduce the number of parameters as as to as few as possible and so it achieved alex nut level accuracy with 50 times fewer parameters and like a tiny model and the reason or the way it did it is by always adding these one by one filters which can really squeeze the number of channels so number of channels kind of never expands and this is actually work from burkland by forest ian dola who's now at tesla um so if you look at all these networks and you plot them by you know we can plot them by just the
Full Stack Deep Learning - Spring 2021,2B,1025,Architecture Comparisons,the top one image that accuracy and the best ones are you know up in the 80s you can also plot them by both the accuracy and the number of operations and gigaflops that they perform and you can also do the same by the number of parameters that they have so like the memory footprint but this this chart does it by a number of operations and you can try to find the sweet spot like what's the best performance in terms of accuracy we can get at the fewest number of operations and so resnets you know like some versions of resnets are probably in the sweet spot um which you can see there because you can get resnet 50 maybe 10 gigaflops or something like that 8 gigaflops 75 accuracy and in general there's a lot of compute that gets used for this kind of stuff so openly i had a blog post that plotted some notable developments over the last few years versus the number of pedoflops that they used and so um like an another i guess research direction or thing that people have been working on is well how do we perform all this compute in as short of a time as possible right because we have to keep doing more and more compute for these networks but we don't want it to take longer and longer number of uh days so some work has focused on just very fast training and the the secret to that is basically batches that are as large as possible right so if you have a lot of gpus then you can crank up your batch size to as big as maybe 32 000. and then distribute them over you know 1000 gpus with each gpu fully fully loaded and then you can train imagenet you can train resnet50 on the imagenet for 90 epochs which gets pretty good performance in just 15 minutes and there's in fact there's a benchmark for this called the dawn bench from stanford and the whole point of this is like how fast can you train on imagenet or some other data sets or alternatively how cheaply can you train on the imagenet to like 93 accuracy top five accuracy so it's interesting to look at i guess the winner right now is a resnet50 trained in just two minutes and 38 seconds
Full Stack Deep Learning - Spring 2021,2B,1200,"Localization, Detection, and Segmentation Tasks",okay well i'll keep going then so the next thing i want to talk about is not just classification which is what we focused on so far but some other vision tasks so classification is like given an image output the class of the one object in the image right localization is do that but also highlight where that object is in the image detection is given an image output every object class and location in the image right so an image might have multiple things and you should detect all of them and then segmentation is actually label every pixel in the image as belonging either to a specific object or to you know kind of the background default object and then furthermore instant segmentation is like it's not enough to just say these pixels belong to the class dog you have to say you know these pixels belong to dog number one these pixels belong to dog number two so for localization we might actually have a pretty good way to do it with like the stuff we've discussed so far because maybe we can just give an image to the to the same network like a resnet or something but then instead of the one output of the class right which is that soft max over the number of classes we could also output some extra things like the bounding box coordinates like x1 y1 x2 y2 so you can imagine it's like the same network up until the fully connected layers and then you have kind of one head of the network going off to predict the class but then four more heads of the network predicting x1 y1 x2 y2 and that might work but it's not going to work when there's multiple objects because we don't a priori know how many objects they're going to be in the image right so we can't like in this case if we know there's exactly one object we can add four coordinate bounding box coordinate heads to the network but in this case we don't know how many objects are going to be in the image so we don't know how many heads to add so what we can do instead is we can um take our you know classifier that we trained on imagenet that detects any like any number of like detects the class of whatever it sees and then give it patches of the image in turn and then on each patch it will say you know i see nothing or i see a duck and stuff like that and we can actually do that you can just you can just do that it'll be very expensive because each time you give it a patch it'll kind of compute everything from scratch but in fact overlapping patches or like neighboring patches um there's yeah overlapping patches there's a lot of the pixels that are shared between them and you shouldn't have to like recompute your network on those shared pixels this is just a demo of me sliding this network so what you can do is you can actually kind of look at your network and observe that it has some conf layers and then maybe some fully connected layers and you can further further more observe that a fully connected layer can be turned into a one by one conv layer okay so given an image you have to do your comp operations and then a one by one conf can actually serve as a fully connected layer for you and so that's exactly what people started doing with this over feed
Full Stack Deep Learning - Spring 2021,2B,1440,"Overfeat, YOLO, and SSD Methods",paper from 2013 they basically just made it computationally tractable to look in many overlapping regions in an image and for each one predict the class of what's in it and then because you know what the bounding box of the region that it looks that you now have a bounding box so basically given an image you end up with a lot of bounding boxes and for each one there's some notion of what what is in it and what you can do is you can just kind of try to unify that into just a couple of detections via non-maximum suppression so what's not maximum suppression it just means that if you have like overlapping bounding boxes you should keep the one that has the highest detection score and then kind of remove all the ones that are overlapping it significantly and there's a efficient algorithm to do it and then lastly there's a way that these methods get measured and it's called intersection over union so that's just the metric for for uh this detection quality right so given some ground truth bounding box and our detection our detection doesn't have to be exactly the same as the ground truth for it to count as a correct detection it just has to have an intersection over union that's greater than some threshold which usually 0.5 so that overfeed kind of approach is actually what yolo which you might have heard of and the ssd single shot detector it's just that same approach but scaled up so what they do is they take an image put a fixed grid over it and then within each grid they they uh try to find objects in that cell and they also do that localization trick that i shared in the first slide where you not only output the category of the thing in the region you're looking at but also the bounding box of the thing in that region and then after you put that you run non-maximum suppression and that actually works really well and it's really fast so that's what yellow is you only look once um it's nice and fast it's a great off-the-shelf solution so um there's three versions of it that the author the original author has worked on um yellow v3 is the is the last one that the author worked on and so basically it achieves really good you know really good performance on this coco data set which i'll talk about in a second and it runs actually like fast enough to run in real time and yellow v4 is uh the most recent thing that was published just this spring the original author is now actually no longer on the paper so it's kind of an interesting name for this method because it's i don't know it's weird that they kept yellow but yeah it runs at like you know yellow v4 here you can see it runs at like 40 mean average precision at like 90 frames per second on the g on the gpu so that's really quick now what is this coco data set that this method is evaluated on so it's microsoft cocoa common objects in context and it's a very large data set uh there's like 330 000 images 1.5 million object instances 80 categories that an object might belong to and even some captions um and so yeah it gives you instant segmentations of like over 200 000 images so this is what people use now for training these kind of models
Full Stack Deep Learning - Spring 2021,2B,1681,"Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net)",so moving on we talked about methods that basically look everywhere in the image right so you put a grid on the image and you look at every cell on the grid there's an alternative to that which is try to find regions that seem interesting in some way and then only look at those right so given an image a lot of it might be just things that are obviously background and then there might be like a interesting region to look at so the first method to do this was also from berkeley uh by rasguership called rcnn it's a region cnn basically so what it used is a like a external method like a non-deep learning method for finding regions and then basically alexnet for each region and because alexnet has to take square input each region would be just warped to be square no matter what it actually was at first and then it uses that localization trick where the consonant outputs not only the class but also the bounding box parameters and so this is really an interim solution and i was kind of bridging like best uh at the time methods for detection and confidence and then later with the paper called faster rcnn they used a contact for the regional proposal network itself so you run some kind of cnn on the whole image then you run the result of that through a region proposal network which gives you proposals and then the proposals are region of interest pooled which is kind of like non-maximum suppression and then each region of interest would be classified and this was really fast because everything is just done in the continent so the reason proposal network is the secret sauce in that paper and uh yeah it's it's kind of like you know yolo because it does put a grid on the image and then scores each each each uh each cell on the grid for is it interesting as a region to look at so yes the faster rcn on training there's four losses total there's the classifier loss and then there's the binding block regression and that's for both the region proposal network and the object classifier but since we're adding so many different losses you know could we add actually another loss for segmentation and the answer is you could and so that's the paper called mask rcnn uh also from birkeland with georgia and ross um and the idea there is just augment faster rcnn as we described it with some more conflairs with their own loss for instant segmentation so now the regions of interest are not only classified and bounding blocks regress that's where like the corners are predicted but they also go through this instant segmentation module and it gives like really good results by the way so um this is like on the test set it detects you know people animals household objects like it looks really good um so each region right it not only goes to the classification but it now also goes through this segmentation step and the way that's done is it goes through a couple of convolution layers and then the convolution basically outputs a binary image where like some pixels are part of the segmentation and some aren't but what if we wanted to to do that to the whole image right not just to the regions of interest so this is sometimes called fully convolutional nuts or sometimes they're called u-nuts and so the idea there is like i'm going to give you an image that's as the input and the output is the segmentation mask of the whole image of the same size as the image and the image can be pretty large right so maybe it's like 224 pixels but by 224 pixels so if we only do convolutions that don't reduce the spatial size of the image it's going to be way too expensive because like applying convolution to a large input is very memory uh expensive so what you can do is you can encode and then decode right so shrink the image down and then up sample it back to the segmentation mask and we know how to shrink it down because like that's what we do when we classify we shrink a whole image down to just the softmax over like all the classes in it but how do we go from something like that back out to an image sized output so up sampling there's really kind of three ways to do it one is what we can call unpooling so it's the opposite of max pooling so in max pooling right we look at a grid and then within each grid we remember which cell was the maximum and we kind of like store that cell and pass that on but because we remember which cell was the maximum we can actually unpool it and then get back out to uh from the pooled version back to the larger version or we can do this with a convolution operation called a transpose convolution and the idea is that we basically like in this image we have a three by three convolution which looks like a shadow and a two by two input that's in blue and as you slide the convolution over the input you actually generate a larger output so it's like the transpose of the convolution that as we usually do it and then we can combine that with a dilated convolution which uh which basically like increases the receptive field of a convolution and so you lose less information as you down sample an up sample so moving on you can do the same kind of
Full Stack Deep Learning - Spring 2021,2B,2073,"Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation)",thing for actually inferring 3d shape so this is called mesh rcnn and as you might guess it's now introducing not only you know a classification branch a bounding box prediction branch a segmentation branch but now also a voxel branch where the output is going to be a 3d mesh and incredibly it works pretty well of course you know we need label data we need data that has a 2d image and it's 3d mesh because otherwise there's like no real ground truth that we can use for the loss function there is such a data set it's called shape net so it has 55 object categories 51 000 3d models so that's that's nice we might want to detect face landmarks so that's like uh you know points on your nose lips eyes and jawline i guess that if you were able to detect them you might be able to animate an avatar like iphone style or snapchat style whatever um once again we can use a confinet for this we just need label data of the right type so annotated faces in the wild is that kind of data produ gives you 25 000 faces with 21 landmarks on each face or we might want to do pose estimation so we want to detect joint locations so not just the segmentation of a person but actually like the joint positioning of that person totally possible we just add like another you know output of the continent another loss we just need to label data coco actually has this has 250 000 people with key points so basically you know any type of vision task where there's like a lot of label training data is a good match for for deep learning because confidence are really good at representing the whatever information is in the image and if you give them the type of data that they need to output and a proper loss function like they they learn it um so moving on a little bit you know confidence are powerful but they can be brittle and they can be
Full Stack Deep Learning - Spring 2021,2B,2220,Adversarial Attacks,brittle in like surprising ways which which is kind of scary so the area of research called adversarial attacks um has to do with that and there's two ways to do this research one is white box which means that you know can you make an attack if you have actually access to the model parameters or black box which means if you don't know anything about the model can you still you know produce an adversarial attack on it and uh you know the intuition is that what the neural nets are doing is they're trying to like basically learn this very complicated function of how the real world as captured in two-dimensional photography maps to you know categories or segmentations or anything and the real world is like highly variable and high dimensional and the neural network has not seen all of it right it's just seen what's in the training set and it's really good at representing what's in the training set and it learns like this manifold that's representative of the real world but it's it's you know what it learns if it stays on the manifold it's pretty good but if it's pushed off of it it becomes like very wrong very quickly and so you can see on the left here you have an image of a panda the neural network thinks it's a panda with 57 confidence and then you add what seems to me to be a random noise but it's actually something that the network would consider a nematode right and if you add them together you get the same image of a panda to my eyes but to a neural network it now looks like a given with 99 confidence and scarily you can actually um you know not only add noise to an image but you can actually add real things to the world such that when you take an image of them they mess up the neural network so there's uh like for self-driving cars this would be a big area of concern can i actually can anyone print something and then put it on road signs that just make the neural network think that you know the road sign is a like a stop sign as a speed limit sign for example so it doesn't stop at the stop sign um oh here's a little meme state-of-the-art neural network or one noisy boy you know who would win so the basic way to like find that noisy boy is is in trying to like find inputs that that push the the gradient of the network towards some class very strongly and then to defend against such attacks you can do a number of things you can try to include adversarial examples like this in the training set and this doesn't seem to work very well in peer claim you can try to smooth the decision boundaries between classes via some methods such that it's not like so brittle and this can be called this is sometimes called defensive distillation i think i have a slide on it coming up no i don't so yeah really briefly it's basically defensive distillation would would involve training a neural network on the raw training data and that can output classes but that network is brittle to adversarial attacks and then you train a second neural network that learns to give the same outputs as the first neural network so it doesn't necessarily see the the data it only sees like the the outputs of the other of the other network which is sometimes called network distillation another thing that's really interesting in computer vision is style transfer so you might have seen images like this
Full Stack Deep Learning - Spring 2021,2B,2456,Style Transfer,we have like an image that we want to transfer some style to and then take some painterly image like a van gogh starry night and then apply it to the photograph that we took and now we have this cool like we transfer the style of one image onto the content of another and the way it basically works is on this is a gift that should be playing so you have your style target which you run through a neural network and you take some outputs of the neural network into this gray matrix and then you have some random pixels which you'll optimize running through the network and then there's two constraints that you add you want these like green matrices to be very close together and you also want the outputs of the neural network uh images to be very close together and if you do that then it's kind of like one is a constraint on style and the other is a constrained content and if you like optimize it optimize those random pixels that you start with you get an image that satisfies the content of your target image and the style of your style image we have a reading this week about it because it's quite interesting and then of course there's gans generative adversarial networks which are able to produce fake images of you know very lifelike fake images uh and we'll actually talk more about them in the research directions lecture a cool way to learn more is this papers with code um website and if you go to like browse state-of-the-art and computer vision then you actually see like a lot of benchmarks like semantic segmentation classification image generation so if you ever want to learn more about anything in particular like let's say image generation you can um see some common benchmarks that are used and then like the best methods on them and uh and then also like the papers that describe those methods so this is a great website papers with code
fast.ai 2022 - Part 1,1,0, Introduction,"Welcome to Practical Deep Learning for coders, lesson one. This is version five of this course, and it's the first new one we've done in two years. So, we've got a lot of cool things to cover! It's amazing how much has changed. Here is an xkcd from the end of 2015."
fast.ai 2022 - Part 1,1,25, What has changed since 2015,"Who here has seen xkcd comics before? …Pretty much everybody. Not surprising. So the basic joke here is… I'll let you read it, and then I'll come back to it. So, it can be hard to tell what's easy and what's nearly impossible, and in 2015 or at the end of 2015 the idea of checking whether something is a photo of a bird was considered nearly impossible. So impossible, it was the basic idea of a joke. Because everybody knows that that's nearly impossible. We're now going to build exactly that system for free in about two minutes! So, let's build an “is it a bird” system. So, we're going to use python, and I'm going to run through this really quickly."
fast.ai 2022 - Part 1,1,80, Is it a bird,"You're not expected to run through it with me because we're going to come back to it. But let's go ahead and run that cell. Okay, so what we're doing is we're searching DuckDuckgo for images of bird photos and we're just going to grab one and so here is the url of the bird that we grabbed. Okay, we'll download it. Okay, so there it is. So we've grabbed a bird and so okay we've now got something that can download pictures of birds. Now we're going to need to build a system that can recognize things that are birds versus things that aren't birds, from photos. Now of course computers need numbers to work with, but luckily images are made of numbers."
fast.ai 2022 - Part 1,1,129, Images are made of numbers,"I actually found this really nice website called pixby where I can grab a bird, and if I wiggle over it (let's pick its beak) you'll see here that that part of the beak was 251 brightness of red, 48 of green, and 21 of blue. So that's RGB. And so you can see as I wave around, those colors are changing (those numbers). And so this picture, the thing that we recognize as a picture, is actually 256 x 171 x 3 numbers, between 0 and 255, representing the amount of red, green and blue on each pixel. So that's going to be an input to our program, that's going to try and figure out whether this is a picture of a bird or not. Okay, so let's go ahead and run this cell, which is going to go through… (and I needed"
fast.ai 2022 - Part 1,1,209, Downloading images,"bird and non-bird but you can't really search Google images or DuckDuckGo images for not a bird, it just doesn't work that way. So I just decided to use forest - I thought okay pictures of forest versus pictures of birds sounds like a good starting point.) So I go through each of: forest, and bird. And I search for forest photo and bird photo, download images, and then resize them to be no bigger than 400 pixels on a side - just because we don't need particularly big ones and it takes a surprisingly large amount of time just for a computer to open an image. Okay, so we've now got 200 of each. I find when I download images I often get a few broken ones and if you try and train a model with broken images it will not work. So here's something which just verifies each image and unlinks - so deletes the ones that don't work"
fast.ai 2022 - Part 1,1,265, Creating a DataBlock and Learner,"Okay, so now we can create what's called a data block. So after I run this cell you'll see that I've basically… I'll go through the details of this later, but… a data block gives fast.ai (the library) all the information it needs to create a computer vision model. And so in this case we're basically telling it… get all the image files that we just downloaded. And then we say show me a few up to six, and let's see… yeah, so we've got some birds, forest, bird, bird, forest. Okay, so one of the nice things about doing computer vision models is it's really easy to check your data because you can just look at it - which is not the case for a lot of kinds of models. Okay, so we've now downloaded 200 pictures of birds, 200 pictures of forests, so we'll"
fast.ai 2022 - Part 1,1,318, Training the model and making a prediction,"now press run. And this model is actually running on my laptop, so this is not using a vast data center. It's running on my presentation laptop. And it's doing it at the same time as my laptop is streaming video, which is possibly a bad idea. And so what it's going to do is it's going to run through every photo out of those 400, and for the ones that are forest it's going to learn a bit more about what forest looks like and for the ones that are bird it'll learn a bit more about what bird looks like. So overall it took under 30 seconds, and believe it or not, that's enough to finish doing the thing which was in that xkcd comic. Let's check by passing in that bird that we downloaded at the start. This is a bird. Probability it's a bird: 1.0000 (rounded to the nearest four decimal places). So something pretty extraordinary has happened since late 2015, which is literally something that has gone from so impossible it's a joke to so easy that I can run it on my laptop computer in (I don't know how long it was) about two minutes. And so hopefully that gives you a sense that creating really interesting, you know real working programs with deep learning is something that… doesn't take a lot of code, didn't take any math, didn't take more than my laptop computer. It's pretty accessible in fact. So that's really what we're going to be learning about over the next seven weeks. So where have we got to now with deep learning? Well it moves so fast, but even in the last few weeks we've taken it up another notch"
fast.ai 2022 - Part 1,1,440, What can deep learning do now,"as a community. You might have seen that something called DALLꞏEꞏ2 has been released which uses deep learning to generate new pictures. And I thought this was an amazing thing that this guy nick did where he took his friends twitter bios and typed them into the DALLꞏEꞏ2 input and it generated these pictures. So this guy's… he typed in commitment, sympathetic, psychedelic, philosophical, and it generated these pictures. So I'll just show you a few of these. I'll let you read them… I love that. That one's pretty amazing I reckon! actually. I love this. Happy Sisyphus has actually got a happy rock to move around. So this is like, um, yeah, I don't know. When I look at these I still get pretty blown away that this is a computer algorithm using nothing but this text input to generate these arbitrary pictures. In this case of fairly, you know, complex and creative things. So the guy who made those points out, this is like… he spends about two minutes or so, you know, creating each of these. Like he tries a few different prompts and he tries a few different pictures, you know, and so he's given an example here of… like when he types something into the system… like, here's an example of like 10 different things he gets back when he puts in “expressive painting of a man shining rays of justice and transparency, on a blue bird twitter logo.” So it's not just, you know, DALLꞏEꞏ2, to be clear. There's, you know, a lot of different systems doing something like this now. There's something called MidJourney, which this twitter account posted: a “female scientist with a laptop writing code, in a symbolic, meaningful, and vibrant style.” This one here is “an HD photo of a rare psychedelic pink elephant.” And this one I think is the second one here (I never know how to actually pronounce this.) This one's pretty cool “a blind bat with big sunglasses holding a walking stick in his hand.” And so when actual artists, you know, this for example, this guy said he knows nothing about art, you know he's got no artistic talent, it's just something you know, he threw together. This guy is an artist who actually writes his own software based on deep learning and spends, you know, months on building stuff, and as you can see, you can really take it to the next level. It's been really great actually to see how a lot of fast.ai alumni with backgrounds as artists have gone on to bring deep learning and art together, and it's a very exciting direction. And it's not just images to be clear, you know one of another interesting thing that's popped up in the last couple of weeks is google's pathways language model which can take any arbitrary english as text, a question, and can create an answer which not only answers"
fast.ai 2022 - Part 1,1,633, Pathways Language Model (PaLM),"the question but also explains its thinking (whatever it means for a language model to be thinking.) One of the ones I found pretty amazing was that it can explain a joke. I'll let you read this… So, this is actually a joke that probably needs explanations for anybody who's not familiar with TPUs. So it, this model, just took the text as input and created this text as output. And so you can see, you know, again, deep learning models are doing things which I think very few, if any of us, would have believed would be maybe possible to do by computers even in our lifetime This means that there is a lot of practical and ethical considerations. We will touch on them during this course but can't possibly hope to do them justice. So I would certainly encourage you to check out ethics.fast.ai to see our whole data ethics course, taught by my co-founder Dr Rachel Thomas, which goes into these issues in a lot more detail. All right, so as well as being an AI researcher at the University of Queensland and fast.ai, I am also a homeschooling primary school teacher and for that reason I study education a lot. One of the people who I love in education is a guy named Dylan Williams and he has this great approach in his classrooms of figuring out how his students are getting along, which is to put a coloured cup on their desk - green to mean that they're doing fine, yellow cup to mean I'm not quite sure, and a red cup to mean I have no idea what's going on. Now since most of you are watching this remotely I can't look at your cups and I don't think anybody bought coloured cups with them today, so instead we have an online version of this. So what I want you to do is go to cups.fast.ai/fast - that's cups.fast.ai/fast - and don't do this if you're, like, a fast.ai expert who's done the course five times - because if you're following along that doesn't really mean much obviously. This is really for people who are, you know, not already fast.ai experts. And so click one of these colored buttons. And what I will do, is I will go to the teacher version and see what buttons you're pressing. All right! So, so far people are feeling we're not going too fast on the whole. We've got one, nope not one, brief red. Okay! So, hey Nick, this url, the same thing with teacher on the end, if you can you keep that open as well and let me know if it suddenly gets covered in red. If you are somebody who's red, I'm not going to come to you now because there's not enough of you to stop the class. So it's up to you to ask on the forum or on the youtube live chat, and there's a lot of folks luckily who will be able to help you, I hope. All right! I wanted to do a big shout out to Radek. Radeck created cups.fast.ai for me. I said to him last week I need a way of seeing coloured cups on the internet and he wrote it in one evening. And I also wanted to shout out that Radek just announced today that he got a job at Nvidia AI and I wanted to say, you know, that fast.ai alumni around the world very very frequently, like every day or two, email me to say that they've got their dream job. Yeah… If you're looking for inspiration on how to get into the field I couldn't recommend nothing… nothing would be better than checking out Radek's work. And he's actually written a book about his journey. It's got a lot of tips in particular about how to take advantage of fast.ai - make the most of these lessons. And so I would certainly… so check that out as well. And if you're here live he's one of our TAs as well so you can say hello to him afterwards. He looks exactly like this picture here. So I mentioned I spent a lot of time studying education both for my home schooling duties and also for my courses, and you'll see that there's something a bit different, very different,"
fast.ai 2022 - Part 1,1,940, How the course will be taught. Top down learning,"about this course… which is that we started by training a model. We didn't start by doing an in-depth review of linear algebra and calculus. That's because two of my favorite writers and researchers on education Paul Lockhart and David Perkins, and many others talk about how much better people learn when they learn with a context in place. So the way we learn math at school where we do counting and then adding and then fractions and then decimals and then blah blah blah and you know, 15 years later we start doing the really interesting stuff at grad school. That is not the way most people learn effectively. The way most people learn effectively is from the way we teach sports, for example, where we show you a whole game of sports. We show you how much fun it is. You go and start playing sports, simple versions of them, you're not very good right and then you gradually put more and more pieces together. So that's how we do deep learning. You will go into as much depth as the most sophisticated, technically detailed classes you'll find - later, right! But first you'll learn to be very very good at actually building and deploying models. And you will learn why and how things work as you need, to get to the next level. For those of you that have spent a lot of time in technical education (like if you've done a phd or something) will find this deeply uncomfortable because you'll be wanting to understand why everything works from the start. Just do your best to go along with it. Those of you who haven't will find this very natural. Oh! And this is Dylan Wiliam, who I mentioned before - the guy who came up with the really cool cups things. There'll be a lot of tricks that have come out of the educational research literature scattered through this course. On the whole I won't call them out, they'll just be there, but maybe from time to time we'll talk about them. All right! So before we start talking about how we actually built that model and how it works, I guess I should convince you that I'm worth listening to. I'll try to do that reasonably quickly, because I don't like tooting my own horn, but I know it's important. So the first thing I mentioned about me, is that me and my friend Silvain wrote this extremely popular book “Deep Learning for Coders” and that book is what this course is quite heavily based on. We're not going to be using any material from the book directly, and you might be surprised by that, but the reason actually is that the educational research literature shows that people learn things best when they hear the same thing in multiple different ways. So I want you to read the book and you'll also see the same information presented in a different way, in these videos. So on,e of the bits of homework after each lesson will be to read a chapter of the book. A lot of people like the book. Peter Norvig, Director of Research, loves the book. In fact his ones here “one of the best sources for a programmer to become proficient in deep learning.” Eric Topple loves the book. Hal Varian Emeritus Professor at Berkeley, Chief Economist, Google, likes the book. Jerome Pecente who is the head of AI at Facebook likes the book. A lot of people like the book, so hopefully you'll find that you like this material as well. I've spent about 30 years of my life working in and around machine learning including building a number of companies that relied on it. And became the highest ranked competitor in the world on Kaggle in machine learning competitions. My company Enlitic, which I founded, was the first company to specialize in deep learning"
fast.ai 2022 - Part 1,1,1165, Jeremy Howard’s qualifications,"for medicine, and MIT voted it one of the 50 smartest companies in 2016, just above Facebook and Spacex. I started fast.ai with Rachel Thomas and that was quite a few years ago now, but it's had a big impact on the world already, including work we've done with our students, has been globally recognized, such as our win in the DAWNBench competition which showed how we could train big neural networks faster than anybody in the world, and cheaper than anybody in the world. And so that was a really big step in 2018, which actually made a big difference. Google started using our special approaches in their models. Nvidia started optimizing their stuff using our approaches. So it made quite a big difference there. I'm the inventor of the ULMFiT algorithm which according to the Transformers book was one of the two key foundations behind the modern NLP revolution. This is the paper here. And actually, you know, interesting point about that, it was actually invented for a fast.ai course. So the first time it appeared was not actually in the journal. It was actually in lesson four of the course, I think, the 2016 course, if I remember correctly. And, you know, most importantly of course, I've been teaching this course since Version One. And this is actually, I think, this is the very first version of it (which even back then was getting hbr's attention) a lot of people have been watching the course, and it's been, you know, really widely used. Youtube doesn't show likes anymore, so I have to show you our likes for you. You know it's been amazing to see how many alumni have gone from this to, you know, to really doing amazing things, you know. And so for example Andrej Karpathy told me that at Tesla, I think he said, pretty much everybody who joins Tesla in AI is meant to do this course. I believe at OpenAI, they told me that all the residents joining there first do this course. So this, you know this course, is really widely used in industry and research for people, and they have a lot of success. Okay, so there's a bit of brief information about why you should hopefully keep going with this. All right so let's get back to what's happened here. Why are we able to create a bird recognizer in a minute or two? And why couldn't we do it before? So I'm going to go back to 2012 and in 2012 this was how image recognition was done."
fast.ai 2022 - Part 1,1,1358, Comparison between modern deep learning and 2012 machine learning practices,"This is the computational pathologist - it was a project done at Stanford. A very successful, very famous project that was looking at the five-year survival of breast cancer patients by looking at their histopathology image slides. Now, so this is, like, what I would call a classic machine learning approach. And I spoke to the senior author of this, Daphne Koller, and I asked her why they didn't use deep learning and she said “well it just, you know, it wasn't really on the radar at that point.” So this is like a pre-deep-learning approach. And so the way they did this was they got a big team of mathematicians and computer scientists and pathologists and so forth to get together and build these ideas for features, like relationships between epithelial nuclear neighbors. Thousands and thousands actually they created of features, and each one required a lot of expertise from a cross-disciplinary group of experts at Stanford. So this project took years, and a lot of people, and a lot of code, and a lot of math. And then once they had all these features they then fed them into a machine learning model - in this case, logistic regression, to predict survival. As I say it's very successful, right, but it's not something that I could create for you in a minute at the start of a course. Tthe difference with neural networks is neural networks don't require us to build these features. They build them for us! And so what actually happened was, in I think it was 2015, Matt Zeiler and Rob Fergus took a trained neural network and they looked inside it to see what it had learned. So we don't give it features, we ask it to learn features."
fast.ai 2022 - Part 1,1,1471, Visualizing layers of a trained neural network,"So when Zeiler and Zeiler looked inside a neural network, they looked at the actual weights in the model and they drew a picture of them. And this was nine of the sets of weights they found. And this set of weights, for example, finds diagonal edges. This set of weights finds yellow to blue gradients. And this set of weights finds red to green gradients, and so forth, right. And then down here are examples of some bits of photos which closely matched, for example, this feature detector. And deep learning, I mean, is deep because we can then take these features and combine them to create more advanced features. So these are some layer two features. So there's a feature, for example, that finds corners. And a feature that finds curves. And a feature that finds circles. And here are some examples of bits of pictures that the circle finder found. And so remember with a neural net which is the basic function used in deep learning, we don't have to hand code any of these or come up with any of these ideas. You just start with actually a random neural network and your feed it examples and you have it learn to recognize things, and it turns out that these are the things that it creates for itself. So you can then combine these features. And when you combine these features it creates a feature detector, for example, that finds kind of repeating geometric shapes. And it creates a feature detector, for example, that finds kind of frilly little things, which it looks like is finding the edges of flowers. And this feature detector here seems to be finding words. And so the deeper you get the more sophisticated the features it can find are. And so you can imagine that trying to code these things by hand would be, you know, insanely difficult, and you wouldn't know even what to encode by hand, right! So what we're going to learn is how neural networks do this automatically, right, but this is the key difference of why we can now do things that previously we just didn't even conceive of as possible, because now we don't have to hand code the features we look for. They can all be learned. it's important to recognize we're going to be spending some time learning about building image based algorithms and image-based algorithms are not just for images and in fact this is going to be a general theme. We're going to show you some foundational techniques but with creativity these foundational techniques can be used very widely. So for example, an image recognizer can also be used to classify sounds. So this was an example from one of our students who posted on the forum and said for their"
fast.ai 2022 - Part 1,1,1660, Image classification applied to audio,project they would try classifying sounds and so they basically took sounds and created pictures from their waveforms and then they used an image recognizer on that and they got a state-of-the-art result by the way. Another of our students on the forum said that they did something very similar to take time series and turn them into pictures and then use image classifiers. Another of our students created pictures from mouse movements from... from users of a computer
fast.ai 2022 - Part 1,1,1688, Image classification applied to time series and fraud,"system. So the clicks became dots and the movements became lines and the speed of the movement became colors and then used that to create an image classifier. So you can see with... with some creativity there's a lot of things you can do with images. There's something else I wanted to point out which is that as you saw when we trained a real working bird recognizer image model we didn't need lots of math; there wasn't any. We didn't need lots of data. We had 200 pictures. We didn't need lots of expensive computers; we just used my laptop. This is generally the case for the vast majority of deep learning that you'll need in... in real life. There will be some math that pops up during this course but we will teach it to you as needed or we'll refer you to external resources as... as needed but it'll just be the little bits that you actually need. You know the myth that deep learning needs lots of data, I think, is mainly passed along by big companies that want to sell you computers to store lots of data and to process it. We find that most real world projects don't need extraordinary amounts of data at all and as you'll see there's actually a lot of fantastic places you can do state-of-the-art work for free nowadays which is... which is great news. One of the key reasons for this is because of something called transfer learning which we'll be learning about a lot during this course and it's something which very few people are aware of the pay-off. In this course we'll be using Pytorch. For those of you who are not particularly close to the deep learning world, you might have heard of Tensorflow and not of Pytorch. You might be surprised to hear that Tensorflow has been dying in popularity in recent years"
fast.ai 2022 - Part 1,1,1816, Pytorch vs Tensorflow,"and Pytorch is actually growing rapidly and in… in research repositories amongst the top papers, Tensorflow is a tiny minority now compared to Pytorch. This is also great research that's come out from Ryan O'Connor. He also discovered that... the majority of people that were doing Tensorflow in 2018 researchers, the majority have now shifted to Pytorch and I mention this because what people use in research is a very strong leading indicator of what's going to happen in industry because this is where you know all the new algorithms are going to come out. this is where all the papers are going to be written about. it's going to be increasingly difficult to use Tensorflow. We've been using Pytorch since before it came out, before the initial release because we knew just from technical fundamentals, it was far better. So this course has been using Pytorch for a long time. I will say however that Pytorch requires a lot of hairy code for relatively simple things. This is the code required to implement a particular optimizer called AdamW in plain Pytorch."
fast.ai 2022 - Part 1,1,1903, Example of how Fastai builds off Pytorch (AdamW optimizer),"I actually copied this code from the Pytorch repository so as you can see there's a lot of it. This gray bit here is the code required to do the same thing with fast.ai. fast.ai is a library we built on top of Pytorch. This huge difference is not because Pytorch is bad. It's because Pytorch is designed to be a strong foundation to build things on top of, like fast.ai. So... When you use fast.ai - the library, you get access to all the power of Pytorch as well but you shouldn't be writing all this code if you only need to write this much code, right? The problem of writing lots of code is that that's lots of things to make mistakes with, lots of things to, you know, not have best practices in, lots of things to maintain. In general we've found, particularly with deep learning: less code is better. Particularly with fastai, the code you don't write is code that we've basically found kind of best practices for you. So when you use the code that we've provided for you, you know you'll generally find you get better results. So... so fast.ai has been a really popular library and it's very widely used in industry, in academia, and in teaching and as we go through this course we'll be seeing more and more pure Pytorch as we get deeper and deeper underneath to see exactly how things work. The fast.ai library just won the 2020 best paper award for the paper about it in Information so again you can see it's a very well regarded library. Okay so… Okay we're still green, that's good. So you may have noticed something interesting, which is that I'm actually running code in these slides. That's because these slides are not in PowerPoint. These slides are in a Jupyter notebook. Jupyter notebook is the environment in which you will be doing most of your computing. It's a web-based application which is extremely popular and widely used in industry and in academia and in teaching and it is a very very very powerful way to to experiment and explore and to build. Nowadays I would say most people at least most students run jupyter notebooks not on their own computers particularly for data science but on a cloud server of which there's quite a few and as I mentioned earlier if you go to course.fast.ai you can see how to use various different cloud servers. One I'm going to show an example of is Kaggle. So Kaggle doesn't just have competitions but it also has a cloud notebook server and I've got quite a few examples there."
fast.ai 2022 - Part 1,1,2118, Using cloud servers to run your notebooks (Kaggle),"So let me give you a quick example of how we use Jupyter notebooks. To... to... to build stuff, to... to experiment, to explore. So on kaggle, if you start with somebody else's notebook... so why don't you start with this one Jupyter notebook 101. If it's your own notebook you'll see a button called edit. If it's somebody else's, that button will say copy and edit. If you use somebody's notebook that you like, make sure you click the upvote button to encourage them and to help other people find it before you go ahead and copy and edit. And once we're in edit mode we can now use this notebook and to use it we can type in any arbitrary expression in python and click run and the very first time we do that it says session is starting it's basically launching a virtual computer for us to run our code. This is all free. In a sense, it's like the world's most powerful calculator. It's a calculator where you have all of the capabilities of the world's I think most popular programming language – certainly it and javascript would be the top two – directly at your disposal. So, python does know how to do one plus one, and so you can see here it spits out the answer. I hate clicking I always use keyboard shortcuts so instead of clicking this little arrow, you just press shift enter to do the same thing but as you can see there's not just calculations here, there's also prose, and so jupyter notebooks are great for explaining to you the version of yourself in six months time, what on earth you were doing, or to your co-workers, or the people in the open source community, or the people you're blogging for etc, and so you just type prose, and as you can see when we create a new cell, you can create a code cell which is a cell that lets you type calculations, or a markdown cell which is a cell that lets you create prose. And the prose uses formatting in a little mini language called “markdown”. There's so many tutorials around I won't explain it to you but it lets you do things like links and so forth. So I'll let you follow through the tutorial in your own time because it really explains to you what to do. One thing to point out is that sometimes you'll see me use cells with an exclamation mark at the start. That's not Python, that's a bash shell command, okay? so that's what the exclamation mark means. As you can see you can put images into notebooks, and so the image I popped in here was the one showing that Jupyter won the 2017 software system award which is pretty much the biggest award there is for this kind of software. Okay, so that's the basic idea of how we use notebooks. So let's have a look at how we do our bird or not bird model. One thing I always like to do when I'm using something like colab or kaggle cloud, cloud platforms that I'm not controlling is, make sure that I'm using the most recent version of any software. So my first cell here is exclamation mark pip install minus u , (that means upgrade)"
fast.ai 2022 - Part 1,1,2325, Bird or not bird? & explaining some Kaggle features,"q (for quiet) fast.ai. So that makes sure that we have the latest version of fast.ai, and if you always have that at the start of your notebooks you're never going to have those awkward forum threads where you say “why isn't this working?” and somebody says to you “oh you're using an old version of some software!” So, you'll see here this notebook is the exact thing that I was showing you at the start of this lesson, So, if you haven't done much python, you might be surprised about how little code there is here, and so python is a concise but not too concise language. You'll see that there's less boilerplate than some other languages you might be familiar with, and I'm also taking advantage of a lot of libraries so fast.ai provides a lot of convenient things for you. Oh I forgot to import… So, to use a external library we use import to “import a symbol from a library”. fast.ai has a lot of libraries we provide. They generally start with “fast something” so for example to make it easy to download"
fast.ai 2022 - Part 1,1,2415, How to import libraries like Fastai in Python,"a url, “fastdownload” has download_url(). To make it easy to create a thumbnail, we have Image.to_thumb() and so forth. So, I always like to view – as I'm building a model – my data at every step. So that's why I first of all grab one bird, and then I grab one forest photo, and I look"
fast.ai 2022 - Part 1,1,2442, Best practice ,"at them to make sure they look reasonable. And once I think, “okay they look okay”, then I go ahead and download. And, so, you can see fast.ai has a download_images() where you just provide a list of urls so that's how easy it is, and it does that in parallel. So it does that, you know, surprisingly quickly. One other fast.ai thing I'm using here is resize_images(). You generally… you'll find that for computer vision algorithms you don't need particularly big images, so I'm resizing these to a maximum size length of 400 because it just… it's actually much faster because gpus are so quick for big images, most of the time can be taken up just opening it. The neural net itself often takes less time. So that's another good reason to make them smaller. Okay, so the main thing I wanted to tell you about was this data block command. So the data block is the key thing that you're going to want to get familiar with as deep learning practitioners at the start of your journey because the main thing you're going"
fast.ai 2022 - Part 1,1,2520, Datablocks API overarching explanation,"to be trying to figure out is how do I get this data into my model? Now that might surprise you. You might be thinking we should be spending all of our time talking about neural network architectures, and matrix multiplication and gradients and stuff like that, the truth is very little of that comes up in practice, and the reason is, that at this point the deep learning community has found a reasonably small number of types of model that work for nearly all the main applications you'll need; and fast.ai will create the right type of model for you the vast majority of the time. So all of that stuff about tweaking neural network architectures and stuff… I mean we'll get to it eventually in this course but you might be surprised to discover that it almost never comes up. Kind of like if you ever did like a computer science course or something and they spent all this time on the details of compilers and operating systems, and then you get to the real world and you never use it again. So this course is called practical deep learning, and so we're going to focus on the stuff that is practically important. Okay, so our images have finished downloading, and two of them were broken so we just deleted them. Another thing you'll note by the way if you're a keen software engineer is, I tend to use a lot of functional style in my programs I find for kind of the kind of work I do that a functional style works very well if you're, you know a lot of people in python are less familiar with, that it's more, maybe comes, more from other things. So yeah that's why you'll see me using stuff like map and stuff quite a lot. Alright so a data block is the key thing you need to know about if you're going to know how to use different kinds of data sets, and so these are all of the things, basically, that you'll be providing. And so what we did when we designed the data block was we actually looked and said “okay over hundreds of projects what are all the things that change from project to project to get the data into the right shape”?, and we realized we could basically split it down into these five things. So the first thing that we tell fast.ai is “what kind of input do we have”?, and so then, so there are lots of blocks in fast.ai for different kinds of inputs so we said “ah, the input is an image!; “what kind of output is there?”, “what kind of label?.”"
fast.ai 2022 - Part 1,1,2680, Datablocks API parameters explanation,"The output's a category, so that means it's one of a number of possibilities. So that's enough for fast.ai to know what kind of model to build for you. So what are the items in this model? what am I actually going to be looking at to look to train from? this is a function! In fact you might have noticed if you were looking carefully that we use this function here. It's a function which returns a list of all of the image files in a path based on extension, so every time it's going to try and find out what things to train from, it's going to use that function. In this case we'll get a list of image files. Something we'll talk about shortly is that it's critical that you put aside some data for testing the accuracy of your model that's called a “validation set.” It's so critical that fast.ai won't let you train a model without one, so you actually have to tell it how to create a validation set, how to set aside some data, and in this case we say “randomly, set aside 20% of the data.” Okay, next question, then you have to tell fast.ai is “how do we know the correct label of a photo”, how do we know if it's a bird photo or a forest photo? and this is another function, and this function simply returns the parent folder of of a path and so in this case we saved our images into either forest or bird. So that's where the labels are going to come from And then finally, most computer vision architectures need all of your inputs as you train to be the same size, so “item transforms” are all of the bits of code that are gonna run on every item, on every image in this case, and we're saying okay we want you to resize each of them to being 192 by 192 pixels. There's two ways you can resize: you can either crop out a piece in the middle, or you can squish it, and so we're saying “squish it”. So that's the data block that's all that you need, and from there we create an important class called data loaders. Data loaders are the things that, actually, pytorch iterates through to grab a bunch of your data at a time. The way it can do it so fast is by using a GPU which is something that can do thousands of things at the same time and that means it needs thousands of things to do at the same time so a data loader will feed the training algorithm with a “bunch” of your images at once; in fact we don't call it a bunch we call it a “batch” or a “mini batch”. And so, when we say show “batch” that's actually a very specific word in deep learning. It's saying show me an example of a batch of data that you would be passing into the model, and so you can see showbatch gives you – tells you – two things: the input which is the picture, and the label, and remember! the label came by calling that function. So, when you come to building your own models you'll be wanting to know what kind of splitters are there? what kinds of labeling functions are there? and so forth what's wrong button you'll be wanting to know what kind of labeling functions are there and what kind of splitters are there and so forth and so docs.fast.ai is where you go to get that information. Often the best place to go is to choose the tutorials. So for example here's a whole data block tutorial, and there's lots and lots of examples, so"
fast.ai 2022 - Part 1,1,2920, Where to find fastai documentation,"hopefully you can start out by finding something that's similar to what you want to do and see how we did it, but then of course there's also the underlying api information so here's data blocks! Okay. How are we doing? Still doing good! Alright. So at the end of all this we've got an object called “dls” that stand for “data loaders” and that contains iterators that pytorch can run through to grab batches of randomly split out training images to train the model with, and validation images to test the model with. So now we need a model. The critical concept here in fastai is called a “learner”. A “learner” is something which combines a model, which is, that is, the actual neural network function we’ll be training, and the data we use to train it with; and that's"
fast.ai 2022 - Part 1,1,2994, Fastai’s learner (combines model & data),"why you have to pass in two things: the data which is the data loaders object, and a model. And so the model is going to be the actual neural network function that you want to pass in, and as I said there's a relatively small number that basically work for the vast majority of things you do. If you pass in just a bare symbol like this it's going to be one of fast.ai's built-in models but what's particularly interesting is that we integrate a wonderful library by Ross Wightman called “timm” (the pytorch image models) which is the largest collection of computer vision models in the world, and at this point fast.ai is the first and only"
fast.ai 2022 - Part 1,1,3040, Fastai’s available pretrained models,"framework to integrate this. So you can use any one of the pytorch image models and one of our students Amanamoro was kind enough to create this fantastic documentation where you can find out all about the different models. And if we click on here, you can get lots and lots of information about all the different models that Ross has provided. Having said that, the model family called “resnet” are probably going to be fine for nearly all the things you want to do, but it is fun to try different models out so you can type in any string here to use any one of those other models Okay so if we run that, let's see what happens okay? So this is interesting… So when I ran this… so, remember on kaggle it's creating a new virtual computer for us, so it doesn't really have anything ready to go, so when I ran this the first thing it did was it said “downloading resnet18.pth” What's that? well, the reason we can do this so fast is, because somebody else has already trained"
fast.ai 2022 - Part 1,1,3122, What’s a pretrained model?,"this model to recognize over 1 million images of over 1 000 different types; something called the “ImageNet” dataset, and they then made those weights available – those parameters– available on the internet for anybody to download. By default on fast.ai when you ask for a model, we will download those weights for you, so that you don't start with a random network that can't do anything. You actually start with a network that can do an awful lot, and so, then something that fast.ai has, that's unique, is this fine_tune method, which, what it does is: it takes those pre-trained weights we downloaded for you, and it adjusts them in a really carefully controlled way to just teach the model the differences between your data set, and what it was originally trained for. That's called “fine tuning”; hence the name. So that's why you'll see this downloading happen first, and so as you can see at the end of it, this is the error right here after a few seconds it's a 100% accurate So we now have a learner, and this learner has been, has started with, a pre-trained model that's been fine-tuned for the purpose of recognizing bird pictures from forest pictures. So you can now call dot predict on it, and dot predict you pass in an image, and so this is how you would then deploy your model. So in the code you have whatever it needs to do."
fast.ai 2022 - Part 1,1,3228, Testing your model with predict method,"So in this particular case this person had some reason that he needs the app to check whether they're in the national park, and whether it's a photo of a bird, so at the vet where they need to know if it's a photo of a bird it would just call this one line of code learn.predict(); and so that's going to return whether it's a bird or not as a string, whether it's a bird or not as an integer, and the probability that it's a non-bird or a bird; and so that's why we can print these things out. Okay so that's how we can create a computer vision model. What about other kinds of models? right? There's a lot more in the world than just computer vision. A lot more than just image recognition. Well even within computer vision, there's a lot more than just image recognition. For example, for example, there's segmentation! So segmentation, maybe the best way to explain segmentation is to show you the result of this model."
fast.ai 2022 - Part 1,1,3308, Other applications of computer vision. Segmentation,"Segmentation is where we take photos – in this case of road scenes – and we color in every pixel according to what it's a, what is it. So in this case we've got brown as cars, blue is fences I guess? red is buildings? or brown? And so on the left here some photos that some somebody has already gone through, and classified every pixel of, every one of these images according to what that pixel is a pixel of, and then on the right is what our model is guessing, and as you can see it's getting a lot of the pixels correct and some of them it’s getting wrong. It's actually amazing how many is getting correct because this particular model I trained in about 20 seconds using a tiny, tiny, tiny, amount of data. So you know, again, like, you would think this would be a particularly challenging problem to solve, but it took about 20 seconds of training to solve it, not amazingly well, but pretty well. If I trained it for another two minutes it'd probably be close to perfect. So this is called “segmentation”. Now you'll see that there's very, very, little data required, and … sorry, very little CODE required, and the steps are actually going to look quite familiar."
fast.ai 2022 - Part 1,1,3408, Segmentation code explanation,"In fact in this case we're using an even simpler approach. Earlier on, we used “data blocks”. Data blocks are a kind of intermediate level very … flexible approach that you can take to handling almost any kind of data, but for the kinds of data that occur a lot, you can use these special data loaders classes which kind of lets you use even less code. So, in this case, to create data loaders for segmentation, you can just say: “okay I'm going to pass you in a function for labeling”; and you can see here it's got pretty similar things that we pass in, to what we passed in for data blocks before. So our file names is get_image_files() again, and then our label function is something that grabs this path, and the codes. So the labels for further segmentation, sorry, the codes. So, like, “what does each code mean?” is going to be this text file, but you can see the basic information we're providing is very very similar regardless of whether we're doing segmentation or object recognition; and then the next steps are pretty much the same. We create a learner for segmentation. We create, so, we've got a UNet learner which we'll learn about later, and then again, we call fine_tune() so that is it! and that's how we create a segmentation model! What about stepping away from computer vision? So perhaps the most widely used kind of model used in industry is tabular analysis. So, taking things like spreadsheets and database tables, and trying to predict columns of those."
fast.ai 2022 - Part 1,1,3512, Tabular analysis with fastai,"So, in tabular analysis, it really looks very similar to what we've seen already, We grabbed some data and you'll see when I call this untar_data(), this is the thing in fast.ai that downloads some data and decompresses it for you and there's a whole lot of urls provided by fast.ai for all the, kind of common data sets that you might want to use; you know all the ones that are in the book? or lots of data sets that are kind of widely used in learning and research? So, it makes life nice and easy for you. So again, we're going to create data loaders, but this time it's tabular data loaders, but we provide pretty similar kind of information to what we have before. Couple of new things we have to tell it. “Which of the columns are categorical?” so they can only take one of a few values, and “which ones are continuous” so they can take, basically, any real number, and then again, we can use the exact same show_batch() that we've seen before, to see the data, and so, fast.ai uses a lot of something called “type dispatch” which is a system that's particularly popular in in a language called"
fast.ai 2022 - Part 1,1,3582, show_batch method explanation,"Julia, to basically, automatically, do the right thing for your data, regardless of what kind of data it is. So, if you call show_batch() on something you should go get back something “useful” regardless of what kind of information you provided. So for a table it shows you on the information in that table. This particular data set is a data set of whether people have less than fifty thousand dollars or more than fifty thousand dollars in salary for different districts based on demographic information in each district. So, to build a model for that data loader, we do as always “something” underscore “learner”. In this case it's a tabular learner. Now this time we don't say: “fine_tune()”; we say “fit”. Specifically, fit_one_cycle(). That's because for tabular models, there's not generally going to be a pre-trained model that already does something like what you want, because every table of data is very different, whereas pictures often have a similar theme, you know? They're all pictures. They all have the same kind of “general idea” of what pictures are. So that's why it generally doesn't make too much sense to fine-tune a tabular model. So instead, you just fit. So there's one difference there. I'll show another example. Okay, so, collaborative filtering. “Collaborative filtering” is the basis of most recommendation systems today. It's a system where we basically take data set that says: which users liked which products,"
fast.ai 2022 - Part 1,1,3685, Collaborative filtering (recommendation system) example,"or which users used which products, and then we use that to guess what other products those users might like based on finding similar users, and what those similar users like. The interesting thing about collaborative filtering is that when we say “similar users,” we're not referring to similar demographically, but similar in the sense of people who liked the same kinds of products. So, for example, if you use any of the music systems like Spotify, or Apple music, or whatever, it'll ask you first, like, “what's a few pieces of music you like,” and you tell it, and then it says “okay well maybe let's start playing this music for you,” and that's how it works. It uses collaborative filtering. So we can create a collaborative filtering data loaders in exactly the same way that we're used to, by downloading and decompressing some data, create our collab data loaders. In this case we can just save from csv, and pass in a csv, and this is what collaborative filtering data looks like. It's going to have, generally speaking, a user id, some kind of product id; in this case, a movie, and a rating. So, in this case this user gave this movie a rating of 3.5 out of 5, and so, again, you can see show_batch, right? So use show batch. You should get back some useful visualization of your data regardless of what kind of data it is, and so again, we create a “learner.” This time the “collaborative filtering” learner, and you pass in your data. In this case we give it one extra piece of information, which is – because this is not predicting a category, but it's predicting a real number – we tell it: “what's the possible range.” The actual range is one to five, but for reasons you'll learn about later, it's a good idea to actually go from a little bit lower than the possible minimum, to a little bit higher. That's why I say 0.5 to 5.5, and then fine-tune. Now, again, you know we don't really need to fine-tune here because there's not really such a thing as a pre-trained collaborative filtering model. We could just say “fit” or “fit_one_cycle”, but actually fine_tune() works fine as well. So, after we train it for a while, this here is the “mean squared error”, so it's basically, that: “on average how far off are we for the validation set”. And you can see as we train, and it's literally so fast (it's less than a second each epoch) that error goes down and down, and for any kind of fast.ai model you can always call show_results() , and get something sensible. So in this case it's going to show a few examples of users and movies. Here's the actual rating that user gave that movie and here's the rating that the model predicted. Okay, so, apparently a lot of people on the forum are asking how I'm turning this notebook into a presentation? So, I'll be delighted to show you, because I'm very pleased that these people made this thing for free for us to use. It's called “Rise”, and all I do is, it's a notebook extension, and in your notebook it gives you an extra little thing on the side where you say which things are slides, or which things are fragments, and a fragment just being…"
fast.ai 2022 - Part 1,1,3908, How to turn your notebooks into a presentation tool (RISE),"so, this is a slide, that's a fragment. So if I do that you'll see it starts with a slide and then the fragment gets added in. Yeah, that's about all there is to it! Actually it's pretty great, and it's very well documented. You know I'll just mention, like what do I make with Jupyter notebooks: this entire book was written entirely in Jupyter notebooks."
fast.ai 2022 - Part 1,1,3945, What else can you make with notebooks?,"Here are the notebooks. So if you go to the fastai fastbook repo… if you go to the fastai fastbook repo, you can read the whole book, and because it's all in notebooks, every time we say: “here's how you create this plot,” or “here's how you train this model,” you can actually create the plot, or you can actually train the model, because it's all notebooks. The entire fast.ai library is actually written in notebooks. So you might be surprised to discover that if you go to fast.ai fast.ai that the source code for the entire library is notebooks, and so the nice thing about this is that, you know, the source code for the fast.ai library has actual pictures of the actual things that we're building for example. What else have we done with notebooks? oh! blogging! I love blogging with notebooks, because when I want to explain something I just write the code, and you can just see the outputs, and it all just works. Another thing you might be surprised by, is all of our tests and continuous integration are also all in notebooks. So every time we change one of our notebooks… every time we change one of our notebooks, hundreds of tests get run automatically, in parallel, and if there's any issues we will find out about it. So, yeah, notebooks are great! and rise is… is a really nice way to do slides in notebooks. Alright. So, what can deep learning do at present? We're still scratching this the tip of the iceberg even though it's a pretty well hyped you know heavily marketed technology this pro… you know when we started in 2014 or"
fast.ai 2022 - Part 1,1,4086, What can deep learning do presently?,"so, you know, not many people were talking about deep learning, and really there was no accessible way to get started with it. There were no pre-trained models you could download, you know. There was just starting to appear some of the first open source software that would run on gpus, but yeah, I mean, but despite the fact that today there's a lot of people talking about deep learning, we're just scratching the surface. Every time pretty much somebody says to me I work in domain x, and I thought I might try deep learning out to see if it can help, and I see them a few months later, and I say “how did it go?” they nearly always say: “well we just broke the state-of-the-art results in our field.” So, you know, when I say these are things that it's currently state of the art for – these are kind of the ones that people have tried so far – but still, most things haven't been tried. So, in NLP, deep learning is the state of the art method in all these kinds of things and a lot more; computer vision, medicine, biology recommendation systems, playing games, robotics… I mean it's just… I've tried… I've tried elsewhere to make bigger lists, and I just end up with pages and pages and pages! so yeah it's… it's generally speaking you know, if it's something that a human can do reasonably quickly, like look at a go board and decide if it looks like a good go board or not, even if it needs to be an ex even… if it needs to be an expert human, then that's probably something that deep learning will be pretty good at. If it's something that takes a lot of logical thought processes over an extended period of time, you know, particularly if it's not based on much data, maybe not. You know, like, “who's going to win the next election”, or something like that. That'd be kind of broadly how I try to decide: is your thing useful for deep… you know, good for deep learning or not. You know it's been a long time to get to this point. Yes, deep learning is incredibly powerful now, but it's taken decades of work. This was the first neural network. Remember, neural networks are the basis of deep learning. So this was back in 1957."
fast.ai 2022 - Part 1,1,4233, The first neural network ,"The basic ideas have not changed much at all, but you know we do have things like gpus now, and solid-state drives, and stuff like that, and of course much more data, just is available, now, but this has been decades of really hard work by a lot of people to get to this point. So let's kind of take a step back, and talk about, like, what's what's going on in these models, and I'm going to describe the basic idea of machine learning largely as it was described by Arthur Samuel in the late 50s when it was invented, and I'm going to kind of do it with these graphs; oh which, by the way, you might find fun… these graphs are themselves… created with Jupyter notebooks. So these are graphviz descriptions that are going to get turned into these, so there's a little sneak peek behind the scenes for you? So, let's start with kind of a graph of, like, well, what does a normal program look like? right? so, in the pre deep learning, and machine learning days, well, you know, you'd have… you still have inputs, and you still have results. Right? and then you code a program in the middle, which is, you know, a bunch of conditionals, and loops, and setting variables, and blah blah blah. Okay. A “machine learning model” doesn't look that different… but… The program has been replaced with something called “a model,” and we don't just have"
fast.ai 2022 - Part 1,1,4358, Machine learning models at a high level,"inputs now, we now also have weights, which are also called “parameters,” and the key thing is this: the model is not any more a bunch of conditionals, and loops, and things. It's a mathematical function. In the case of a neural network, it's a mathematical function that takes the inputs, multiplies them together by the weights – by one set of weights – and adds them up; and then it does that again for a second set of weights and adds them up, does it again for a third set of weights, and adds them up and so forth. It then takes all the negative numbers, and replaces them with zeros, and then it takes those as inputs to the next layer. It does the same thing; multiplies them a bunch of times, and adds them up, and it does that a few times, and that's called “a neural network.” Now, the model therefore, is not going to do anything useful, unless these weights are very carefully chosen. And, so the way it works is, that we actually start out with these weights as being random. So initially, this thing doesn't do anything useful at all! So, what we do –- the way arthur samuel described it back in the late 50s (the inventor of machine learning) – is, he said: “okay, let's take the inputs, and the weights, put them through our model.” He wasn't talking particularly about neural networks. He's just like… whatever model you like… get the results, and then let's decide how “good” they are, right? So, if for example, we're trying to decide: “is this a picture of a bird?” and the model said – which initially is random – says “this isn't a bird,” and actually it IS a bird, we would say: “oh you're wrong!” So we then calculate “the loss.” So, the loss is a number that says how good were the results. So that's all pretty straightforward, you know. We could, for example, say oh what's the accuracy? We could look at 100 photos and say which percentage of them did it get right. No worries. Now the critical step is this arrow. We need a way of updating the weights that is coming up with a new set of weights that are a bit better than the previous set. Okay. And by a bit better we mean it should make the loss get a little bit better. So we've got this number that says how good is our model and initially it's terrible, right? It's random. We need some mechanism of making a little bit better. If we can just do that one thing then we just need to iterate this a few times. Because each time we put in some more inputs and... and put in our weights and get our loss and use it to make it a little bit better, then if we make it a little bit better enough times, eventually it's going to get good. Assuming that our model is flexible enough to represent the thing we want to do. Now remember what I told you earlier about what a neural network is: which is basically multiplying things together and adding them up and replacing the negatives with zeros and you do that a few times, that is provably an infinitely flexible function. So it actually turns out that that incredibly simple sequence of steps, if you repeat it a few times, you do enough of them, can solve any computable function. And something like generate an artwork based off somebody's twitter bio is an example of a computable function, right? Or translate English to Chinese is an example of a computable function. So they're not the kinds of normal functions you do in year eight math right but they are computable functions. and so therefore if we can just create this step then and use the neural network as our model then we're good to go. In theory we can solve anything given enough time and enough data and so that's exactly what we do. And so once we've finished that training procedure we don't need the loss anymore and even the weights themselves, we can integrate them, kind of into the model, right? We finish changing them so we can just say that's now fixed and so once we've done that we now have something which takes inputs, puts them through a model and gives us results. It looks exactly like our original idea of a program and that's why we can do what I described earlier, that is once we've got that learn.predict for our bird recognizer we can insert it into any piece of computer code, right? Once we've got a trained model it's just another piece of code we can call with some inputs and get some outputs. Deploying machine learning models in practice can come with a lot of, you know, little tricky details but the basic idea in your code is that you're just going to have a line of code that says learn.predict and then you just fit it in with all the rest of your code in the usual way. And this is why: because a trained model is just another thing that maps inputs to results. Okay. All right. so as we come to wrap up this first lesson, for those of you that are already familiar"
fast.ai 2022 - Part 1,1,4707, Homework,"with notebooks and Python, there's... you know... this has got to be pretty easy for you. You're just going to be using some stuff that you're already familiar with and some slightly new libraries. For those of you who are not familiar with Python, you know, you... you're biting into a big thing here. There's obviously a lot you're going to have to learn and to be clear I'm... I'm not going to be teaching Python in this course but we do have links to great python resources in the forum. So check out... check out that thread. Regardless of where you're at, the most important thing is to experiment and so experimenting could be as simple as just running those Kaggle notebooks that I've shown you just to see them run. You could try changing things a little bit. I'd really love you to try doing the... the bird or forest exercise but come up with something else. Maybe try to use three or four categories rather than two, you know. Have a think about something that you think would be fun to try. Depending on where you're at, you know, push yourself a little bit but not too much. So make sure you get something finished before the next lesson. Most importantly, read chapter one of the book. It's got much the same stuff that we've seen today but presented in a slightly different way. And then come back to the forums and present what you've done in the share your work here thread. After the first time we did this in year one of the course we got over a thousand replies and of those replies it's amazing how many of them have ended up turning into new startups, scientific papers, job offers. It's been really cool to watch people's journeys and some of them are just plain fun, you know. So this person classified different types of Trinidad and Tobago people. So you know, people do stuff based on where they live and what their interests are. I don't know if this person is particularly interested in zucchini and cucumber but they made a zucchini and cucumber classifier. I thought this was a really interesting one: classifying satellite imagery into what city it's probably a picture of. Amazingly accurate actually. 85 percent with 110 classes. Panama city bus classifier. Batik cloth classifier. This one, you know, very practically important, you know, recognizing the state of buildings. We've had quite a few students actually move into disaster resilience based on satellite imagery using exactly this kind of work. We've already actually seen this example. Ethan Sutin the... the sound classifier and I mentioned it was state of the art. He actually checked up the dataset's website and found that he beat the state of the art for that. Alena Harley did tumor-normal sequencing. So she was at Human Longevity International. So she actually did three different really interesting pieces of cancer work during that first course if I remember correctly. And I showed you this picture before. What I didn't mention is actually this... this student Gleb was a software developer at Splunk, a big NASDAQ listed company. And this student project he did turned into a new patented product at Splunk and a big blog post and the whole thing turned out to be really cool. Basically something to identify fraudsters using image recognition with these pictures we discussed. One of our students built this startup called Envision. Anyway there's been lots and lots of examples. So all of this is to say, you know, have... have a go at you know, starting something. Create something you think would be fun or or interesting and share it in the forum. If you're a total beginner with Python, you know, then start... start with something simple. But I think you'll find people very encouraging and if you've done this a few times before then try to push yourself a little bit further. And don't forget to look at the quiz questions at the end of the book and see if you can answer them all correctly. All right, thanks everybody, so much for coming. Okay. Thanks so much for coming everybody. Bye."
fast.ai 2022 - Part 1,2,0,Introduction,"Hi everybody. Welcome to lesson two. Thanks for coming back… slight change of environment here, we had a bit of an “administrative issue” at our university — somebody booked our room — so I'm doing this from the study at home. so sorry about the lack of decorations behind me. I'm actually really really pumped about this lesson. It feels like going back to what things were like in the very early days, because we're doing some really new, really cool stuff, which… you know… stuff that hasn't really been in courses like this before. So, I'm super super excited. So thanks a lot for coming back after lesson one, and I hope it's worth you coming back. I think… I think you're gonna love it! I am, yeah, I'm really excited about this."
fast.ai 2022 - Part 1,2,55,Reminder to use the fastai book as a companion to the course,"Now remember that, you know, the course goes with the book, so be sure that you're following. I mean, not following along in the book, because we're covering similar things in different directions… but, like, read the book as well, and remember the book is entirely available for free as well. You can go to the fast.ai fastbook repo to see the notebooks, or through course.fast.ai you can read it there — for example, through Colab. And also remember that the book… I mean… the book has a lot of stuff that we didn't cover in the course like, you know, stuff I find pretty interesting about the history of neural networks, some of which has some really interesting personal stories actually, as you'll read here. And at the end of each chapter there is a quiz. Remember it's not a bad idea before you watch the video to read the quiz, so if you want to read the Chapter Two quiz, you know, and then come back, that's not a bad idea. Then make sure that you can do the quiz after you've watched the video, and you've read chapter two of the book."
fast.ai 2022 - Part 1,2,126,aiquizzes.com for quizzes on the book,"Something I didn't mention last week is there's also a very cool thing that Radek — who I mentioned last week — has written, called aiquizzes.com which is a site specifically for quizzes about the book. And it actually uses repetitive based learning techniques to make sure that you never forget. So, do check out aiquizzes.com. It's all brand new questions, they're different to the ones in the book, and they're really nicely curated, and put together. So check out aiqquizzes.com as well."
fast.ai 2022 - Part 1,2,156,"Reminder to use fastai forums for links, notebooks, questions, etc.","Remember, as well as course.fast.ai, there's also forums.fast.ai. So course.fast.ai is where you want to go to get, you know, links to all the notebooks, and kaggle stuff, and all that stuff. You'll also find on forums.fast.ai every lesson has an official topic, you know, with all the information you'll need. Generally there'll be a bit more info on the forums, we try to keep the course lean and mean, and the forums are a bit more detailed. So if you find — in this case you are gonna look for the Lesson Two official topic but here's the Lesson One official topic so far. So from the Lesson One official topic, already, after just a few days since I recorded it — we haven't even launched the course so it's just the people doing it live — there's already a lot of replies, and that can get pretty overwhelming. So be aware that there's a button at the bottom of my post that says summarize this topic."
fast.ai 2022 - Part 1,2,222,How to efficiently read the forum with summarizations,"And if you hit that, then you'll just see the most upvoted replies, and that's a really good way to just make sure that you hit on the main stuff. So there's the button, and here's what it looks like after you hit it. You'll just get… you'll just get the upvoted stuff from fast.ai legends like Sanyam and Tanishq. So hopefully you'll find that a useful way to use the forum."
fast.ai 2022 - Part 1,2,253,Showing what students have made since last week,"So one of the cool things about this week is I… as promised, put up the “show us what you've made” post, and already a lot of people have posted. I took this screenshot a few days ago, it's way above 39 replies already, if I'm… if I remember correctly. I had a lot of trouble deciding which ones to share because they're all so good, so I've actually decided to kind of, you know, go the easy route, and I just picked the firsts. So I'm just going to show you the first ones that were posted because they're all so good. So the first, the very very first one to be posted is a damaged car classifier… so that worked out pretty well — it looks like. And I really liked what Matt, the creator, said about this, is that… you know, wow, it's a bit uncomfortable to run this code, I don't really understand yet but I'm just doing it. And so I'm like, yeah, good on you Matt for just… for just doing it. That's the way to get started, it's all going to make sense, don't worry. Very nice to see that the next one posted was actually a blog post in fast pages — very nice to see — so just describing some stuff, some experiments that they ran over the week and what did they find. Next one was the amazing beard detector which if I understand correctly was mainly because it's very easy to get from bird to beard by just changing one letter to two, and this is doing a very good job of finding gentlemen with beards. Very nice. And then this one is… another level again, it's a whole in-production web app to classify food, which is kind of like extra credit . Apparently we're up to 80 replies now in that thread, thank you Sanyam, very cool. So, you know, obviously… so this was actually created by Suvash who's been doing the courses for a few years now, I believe. And so you know, one day you too might be able to create your very own web app and put it in production. And when I say one day, more specifically… today! I'm actually going to show you how to do this right now! So it's actually quite a lucky coincidence that Suvash put this up there because it's exactly the topic that we're going to pick today."
fast.ai 2022 - Part 1,2,405,Putting models into production,"So, how do we go about putting a model in production? Step one, is… well… you've kinda done step one, right? Step one is… step one, two, three, four… is to figure out what problem you want to solve, figure out how to find the data for it, go gather some data, and so forth. So what's the kind of first step after you've got your data? The next step is data cleaning, and if you go to Chapter 2 of the book, which I'm going to go ahead and open up now… So here is the book… so you can open it in Colab directly from the course, or if you've cloned it to your computer, or whatever, you can… you can do it there. So remember, course.fast.ai will run you through exactly how to run these notebooks, and so you can see Chapter 2 is all about putting stuff in production, and so here is Chapter 2. All right, and so remember we hit Shift + Enter to run cells, okay? …to execute them. And so we're going to go to the part of the book where we start cleaning the data. So I'll click on Navigate, and we'll go down here… gathering data… there we are! So we could do a quick bit of revision first. By the way, I will mention, a lot of people ask me"
fast.ai 2022 - Part 1,2,490,Jupyter Notebook extensions,"what are the little tricks I use for getting around a Jupyter notebook so quickly and easily. One of the really nice ones as you'll see is this navigate menu, which actually doesn't appear by default. So if you install something called Jupyter Notebook Extensions - Jupyter… Notebook… Extensions… and so you just… you just “pip install” them, follow the instructions and then restart Jupyter. Obviously this… Colab already has a table of contents by the way so this is just if you're using something local, for example, then you'll see here that this “Nbextensions” thing will appear, and if you click on “Table of Contents (2)” that gives you this handy navigation bar. The other thing I really like is this one here called collapsible headings, and that's the one which gives me these nice little things here, to close, and open up. And actually that's not even the best part. The best part for me is if I hit the right arrow it goes to the end of a section, and if I hit the left arrow it goes to the start of a section. So it's like, if I want to move around sections I just press up left, down right, down right, very handy, and if you hit left again when you're here it 'll close it up. Hit right again here, open it up. So that's collapsible headings. Anyway, a couple of really handy things, and we'll be talking a lot more about getting your notebook set up today in a moment. Okay, so one thing you'll notice is in the book, we use the Bing API for searching images. I've"
fast.ai 2022 - Part 1,2,589,Gathering images with the Bing/DuckDuckGo,"just gone ahead, and replaced “bing” with “ddg” because the Bing API requires getting an SDK key which honestly, it’s like the hardest thing in deep learning is figuring out the Bing Azure website, and getting that sorted out. ddg doesn't! So it's basically exactly the same… (and you can…) I'll share this notebook as well, on the course website in the forum but all I've basically done is I've replaced bing with ddg, and got rid of the key. So then, just like we did last week, we can search for things, and so in the book we did a bear detector because at the time I wrote it my then toddler was very interested in me helping identify teddy bears, and I certainly didn't want her accidentally cuddling a grizzly bear, so we show how here how we can search for grizzly bears, just like last week. Something that loops through grizzly bears, black bears, and teddy bears, just like last week. Get rid of the ones that failed, just like last week. And one thing a few people have asked on the forum is how do i find out more information"
fast.ai 2022 - Part 1,2,670,How to find information & source code on Python/fastai functions,"about basically any python or fastai or pytorch thing. There's a few tips here in the book. One is that if you put a double question mark next to any function name you'll actually get the whole source code for it, and by the same token if you put a single question mark you'll get a brief… you know… a little bit of information. If you've got nbdev installed (I think it's nbdev you need) then you can type doc, and that will give you perhaps most importantly a link straight to the documentation where you can find out more information and generally there will be examples as well. And also a link here to the source code, if you want to… (let's do that with a control okay) …a link to the source code and that way you can jump around. And notice that in github in the source code you can click on things and jump to their definition. So it's kind of a nice way of skipping around to understand exactly what's going on. Okay, so lots of great ways of getting help. But what I promised you is that we're going to now clean the data. So I'm going to tell you"
fast.ai 2022 - Part 1,2,765,Cleaning the data that we gathered by training a model,"something that you might find really surprising. Before you clean the data, you train a model. Now, I know that's going to sound really backwards to what you've probably heard a thousand times, which is that first you… ~(build/you/train) …you clean your data and then you train your model. But I'm going to show you something really amazing. First we're going to train a model and you'll see why in a moment. So to train a model, just like before, we use a DataBlock to grab our data loaders — there's lots of information here in the book about what's going on here. There we go… and so then we can call show_batch to see them as per usual. There's a little side bar here in the book, I'll quickly mention, which is about the different ways we can resize. I think we briefly mentioned it last week… we can squish…"
fast.ai 2022 - Part 1,2,817,Explaining various resizing methods,"last week I used a string, you can use a string or this kind of enum like thing that we have. You can see with a squish you can end up with some very thin bears, right? So this is the real size, the shape, of the bear. Here its become thin but you can see now we've got all of its cubs… (are they called clubs? yeah bear cubs) …so it squished it to make sure we can see the whole picture. Same one here. This one was out of the picture, we squished it and this guy now looks weirdly thin, but we can see the whole thing. So that's squishing! Or else this one here is cropping. It's cropped out just the center of the image so we get a better aspect ratio but we lose some stuff. This is so we can get square images. And the other approach is we can use “pad.” And so you can pad with various different things. If you pad with zeros, which is black, you can see here now we've got the whole image and the correct aspect ratio. So that's another way we can do it and you know, different situations, you know, result in different quality models. You can try them, all it doesn't normally make too big a difference. So I wouldn't worry about it too much. I'll tell you one though that is very interesting is RandomResizedCrop. So instead of saying resize, we can say RandomResizedCrop. And if we do that"
fast.ai 2022 - Part 1,2,890,RandomResizedCrop explanation,"you'll see we get a different bit of an image every time. So, during the week… this week somebody asked on the forum… I'm trying to… (this is a really interesting idea, which it turned out worked slightly) they wanted to recognize pictures of French and German texts. So obviously this is not the normal way you would do that, but just for a bit of experiment (and I love experiments, so) they had very big scans of documents and they wanted to figure out whether it was french or german just by looking at images and they said the pictures were too big what should I do? I said, use RandomResizedCrop and that way you'd grab different bits of the image, and this is very nice because you could run lots and lots of epochs and get slightly different pictures each time. So this is a very good technique. And"
fast.ai 2022 - Part 1,2,950,Data augmentation,"this idea of getting different pictures each time from the same image is called “data augmentation.” And again, I'm not going to go into too much detail about data augmentation, because it's in the book, but i'll just quickly, you know, point out here that if you use this thing called “aug_transforms” so augmentation transforms, and here I have multiplied them by two – so I've made them super big so you can see them more easily. You can see that these teddies are getting turned, and squished, and warped, and recolored, and saturated, all this stuff to make every picture different. And generally speaking if you're training for more than about five or ten epochs, which you'll probably want to do most of the time unless you've got a super easy problem to solve, you'll probably want to use RandomResizedCrop and these “aug_transforms”. Don't put the mult=2, just leave that empty. I'm just putting it there so you can see them more clearly. So I've got an interesting question here from Alex in our audience, which is: “Is this…"
fast.ai 2022 - Part 1,2,1017,Question: Does fastai's data augmentation copy the image multiple times?,"is this copying the image multiple times? …doing something like this? …or something like this?” and the answer is “no we're not copying the image. What happens is that image… so each epoch, every image gets read. And what happens here is though… is… is kind of… in memory, in RAM, this… the image is being “warped”, right? it's being… we're cropping it and recoloring it, and so forth. So it's a real time process that's happening during model training. So, there's no copies being stored on your computer, but effectively it's almost like there's infinitely slightly different copies because that's what the model ends up seeing. So, I hope that makes sense Alex, and everybody else, so that's a great question! Okay, so we've got… we're going to use RandomResizeCrop, we're going to use augmentation transforms, so that we can get at DataLoaders from that and then we can go ahead and train our model. It takes about a minute. In this case we only did four epochs of fine tuning, and we'll talk about why there's five here later in the course, but four main epochs of fine tuning. So we probably didn't really need RandomResizedCrop and aug_transforms because there's so few epochs, but you know, if you want to run more epochs this is a good approach. Under three percent error! that's good! Okay, so remember I said we're going to train a model before we clean?"
fast.ai 2022 - Part 1,2,1110,Training a model so you can clean your data,"Okay so let's go ahead and train it. So while that's training… so it's running on my laptop, which only has a 4GB GPU, it's pretty basic, but it's enough to get started. While that's training we'll take a look at the next one. So the first thing we're going to look at is the confusion matrix and the confusion matrix is something that… it only is meaningful for when your labels are categories, right!"
fast.ai 2022 - Part 1,2,1140,Confusion matrix explanation,"And what it says is how/what category errors are you making. And so this is showing that the model that we've got at this point… there were two times when there was actually a grizzly bear and it thought it was a black bear, and there was two times when there was actually a black bear, and it thought it was a grizzly bear, and there was no times that it got teddies wrong, which makes sense right because teddies do look quite different to both. In a lot of situations when you look at this, it'll kind of give you a real sense of, like okay, well what are the hard ones? right? So for example if you use the pets data set that we quite often play with in the book and the course, this classification [**] matrix for different breeds of pets, you know, really shows you which ones are difficult to identify, and I've actually gone in, and like, read wikipedia pages, and pet breeding reports about how to identify these particular types, because they're so difficult, and even experts find it difficult. And one of the things I've learned from doing the course actually is… black bears and grizzly bears are much harder to pick apart than I had realized... so I'm not even going to try! But I'll show you, the really interesting thing we can do with this model, is that now we've created this classification interpretation object, which we use for confusion matrix, we can say plot_top_losses."
fast.ai 2022 - Part 1,2,1233,plot_top_losses explanation,"We can say plot top losses, and this is very interesting. What it does is, it tells us the places where the “loss” is the highest. Now if you remember from the last lesson, the loss is that measurement – of how good our model is – that we take after each time we run through an item of data. A loss will be “bad” if we predict wrongly, and we're very confident about that prediction. So here's an example where we predicted… (here's the order here: prediction, actual loss, probability) …where we predicted grizzly and it was actually a black, and we were 96% sure (our model was) that it's a grizzly. Now, I don't know enough about bears to know whether the model made a mistake or whether this actually is a picture of a grizzly bear… but so, an expert would obviously go back and check those out… right? Now, you'll notice a couple here... it's got grizzly/grizzly, teddy/teddy… they're actually correct, right? So why is this loss “bad”? When it's correct? And the reason is… because it wasn't very confident. It was only 66% confident, right? So here's a teddy… it's only 72% confident right? So you can have a bad loss, either by being wrong and confident, or being right and unconfident. Now the reason that's really helpful, is that now we can use something called the"
fast.ai 2022 - Part 1,2,1330,ImageClassifierCleaner demonstration,"fast.ai ImageClassifierCleaner to clean up the ones that are wrongly labeled in our data set. So when we use the ImageClassifierCleaner it actually runs our models. That's why we pass it “learn”, right? And I mentioned that I don't know much about black bears and grizzly bears but I do know a lot about teddy bears, so I'll pick teddy bears. And if I click teddy bears it's now showing me all the things in the training set (you can pick training or valid) that were marked as teddy bears and here's what's really important: they're ordered by loss, so they're ordered by confidence, right? So I can scroll through just the first few and check they're correct, right? And oh, here's a mistake, right? So when I find one that was wrongly gathered I can either, put it…if it's in the wrong category, I can choose the correct category… or, if it shouldn't be there at all, I click delete. So here I'll go ahead and click delete, right? So you can see some reasons that some of these are hard, like for example here's two teddies, which is just, I guess, confusing since it doesn't see that often. This one here is a bit weird looking, it looks almost like a wombat. This is an awful lot of teddies. This one maybe is just a bit hard to see from the background, but these, otherwise, they look fine… Fine, so we just look through the first few and if you don't see any problem or problems in the first year, you're probably fine. So that's cleaned up our training set, let's clean up our validation set as well. So here's that one it had trouble with, I don't know why it had trouble with that one but so be it and we'll have a quick scroll through… Okay I'm not really sure that's a bear so I'm just going to go ahead and delete it. It's a teddy something… but, you know, it's a problem. Okay, that's not a teddy either… So you see the idea, right? So after we've done that, what that does is… the cleaner has now stored a list of the ones that we changed and the list of the ones we deleted, so we can now go ahead and run this cell. And so that's going to go through a list of all of the indexes that we said to delete, and it will delete those files. And we'll go through all the ones we set to change, and it will move them to the new folder, there we go, done! So, this is like, not just something for image models. It's just… it's actually… a really powerful technique that almost nobody knows about and uses, which is, before you start data cleaning, always build a model to find out what things are difficult to recognize in your data and to find the things that the model can help you find data problems. And then as you see them, you'll kind of say “okay, I see the kinds of problems we're having” and you might find better ways to gather the next data set, or you might find ways to kind of automate some of the cleaning and so forth. Okay, so, that is data cleaning and since I only have a 4GB GPU it's very important for me"
fast.ai 2022 - Part 1,2,1528,CPU RAM vs GPU RAM (VRAM),"to close and halt, because that will free up the memory. So it's important to know… on your computer your normal RAM doesn't really get filled up, because if you use up too much RAM, what will happen is that instead your computer will start (it's called) swapping, which is basically to save that RAM onto the hard disk to use it later. GPUs can't swap. GPUs, when they run out of RAM, that's it, you're done. So you need to make sure that you close any notebooks that are using the GPU that you're not using and really only use one thing at a time on the GPU, otherwise you're almost certainly run out of memory. So we've got the first few reds starting to appear. So remember to ask and, in terms of the yellows, it's important to know, as you watch the video, I'm not asking you to run all this code, okay? The idea is to kind of watch it, and then go back and pause, you know, as you go along. Or you can just stop, try, stop, try. The approach I really like — and a lot of students really like — for watching these videos is to actually watch the entire thing without touching the keyboard, to get a sense of what the video is about, and then go back to the start and watch it again and follow along. That way, at every point you know what it is you're doing, you know what's going to happen next. That can actually save you some time. It's a bit of an unusual way because, obviously like, real life lectures you can't do that, you can't rewind the professor and get them to say it again, but it's a good way to do it here."
fast.ai 2022 - Part 1,2,1638,Putting your model into production,"So now that we've cleaned our data, how are we going to put it into production? Well in the book we use something called Voilà. And it's… it's pretty good! But there's actually something that I think most of you are probably going to find a lot more useful nowadays, which is something called HuggingFace Spaces, and there's a couple of things you can use with that. We're going to look at something called Gradio today. And there isn't a chapter about this in the book but that doesn't matter because Tanishq Abraham — who's actually one of the TAs in the course — has written a fantastic blog post about really everything we're going to cover today. So there's a link to that from the forum and from the course page, so this is like the equivalent of the chapter of the book, if you like. And I would be remiss if I didn't stop for a moment and call out Tanishq in a big way for two reasons: the first is he is one of the most helpful people in the fastai community, he's been around quite a few years, incredibly tenacious, thoughtful and patient; and also because I have this fantastic picture of him a few years ago with Conan when he was a famous child prodigy, so now you know what happens to famous child prodigies when they grow up – they became even more famous fastai community members and deep learning experts. So you should definitely check out this video of him telling jokes to Conan. I think he's still only 18 actually, this is probably not that many years ago. So thank you very much Tanishq for all your help in the community and sorry for embarrassing you with that picture of you as a nine-year-old — I'm not really, haha. Okay, now, the thing is for doing Gradio and HuggingFace Spaces, well, it's easy enough to start. Okay, we start over here on the HuggingFace Spaces page — which we've linked to from the forum in the course. And we're going to put a model in production, where we're going to take the model we trained, and we are going to basically copy it to this HuggingFace Spaces server and write a user interface for it. So… let's go! Create new space. Okay, so, you can just go ahead and say, all right… so obviously you sign up… the whole thing's free. Basically everything I'm showing you in this entire course, you can do for free, that's the good news. Okay, so, give it a name, just create something minimal. I always use the Apache license because it means other people can use your work really easily but you don't have to worry too much about patents. As I say there's a few different products you can use with it, we're going to use Gradio, also free. If you make it public then you can share it, which is always a good idea when you're a student, particularly to… to really be building up that portfolio. Okay, so we're done. We've created a space. Now, what do we do next? Well, Spaces works through Git, now, most software developers"
fast.ai 2022 - Part 1,2,1820,Git & Github desktop,"will be very familiar with Git. Some data scientists might not be. And so Git is a very, very useful tool. I'm not going to talk about it in detail, but let's kind of quickly learn about how to use it, right? Now, Git, you can use it through something called Github Desktop which is actually pretty great and even people who use Git through the console should probably be considering using Github Desktop as well, because some things are just much faster and easier in it, in fact I was talking to my friend Hamil today and I was like, oh, help, I've accidentally committed this two things by mistake, what's the easiest way to revert it, and he used to work at Github and I thought he was going to have some fancy console command and he's like “oh, you should use Github Desktop and you can just click on it.” Oh! That's a great idea. So that's… that's useful. But most of the time we do use Git from the console, from the terminal. If you're a Linux user or a Mac user you've already got a terminal, very straightforward, no worries."
fast.ai 2022 - Part 1,2,1890,For Windows users,"If you're a Windows user I've got good news, nowadays Windows has a terrific terminal. It's called Windows Terminal, you get it from the Microsoft Store, so in fact every time you see me using a terminal, I'm actually using that Windows Terminal, works very well. God knows why you'd want it to, with all these ridiculous colors but, there you go. Now, what do you want to be running inside your terminal? Obviously, if you're in Linux or Mac you've already got a shell set up. In Windows, you almost certainly want to use Ubuntu. So Windows, believe it or not, can actually run a full Linux environment and to do it, is right typing a single line, which is this… So, if you go to (just google for WSL install) run PowerShell as administrator, paste that command, wait about five minutes reboot, you're done. You now have a complete Linux environment. Now, one of the reasons I'm mentioning this is, I'm going to show you how to do stuff on your own machine now, and, so, this is like going to a bit of an extra level of geekery, which some data scientists may be less familiar with. So you know, don't be worried about the terminal, you're gonna.. I think you're gonna… find it really helpful and much less scary than you expect. And, I particularly say, like for me, I choose to use Windows and that's because I get, you know, all the nice Windows GUI apps and I can draw on my screen, and do presentations, and I have a complete Linux environment as well. And that Linux environment uses my GPU and everything, so for me, you know, my first choice is to use Windows, my second choice, by not very much, really like it, would be to use Linux. Mac is a little bit harder, but it's still usable. Some things are a little bit trickier on Mac but, you should be fine. Okay, so, whatever you've got, at this point you've now got a terminal available. And so, in your terminal… (one of the really nice things about using a terminal is you don't have to follow lots of instructions about click here, click here, click here. You just copy and paste things.) So I'm just going to… you just copy this, and you go over to your terminal, and you paste it in, and you run it, and after you do that, you'll find that you've now got a directory. And so that new directory initially is empty and they tell you, okay, go ahead and create a file with this in it. Okay, so, how do you create a file with that in it, when we're in here, in our Linux environment on Windows, or in the terminal on Mac, or whatever? Well, all you do in Windows, if you just type “explorer.exe .”, it'll open up explorer here, or better still, on either Mac, or Linux, or Windows… So, yeah, regardless of what computer (type of computer on), you can just type “code .” and it will pop up Visual Studio Code and open up your folder. And so then you can just go ahead and… (if you haven't used VS Code before it's really well worth taking a few minutes to read some tutorials, it's a really great IDE) …and so you can go ahead and create an app.py file, like they tell you to, app.py file, containing what they told you to put in it. Here it is, here. All right, we're nearly there. So you can now go ahead and save that, and then you need to commit it, to Gradio… (not Gradio…) to HuggingFace Spaces. So one really easy way is just, in Visual Studio itself, you can just click here, and that'll give you a place where you type a message, and you hit tick, and it'll send it off to HuggingFace Spaces for you. So once you've done that, you can then go to… back to… the exact same website you're in before: “huggingface.co/spaces/jhp00/minimal”, and what you'll find now is that — it'll take about a minute, to build your website. And it's… the website it's building is going to have a Gradio interface with a text input, a text output, and it's going to run a function called “greet()” on the input, and my function called “greet()” will return “Hello {name}”. So that's what it's going to do. Now there it goes, let's try it! We'll say hello to Tanishq. I'm not always very good at remembering how to spell his name, I think it's like that. And there you go, so you can see it's put the output, for our input. So not a very exciting app but we now have, to be fair, an app running in production, right?"
fast.ai 2022 - Part 1,2,2220,Deploying your deep learning model,"Now, I told you we'd have a deep learning model running in production so now we have to take the next step which is to turn this into a deep learning model. All right, so, first we're going to need a deep learning model. And there's a few different ways we can get ourselves a deep learning model, but basically we're gonna have to train one. So, I've got a couple of examples: I've got a Kaggle example and a Colab example. Maybe I'll quickly show you both. They're gonna be the same thing. And I'm just going to create a dog or a cat classifier. Okay, so here's our Kaggle model,"
fast.ai 2022 - Part 1,2,2258,Dog/cat classifier on Kaggle,"I'll click on edit so you can actually see what it looks like in edit view. Now, Kaggle already has fastai installed but I always put this first, just to make sure we've got the latest version. And obviously import stuff, we… So we're going to grab the pets data set, a function to check whether it's a cat, that's our labeling function for our ImageDataLoaders — remember this is just another way of doing DataBlocks, it's like a little shorthand. And we create our learner, and we fine tune it. Okay, so, that's all stuff we've seen before. So, in Kaggle every notebook has an edit-view (which is what you just saw) and a reader-view. And so you can share your notebook — if you want to. And then anybody can read the reader-view, as you see. And so, here, you can see, it shows you what happened when I ran it. And so, I trained it, it took… you know, the GPUs on Kaggle are a bit slower than most modern GPUs but they're still fast enough, I mean, it takes five minutes. And there's one bit at the end here,"
fast.ai 2022 - Part 1,2,2335,Exporting your model with learn.export,"which you haven't seen before, which is I got learn.export and I give it a name. Now, that's going to create a file containing our trained model, and… that's the only thing! Creating this file is the only thing you need a GPU for, right? So you do that on Kaggle, or on Colab. So here's exactly the same thing on Colab… you can see “pip install”, is_cat, our data, ImageDataLoaders, so I've got a show_batch here as well, just for fun. Create my learner and then export. So, while we wait I might go ahead and just run that. One nice thing about Kaggle is once you've run it and saved it, you can then go to the data tab"
fast.ai 2022 - Part 1,2,2380,Downloading your model on Kaggle,"and, here is basically anything you've saved, it's going to appear here, and here it is: model.pickle, right? So now I can go ahead and download that and that will then be downloaded to my downloads folder. And then I need to copy it into the same directory that my HuggingFace Spaces App is in. Now my HuggingFace Spaces App is currently open in my terminal, on Mac you can type “open .” or in Windows you can type “explorer.exe .” and that'll bring up your finder or explorer in that directory. And so then you can just paste that, that thing you downloaded into this directory. Something by the way, in Windows I do, which I find really helpful is, I actually grab my… my home directory in Linux and I pin it to my Quick Access and that way I can always jump in Windows straight to my Linux files. Not really something you have to worry about on Mac because it's all kind of integrated, but on Windows they're like, kind of like, two separate machines. Okay, so… Let's do… so, I created a Space called testing and I downloaded my “model.pkl” and I pasted it into testing. So now we need to know how do we do predictions on a saved model? So we've got a notebook for that… Okay, so we've got a notebook for that"
fast.ai 2022 - Part 1,2,2490,How to take a model you trained to make predictions,"and… so I'm going to take you through how we use a model that we've trained to make predictions. There's a few funny things with hash pipe which I'll explain in a moment, just ignore those for now. So we import fastai as usual. We import Gradio as we did before. And we copy in the exact same is_cat definition we had before. That's important, any external functions that you used in your labeling, need to be included here as well because that learner refers to those functions, okay? It saves... that learner saves everything about your model, but it doesn't have the source code of the function, so you need to keep those with you. So, let's try running this. So, for example, I just grabbed… (as you might have seen in my explorer) I just popped a dog picture there. And so we can create a python image library image from that dog, turn it into a slightly smaller one so it doesn't overwhelm my whole screen and there is a picture of a dog. So, how do we make predictions of whether that's a dog or a cat? Well, it's very simple, all we do is… instead of training a learner, we use load_learner(). We pass in the file name that we saved, and that returns a learner. This learner is exactly the same as the learner you get when you finish training. So here we are, here's Colab, right? We've just been training a learner, so at the end of that, there's a learner that's been trained, and so we… We kind of froze it in time, something called a pickle file — which is a python concept. It's like a frozen object, we saved it to disk, we transferred it to our computer, and we've now loaded it, and we've now unfreezed thought it. This is our unpickled learner and we can now do whatever we like with that. So one of the things that…"
fast.ai 2022 - Part 1,2,2610,learn.predict and timing,"One of the methods that a learner has is a dot predict method, so, if I run it, you can see, even on my laptop, it's basically ~(instance) instant. In fact we can see how long it took. If you — in Jupyter things that start with percent are called magics, they're special Jupyter things, so for example, there's a thing to see how long something takes. There you go, okay, so it took 54 milliseconds to figure out that this is not a cat. So it's returning two things: is it a cat? (as a string); is it a cat? (as a zero or one), and then the probability that it's a dog, and the probability that it's a cat. So the probability of zero (false) and one (true) of is it a cat? so definitely a dog. So"
fast.ai 2022 - Part 1,2,2662,Shaping the data to deploy to Gradio,"we now want to create a Gradio interface which basically has this information. So Gradio requires us to give it a function that it's going to call, so here's our function. So we're going to call predict, and that returns as we said three things: the prediction is a string; the index of that; and the probabilities of whether it's a dog or a cat. And what Gradio wants, is it wants to get back a dictionary containing each of the possible categories — which in this case is dog or cat — and the probability of each one. So, if you haven't done much python before, a dict of a zip may be something you haven't seen. Very handy little idiom well worth checking out. Ditto if you haven't seen map before, anyway, here it is. One slightly annoying thing about Gradio at the moment is that it doesn't handle Pytorch Tensors, you can see here Pytorch is not returning normal numbers, it's returning Tensors. It's not even returning Numpy Arrays. In fact radio can't handle Numpy either, so we have to change everything just to a normal float, so that's all that this is doing, it's changing each one to a float. So, for example, if I now call classify_image() with our doggy image, we get back a dictionary of a dog, yes definitely; cat, definitely not. So now we've got all that we can go ahead and create a Gradio interface."
fast.ai 2022 - Part 1,2,2747,Creating a Gradio interface,"So Gradio interface is something where we say, well, what function do you call to get the output, what is the input — in this case we say, oh, the input is an image — and so check out the Gradio docs, it can be all kinds of things like a webcam picture, or a text, or you know, all kinds of things. Give it a shape that it's going to put it into, the output's just going to be a label so we're going to create very, very simple interface. And we can also provide some examples, and so there's a dog, a cat, and a dunno — which I'll tell you about in a moment — which you'll see here there's a dog, and a cat, and a don't know. So once I launch it, it says, okay, that's now running on this url. So if I open that up; you can see now, we have just like Suvash, we have our own — not yet in production, but running on our own box — classifier. So let's check: dog, so you can click it and upload one, or just choose the examples… Here they are! So it's running on my own laptop, basically instant. And I really have to tell you the story about this guy here, this is the dunno. Submit, wait, why is it saying 100? Normally this says like 50 50. That's a bummer, this model's got… messed up my whole story. So last time I trained this model, and I ran it on the… dunno, it said, it said like it's almost exactly 50 50. And the way we found this picture is… I showed my six-year-old daughter, she's like: “what are you doing dad?”, like, I'm coding, “what are you coding?” Oh, you know, dog cat classifier. She checks it out and her first question is: “can I take your keyboard for a moment”, and she goes to google, and she's like: what is a dog mixed with a cat called. Like, there's no such thing as a dog mixed with a cat. Anyway, she goes to the images tab, and finds this picture, and she's like: “look there's a dog mixed with a cat”. She said: “run it on that dad!, run it on that!” And I ran it and it was like: 50-50. It had no idea if it's a dog or a cat. Now, this model I just retrained today, now it's sure it's a cat. So there you go, I think I used a slightly different training schedule or something, or gave it an extra epoch, anyway, so that's a dog-cat, but apparently it's a cat. I guess it is a cat. It's probably right. I shouldn't have trained it for as long. Okay, so there's our interface. Now, that's actually running, so you actually have to click the start button to stop it running so otherwise you won't be able to do anything else in your notebook. Okay,"
fast.ai 2022 - Part 1,2,2905,Creating a Python script from your notebook with #|export,"so, now we have to turn that into a python script. So, one way to turn it into a python script would be to copy and paste into a python script all the things that you need. I need to copy and paste into a python script all these parts of this that you need, so, for example, we wouldn't need this — it's just to check something out, we wouldn't need this — it was just experimenting. This was just experimenting. We'd need this, right? So, what I did… is I went through and I wrote “hash pipe export” at the top of each cell that contains information that I'm going to need in my final script, and then — so they're the steps, right? And then, at the very bottom here, I've imported something called notebook2script from nbdev, and if I run that, and pass in the name of this notebook, that creates a file for me called app.py containing that script. So this is a nice easy way to — like when you're working with stuff that's expecting a script and not a notebook, like HuggingFace Spaces does. It's fine to just copy and paste into a text file if you like, but I really like this way of doing it because that way I can do all of my experimentation in a notebook, and when I'm done, I just have a cell at the bottom I just run and export it. How does it know to call it app.py? That's because there's a special thing at the top default export: default_exp, which says what python file name to create. So that's just a little trick that I use. So now we've got an app.py. We need to upload this to Gradio, how do we do that? You just… you just push it to Git. So… you can either do it with Visual Studio Code, or you can type “git commit”. And then “git push”. And once you've done that…if we change minimal to testing… I think this hopefully might still be running my previous model — because I didn't push it, and that way, we can see our crazy dog-cat."
fast.ai 2022 - Part 1,2,3047,Hugging Face deployed model,"All right, so here it is, you can see it running in production. So now this is something that anybody can — if you set it to public — anybody can go here and check out your model, and so they can upload it. And so, here's my doggy. Yep, definitely a dog. Cat, yeah I think I might have trained this for an epoch or two less, so it's less confident. Yeah definitely a cat. Dog-cat… hey dog-cat… hmm, still thinks it's definitely a cat. Oh well, so weird. Okay, so, that is…Okay, so, that is an example of getting a simple model in production. There's a couple of questions from the forum, from the community. Okay so one person's asking: “what's the difference between a pytorch model and a fastai learner?” Okay, that's fine, we'll get to that shortly. Don't know if it'd be the lesson, it might be this lesson or the next lesson. And then somebody else asked, basically it's asking: “how many epochs do we train for?” So, as you train a model your error rate, as you can see, it improves."
fast.ai 2022 - Part 1,2,3132,How many epochs do you train for?,"And so the question is: should I run more? Should I increase the number of epochs? This is doing three epochs, right? Here's my three epochs plus one to get started. Look, it's up to you, right? I mean, for this, is here saying: there's a one percent error. I'm… I'm okay with the one percent error, you know, if you want it to be better, then you could use more data augmentation, and you could train it for longer. If you train for long enough, as we'll learn about soon, and then… maybe the next lesson. If you train for long enough, your error rate actually starts getting worse. And, you'll see, we'll learn about why. So basically yeah, you can train until it's good enough or until you've run out of patience, or time, or run out of compute, or until you… or until the error rate starts getting worse. Okay, oh, and then in Colab, how do you grab your model?"
fast.ai 2022 - Part 1,2,3196,How to export and download your model in Google Colab,"All you need to do in Colab is, after you've exported it, is, if you go into their file browser you'll actually see it here, right? And you can click download. It's a bit weird, it doesn't like, pop up a box saying where do you want to download it to, but instead this kind of progress circle thing pops up, and so depending on how big it is, and so forth, it can take a few minutes. And once that circle fills up then it'll, the browser thing will finally pop up and say, okay, you can save it. Okay so that's how you… that's how you actually grab your model. So as you can see, the step where you actually need a GPU, you can use these totally free resources: Colab, Kaggle, and there are other ones we'll talk about in future lessons. And then you can do everything else on your own computer, including the predictions, the predictions are fast, right? So, you really don't need to use a GPU for that, unless you're doing thousands of them. Okay, here it goes, now it's asking me to save it."
fast.ai 2022 - Part 1,2,3265,"Getting Python, Jupyter notebooks, and fastai running on your local machine","Okay, so now one big issue is, we needed… to run it on our computer… we needed python and Jupyter Notebooks running on our computer, so how do you do that? Because this is where often people get in all kinds of trouble… “I'm trying to figure out how to get this all working.” So, the good news is we've actually got something that makes it very, very straightforward. It's called fastsetup. There's really only just one part of it you need, so let me show you…. it's… it's… actually a Git repository on Github. Github's the place where most Git repositories live. So if you go to Github fastai/fastsetup you'll see it. And so what you can do is you can now grab this whole repository, just by clicking here on code, and if you've got Github Desktop installed, click on open with Github Desktop and, as you'll see, it brings this up saying: okay I'm ready to save this for you, so I click clone, so it's making a copy of it. There we go… So, basically, once you've cloned it, you'll then find there's a file in there called setup-conda.sh… which, you know, the details don't really matter, it's pretty short, but, that's the thing that's going to install python for you. So at that point you can just run “./setup-conda.sh” and it'll run this installer. Now, if you've got Linux or Mac you've already got python on your machine. Don't use that python!! And the reason is because that python, it's called the system python, it's used by your computer to do computery stuff, right? It's actually… it's actually needed. You don't want to be messing with it, I promise you, like… it's, it always leads to disaster, always! You want your own development version of python, it's also going to make sure you've got the latest version, and all the libraries you want, and, by far, the best one for you is almost certainly going to be these conda based python distributions, so if you run setup conda you'll get the one that we recommend. And, the one we recommend at the moment is something called mambaforge. So basically once you run it, you'll find that you've now — you close your terminal and reopen it — you'll find you've now got one extra command, which is called mamba, and mamba lets you install stuff. So once you've run it, you'll be able to go “mamba install fastai”. And that's gonna… actually we should probably, I should mention this, actually more, a bit more detail about how to install it correctly. If we go to docs.fast.ai, installing. Yeah, okay, we actually want to do “conda install -c fastchan fastai”. So let's just copy and paste. Oh sorry, not actually… and then, the other thing I'll say is, instead of using conda, replace conda with mamba because nowadays it's much faster. So “mamba install -c fastchan fastai”. Now, this is going to install everything you need. It's going to install Pytorch, it's going to install Numpy, it's going to install fastai… and so forth, right? And… so obviously I've already got it. And then the other thing you'll want to do is install nbdev, so you can do exactly the same thing for nbdev. You don't have to, right? It's just that… but that'll install Jupyter for you, amongst other things. And so at that point you can now, you can now use Jupyter. And so the way Jupyter works is — you can see it over here. This is my… (I'll go ahead and close it so we can start again.) So basically, to use Jupyter you just type “jupyter notebook”, okay? And when you run it, it'll say: okay, we're now running a server for you, and so if you click on that hyperlink, it'll pop up this, okay?, which is exactly what you see me use all the time. Okay so, you know, that hopefully is enough to kind of get you started with python, and with Jupyter Notebook… The other way people tend to install software is using something called pip instead of mamba. Pretty much anything you can do with mamba you can also do with pip, but if you've got a GPU, pip isn't going to install things generally so that it works on your GPU, you have to install lots of other stuff, which is annoying, so, that's why I kind of tell people to use mamba, but you can use pip otherwise. Still a little bit of red. Please let us know how we can help you again. Okay so let's see how we’re going with our steps. I forgot I had these steps here to remind myself. We created a space, tick. We created a basic interface, tick. Okay we got git setup, we got conda set up, or mamba. So mamba and conda are the same thing, mamba is just a much faster version and we'll keep some notes on the course website because at the moment they're actually working on including the speed ups from mamba into conda. So at some point maybe it'll be fine to use conda again. At the moment conda is way too slow, so don't use it. Okay we've done dogs versus cats no problem. Yeah so we could also look at pet breeds (well yeah we'll briefly look at that.) Okay, we've used an exported learner, no problem. We’ve used nbdev, no problem. Oh! okay… try the api… alright this is interesting."
fast.ai 2022 - Part 1,2,3650,"Comparing deployment platforms: Hugging Face, Gradio, Streamlit","So I think we can all agree, hopefully, that this is pretty cool that we can provide to anybody who wants to use it, for free, a real working model, and you know, with gradio there's actually, you know, a reasonable amount of flexibility, around like how you can make your website look, you know, using these various different widgets. It's not amazingly flexible, but it's flexible enough to kind of… it's really just for prototyping, right. So gradio has a lots of widgets and things that you can use. The other main platform at the moment that HuggingFace Spaces supports is called streamlit. streamlit is more flexible, I would say, than gradio - not quite as easy to get started with but you know, it's kind of that nice in between, I guess, so also a very good thing you know again mainly for, kind of, building prototypes. But at some point you're going to want to build more than a prototype. You want to build an app, right, and one of the things I really like about gradio in HuggingFace Spaces is… there's a button down here"
fast.ai 2022 - Part 1,2,3733,Hugging Face API,"“view the api”. So we can actually create any app we want and the key point is that the thing that does the actual model predictions for us is going to be handled by HuggingFace Spaces Gradio, right, and then we can write a Javascript application that then talks to that. Now there's going to be two reactions here … Anybody who's done some front-end engineering is going to be like, “oh great, I can now literally create anything in the world, because I just write any code and I can do it and they'll be excited.” And a lot of data scientists might be going “uh oh… I have no idea how to use javascript, it's not in my, you know, inventory.” So this is again where I'm going to say: look, don't be too afraid of Javascript. I mean obviously one option here is just to kind of say, “hey I've got a model” and throw it over to the wall to your mate who does know Javascript and say please create a Javascript interface for me. But let me just give you a sense of how really not hard this actually is. So there's a… end point. There's now a url that's running with our model on it and if you pass it some data… an image… some image data… to this… to this url, it's going to return back the dictionary. So it's going to do exactly the same thing that this UI does but as an api, as a function we can call. And so it's got like, examples here of how to call it… so for example, i can actually… let me show you the api as an example using that minimal interface we had … because it's just going to be a bit simpler. So if I click curl and I copy that… copy that… and paste. So you can see there … (oh that's not a great example passing in hello world) …if I pass in Tanishq again, let's see how I'm going with his name Tanishq. Here you can see it returns back “Hello Tanishq!” So this is how these apis work, right. So we can use javascript to call the api and we've got some examples. So I've created a website, and here is my website “tinypets”"
fast.ai 2022 - Part 1,2,3900,Jeremy's deployed website example tinypets,"and on this website as you can see it's not the most amazingly beautiful thing but it's a website. It's a start right, and up here I've got some examples, here we go, single file, click, choose file, click… and in this example I'm actually doing full pet classification. So I actually trained a model to classify breed, which we'll talk about more next week, rather than just dog versus cats. So let's pick a particular breed… and we run it… Oh! And there it is! Now not very amazing right, but the fact is that this is now a Javascript App means we have no restrictions about what we can do. And let's take a look at that HTML… that's it! It easily fits in a screen, right, and the basic steps are not crazy, right, it's basically… we create an import for our photo, we add an event listener that says when you change the photo call the read function, the read function says create a file reader, read the file and when you finished loading, call loaded, and then loaded says… fetch that… now that path there… is… that path there! Right! Except we're doing the full pets one. So this is basically just copied and pasted from their… from their sample. And then grab the JSON, and then grab from the data the first thing… the confidences… the label, and then set the HTML. So as you can see it's like (okay) if you haven't used javascript before these are all new things, right, but they're not… it's not harder than python, right, it's just… it's just another just another language to learn. And so from here you can start to build up, right! So for example we've created a multi-file version. So with the multi-file version, let me show you… multi-file… choose… so we can now click a few… so we've got a newfie, a ragdoll, a basset hound, and some kind of cat. I'm not much of a cat person. So we chose four files and bang they've all been classified! Apparently it's a Bengal! (I wouldn't know) Here's our newfoundland. So there's the multifile version, and if you look at the code, it's not much more, right, it's now just doing… it's getting all the files and mapping them to read, and now appending each one, so not much more code at all. And as you might have seen on our site here, there's a few more examples, which is some of the community during the week, has created their own versions. So this one here is… I think this is… the yeah… this is"
fast.ai 2022 - Part 1,2,4103,Get to know your pet example by aabdalla,"from one of the gradio guys... they called it “get to know your pet.” So if i choose a pet… I kind of… I really like this because it actually combines two models. So first of all it says, oh! It's a basset hound, and then it lets me type in and ask things about it. So i can see, “oh, what kind of tail does it have?...” search, and so that's now going to call an NLP model, which asks about this… it's a curved saber tail. There we go! “What maintenance does it need?” So again, like here you can kind of see how (oh) a basset hound’s ears must be cleaned inside and out frequently. So this is like combining models.So you can see this is something that you couldn't do with just a kind of a… a ready to go interface. And so the next thing i wanted to point out is how did we create the website? I showed you how to create an HTML file, but like how do you create those, and how do you make a website out of them? Well watch this! Let's… here's…. here's…."
fast.ai 2022 - Part 1,2,4184,Source code explanation,"here's the source code to our most basic version, okay! So i could just save this… there we go… Okay so we could open that with Visual Studio Code… and… what we could actually do is, we could… here it just is in explorer or mac & finder. I could just double click on it and here it is! It's a working app!! So you can see i don't need any software installed on my computer to use a Javascript App, right! It's a single file… I just run it in a browser… a browser is our complete execution environment. It's a got a debugger. It's got the whole thing. So here you can (yeah) here you can see it's… it's just calling out to this external hacking faces endpoint, so I can do it all sitting here on my computer. So once I've got my HTML file that's working fine on my computer, in VSCode, how do I then put it on the web, so that other people can use it? Again, the whole thing's free. There's a really cool thing called github pages, which basically"
fast.ai 2022 - Part 1,2,4268,Github Pages,"will host your website for you. And because it's just Javascript, it'll… it'll all work just fine! The easiest way to create a github pages site in my opinion, is to use something called fastpages, which is a fast.ai thing! And… basically all you do… is you follow the setup process. So first it does it's… let's just go through it… so it says generate a copy by clicking on this link. So I click the link, all right, okay, give it a name… (I try to make everything public! I always think it's good good practice… you don't have to) …create repo, generating, okay, and then there's basically two more steps, takes about five minutes. We don't have five minutes, so I'll show you the one that I've already built, which is “fast.ai/tinypets” And so once it's done you'll basically end up with this empty site which, again, you just go “code” “open with github desktop” or “open with visual studio” whatever… so “open with github desktop”... or you can copy & paste this to your terminal… and so any one of those is going to get you this whole thing on your computer. You can save your HTML files there… push it back up to github. And what you'll find is we… we've… FastPages will show you the link to the website that is created for you. Now, the website that's created for you, you can make it look however you want using something called a “Theme”. So you'll see it's created a file called config.yml where you can pick a theme. So, in this case, I picked a theme called “alembic” for no particular reason. So Github Pages uses something called “Jekyll”, and so any “Jekyll” theme will, will basically work. So I picked out this theme, and so, as a result, when I now save things into this repo, they will automatically appear in this website, and the files automatically appear up here in this list. So, if you look at my index, that's the home page… the entire file is just this, right! The only slightly weird thing is at the top of every Github Pages file, you have to have three dashes, title, and layout, and three dashes. It's called “front meta”. And so, once you do that… and save it, it will appear in your website. So, something else I did then, I was like, okay, well… that's all very well that fast.ai has created this website, but I don't really like what it looks like. I want to create a different version. No worries. You can go to “fast.ai/tinypets” and click “Fork”. And, when you click “Fork”, it's going to create your own copy. So I did that under my personal account which is jph00. And look! Now I've got my own version of it, and now I can make changes here. So I made a few changes. One change I made was: I went to config.yml and I changed the theme… to “pages-themes/hacker”. So once you fork, one thing you have to do, which normally FastPages does for you, is you do have to go to “Settings”, and click “Pages”, and actually enable “Github Pages”. So you basically have to, by default, it's turned off, so here you'll just have to turn it on. So use the “Master Branch Root Save” and then it'll say, “No worries, it's ready to be published”. And so I changed the config.yml file to point at a different theme. And so, if you look at now… the jph's Tiny Pets… it's different! Okay? So it's got the same info, but it's much more “hackerish” because jph00 is a serious hacker, as you can tell from his website. So, anyway, look, it's a very brief taste of this kind of world of Javascript and websites and so forth, but I wanted to give you a sense of, like, you know, you don't need any money. You don't need any IDEs. You know, you don't really need much code to get started with writing your own web apps, and, thanks to hugging face spaces, you know, they'll even host your model for you and all you need to do is just have the magic string, as a thing to call. Okay! So… signing out: Hacker Jeremy Howard. Thanks very much for watching, and, in the next lesson, we're going to be digging into some natural language processing. We're going to be doing some of the same stuff, but we're going to be doing it with language rather than pictures. And we're going to be diving under the hood to see how these models actually work. We're going to learn about things like stochastic gradient descent, and we might even be having to brush off a little bit of calculus. I hope I haven't put you off by saying the “C” word. I will see you next time! Thanks all! Bye!"
fast.ai 2022 - Part 1,3,0,Introduction and survey,"Hi everybody, and welcome to lesson three of Practical Deep Learning for Coders. We did a quick survey this week to see how people feel that the course is tracking and over half of you think it's about the right pace, and of the rest, some of you think it's a bit slow and some of you think it's a bit… sorry… think it's a bit slow and some of you think it's a bit fast. So, hopefully that's about the best we can do generally speaking. The first two lessons are a little more easy pacing for anybody who's already familiar with the basic technology pieces, and then the later lessons get, you know, more into some of the foundations. Today we're going to be talking about things like matrix multiplications and gradients and calculus and stuff like that. So, for those of you who are more mathy and less computery you might find this one more comfortable, and vice versa. So remember that there is an official course updates thread where you can see all the up-to-date info about everything you need to know, and of course the course website as well. So by the time, you know, you watch the video of the lesson it's pretty likely that, if you come across a question, or an issue, somebody else will have, so definitely search the forum and check the facts first and then, of course, feel free to ask a question yourself on the forum if you can't find your answer. One thing I did want to point out which you'll"
fast.ai 2022 - Part 1,3,96,Lesson 0 How to fast.ai,"see in the lesson thread and the course website is there is also a Lesson Zero. Lesson Zero is based heavily on Radek’s book Meta Learning, which internally is based heavily on all the things that I've said over the years about how to learn fastai. It's… we try to make the course full of titbits about the science of learning itself and put them into the course. It's a different course to probably any other you've taken and it's, I strongly recommend watching Lesson Zero, as well. The last bit of Lesson Zero is about how to set up a Linux box from scratch — which you can happily skip over unless that’s of interest — but the rest of it is full of juicy information that I think you'll find useful. So the basic idea of what to do, to do a fast.ai lesson"
fast.ai 2022 - Part 1,3,145,How to do a fastai lesson,"is: watch the lecture and I generally, you know, on the video, recommend watching it all the way through without stopping once and then go back and watch it with lots of pauses, running the notebook as you go. Because otherwise you're kind of like running the notebook without really knowing where it's heading, if that makes sense. And the idea of writing the notebook is, you know, there's a few notebooks you could go through, so obviously there's the book! So going through chapter one of the book, going through chapter two of the book as notebooks, running every code cell and experimenting with inputs and outputs to try and understand what's going on. And then trying to reproduce those results, and then trying to repeat the whole thing with a different data set. And if you can do that last step, you know, that's quite a stretch goal, particularly at the start of the course, because there's so many new concepts but that really shows that you… you've got it sorted. Now, this third bit: “reproduce results”, I recommend using… you'll find in the fastbook repo — so the repository for the book — there is a special folder called “clean”, and “clean” contains all of the same chapters of the book but with all of the text removed — except for headings — and all the outputs removed, and this is a great way for you to test your understanding of the chapter, is… before you run each cell, try to say to yourself, “okay, what's this for and what's it going to output? — if anything.” And if you, kind of, work through that slowly… that's a great way… and any time you're not sure, you can jump back to the version of the notebook with the text to remind yourself and then head back over to the clean version. So, there's an idea for something which a lot of people find really useful for self-study. I say"
fast.ai 2022 - Part 1,3,268,How to not self-study,"self-study but, of course, as we've mentioned before, the best kind of study is study done, to some extent, with others — for most people. You know, the research shows that you're more likely to stick with things if you're doing it as kind of a bit of a social activity. There are… the forums are a great place to find and create study groups, and you'll also find on the forums a link to our Discord server. So… yes, our Discord server where there are some study groups there as well. So, you know, in person study groups, virtual study groups, are a great way to, you know, really make good progress and find other people at a similar level to you. If there's not a study group going at your level, in your area, in your time zone; create one, so just post something saying: Hey! Let's create a study group. So this week there's been a lot of fantastic activity. I can't show all of it so, what I did was, I used the summary functionality"
fast.ai 2022 - Part 1,3,328,Highest voted student work,"in the forums to grab all of the things with the highest votes and so, I just quickly show a few of those. We have a Marvel Detector created this week. Identify your favorite marvel character. I love this: a rock, paper, scissors game where you actually use pictures of the rock, paper, scissors symbols — and apparently the computer always loses, that's my favorite kind of game. There is a lot of Elon around, so very handy to have an Elon Detector to, you know, either find more of him if that's what you need, or maybe less of him. I thought this one was very interesting. I love these kind of really interesting ideas there's like… “gee I wonder if this would work?” Can you predict the average temperature of an area based on an aerial photograph? And the… and apparently the answer is yeah! Actually you can predict it pretty well. Here in Brisbane, it was predicted, I believe to within one and a half celsius. I think this student is actually a genuine meteorologist, if I remember correctly – he built a cloud detector. So, then, building on top of the What's Your Favorite Marvel Character, there's now… also this is a Marvel character. My daughter loves this one: what dinosaur is this? And, I'm not as good about dinosaurs as I should be. I feel like there's 10 times more dinosaurs than there was when I was a kid, so I never know their names, so this is very handy. This is cool, choose your own adventure, where you choose your path using facial expressions. And, I think this music genre classification is also really cool. Brian Smith created a Microsoft Power App application that actually runs on a mobile phone. That's pretty cool. Wouldn't be surprised to hear that Brian actually works at Microsoft, so, also an opportunity to promote his own stuff there. I thought this art movement classifier was interesting in that, like, there's a really interesting discussion on the forum about what it actually shows, about similarities between different art movements. And I thought this redaction detector project was really… was really cool as well – and there's a whole tweet thread and blog post and everything about this one particularly great piece of work. Okay so I'm going to quickly show you a couple of little tips before"
fast.ai 2022 - Part 1,3,476,Pets breeds detector,"we kind of jump into the mechanics of what's behind a neural network, which is, I was playing a little bit with how do you make your neural network more accurate during the week and so I created this Pet Detector. And this pet detector is not just predicting… predicting dogs or cats but what breed is it. That's obviously a much more difficult exercise. Now, because I put this out on HuggingFace Spaces, you can download and look at my code, because, if you just click files and versions on the space — which you can find a link on the forum and the course website — you can see them all here and you can download it to your own computer. So I'll show you what I've got here. Now one thing I'll mention is today"
fast.ai 2022 - Part 1,3,532,Paperspace,"I'm using a different platform, so in the past I've shown you Colab and I've shown you Kaggle and we've also looked at doing stuff on your own computer — not so much training models on your computer, but using the models you've trained to create applications. Paperspace is a another website — a bit like Kaggle and Google — but in particular they have a product called Gradient Notebooks, which is — at least as I speak (and things change all the time, so check the course website) — but, as I speak, in my opinion, is by far the best platform for running this course and for, you know, doing experimentation. I'll explain why as we go. So, why haven't I been using the past two weeks? Because I've been waiting for them to build some stuff for us to make it particularly good and they just… they just finished. So, I've been using it all week and it's totally amazing. This is what it looks like… so you've got a machine running in the cloud, but the thing that is very special about it is it's a… it's a real… it's a real computer you're using. It's not like that kind of weird virtual version of things that Kaggle or Colab has,"
fast.ai 2022 - Part 1,3,616,JupyterLab,"so if you whack on this button down here you'll get a full version of Jupyter Lab, or you can switch over to a full version of classic Jupyter Notebooks. And, I'm actually going to do stuff in Jupyter Lab today because it's a pretty good environment for beginners who are not familiar with the terminal — which I know a lot of people in the course are in that situation — you can do really everything kind of graphically. There's a file browser, so here you can see I've got my pets repo. It's got a Git repository thing, you can pull and push to Git. And then you can also open up a terminal, create new notebooks and so forth. So what I tend to do with this is, I tend to go into a full screen so it's kind of like its own whole IDE, and so you can see I've got here my terminal, and here's my notebook. They have free GPUs and most importantly there's two good features: one is that you can pay, I think it's eight or nine dollars a month to get better GPUs, and basically as many as, you know, as many hours as you want. And they have persistent storage so, with Colab — if you've played with it — you might have noticed it's annoying you have to muck around with saving things to Google Drive and stuff. On Kaggle there isn't really a way of, kind of, having a persistent environment, whereas on Paperspace you have, you know, whatever you save in your storage it's going to be there the next time you come… come back. So, I'm going to be adding walkthroughs of all of this functionality, so look at… so if you're interested in really taking advantage of this, check those out."
fast.ai 2022 - Part 1,3,731,Make a better pet detector,"Okay so I think the main thing that I wanted you to take away from Lesson Two isn't necessarily all the details of: how do you use a particular platform to train models and deploy them into applications through JavaScript or online platforms, but the key thing I wanted you to understand was the concept. There's really two pieces: there's the training piece, and at the end of the training piece you end up with this model.pkl file, right? And once you've got that, that's now a thing where you feed it inputs and it spits out outputs based on that model that you trained, and then… so you don't need, you know, because that happens pretty fast, you generally don't need a GPU once you've got that trained. And so then there's a separate step, which is deploying. So I'll show you how I trained my pet classifier. So you can see I've got two IPython notebooks: one is “app” which is the one that's going to be doing the inference in production; one is the one where I train the model. So this first bit I'm going to skip over because you've seen it before, I create my ImageDataLoaders, check that my data looks okay with show_batch, train a ResNet34 and I get 7% accuracy… so that's pretty good."
fast.ai 2022 - Part 1,3,827,Comparison of all (image) models,"But check this out, there's a link here to a notebook I created… (actually most of the work was done by Ross Wightman) …where we can try to improve this by finding a better architecture. There are, I think, at the moment — in the Pytorch image models libraries — over 500 architectures and we'll be learning over the course, you know, what they are? How they differ? But, you know, broadly speaking they're all mathematical functions, you know, which are basically matrix multiplications and these non-linearities such as RELUs — that we'll talk about today. Most of the time those details don't matter, what we care about is three things: How fast are they? How much memory do they use? and how accurate are they? And so, what I've done here with Russ is we've grabbed all of the models from Pytorch image models and you can see all the code we've got (there's very, very little code) to create this plot. Now… (my screen resolution resolution's a bit, there we go, let's do that) And so, on this plot, on the x-axis we've got seconds per sample: so how fast is it - so, to the left is better, it's faster. And on the right is how accurate is it, so how accurate was it on ImageNet in particular. And so, generally speaking, you want things that are up towards the top and left. Now we've been mainly working with ResNet and you can see down here, here's ResNet18. Now resonant 18 is a particularly small and fast version for prototyping. We often use ResNet34, which is this one here. And you can see this, kind of like, classic model that's very widely used (actually nowadays isn't the state of the art anymore.) So we can start to look up at these"
fast.ai 2022 - Part 1,3,949,Try out new models,"ones up here and find out some of these better models. The ones that seem to be the most accurate and fast are these LeVit models, so I tried them out on my pets and I found that they didn't work particularly well. So I thought, okay, let's try something else out; so next up I tried these ConvNeXT models. And this one into here, it was particularly interesting, it's kind of like super high accuracy — it's the, you know, if you want 0.001 seconds inference time, it's the most accurate. So I tried that! So how do we try that? All we do is, I can say… so the Pytorch image models is in the Timm module, so at the very start I imported that. And we can say: list_models and pass in a glob, a match. And so this is going to show all the ConvNeXT models, and here I can find the ones that I just saw. And all I need to do is, when I create the vision learner, I just put the name of the model in as a string, okay? So, you'll see, earlier this one is not a string, that's because it's a model that fastai provides, the library. Fastai only provides a pretty small number, so if you install Timm — so you'll need to “pip install timm” or “conda install timm” — you'll get hundreds more and you put that in a string. So if I now train that, the time for these epochs goes from 20 seconds to 27 seconds, so it is a little bit slower, but the accuracy goes from 7.2 percent down to 5.5 percent. So, you know, that's a pretty big relative difference... 7.2 divided by 5.5... Yeah, it's about a thirty percent improvement. So, that's pretty fantastic and, you know, it's been a few years, honestly, since we've seen anything really beat ResNet that's widely available and usable on regular GPUs, so this is, this is a big step and so this is a, you know, there's a few architectures nowadays that really are probably better choices a lot of the time, and these Conv… so, if you are not sure what to use, try these ConvNeXT architectures. You might wonder what the names are about, obviously tinys, more large, et cetera, is how big is the model, so that'll be how much memory is it going to take up, how fast is it. And then these ones here that say in22ft1k, these ones have been trained on more data, so ImageNet, there's two different image data sets: there's one that's got a thousand categories of pictures, and there's another one that's about 22,000 categories of pictures. So this is trained on the one with 22,000 categories of pictures. So these are generally going to be more accurate on, kind of, standard photos of natural objects. Okay, so from there I exported my model and that's there, okay? So now I've trained my model and I'm all done. You know, other things you could do obviously is add more epochs, for example, that image augmentation; there's various things you can do, but, you know, I found this is actually pretty, pretty hard to beat this by much. If any of you find you can do better, I'd love to hear about it."
fast.ai 2022 - Part 1,3,1162,Get the categories of a model,"So then to turn that into an application, I just did the same thing that we saw last week, which was to load the learner; now this is something I did want to show you: the learner, once we load it and call predict, spits out a list of 37 numbers. That's because there are 37 breeds of dog and cat, so these are the probability of each of those breeds. What order are they in? That's an important question. The answer is that fastai always stores this information about categories — this is a category, in this case a dog or cat breed — in something called the vocab object, and it's inside the data loaders. So we can grab those categories, and that's just a list of strings, just tells us the order. So if we now zip together the categories and the probabilities, we'll get back a dictionary that tells you… well… like so. So here's that list of categories and here's the probability of each one. And this was a basset hound, so there you can see… Yep! almost certainly a basset hound. So from there, just like last week, we can go and create our interface and then… and then launch it and there we go. Okay so, what did we just do really? What is this magic model.pkl file."
fast.ai 2022 - Part 1,3,1240,What’s in the model,"So, we can take a look at the model.pkl file – it's an object type called a learner and a learner has two main things in it: the first is the list of pre-processing steps that you did to turn your images into things of the model, and that's basically this information here. So it's your data blocks or your ImageDataLoaders or whatever. And then the second thing, most importantly, is the trained model! And so, you can actually grab the trained model by just grabbing the .model attribute — so I'm"
fast.ai 2022 - Part 1,3,1283,What does model architecture look like,"just going to call that m — and then if I type m, I can look at the model. And so, here it is… lots of stuff, so what is this stuff? Well, we'll learn about it all, over time, but basically what you'll find is: it contains lots of layers, because this is a deep learning model, and you can see it's kind of like a tree, that's because lots of the layers themselves consist of layers. So there's a whole layer called the TimmBody, which is most of it. And then right at the end there's a second layer called Sequential, and then the TimmBody contains something called “model”, and it can then contain something called “stem”, and something called “stages”. And then “stages” contain 0, 1, 2, etc."
fast.ai 2022 - Part 1,3,1335,Parameters of a model,"So what is all this stuff? Well, let's take a look at one of them. So to take a look at one of them, there's a really convenient method in Pytorch called get_submodule where we can pass in a, kind of, a dotted string navigating through this hierarchy. So “0.model.stem.1” goes zero, model stem, one. So this is going to return this LayerNorm2d thing. So what is this layer Norm2d thing? Well the key thing is: it's got some code, it's with the mathematical function that we've talked about, and then the other thing that we learned about is it has parameters. So we can list its parameters and look at this: it's just lots and lots and lots of numbers. Let's grab another example, we could have a look at “0.model.stages.0.blocks.1.mlp.fc1” and parameters… Another big bunch of numbers. So, what's going on here? What are these numbers and where on earth did they come from? And how come these numbers can figure out whether something is a basset hound or not? Okay so… To answer that question we're going to have a look at a Kaggle notebook"
fast.ai 2022 - Part 1,3,1416,Create a general quadratic function,"“How does a neural network really work?” I've got a local version of it here, which I'm going to take you through. And the basic idea is: machine learning models are things that fit functions to data. So we start out with a very, very flexible — in fact an infinitely flexible — as we've discussed function, a neural network, and we get it to do a particular thing, which is to recognize the patterns in the data examples we give it. So let's do a much simpler example than a neural network, let's do a quadratic. So let's create a function f which is 3 x² + 2 x + 1. Okay so it's a quadratic, with coefficients three, two and one. So, we can plot that function f, and give it a title — if you haven't seen this before things between dollar signs is what's called latex, it's basically how we can create kind of typeset mathematical equations. Okay, so let's run that. And so here you can see the function, here you can see the title I passed it, and here is our quadratic. Okay, so what we're going to do is… we're going to imagine that we don't know that's the true mathematical function we're trying to find — because it's obviously much simpler than the function that figures out whether an image is a basset hound or not — that we're just going to start super simple. So this is the real function and we're going to try to recreate it from some data. Now it's going to be very helpful if we have an easier way of creating different quadratics, so I've defined a kind of a general form of a quadratic here… with coefficients a, b, and c; and at some particular point x, it's going to be a x squared plus b x plus c. And so let's test that. Okay so, that's for x equals 1.5, that's 3 x squared plus 2 x plus 1, which is the quadratic we were… did before. Now we're going to want to create lots of different quadratics to test them out and find which one's best, so this is a somewhat advanced but very, very helpful feature of python that's worth learning if you're not familiar with it, but it's used in a lot of programming languages, it's called a partial application of a function. Basically I want this exact function, but I want to fix the values of a, b and c, to pick a particular quadratic. And the way you fix the values of the function is: you call this thing in python called “partial”, and you pass in the function, and then you pass in the values that you want to fix. So for example, if I now say make a quadratic 3, 2, 1; that's going to create a quadratic equation with coefficients 3, 2, and 1. And you can see, if I then pass in — so that's now f — if I pass in 1.5, I get the exact same value I did before. Okay, so, we've now got an ability to create any quadratic equation we want by passing in the parameters of the coefficients of the quadratic — and that gives us a function that we can then just call as just like any normal function, so that only needs one thing now, which is the value of x, because the other three a, b, and c are now fixed. So if we plot that function, we'll get exactly the same shape, because it's the same coefficients."
fast.ai 2022 - Part 1,3,1640,Fit a function by good hands and eyes,"Okay, so, now I'm going to show an example of some data… some data that matches the shape of this function. But in real life, data is never exactly going to match the shape of a function, it's going to have some noise. So here's a couple of functions to add some noise. So you can see, I've still got the basic functional form here, but this data is a bit dotted around it. The level to which you look at how I implemented these is entirely up to you, it's not like super necessary, but it's all stuff which, you know, the kind of things we use quite a lot so this is to create normally distributed random numbers… this is how we set the seed so that each time I run this I gotta get the same random numbers. This one is actually particularly helpful, this creates a tensor, so in this case a vector that goes from -2 to 2, in equal steps and there's twenty of them, that's why there's 20 steps along here. So then my y-values is just f(x), with this amount of noise added. Okay, so — as I say — the details of that don't matter too much, the main thing to know is we've got some random data now. And so, this is… the idea is now we're going to try to reconstruct the original quadratic equation, find the one which matches this data. So how would we do that? Well, what we can do is, we can create a function called plot quadratic that, first of all, plots our data, as a scatter plot, and then it plots a function, which is a quadratic – the quadratic we pass in. Now, there's a very helpful thing for experimenting in Jupyter Notebooks, which is the @interact function. If you add it on top of a function, then it gives you these nice little sliders. So here's an example of a quadratic with coefficients 1.5 ,1.5, 1.5. And it doesn't fit particularly well. So how would we try to make this fit better? Well, I think what I'd do is I take the first slider, and I would try moving it to the left and see if it looks better or worse. That looks worse to me, I think it needs to be more curvy, so let's try the other way. Yeah, that doesn't look bad. Let's do the same thing for the next slider, have it this way… no, I think that's worse, let's try the other way… okay, final slider… try this way, no, it's worse… this way… So you can see what we can do, we can basically pick each of the coefficients one at a time. Try increasing a little bit, see if that improves it. Try decreasing it a little bit, see if that improves it. Find the direction that improves it, and then slide it in that direction a little bit. And then when we're done, we can go back to the first one and see if we can make it any better. Now we've done that! And actually you can see that's not bad, because I know the answer's meant to be three, two, one. So they're pretty close. And, I wasn't cheating, I promise. That's basically what we're gonna do. That's basically how those parameters are created. But we, obviously, don't have time, because the, you know, big fancy models have,"
fast.ai 2022 - Part 1,3,1858,Loss functions,"often, hundreds of millions of parameters, we don't have time to try a hundred million sliders. So we need something better. Well the first step is, we need a better idea of like, when I move it, is it getting better or is it getting worse? So, if you remember back to Arthur Samuel's description of machine learning that we learned about on Chapter One of the book, and in Lesson One… we need some… something we can measure, which is a number that tells us how good is our model. And if we had that, then as we moved these sliders, we could check to see whether it's getting better or worse. So this is called a loss function. So, there's lots of different loss functions you can pick but perhaps the most simple and common is mean-squared-error, which is going to be… so it's going to get in our predictions, and it's got the actuals. And we're going to go predictions minus actuals squared and take the mean. So that's mean squared error. So, if I now rerun the exact same thing I had before, but this time I'm going to calculate the loss, “the mse,” between the values that we predict, f(x); remember where “f” is the quadratic we created and the actuals… “y”. And this time I'm going to add a title to our function, which is the loss. So now let's do this more rigorously. We're starting at a mean squared error of 11.46, so let's try moving this to the left and see if it gets better… no, worse, so I’ll move it to the right. All right, somewhere around there. Okay, now let's try this one. Okay, best when I go to the right. Okay, what about “c”? 3.91. It's getting worse, so I keep going (sorry about that). And so now we can repeat that process, right? So we've had each of a, b and c move a little bit. Let's go back to a, can I get any better than 3.28? Let's try moving left: yeah, left was a bit better. And for b, let's try moving left: worse. Right was better. And finally c, move to the right… Oh, definitely better. There we go. Okay so, that's a more rigorous approach, it's"
fast.ai 2022 - Part 1,3,2019,Automate the search of parameters for better loss,"still manual, but at least we can, like, we don't have to rely on us to kind of recognize: does it look better or worse? So finally we're going to automate this. So the key thing we need to know is: for each parameter, when we move it up, does the loss get better? Or when we move it down, does the loss get better? One approach would be to try it, right? We could manually increase the parameter a bit and see if the loss improves, and vice versa. But there's a much faster way, and the much faster way is to calculate its derivative. So if you've forgotten what a derivative is, no problem, there's lots of tutorials out there, you could go to Khan Academy or something like that, but in short, the derivative is what I just said. The derivative is a function that tells you: if you increase the input, does the output increase or decrease? …and by how much. So that's called the slope, or the gradient. Now the good news is Pytorch can automatically calculate it for you. So, if you went through horrifying months of learning derivative rules in year 11, and worried you're going to have to remember them all again, don't worry, you don't! You don't have to calculate any of this yourself, it's all done for you. Watch this. So the first thing to do, is we need a function that takes the coefficients of the quadratic a, b and c as inputs. I'm going to put them all in a list, you'll see why in a moment, I kind of call them parameters. We create a quadratic, passing in those parameters a, b and c. This star on the front is a very, very common thing in python, basically it takes these parameters and spreads them out to turn them into a, b and c, and pass each of them to the function. So we've now got a quadratic with those coefficients and then we return the mean squared error of our predictions against our actuals. So this is a function that's going to take the coefficients of a quadratic and return the loss. So let's try it. Okay so if we start with a, b and c of 1.5, we get a mean squared error of 11.46. It looks a bit weird, it says it's a tensor, so don't worry about that too much, in short, in Pytorch everything is a tensor. A tensor just means that you don't… it doesn't just work with numbers, it also works with lists or vectors of numbers… that's called a 1d-tensor. Rectangles of numbers, so tables of numbers is called a 2d-tensor. Layers of tables of numbers, that's called a 3d-tensor, and so forth. So in this case this is a single number, but it's still a tensor, that means it's just wrapped up in the Pytorch machinery that allows it to do things like calculate derivatives. But it's still just the number 11.46. So what I'm going to do is I'm going to create my parameters a, b and c; and I'm going to put them all in a single 1d-tensor. Now, 1d-tensor is also known as a rank-1-tensor. So this is a rank 1 tensor and it contains the list of numbers 1.5, 1.5, 1.5. And then I'm going to tell Pytorch that I want you to calculate the gradient for these numbers, whenever we use them in a calculation. And the way we do that is we just say: requires_grad_(). So here is our tensor, it contains 1.5 three times and it also tells us it's… we flagged it to say, please calculate gradients for this particular tensor, when we use it in calculations. So let's now use it in the calculation, we're going to pass it to that quad_mse, that's the function we just created that gets the mse — the mean squared error — for a set of coefficients and not surprisingly it's the same number we saw before: 11.46. Okay, not very exciting, but there is one thing that's very exciting, which is added an extra thing to the end called grad function, and this is the thing that tells us that, if we wanted to, Pytorch knows how to create — calculate — the gradients for our inputs. And to tell Pytorch: yes please, go ahead and do that calculation, you call “backward()” on the result of your loss function. Now when I run it nothing happens, or it does look like nothing happens… But what does happen is: it's just added an attribute called grad, which is the gradient to our inputs “abc”. So, if we run this cell, this tells me that if I increase a, the loss will go down. If I increase b, the loss will go down a bit less, you know. If they increase c, the loss will go down. Now, we want the loss to go down, right? So that means we should increase a, b and c. Well, how much by? Well, given that a is… says, if you increase a, even a little bit, the loss improves a lot, that suggests we're a long way away from the right answer, so we should probably increase this one a lot, this one the second most, and this one the third most. Okay so this is saying, when I increase this parameter, the loss decreases. So in other words, we want to adjust our parameters a, b and c by the negative of these. We want to increase, increase, increase. So we can do that by saying, okay, let's take our “abc” minus equals (so that means equals “abc” minus) the gradient. But, we're just going to like decrease it a bit, we don't want to jump too far, okay? So just we're just going to go a small distance. So we're going to, we're just going to somewhat arbitrarily pick 0.01. So that is now going to create a new set of parameters which are going to be a little bit bigger than before, because we subtracted negative numbers. And we can now calculate the loss again. So remember, before it was 11.46, so hopefully it's going to get better. Yes it did, 10.11. There's one extra line of code, which we didn't mention, which is with torch.no_grad(). Remember earlier on, we said that the parameter abc requires grad, and that means Pytorch will automatically calculate its derivative when it's used in a… in a function. Here it's being used in a function, but we don't want the derivative of this, this is not our loss, right? This is us updating the gradients. So this is basically the standard inner part of the Pytorch loop, and every neural net deep learning machine… pretty much every machine learning model, at least of this style, that you’ll build, basically looks like this. If you look deep inside fastai source code, you'll see something that basically looks like this. So we could automate that, right? So let's just take those steps, which is, we're going to calculate — let's go back to here — we're going to calculate the mean squared error for our quadratic. Call backward and then subtract the gradient times the small number from the gradient. So let's do it five times. So, so far we're up to a loss of 10.1. So we're going to calculate our loss, call dot backward to calculate the gradients and then: with no_grad, subtract the gradients times a small number, and print how we go. And there we go, the loss keeps improving. So we now have… some coefficients. And there they are: 3.2, 1.9, 2.0. So they're definitely heading in the right direction. So that's basically how we do… it's called optimization. Okay, so you'll hear a lot in deep learning about optimizers, this is the most basic kind of optimizer, but they're all built on this principle, of course, it's called gradient descent. And you can see why it's called gradient descent: we calculate the gradients and then do a descent, which is, we're trying to decrease the loss. So, believe it or not,"
fast.ai 2022 - Part 1,3,2565,The mathematical functions,"that's the entire foundations of how we create those parameters. So we need one more piece, which is what is the mathematical function that we're finding parameters for? We can't just use quadratics, right? …because it's pretty unlikely that the relationship between parameters and whether a pixel is part of a basset hound is a quadratic. It's going to be something much more complicated. No problem, it turns out that we can create an infinitely flexible function from this one tiny"
fast.ai 2022 - Part 1,3,2598,ReLu: Rectified linear function,"thing: this is called a Rectified Linear Unit. The first piece, I'm sure you will recognize, it's a linear function. We've got our output y, our input x, and coefficients m and b. This is even simpler than our quadratic, and this is a line. And torch.clip() is a function that takes that output, y, and if it's greater than that number, it turns it into that number. So in other words this is going to take anything that's negative, and make it zero. So this function is going to show two things: calculate the output of a line, and if it is ~(bigger,) ah, smaller than zero, it'll make it zero. So that's rectified_linear(). So let's use partial to take that function and set the m and b to one and one. So this is now going to be, this function here, will be: y equals x plus one followed by this torch.clip(). And here's the shape, okay. As you'd expect, it's a line until it gets under zero, when it becomes, well, it's still a line, it's a… becomes a horizontal line. So we can now do the same thing, we can take this plot function and make it interactive using interact. And we can see what happens when we change its two parameters: m and b. So we're now plotting the rectified linear and fixing m and b. So m is the slope, okay, and b is the intercept, with a shift up and down. Okay, so that's how those work. Now, why is this interesting? Well, it's not interesting of itself,"
fast.ai 2022 - Part 1,3,2717,Infinitely complex function,"but what we could do is, we could take this rectified linear function and create a double ReLU, which adds up two rectified linear functions together. So there's some slope m1, b1 – some second slope m2, b2 – we're gonna calculate it at some point x. And so let's take a look at what that function looks like if we plot it. And you can see what happens is we get this downward slope, and then a hook, and then an upward slope. So if I change m1 it's going to change the slope of that first bit, and b1 is going to change its position, okay. And I'm sure you won't be surprised to hear that m2 changes the slope of the second bit and b2 changes that location. Now, this is interesting… why? Because we don't just have to do a double ReLU, we could add as many ReLUs together as we want, and if we add as many ReLUs together as we want, then we can have an arbitrarily squiggly function, and with enough ReLUs, we can match it as close as we want, right? So you could imagine incredibly squiggly, like I don't know, like an audio waveform of me speaking and if I gave you 100 million ReLUs to add together, you could almost exactly match that. Now, we want functions that are not just… that we've plotted in 2d, we want things that can have more than one input, but you can add these together across as many dimensions as you like. And so, exactly the same thing will give you a ReLU over surfaces, or a ReLU over 3d, 4d, 5d and so forth. And it's the same idea, with this incredibly simple foundation you can construct an arbitrarily, accurate, precise model. Problem is, you need some numbers for them, you need parameters. Oh! no problem, we know how to get parameters, we use gradient descent. So believe it or not, we have just derived deep learning! Everything from now on is tweaks to make it faster, and make it need less data, you know, this is… this is it!! Now, I remember a few years ago when I said something like this in a class, somebody on the forum was like, this reminds me of that thing about how to draw an owl. Jeremy's basically saying, okay, step one: draw two circles; step two: draw the rest of the owl. The thing I'm… I find I have a lot of trouble explaining to students is: when it comes to deep learning, there's nothing between these two steps. When you have ReLUs getting added together, and gradient descent to optimize the parameters, and samples of inputs and outputs that you want, the computer draws the owl, right? that's, that's, that's it, right? So we're going to learn about all these other tweaks, and they're all very important, but when you come down to like trying to understand something in deep learning, just try to keep coming back to remind yourself of what it's doing, which it's using gradient descent to set some parameters to make a wiggly function which is basically the addition of lots of rectified linear units or something very similar to that, match your data."
fast.ai 2022 - Part 1,3,2961,A chart of all image models compared,"Okay, so we've got some questions on the forum. Okay, so, the question from Zakia, with six upvotes… (so for those of you watching the video, what we do in the lesson is we want to make sure that the questions that you hear answered are the ones that people really care about so, we pick the ones which get the most upvotes.) This question is: “is there perhaps a way to try out all the different models and automatically find the best performing one?” Yes, absolutely you can do that, so… If we go back to our training script, remember, there's this thing called list_models() and it's a list of strings, so you can easily add a for loop around this that basically goes, you know, for architecture in timm.list_models and you could do the whole lot, which would be like that, and then you could… do that, and a way you go. It's going to take a long time for 500 and something models, so generally speaking, like, I've never done anything like that myself, I would rather look at a picture like this and say like: Okay, where am I in? The vast majority of the time — this is something, this would be the biggest error and number one mistake of beginners I see — is that they jump to these models from the start of a new project. At the start of a new project I pretty much only use ResNet18 because I want to spend all of my time trying things out. I'm going to try different data augmentation. I'm going to try different ways of cleaning the data. I'm going to try, you know, different external data I can bring in. And so I want to be trying lots of things and I want to be able to try it as fast as possible, right? So… Trying better architectures is the very last thing that I do. And what I do is, once I've spent all this time, and I've got to the point where I've got… okay, I've got my ResNet18, well maybe, you know, ResNet34 because it's nearly as fast. And I'm like, okay well, how accurate is it? How fast is it? Do I need it more accurate, for what I'm doing? Do I need it faster, for what I'm doing? Could I accept some trade-off to make it a bit slower, to make it more accurate? And so then I'll have a look and I'll say: okay well, I kind of need to be somewhere around 0.001 seconds and so I try a few of these. So that would be how I would think about that."
fast.ai 2022 - Part 1,3,3131,Do I have enough data?,"Okay, next question from the forum is around: “How do I know if I have enough data? What are some signs that indicate my problem needs more data?” I think it's pretty similar to the architecture question. So, you've got some amount of data. Presumably you've, you know, you've started using all the data that you have access to, you've built your model, you've done your best. Is it good enough? Do you have the accuracy that you need for whatever it is you're doing? You can't know until you've trained the model, but as you've seen, it only takes a few minutes to train a quick model. So, my very strong opinion is that the vast majority of projects I see in industry wait far too long before they train their first model. You know, in my opinion you want to train your first model on day one with whatever CSV files or whatever that you can hack together. And you might be surprised that none of the fancy stuff you're thinking of doing is necessary because you already have a good enough accuracy for what you need. Or you might find quite the opposite, you might find that, oh my god, we're basically getting no accuracy at all, maybe it's impossible. These are things you want to know at the start, not at the end. We'll learn lots of techniques both in this part of the course and in Part Two about ways to really get the most out of your data. In particular there's a reasonably recent technique called semi-supervised learning which actually lets you get dramatically more out of your data, and we've also started talking already about data augmentation, which is a classic technique you can use. So, generally speaking, it depends how expensive is it going to be to get more data. But also what do you mean when you say “get more data.” Do you mean more labeled data? Often it's easy to get lots of inputs and hard to get lots of outputs. For example, in medical imaging — where I spent a lot of time — it's generally super easy to jump into the radiology archive and grab more CT scans. But it might be very difficult and expensive to, you know, draw segmentation masks, and pixel boundaries and so forth, on them. So often, you can get more, you know, in this case images, or texts, or whatever. And maybe it's harder to get labels. And again, there's a lot of stuff you can do, using things like we'll discuss: semi-supervised learning to actually take advantage of unlabeled data as well. Okay, a final question here: “In the quadratic example, where we calculated the initial"
fast.ai 2022 - Part 1,3,3296,Interpret gradients in unit?,"derivatives for a, b and c, we got values of -10.8, -2.4, etc. What unit are these expressed in? Why don't we adjust our parameters by these values themselves?” So I guess the question here is: “Why are we multiplying it by a small number?” …which in this case is 0.01. Okay let's take those two parts of the question. What's the unit here? The unit is: ~(for each increase in “x” of one, how much does…) sorry, for each increase in “a” of one, so if I increase “a” from, in this case, we're at 1.5; so if we increase from 1.5 to 2.5, what would happen to the loss? And the answer is: it would go down by 10.9887. Now, that's not exactly right because it's kind of like… it's kind of like, in an infinitely small space, right? …because actually it's going to be curved, right? But it's, if it stays…it stayed at that slope, that's what would happen. So if we increased “b” by one, the loss would decrease — if it stayed constant, you know, if the slope stayed the same — the loss would decrease by minus 2.122."
fast.ai 2022 - Part 1,3,3383,Learning rate,"Okay, so why would we not just change it directly by these numbers? Well, the reason is… the reason is that if we… have some function that we're fitting, and, there's some kind of interesting theory that says that once you get close enough to the the optimal value, all functions look like quadratics anyway, all right, so we can kind of safely draw it in this kind of shape, because this is what they end up looking like if you get close enough. And we're like, let's say we're way out over here, okay, so we're measuring — I use my daughter's favorite pens, the nice sparkly ones — so, we're measuring the slope here. There's a very steep slope, right? So that seems to suggest we should jump a really long way. So we jump a really long way, and what happened? Well we jumped way too far. And the reason is that that slope decreased, as we moved along, and so that's generally what's going to happen, right? Particularly as you approach the optimal, generally the slope is going to decrease. So that's why we multiply the gradient by a small number. And that small number is a very, very, very important number; it has a special name… It's called the learning rate. And this is an example of a hyperparameter. It's not a parameter, it's not one of the actual coefficients of your function, but it's a parameter you use to calculate the parameters. Very meta, right? It's a hyperparameter. And so it's something you have to pick. Now we haven't picked any yet in any of the stuff we've done (should I remember) and that's because fastai generally picks reasonable defaults — for most things — but later in the course we will learn about how to try and find really good learning rates, and you will find sometimes you need to actually spend some time finding a good learning rate. You could probably understand the intuition here. If you pick a learning rate that's too big, you'll jump too far and so you'll end up way over here, and then you will try to then jump back again, and you'll jump too far the other way, and you'll actually diverge. And so if you ever see, when your model's training, it's getting worse and worse, probably means your learning rate is too big. What would happen on the other hand if you pick a learning rate that's too small? Then you're going to take tiny steps and, of course, the flatter it gets the smaller the steps are going to get, and so you're going to get very, very bored. So finding the right learning rate is a compromise between the speed at which you find the answer and the possibility that you're actually going to shoot past it and get worse and worse. Okay, so one of the bits of feedback I got quite a lot in the survey is that people want a break halfway through, which I think is a good idea. So I think now is a good time to have a break, so let's come back in 10 minutes at 25 past seven."
fast.ai 2022 - Part 1,3,3614,Matrix multiplication,"Okay, hope you had a good rest, have a good break, I should say. So I want to now show you, a really really important mathematical computational trick, which is, we want to do a whole bunch of ReLUs. All right. So we're going to be wanting to do a whole lot of m * x + b. And we want… don't just want… to do m * x + b. We're going to want to have, like, lots of variables. So, for example, every single pixel of an image would be a separate variable, so we're going to multiply every single one of those, times some coefficient, and then add them all together, and then do… the crop… the ReLU. And then we're going to do it a second time with a second bunch of parameters and then a third time and a fourth time and fifth time. It's going to be pretty inconvenient to write out 100 million ReLUs, but so happens there's a mathematical… a single mathematical operation… that does all of those things for us, except for the final replace negatives with zeros, and it's called matrix multiplication. I expect everybody at some point did matrix multiplication at high school. I suspect also a lot of you have forgotten how it works. When people talk about linear algebra in deep learning, they give the impression you need years of graduate school study to learn all this linear algebra. You don't!! Actually all you need almost all the time is matrix multiplication and it couldn't be simpler. I'm going to show you a couple of different ways. The first is, there's a really cool site called “matrixmultiplication.xyz”. You can put in any matrix you want, so I'm going to put in (oops) this one. So this matrix is saying I've got three rows of data with three variables, so maybe they're tiny tiny images of three pixels and the value of the first one is “1 2 1” the second is “0 1 1” and the third is “2 3 1” - so those are our three rows of data. These are our three sets of coefficients, so we've got “a b c” in our data so I guess you'd call it “x1 x2 x3” and then here's our first set of coefficients “a b c” “2 6 1” and then our second set is “5 7 8”. So here's what happens when we do matrix multiplication… that second… this matrix here of coefficients, gets flipped around and we do… this is the multiplications and additions that i mentioned, right! So multiply, add, multiply, add, multiply, add. So that's going to give you the first number because that is the left hand column of the second matrix times the first row, so that gives you the top left result. So the next one is going to give us two results, right! So we've got now the right hand one with the top row and the left hand one with the second row. Keep going down, keep going down, and that's it!! That's what matrix multiplication is - it's multiplying things together and adding them up. So there'd be one more step to do, to make this a layer of a neural network, which is if this had any negatives we would replace them with zeros. That's why matrix multiplication is “the” critical foundational mathematical operation in basically all of deep learning. So the GPUs that we use… the thing that they are good at is this, matrix multiplication. They have special cores called tensor-cores, which we can basically only do one thing, which is to multiply together two four by four matrices, and then they do that lots of times for bigger matrices."
fast.ai 2022 - Part 1,3,3862,Build a regression model in spreadsheet,"So I'm going to show you an example of this. We're actually going to build a complete machine learning model on real data in the spreadsheet. So fastai has become kind of famous for a number of things, and one of them is using spreadsheets to create deep learning models. We haven't done it for a couple of years, so I'm pretty pumped to show this to you. What I've done is I went over to Kaggle, where there's a competition I actually helped create many years ago called Titanic, and it's an ongoing competition, so 14 000 people have entered it, or 14000 teams have entered it so far. It's just a competition for a bit of fun, there's no end date, and the data for it, is the data about, who… (here it is training data) …who survived and who didn't, from the real titanic disaster. And so I… I clicked here on the download button, to grab it on my computer, that gave me a CSV, which I opened up in Excel. The first thing I did then was I just removed a few columns that clearly were not going to be important things, like the name of the passengers, the passenger id, just to try to make it a bit simpler. And so I've ended up with, each row of this is one passenger. The first column is the dependent variable. The dependent variable is the thing we're trying to predict. Did they survive? And the remaining are some information, such as what class of the boat - first, second, or third class, their sex, their age, how many siblings in the family. “Par-ch”, I think is parents or something… So you should always look for a data dictionary, right? To find out what's… what… number of parents and children, okay. What was their fare? And which of the three cities did they embark on, Cherbourg, Queenstown, Southampton? Okay, so there's our data. Now when I first grabbed it, I noticed that there were some people with no age. Now there's all kinds of things we could do for that, but for this purpose, I just decided to remove them. And I found the same thing for Embarked. I removed the blanks as well. But that left me with nearly all of the data. Okay, so then I've put that over here. Here's our data with those rows removed, and… okay, that's the… so these… these are the columns that came directly from Kaggle. So basically what we now want to do is we're going to multiply each of these by a coefficient. How do you multiply the word, Male, by a coefficient? And how do you multiply “S” by a coefficient? You can't. So I converted all of these to numbers. Male and Female were very easy. I created a column called isMale, and as you can see there's just an IF statement that says, if sex is male then it's one, otherwise it's zero. And we can do something very similar for Embarked. We can have one column called “did they embark in Southampton?”. Same deal, and another column for did they… what's it called? Cherbourg? Did they embark in Cherbourg? And now P-class is one, two, or three, which is a number, but it's not really… it's not really a continuous measurement of something. There isn't one or two or three things. They're different levels. So I decided to turn those into similar things, into these binary. These are called “binary categorical variables”. So, are they first class? and are they second class? Okay, so that's all that. The other thing that I was thinking, well, you know, then I kind of tried it and checked out what happened, and what happened was the people with… so I… I created some random numbers. So to create the random numbers, I just went: equals RAND, right? And I copied those to the right, and then I just went COPY and I went PASTE VALUES. So that gave me some random numbers. And that's my, like… so just because like, I was like… Before I said: all A, B and C, let's just start them at 1.5, 1.5, 1.5. What we do in real life is we start our parameters at random numbers that are a bit more or a bit less than zero. So these are random numbers. Actually, sorry I slightly lied, I didn't use RAND, I used RAND minus 0.5, and that way I got small numbers that were on either side of zero. So, then when I took each of these, and I multiplied them by our fares, and ages, and so forth, what happened was that these numbers here, are way bigger than, you know, these numbers here. And so, in the end all that mattered was, what was their fare. That’s because they were just bigger than everything else. So I wanted everything to basically go from zero to one. These numbers were too big. So what I did up here is, I just grabbed the maximum of this column, the maximum of all the fares is $512. And so then… actually I do age first… I did a maximum of age, because similar thing, right? There's 80 year olds and there's two year olds. And so, then over here I just did, okay well, what's their age divided by the maximum. And so that way, all of these are between zero and one. Just like all of these are between zero and one. So that's how I fix… this is called normalizing the data. Now we haven't done any of these things when we've done stuff with fastai. That's because fastai does all of these things for you, and we'll learn about how, right? But it's all these things are being done behind the scenes. For Fare, I did something a bit more, which is I noticed there's some lots of very small fares, and there's also some… a few very big fares. So like seventy dollars, and then seven dollars, seven dollars. Generally speaking when you have lots of really big numbers, and a few small ones… so generally speaking when you've got a few really big numbers and lots of really small numbers, this is really common with… with… with money. You know, because money kind of follows this relationship where a few people have lots of it, and they spend huge amounts of it, and most people don't have heaps. If you take the LOG of something that's like… that has that kind of extreme distribution, you end up with something that's much more evenly distributed. So I've added this here called Log_Fare, as you can see. And these are all around one, which isn't bad. I could have normalized that as well, but I was too lazy, I didn't bother, because it seemed okay. So at this point you can now see that if we start from here, all of these are, all around the same kind of level, right? So none of these columns are going to saturate the others. So now I've got my coefficients, which are, just as I said, they're just random. Okay, and so now I need to basically calculate AX1 plus BX2 plus CX3 plus blah blah blah blah blah blah blah, okay. And so, to do that, you can use SumProduct in Excel. I could have typed it out by hand, which would be very boring, but some product is just going to multiply each of these. This one will be multiplied by… where is it… SibSp, by this one. This one will be multiplied by this one, so forth and then, they get all added together. Now one thing, if you're eagle-eyed, you might be wondering is, in a linear equation we have Y equals MX plus B. At the end there's this constant term. And I do not have any constant term. I've got something here called “Const”, but I don't have any plus at the end. How do we… how's that working? Well there's a nice trick that we pretty much always use in machine learning, which is to add a column of data just containing the number one, every time. If you have a column of data containing the number one every time, then that parameter becomes your constant term. So you don't have to have a special constant term, and so it makes our code a little bit simpler, when you do it that way. It's just a trick, but everybody does it. Okay, so this is now the result of our linear model. So this is not… I'm not even going to do ReLU, right? I'm just going to do a plain regression, right? Now if you've done regression before, you might have learned about it as something you kind of solve with various matrix things. But in fact you can solve a regression using gradient descent. So I've just gone ahead and created a loss for each row, and so the loss is going to be equal to our prediction minus “whether they survived” squared. So this is going to be our squared error, and here they all are, our squared errors. And so here I've just summed them up. I could have taken the Mean. I guess that would have been a bit easier to think about, but SUM is going to give us the same result. So here's our loss, and so now, we need to optimize that using gradient descent. So Microsoft Excel has a gradient descent optimizer in it, called Solver. So I'll click Solver, and I just say, okay, what are you trying to optimize? It's this one here, and I'm going to do it by changing these cells here. And I'm trying to minimize it, and so we're starting a loss of 55.78. Actually, let's change it to Mean, as well. We got Mean or Average? Probably Average. All right, so start at 1.03. So optimize that. And there we go. So it's gone from 1.03 to 0.1. And so we can check the predictions. So the first one gets predicted exactly correctly. It was “they didn't survive”, and we predicted “they wouldn't survive”. Ditto for this one. It's very close. And you can start to see… so this one, you can start to see a few issues here, which is like sometimes it's predicting ~(less than one, sorry) less than zero, and sometimes it's predicting more than one. Wouldn't it be cool if we had some way of… wouldn't be cool if we had some way of constraining it to between zero and one, and that's an example of some of the things we're going to learn about, that make this stuff work a little bit better, right? But you can see it's doing an okay job. So this is not deep learning, this is not a neural net, yet. This is just a regression. So, to make it into a neural net,"
fast.ai 2022 - Part 1,3,4578,Build a neuralnet by adding two regression models,"we need to do it multiple times. So I'm just going to do it twice. So now rather than one set of coefficients, I've got two sets. And again I just put in random numbers. Other than that, all the data's the same. And so now I'm going to have my sum product again. So the first sum product is with my first set of coefficients, and my second sum product is with my second set of coefficients. So I'm just calling them linear-one and linear-two. Now there's no point adding those up together, because if you add up two linear functions together, you get another linear function. We want to get all those wiggles, right? So, that's why we have to do our ReLU. So in Microsoft Excel, Relu looks like this: if the number is less than zero, use zero; otherwise use the number. So that's how we're going to replace the negatives with zeros. And then finally, if you remember from our spreadsheet, we have to add them together. So we add the ReLUs together. So that's going to be our prediction, and then our loss is the same as the other sheet. it's just Survived minus Prediction, Squared. And let's change that to Mean… Not Mean… Average. Okay. So let's try solving that. Optimize $AH$1. And this time we're changing all of those. Solved. So this is using gradient descent. Excel Solver is not the fastest thing, well, but it gets the job done. Okay let's see how we went. 0.08 for our deep learning model versus 0.1 for our regression. So it's a bit better. So there you go. So we've now created our first deep learning neural network from scratch, and we did a Microsoft Excel, everybody's favorite artificial intelligence tool. So that was a bit slow and painful. It’ll be a bit faster and"
fast.ai 2022 - Part 1,3,4711,Matrix multiplication makes training faster,"easier if we used matrix multiplication, so let's finally do that. So this next one is going to be exactly the same as the last one, but with matrix multiplication. So all our data looks the same. You'll notice the key difference now is our parameters have been transposed. So before I had the parameters matching the data in terms of being in columns. For matrix multiplication, the… the expectation is, the way matrix multiplication works… it works… is that you have to transpose this. So it goes, the X and Y is, kind of, the opposite way around. The rows and columns are the opposite way around. Other than that, it's the same. I've got the same… I just copied and pasted the random numbers, so we had exactly the same starting point and so now… our entire… this entire thing here, is a single function which… which is, matrix multiply, all of this, by all of this. And so when i run that, it fills in exactly the same numbers. Make this Average. And so now we can optimize that. Okay, make that a MINIMUM, by changing these. Solve. You should get the same number, 0.08, wasn’t it? Yep, now we do. Okay. So that's just another way of doing the same thing. So you can see that matrix multiplication, it takes like a surprisingly long time, at least for me, to get an intuitive feel for matrix multiplication, as like a single mathematical operation. So i still find it helpful to kind of remind myself, it’s just doing these sum products, and additions. Okay, so that is… that is a deep learning neural network in Microsoft Excel."
fast.ai 2022 - Part 1,3,4861,Watch out! it’s chapter 4,"And the Titanic Kaggle competition, by the way, is a pretty fun, learning competition. If you haven't done much machine learning before, then it's certainly worth trying out, just to kind of get the feel for these… how these all get put together. So this is… So the chapter of the book that this lesson goes with, is Chapter Four. And Chapter Four of the book is the chapter where we lose the most people. Because it's, to be honest, it's hard. But part of the reason it's hard is, I couldn't put this into a book. Okay. So we're teaching it in a very different way in the course, to what's in the book, and you know you can use the two together. But if you've tried to read the book and been a bit disheartened, yeah, try, you know, try following through… through the spreadsheet instead. Maybe try creating, like, if you use Numbers or Google Sheets or something, you could try to create your own kind of version of it on whatever spreadsheet platform you prefer. Or you could try to do it yourself from scratch in Python, you know, if you want to really test yourself. So there's some suggestions. Okay. Question from Victor Guerrero. In the Excel exercise, when Jeremy is doing some"
fast.ai 2022 - Part 1,3,4951,Create dummy variables of 3 classes,"feature engineering, he comes up with two new columns, Pclass_1 and Pclass_2. That is true. Pclass_1 and Pclass_2. Why is there no Pclass_3 column? Is it because Pclass_1… if Pclass_1 is zero and Pclass_2 is zero, then Pclass_3 must be one? So, in a way, two columns are enough to encode the input with the original column? Yes! That's exactly the reason. So, there's no need to tell the computer about things it can kind of figure out for itself. So when you create… These are called dummy variables. So when you create dummy variables for a categorical variable with three levels, like this one, you need two dummy variables. So, in general, categorical variable with n levels needs n-1 columns. Thanks to a good question."
fast.ai 2022 - Part 1,3,5014,Taste NLP,"So what we're going to be doing in our next lesson is looking at natural language processing. So far we've looked at some computer vision, and just now we've looked at some, what we call, tabular data, so… so… kind of spreadsheet type data. Next up we’re… we're going to be looking at natural language processing. So I'll give you a taste of it. So you might want to open up the Getting Started with… Getting Started with NLP for Absolute Beginners Notebook. So here's the Getting Started With NLP for Absolute Beginners Notebook. I will say, as a notebook author, I may sound a bit lame, but I always see when people have uploaded it, it always makes me really happy, so… and it also helps other people find it. So remember to upvote these notebooks, or any other notebooks you… you like. I also always read all the comments. So if you want to ask any questions or make any comments, I enjoy those as well. So natural language processing is about, rather than taking, for example, image data and making predictions, we take text data. That text data, most of the time, is in the form of prose, so like, plain English text. So, you know, English is the most common language used for NLP, but there's NLP models in dozens of different languages nowadays. And if you're a non-English speaker, you'll find that for many languages, there's less resources in non-English languages, and there's a great opportunity to provide NLP resources in your language. This has actually been one of the things that the fastai community has been fantastic at, in the global community, is building NLP resources. For example, the first Farsi NLP resource was created by a student from the very first fast.ai course. The Indic languages, some of the best resources have come out of fastai alumni and so forth. So that's a particularly valuable thing you could look at. So if your language is not well represented, that's an opportunity, not a problem. So some examples of things you could use NLP for? Well perhaps the most common and practically useful in my opinion, is classification. Classification means you take a document – now when I say a document that could just be one or two words, it could be a book, it could be a Wikipedia page, so it could be any length. We use the word “document”, it sounds like that's a specific kind of length but it can be a very short thing or very long thing. We take a document and we try to figure out a category for it. Now that can cover many many different kinds of applications. So, one common one that we'll look at a bit is sentiment analysis. So, for example, is this movie review positive or negative. Sentiment analysis is very helpful from things like marketing and product development - you know in big companies there's lots and lots of, you know, information coming in about your product. It's very nice to quickly sort it out and to kind of track metrics from week to week. Something like figuring out what author wrote the document would be an example of a classification exercise because you're trying to put in a category – in this case is, which author. It gives a lot of opportunity in legal discovery. There's already some products in this area, where in this case the category is: is this legal document in scope or out of scope in the court case? Just organizing documents, triaging inbound emails - so like, which part of the organization should it be sent to? Was it urgent or not? Stuff like that. So these are examples of categories of classification. What you'll find is, when we look at"
fast.ai 2022 - Part 1,3,5249,fastai NLP library vs Hugging Face library,"classification tasks in NLP, is it's going to look very very similar to images. But what we're going to do is we're going to use a different library. The library we're going to use is called Hugging Face transformers, rather than fast.ai, and there's two reasons for that: the main reason why, is because i think it's really helpful to see how things are done in more than one library; and HuggingFace transformers, yeah so… fast.ai has a very layered architecture so you can do things at a very high level with very little code or you can dig deeper and deeper and deeper getting more and more fine-grained. HuggingFace transformers doesn't have the same high level api, at all, that fast.ai has, so you have to do more stuff manually. And so at this point of the course, you know, we're going to actually intentionally use a library which is a little bit less user-friendly in order to see, kind of, what extra steps you have to go through to use other libraries. Having said that, the reason I picked this particular library, is it is particularly good. It has really good models in it, it has a lot of really good techniques in it, not at all surprising because they have hired lots and lots of fast.ai alumni, so they have very high quality people working on it. So, before the next lesson,"
fast.ai 2022 - Part 1,3,5334,Homework to prepare you for the next lesson,"yeah, if you've got time, take it… take a look at this notebook and take a look at the data. The data we're going to be working with is quite interesting. It's from a Kaggle competition which is trying to figure out, in patterns, whether two concepts are referring to the same thing or not, where those concepts are represented as English text. And when you think about it, that is a classification task, because the document is, you know, basically text one blah, text two blah. And then the category is similar or not-similar. And in fact in this case they actually have scores, it's either going to be, basically, zero (0), zero point two five (0.25), point five (0.5), point seven five (0.75), or one (1), of, like, how similar is it. But it's basically a classification task, when you think of it that way. So yeah, you can have a look at the data and, next week, we're going to go through step by step through this notebook. And we're going to take advantage of that, as an opportunity also, to talk about the really important topics of validation sets and metrics, which are two of the most important topics in, not just deep learning, but machine learning more generally. All right, thanks, everybody. I'll see you next week. Bye."
fast.ai 2022 - Part 1,4,0,Using Huggingface,"Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think is the lesson that a lot of the regulars in the community have been most excited about, because it's where we're gonna get some totally new material — totally new topic, we've never covered before. We're going to cover natural language processing (NLP), and you'll find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the fast.ai library, using recurrent neural networks (RNNs). Today we're going to do something else, which is we're going to do Transformers, and we're not even going to use the fast.ai library at all in fact. So, what we're going to be doing today is we're going to be fine-tuning a pre-trained NLP model using a library called Hugging Face Transformers. Now given this is the fast.ai course, you might be wondering why we'd be using a different library other than fast.ai. The reason is that I think that… It's really useful for everybody to have experience and practice of using more than one library. Because you'll get to see the same concepts applied in different ways, and I think that's great for your understanding of what these concepts are. Also, I really like the Hugging Face Transformers library. It's absolutely the state of the art in NLP, and it's well worth knowing. If you're watching this on video, by the time you're watching it, we will probably have completed our integration of the Transformers library into fast.ai. So it's in the process of becoming the main NLP (kind of) foundation for fast.ai. So you'll be able to combine Transformers and fast.ai together. Yeah, so I think there's a lot of benefits to this, and in the end you're going to know how to do NLP, you know, in a really fantastic library. Now the other thing is, Hugging Face Transformers doesn't have the same layered architecture that fast.ai has, which means particularly for beginners, the kind of high level, height… you know… top-tier API that you'll be using most of the time, is not as (kind of) ready to go for beginners, as you're used to from fast.ai. And so that's actually, I think, a good thing. You're up to Lesson Four, you know the basic idea now of how gradient descent works, and… and you know, how parameters are learned as part of a flexible function, I think you're ready to try using a somewhat lower level library that does a little bit less for you. So it's going to be, you know, a little bit more work. It's still… it's a very well designed library, and it's still reasonably high level, but you're going to learn to go a little bit deeper. And that's kind of how the rest of the course in general is going to be. On the whole, is, we're going to get a bit deeper, and a bit deeper, and a bit deeper. Now, so first of all, let's talk about what we're going to be doing with"
fast.ai 2022 - Part 1,4,204,Finetuning pretrained model,"fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now you do. You played with these sliders last week, and hopefully you've all actually gone into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So imagine that your job was to move these sliders, to get this as nice as possible, but when it was given to you, the person who gave it to you said, “Oh! actually slider A, that should be on 2.0, we know for sure. And slider B, we think it's like around two and a half. Slider C, we've got no idea.” Now that'd be pretty helpful, wouldn't it, right? Because you could immediately start focusing on the one we have no idea about, get that in roughly the right spot, and then the one you kind of got a vague idea about, you could just tune it a little bit, and the one that they said was totally confident, you wouldn't move at all. You would probably tune these sliders really quickly. That's what a pre-trained model is. A pre-trained model is a bunch of parameters that have already been fitted, where some of them we’re already pretty confident of what they should be, and some of them we really have no idea at all. And so fine-tuning is the process of taking those ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit. The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm called ULMFiT which was first presented actually in a fast.ai course, I think the very first"
fast.ai 2022 - Part 1,4,314,ULMFit,"fast.ai course. It was later turned into an academic paper by me in conjunction with a then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers and went on to help inspire a huge change, you know, huge kind of step improvement in NLP capabilities around the world, along with a number of other important innovations at the time. This is the basic process that ULMFiT described. Step One was to build something called a language model using basically nearly all of Wikipedia and what the language model did was it tried to predict the next word of a Wikipedia article. In fact every next word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia articles which would say things like, you know “the 17th prime number is… dot dot dot” or “the 40th president of the United States, blah, said at his residence, blah that”. You know, filling in these kinds of things requires understanding a lot about how language is structured, and about the world, and about math, and so forth. So to get good at being a language model a neural network has to get good at a lot of things. It has to understand how language works at a reasonably good level and it needs to understand what it's actually talking about, and what is actually true, and what is actually not true, and the different ways in which things are expressed, and so forth. So this was trained using a very similar approach to what we'll be looking at for fine-tuning but it started with random weights and at the end of it there was a model that could predict more than 30 percent of the time correctly what the next word of a Wikipedia article would be. So in this particular case, for the ULMFiT paper, we then took that and we were trying to… the first task I did actually, for the fast.ai course, back when I invented this, was to try and figure out whether IMDb movie reviews were positive or negative sentiment: Did the person like the movie or not? So what I did was I created a second language model so again the language model here is something that predicts the next word of a sentence but rather than using Wikipedia I took this pre-trained model that was trained on Wikipedia and I ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next word of an IMDb movie review. And then finally I took those weights and I fine-tuned them for the task of predicting whether or not a movie review was positive or negative sentiment. So those were the three steps. This is a particularly interesting approach because this very first model, in fact the first two models, if you think about it they don't require any label. I didn't have to collect any kind of document categories, or do any kind of surveys, or collect anything. All I needed was the actual text of Wikipedia and movie reviews themselves because the labels was: “what’s the next word of a sentence?”. Now, since we built ULMFiT, and we used RNNs (recurrent neural networks) for this, at about the same time-ish that we released this, a new kind of architecture particularly useful for NLP at the time was developed called Transformers. And Transformers were particularly built because"
fast.ai 2022 - Part 1,4,555,Transformer,"they can take really good advantage of modern accelerators like, like Google's TPUs. They didn't really, kind of, allow you to predict the next word of a sentence. It's just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked the model to predict which/what were the words that were deleted, essentially. So it's a pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the same. So today we're going to be looking at models using what's become the, you know, much more popular approach than ULMFiT which is this Transformers masked language model approach. Okay, John do we have any questions? And I should mention we do have a professor from University of Queensland, John Williams, joining us, who will be asking the highest voted questions from the community. What have you got, John? Yeah thanks Jeremy. Look, and we might be jumping the gun here, I suspect this is where you're going tonight but we've got a good question here on the forum which is: “How do you go from a model that's trained to predict the next word, to a model that can be used"
fast.ai 2022 - Part 1,4,652,Zeiler & Fergus,"for classification”? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place to start would be the next slide, kind of give you a sense of this. You might remember in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at visualizations of the first layer of a imagenet classification model and Layer One had sets of weights that found diagonal edges, and here are some examples of bits of photos that successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's some examples of bits of pictures that matched, and then Layer Two combined those and now you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified linear units, the outputs of those, they're called activations, where then themselves run through a matrix multiply, a rectified linear unit, added together, so that now you don't just have to have edge detectors, but Layer Two had corner detectors. And here's some examples of some corners that that corner detector successfully found. And remember, these were not engineered in any way, they just evolved from the gradient descent training process. Layer Two had examples of circle detectors as it turns out, and skipping a bit, by the time we got to Layer Five we had bird and lizard eyeball detectors, and dog face detectors, and flower detectors and so forth. Now, you know, nowadays you'd have something like a resnet50 would be something you'd probably be training pretty regularly in this course so that, you know, you've got 50-layers, not just 5-layers. Now the later layers do things that are much more specific to the training task which is, like, actually predicting really, what is it that we're looking at? The early layers, pretty unlikely you're going to need to change them much, as long as you're looking at, like, some kind of natural photos, right? You're going to need edge detectors and gradient detectors. So what we do, in the fine-tuning process, is… there's actually one extra layer after this, which is the layer that actually says: “What is this?”. You know, it's, it's a dog or a cat or whatever. We actually delete that, we throw it away. So now that last matrix multiply has one output, or one output per category you're predicting. We throw that away, so the model now has that last matrix that's spitting out, you know, depends, but generally a few hundred activations, and what we do is, as we'll learn more shortly in the coming lesson, we just stick a new random matrix on the end of that. And that's what we initially train, so it learns to use these kinds of features to predict whatever it is you're trying to predict. And then we gradually train all of those layers. So that's basically how it's done and so it's a bit hand wavy but we'll, particularly in part two, actually build that from scratch ourselves. And in fact in this lesson, time permitting, we're actually going to start going down the process of actually building a real-world deep neural net in python, so we'll be starting to actually make some progress towards that goal. Okay so let's jump into the notebook. So we're going to look at a Kaggle competition that's"
fast.ai 2022 - Part 1,4,887,US Patent Phrase to Phase Matching Kaggle competition,"actually on as I speak, and I created this notebook called “Getting started with NLP for absolute beginners”. And so the competition is called the “U.S. Patent Phrase to Phrase Matching Competition”. And, so I'm going to take you through, you know, a complete submission to this competition. And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied… they're interesting because this is an actual project, that an actual organization, is prepared to invest money in getting solved, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data about a real problem that real organizations really care about, and a very direct way to measure the, you know, accuracy of your solution, you can't really get better than this. Okay so this is a good place, a good competition to experiment with for trying NLP. Now, as I mentioned here, probably the most widely useful application for NLP is classification and as we've discussed in computer vision, classification refers to taking an object"
fast.ai 2022 - Part 1,4,970,NLP Classification,"and trying to identify a category that object belongs to. So, previously we've mainly been looking at images. Today we're going to be looking at documents. Now, in NLP when we say document, we don't specifically mean, you know, a 20 page long, you know, essay. A document could be three or four words, or a document could be the entire encyclopedia. So a document is just an input to an NLP model that contains text. Now, classifying a document, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca…) a classification task – we try to decide on the category: positive or negative sentiment. Author identification would be taking a document and trying to find the category of author. Legal discovery would be taking documents and putting them into categories according to in- or out-of-scope for a court case. Triaging inbound emails would be putting them into categories of, you know, throw away, send to customer service, send to sales, etc. Right? So classification is a very, very rich area, and for people interested in trying out NLP in real life, I would suggest classification would be the place I would start, for looking for, kind of, accessible, real world, useful problems you can solve right away. Now, the Kaggle competition does not immediately look like a classification competition. What it contains… Let me show you some data… What it contains is data that looks like this. It has a thing that they call “anchor”, a thing they call “target”, a thing they call “context”, and a score. Now these are.. I can't remember exact details but I think these are from patents, and I think on the patents there are various, like, things they have to fill in in the patent, and one of those things is called “anchor”, one of those things is called “target” and in the competition the goal is to come up with a model that automatically determines which anchor and target pairs are talking about the same thing. So a score of one here “wood article” and “wooden article” obviously talking about the same thing. A score of zero here “abatement” and “forest region” not talking about the same thing. So the basic idea is we're trying to guess the score. And it's kind of a classification problem, kind of not. We're basically trying to classify things into either “these two things are the same” or “these two things aren't the same”. It's kind of not because we have not just 1 and 0 but also 0.25, 0.5 and 0.75. There's also a column called “context”, which is, I believe, is like the category that this patent was filed in and my understanding is that whether the anchor and the target count as similar or not depends on, you know, what the patent was filed under. So how would we take this and turn it into something like a classification problem? So the suggestion I make here is that we could basically say, okay, let's put the, you know, some constant string like TEXT1 or FIELD1 before the first column and then something else like TEXT2 before the second column. Oh, and maybe, also the context, I should have as well TEXT3 in the context, and then try to choose a category of meaning similarity: “Different” “Similar” or “Identical”. So you can basically concatenate those three pieces together, call that a document and then try to train a model that can predict these categories. That would be an example of how we can take this, basically, similarity problem, and turn it into something that looks like a classification problem. And we tend to do this a lot in deep learning, is we kind of take problems that look a bit novel and different, and turn them into a problem that looks like something we recognize. All right, so on Kaggle this is a, you know, larger data set that you're going to need a GPU to run."
fast.ai 2022 - Part 1,4,1256,"Kaggle configs, insert python in bash, read competition website","So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically. Personally, you know, I like using things like Paperspace generally better than Kaggle, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's some information here, I won't go through but it basically describes how you can download stuff to Paperspace or your own computer as well if you want to. So I basically create this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd say “if iskaggle” and put those changes. So here, you can see here, if I'm not on Kaggle and I don't have the data yet, then download it. And Kaggle has a little API which is quite handy for doing stuff like downloading data and uploading notebooks and stuff like that, submitting to competitions. If we are on Kaggle then the data's already going to be there for us which is actually a good reason for beginners to use Kaggle as you don't have to worry about grabbing the data at all – it's sitting there for you as soon as you open the notebook. Kaggle has a lot of python packages installed, but not necessarily all the ones you want, and at the point I wrote this they didn't have the Hugging Faces datasets package, for some reason, so you can always just install stuff. So you might remember the exclamation mark means this is not a python command, but a shell command, a bash command. But it's quite neat you can even put bash commands inside python conditionals so that's a pretty cool little trick in notebooks. Another cool little trick in notebooks is that if you do use a bash command like “ls” but you then want to insert the contents of a python variable, just chuck it in parentheses. So, I've got a python variable called “path” and I can go “ls {path}” in parentheses and that will “ls” the contents of the python variable “path”. So there's another little trick for you. All right, so when we “ls” that we can see that there's some CSV files. So what I'm going to do is, kind of, take you through, roughly the process, the kind of process I, you know, went through as, you know… when I first look at a competition. So the first thing is like, already a data set, indeed, what's in it? Okay, so it's got some CSV files. You know, as well as looking at it here, the other thing I would do… is I would go to the competition website… and if you go to “Data”… A lot of people skip over this, which is a terrible idea, because it actually tells you what the dependent variable means, what the different files are, what the columns are, and so forth. So don't just rely on looking at the data itself but look at the information that you're given about the data. So, for CSV files, so CSV files are comma separated values, so they're just text files with a comma between each field, and we can read them using pandas, which for some reason is always called “pd”. Pandas is one of, I guess, like, (I'm trying to think) probably"
fast.ai 2022 - Part 1,4,1491,"Pandas, numpy, matplotlib, & pytorch","like four key libraries that you have to know to do data science in python. And specifically, those four libraries are: numpy… matplotlib… pandas… and pytorch. So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for plotting; pandas we use for tables of data; and pytorch we use for deep learning. Those are all covered in a fantastic book by the author of pandas which, the new version is actually available for free, I believe. “Python for data analysis”. So if you're not familiar with these libraries just read the whole book, it doesn't take too long to get through, and it's got lots of cool tips and it's very readable. I do find a lot of people doing this course… often I see people kind of, trying to jump ahead, and… and want to be like: “Oh I want to know how to, like, create a new architecture” or “Build a speech recognition system” or whatever. But it then turns out that they don't know how to use these fundamental libraries. So it's always good to be bold and be trying to build things, but do also take the time to, you know, make sure you finish reading the fast.ai book and read at least Wes McKinney's book. That would be enough to really give you all the basic knowledge you need, I think. So, with pandas we can read a CSV file and that creates something called a DataFrame, which is just a table of data, as you see. So, now that we've got a DataFrame, we can see what we're working with, and when we ask… when in jupyter we just put the name of a variable containing a DataFrame, we get the first five rows, the last five rows, and the size. So we've got 36,473 rows. Okay, so other things I like to use for understanding a DataFrame is the “describe” method. If you pass “include equals object” that will describe, that will describe, basically all the kind of the string fields, the non-numeric fields. So, in this case there's four of those, and so you can see here that, that anchor field we looked at, there's actually only 733 unique values, okay, so this thing, you can see that there's lots of repetition out of 36,000. So there's lots of repetition. This is the most common one: it appears 152 times. And then “context”, we also see lots of repetition – there's 106 of those contexts. So, this is a nice little method, we can see a lot about the data in a glance. And when I first saw this in this competition I thought: well this is actually not that much language data, when you think about it. The, you know… Each document is very short, you know, three or four words really, and lots of it is repeated. So that's like… as I'm looking through it I'm thinking, like, “what are some key features of this data set?”. And that would be something, I'd be thinking, well, that's, you know, we've got to do a lot with not very much unique data here. So here's how we can just go ahead and create a single string like I described which contains, you know, some kind of field separator, plus the context, the target and the anchor. So we're going to pop that into a field called “input”. Something slightly weird in pandas is there's two ways of referring to a column. You can use square brackets and a string to get the input column or you can just treat it as an attribute. When you're setting it, you should always use the form seen here (... df [‘input’]= …) When reading it you can use either. I tend to use this one because it's less typing. So you can see now we've got this/these concatenated rows. So, head() is the first few rows. So we've now got some documents to do NLP with. Now, the problem is, as you know from the last lesson, neural networks work with numbers. All right, we're going to take some"
fast.ai 2022 - Part 1,4,1766,Tokenization,"numbers and we're going to multiply them by matrices, we're going to replace the negatives with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that for these strings? So there's basically two steps we're going to take. The first step is to split each of these into tokens. Tokens are basically words. We're going to split it into words. There's a few problems with splitting things into words, though. The first is that some languages like chinese don't have words, right, or at least certainly not space separated words. And in fact in chinese it's sometimes… it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even… the pieces are not next to each other. Another reason is that, what we're going to be doing is, after we've split it into words, or something like words, we're going to be getting a list of all of the unique words that appear, which is called the vocabulary, and every one of those unique words is going to get a number. As you'll see later on the bigger the vocabulary, the more memory is going to get used, the more data we'll need to train. In general we don't want a vocabulary to be too big. So instead, nowadays, people tend to tokenize into something called subwords which is pieces of words – so I'll show you what it looks like. So the process of turning it into smaller units like words, it's called tokenization – and we call them tokens instead of words. The token is just like the more general concept of, like, whatever we're putting it into. So we're going to get Hugging Face transformers and Hugging Face datasets doing our work for us, and so, what we're going to do is we're going to turn our pandas DataFrame into a Hugging Face “datasets” Dataset. It's a bit confusing: pytorch has a class called Dataset and Hugging Face has a class called Dataset and they're different things, okay, so this is a Hugging Face Dataset. “Hugging Face datasets” dataset. So we can turn a DataFrame into a Dataset just using the from_pandas method and so we've now got a Dataset. So, if we take a look it just tells us: all right it's got these features, okay? And remember “input” is the one we just created with the concatenated strings and here's those 36,000 rows. Okay, so now we're going to do these two things. Tokenization, which is to split each text up into tokens, and the numericalization, which is to turn each token into its unique id based on where it is in the vocabulary. The vocabulary, remember, being the unique, the list of unique tokens. Now, particularly in this stage: tokenization, there's a lot of little decisions that have to be made. The good news is you don't have to make them because whatever pre-trained model you used the people that pre-trained it made some decisions, and you're going to have to do exactly the same thing, otherwise you'll end up with a different vocabulary to them and that's going to mess everything up. So that means before you start tokenizing you have to decide on what model to use. Hugging Face transformers is a lot like “timm”. It has a library of, I believe, hundreds of models. I guess I shouldn't say Hugging Face transformers. It's really the Hugging Face model hub. 44,000"
fast.ai 2022 - Part 1,4,2000,Huggingface model hub,"models, so even many more even than timm's image models. And so, these models, they vary in a couple of ways. There's a variety of different architectures, just like in “timm” but then something which is different to “timm” is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could type “patent” and see if there's any pre-trained patent: there is. Okay, so there's a patent, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often, thanks to the Hugging Face model hub, you can start your pre-trained model with something that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents. Having said that, there are some just generally pretty good models that work for a lot of things a lot of the time, and deberta-v3 is certainly one of those. This is a very new area. NLP has been, like, practically, really effective for, you know, general users, for only a year or two, whereas for computer vision it's been quite a while. So you'll see, you'll find that a lot of things aren't quite as well bedded down. I don't have a picture to show you of which models are the best or the fastest and the most accurate and whatever, right? This, a lot of this stuff is, like stuff that we're figuring out as a community using competitions like this, in fact. And this is one of the first NLP competitions, actually, in the kind of modern NLP era. So, you know, we've been studying these competitions closely and yes, I can tell you that deberta-v3 is actually a really good starting point for a lot of things so that's why we've picked it. So we pick our model and just like in “timm” for image, you know, models there's often going to be a small, a medium, a large and of course we should start with small, right, because small is going to be faster to train we're going to be able to do more iterations…. and so forth. Okay. So at this point remember the only reason we picked our model is because we have to make sure we tokenize in the same way. To tell transformers that we want to tokenize the same way that the people that built a model did, we use something called AutoTokenizer. It's nothing fancy, it's basically just a dictionary which says: “oh, which model uses which tokenizer?”. So when we say “AutoTokenizer.from_pretrained” it will download the vocabulary and the details about how this particular model tokenized the dataset. So, at this point we can now take that tokenizer and pass a string to it."
fast.ai 2022 - Part 1,4,2200,Examples of tokenized sentences,"So, if I pass the string “G’day folks, I’m Jeremy from fast.ai!” you'll see it's kind of putting it into words, kind of not. So if you've ever wondered whether “g'day” is one word or two you know it's actually three tokens according to this tokenizer. And “I'm” is three tokens. And “fast.ai” has three tokens. This punctuation is a token. And so, you kind of get the idea. These underscores here? That represents the start of a word, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital “I” in the middle of a word versus the start of a word, that kind of means a different thing. So this is what happens when we tokenize this sentence using the tokenizer that the deberta-v3 developers used. So here's a less common (unless you're a big platypus fan like me), less common sentence.: “A platypus is an ornithorhynchus anatinus”. So okay, in this particular vocabulary platypus got its own word, its own token, but ornithorhynchus didn't. And so I still remember grade one, for some reason our teacher got us all to learn how to spell “ornithorhynchus”, so, one of my favorite words. So you can see here it's been split into “_or”, “ni”, “tho”, “rhynch”, “us”. So every one of these tokens you see here is going to be in the vocabulary, right? The list of unique tokens that was created when this, when this particular model, this pre-trained model, was first trained. So somewhere in that list we'll find “_A” (“underscore capital A”), and it'll have a number and so that's how we'll be able to turn these into numbers. So this first process is called tokenization and then the thing where we take these tokens and turn them into numbers is called numericalization. So, our data set, remember we put our string into the “input” field so here's a function"
fast.ai 2022 - Part 1,4,2327,Numericalization,"that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our tokenization function. Tokenization can take a minute or two so we may as well get all of our processes used doing it at the same time to save some time. So if you use the “dataset dot map” it will parallelize that process, and just pass in your function. Make sure you pass “batched=True” so it can do a bunch at a time. Behind the scenes this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with “batched=True” it'll be able to do more stuff at once. So look it only took six seconds, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going to contain exactly the same as our original data set.) No sorry, it's not going to take exactly the same as the original data set, it's going to contain exactly the same input as our original data set and it's also going to contain a bunch of numbers. These numbers are the position in the vocabulary of each of the tokens in the string, so we've now successfully turned a string into a list of numbers. That is a great first step. We can see how this works, we can see for example that we've got “of” at this a separate word, so that's going to be an “_of” in the vocabulary we can grab the vocabulary, look up “_of”, find that it's 265 and check here: yep here it is 265. Okay, so it's not rocket science right? It's just looking stuff up in a dictionary to get the numbers. Okay, so that is the tokenization and numericalization necessary in NLP to turn our documents into numbers to allow us to put it into our model. Any questions so far John? Yeah, thanks Jeremy so there's a couple and this seems like a good time to throw them out – and it's related to how you've formatted your input data into these sentences that"
fast.ai 2022 - Part 1,4,2473,Question: rationale behind how input data was formatted,"you've just tokenized. So one question was really about: How you choose those keywords and the order of the fields that you know, so I guess just interested in an explanation, is it more art or science? how are you… No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards, you know, doesn't matter! We just want some way, something that it can learn from, right? So if I just concatenated it without these headers before each one, it wouldn't know where “abatement of pollution” ended and where “abatement” started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're so flexible… As long as you give it the information somehow, it doesn't really matter how you give it the information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't know, one semicolon here, and two here, and three here. Yeah it's not a big deal. At the level where you're, like, trying to get an extra half a percent to get up the leaderboard of a Kaggle competition you may find tweaking these things makes tiny differences, but in practice you won't generally find it matters too much. Right, thank you. And I guess the second part of that, somebody's asking: If one of their fields was particularly long, say it was a thousand characters, is there any special handling required there? Do you need to re-inject those kinds of special marker tokens? Does it change if you've got much bigger fields that you're trying to learn and query? Yes. Long documents and ULMFiT require no special consideration. IMDb in fact has multi thousand word movie reviews, and it works great. To this day, ULMFiT is probably the"
fast.ai 2022 - Part 1,4,2600,ULMFit fits large documents easily,"best approach for reasonably quickly and easily using large documents. Otherwise, if you use transformer-based approaches, large documents are challenging. Specifically, transformers basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if it works for you, but you know I'd certainly try both. For under 2,000 words, you know, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory. So, Hugging Face transformers has these, you know… As I say it right now, I find them somewhat obscure and not particularly well documented expectations about your data, that you kind of have to figure out, and one of those is that it expects that your target is a column called “labels”. So once I figured that out, I just went, got our tokenized DataSet, and renamed our “score” column to “labels”, and everything started working. I don't know if at some point they'll make this a bit more flexible, but it’s probably best to just call your target “labels” and life will be easy. You might have seen back when I went “ls {path}” that there was another data set there, called test.csv. And if you look at it, it looks a lot like our training set, that's our other CSV that we've been working with, but it's missing the score. The labels. This is called a test set – and so we're going to talk a little bit about that now because my claim here is that perhaps the most important idea in machine learning is the idea of having separate training, validation and test data sets."
fast.ai 2022 - Part 1,4,2755,Overfitting & underfitting,"Test and validation sets are all about identifying and controlling for something called “overfitting” and we're going to try and learn about this through example. This is the same information that's in that Kaggle notebook – I've just put it on some slides here. So I'm going to create a function here called plot_poly and I'm actually going to use the same data that, I don't know if you remember, we used it earlier for trying to fit this quadratic. We created some “x” and some “y” data. This is the data we're going to use and we're going to use this to look at overfitting. The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for those of you that remember, a first degree polynomial is just a line, it's ”y = a x”. A second degree polynomial will be ”y = ax^2 + bx + c”, third degree polynomial we'll have a cubic, fourth degree you know quartic, and so forth. And what I've done here is I've plotted what happens if we try to fit a line to our data. It doesn't fit very well. So what happened here is we… we did a linear regression… and what we're using here is a very cool library called scikit-learn. scikit-learn is something that, you know, I think it'd be fair to say it's mainly designed for kind of classic machine learning methods like, kind of linear regression and stuff like that – I mean, very advanced versions of these things, but it's also great for doing these quick and dirty things. So in this case I wanted to do a… what's called a polynomial regression… which is fitting the polynomial to data and it's just these two lines of code. It's a super nice library. So in this case, a degree one polynomial is just a line, so I fit it, and then I show it with the data, and there it is. Now that's what we call underfit, which is to say there's not enough, kind of, complexity in this model I fit, to match the data that's there. So an underfit model is a problem. It's got to be systematically biased, you know; all the stuff up here, we're going to be predicting too low; all the stuff down here, we're predicting too low; all the stuff in the middle, we’ll be predicting too high. A common misunderstanding is that simpler models are kind of more reliable in some way, but models that are too simple will be systematically incorrect as you see here. What happens if we fit a 10 degree polynomial? That's not great either! In this case it's not really showing us what the actual… Remember this was originally a quadratic. This is meant to match, right? And particularly at the ends here, it's predicting things that are way above what we would expect in real life right? And it's trying to get… really it's trying really hard to get through this point, but clearly this point was just some noise, right? So this is what we call “overfit”. It's done a good job of fitting to our exact data points, but if we sample some more data points from this distribution, honestly we probably would suspect they're not going to be very close to this, particularly if they're a bit beyond the edges. So that's what overfitting looks like. We don't want underfitting or overfitting. Now underfitting is actually pretty easy to recognize, because we can actually look at our training data and see that it's not very close. Overfitting is a bit harder to recognize because the training data is actually very close. Now on the other hand, here's what happens if we fit a quadratic. And here I've got both the real-line and the fit-line and you can see they're pretty close, and that's of course what we actually want. So how do we tell whether we have something more like this, or something more like this. Well what we do is we do something pretty straightforward… is we take our original data set, these points, and we remove a few of them, so let's say 20% of them."
fast.ai 2022 - Part 1,4,3045,Splitting the dataset,"We then fit our model using only those points we haven't removed, and then we measure how good it is by looking at only the points we removed. So in this case let's say we had removed… (I'm just trying to think) If I had removed this point here right, then it might have kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away. The model… the data that we take away and don't let the model see it when it's training is called the “validation set.” So in fast.ai we've seen splitters before, right… The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so things like accuracy, measured only on the validation set. This is really unusual. Most libraries make it really easy to shoot yourself in the foot, by not having a validation set, or accidentally not using it correctly. So fast.ai won't even let you do that. So you've got to be particularly careful when using other libraries. HuggingFace transformers is good about this, so they make sure that they do show you your metrics on a validation set. Now creating a good validation set is not generally as simple as just randomly pulling"
fast.ai 2022 - Part 1,4,3151,Creating a good validation set,"some of your data out of your model, out of the data that you passed… that you train for your model. The reason why is… imagine that this was the data you were trying to fit something to (okay) and you randomly remove some, so it looks like this. That looks very easy doesn't it, because you've kind of like, still got all the data you'd want around the points, and in a time series like this… this is dates and sales… in real life you're probably going to want to predict future dates. So if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate and remove the last couple of weeks. So if this was your validation set and this is your training set, that's going to be actually testing whether you can use this to predict the future, rather than using it to predict the past. Kaggle competitions are a fantastic way to test your ability to create a good validation set, because Kaggle competitions only allow you to submit, generally, a couple of times a day. The dataset that you are scored on in the leaderboard during that time is actually only a small subset… in fact it's a totally separate subset… to the one you'll be scored on, on the end of the competition. And so most beginners on Kaggle overfit. And it's not until you've done it that you'll get that visceral feeling of like: “oh my god, I overfit.” In the real world outside of Kaggle you will often not even know that you overfit – you just destroy value for your organization silently. So it's a really good idea to do this kind of stuff on Kaggle a few times first, in real competitions, to really make sure that you are confident you know how to avoid overfitting – how to find a good validation set and how to interpret it correctly. And you really don't get that until you screw it up a few times. A good example of this was… there was a distracted driver competition on Kaggle – there are these kind of pictures from inside a car, and the idea was that you had to try and predict whether somebody was driving in a distracted way or not, and on Kaggle they did something pretty smart… the test set, so the thing that they scored you on the leaderboard, contained people that didn't exist, at all, in the competition data that you train the model with. So if you wanted to create an effective validation set in this competition, you would have to make sure that you separated the photos, so that your validation set contained photos of people that aren't in the data you're training your model on. There's another one like that, the Kaggle fisheries competition, which had boats that didn't appear… so they were basically pictures of boats and you meant to try to guess/predict what fish were in the pictures. And it turned out that a lot of people accidentally figured out what the fish were by looking at the boat, because certain boats tended to catch certain kinds of fish. And so by messing up their validation set, they were really overconfident of the accuracy of their model. I'll mention in passing, if you've been around Kaggle a bit, you'll see people talk about cross-validation a lot. I'm just going to mention, be very very careful. Cross-validation is explicitly not about building a good validation set, so you've got to be super super careful if you ever do that. Another thing I'll mention, is that scikit-learn conveniently offers something called train_test_split, as does Hugging Face datasets, as does fast.ai – we have something called random splitter. It can be encouraging… it can almost feel like it's encouraging you to use a randomized validation set because there are these methods that do it for you. But yeah, be very very careful, because very very often that's not what you want, okay. So we've learned what a validation set is, so that's the bit that you pull out of your data that you don't train with, but you do measure your accuracy with. So what's a test set? It's basically another validation set, but you don't even use it for tracking your accuracy while you build"
fast.ai 2022 - Part 1,4,3433,Test set,"your model. Why not? Well imagine you tried two new models every day for three months (that's how long a Kaggle competition goes for.) So you would have tried 180 models, and then you look at the accuracy on the validation set for each one. Some of those models you would have got a good accuracy on the validation set, potentially because of pure chance, just a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually overfit using the validation set. So you actually want to know whether you've really found a good model or not. So in fact on Kaggle they have two test sets. They've got the one that gives you feedback on the leaderboard during the competition and a second test set which you don't get to see until after the competition is finished. So in real life you've got to be very careful about this, not to try so many models during your model building process that you accidentally find one that's good by coincidence. And only if you have a test set that you've held out, or you know that. Now that leads to the obvious question which is very challenging, is you spent three months working on a model, worked well on your validation set, you did a good job of locking that test set away in a safe so you weren't allowed to use it, and at the end of the three months you finally checked it on the test set, and it's terrible. What do you do? Honestly you have to go back to square one. You know there really isn't any choice other than starting again. So this is tough, but it's better to know, right. Better to know than to not know, so that's what a test set is for."
fast.ai 2022 - Part 1,4,3540,Metric vs loss,"So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something like “accuracy”. It's a number that tells you: How good is your model? Now on Kaggle this is very easy. What metric should we use? Well they tell us… go to overview, click on evaluation, and find out… and it says: oh, we will evaluate on the Pearson Correlation Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that we will take the derivative of, and find the gradient, and use that to improve our parameters during training? And the answer is: maybe, sometimes, but probably not. For example, consider accuracy. Now, if we were using accuracy to calculate our derivative and get the gradient, you could have a model that's actually slightly better, you know, it's slightly like it's doing a better job of recognizing dogs and cats, but not so much better that it's actually caused any incorrectly classified cat to become a dog. So the accuracy doesn't change at all. So the gradient is zero. You don't want stuff like that. You don't want bumpy functions because they don't have nice gradients – often they don't have gradients at all, they're basically zero nearly everywhere. You want a function that's nice and smooth. Something like, for instance, the average absolute error, mean absolute error, which we've used before. So that's the difference between your metrics and your loss. Now be careful, right, because when you're training your model's spending all of its time trying to improve the loss and most of the time that's not the same as a thing you actually care about, which is your metric. So you've got to keep those two different things in mind. The other thing to keep in mind is that in real life… you can't go to a website and be told what metric to use. In real life… the model that you choose, there isn't one"
fast.ai 2022 - Part 1,4,3687,The problem with metrics,"number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex process often involving humans, both as users and customers and as people, you know, involved in… as part of the process. There's all kinds of things that are changing over time and there's lots and lots of outcomes of decisions that are made. One metric is not enough to capture all of that. Unfortunately, because it's so convenient to pick one metric and use that to say: I've got a good model, that very often finds its way into industry, into government… where people roll out these things that are good on the one metric that happened to be easy to measure. And again and again we found people's lives turned upside down because of how badly they get screwed up by models that have been incorrectly measured using a single metric. So my partner Rachel Thomas has written this article which I recommend you read about “The problem with metrics is a big problem for AI” It's not just an AI thing! There's actually this thing called “Goodhart’s Law” that states “when a measure becomes a target, it ceases to be a good measure.” The thing is… so when I was a management consultant, you know, 20 years ago, we were always kind of part of these strategic things trying to like: find key performance indicators and ways to kind of, you know, set commission rates for sales people and we were really doing a lot of this, like, stuff which is basically about picking metrics and, you know, we see that happen… go wrong in industry all the time. AI is dramatically worse because AI is so good at optimizing metrics, and so that's why you have to be extra, extra, extra careful about metrics, when you are trying to use a model in real life. Anyway, as I said in Kaggle, we don't have to worry about any of that, we're just going to use the “Pearson correlation coefficient” which is all very well as long as you know what the hell the “Pearson correlation coefficient” is… If you don't, let's learn about it. So “Pearson correlation coefficient” is usually abbreviated"
fast.ai 2022 - Part 1,4,3850,Pearson correlation,"using letter “r” and it's the most widely used measure of how similar two variables are. And so, if your predictions are very similar to the real values then the “Pearson correlation coefficient” will be high, and that's what you want. “r” can be between minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct. Generally speaking, in courses or textbooks, when they teach you about the “Pearson Correlation Coefficient”, at that point… at this point, they will show you a mathematical function. I'm not going to do that because that tells you nothing about the “Pearson Correlation Coefficient.” What we actually care about is not the mathematical function, but how it behaves; and I find most people, even who work in data science, have not actually looked at a bunch of data sets to understand how “r” behaves. So let's do that right now so that you're not one of those people. The best way I find to understand how data behaves in real life, is to look at real-life data so there's a data set… scikit-learn comes with a number of data sets, and one of them is called “California housing” and it's a data set where each row is a district… and, it's kind of demographic, sorry it's information… some demographic information about different districts, and about the value of houses in that district. I’m not going to try to plot the whole thing, it's too big, and this is a very common question I have from people is: “how do I plot data sets with far too many points?” The answer is very simple: get less points. So I just randomly grab a thousand points. Whatever you see with a thousand points, is going to be the same as what you see with a million points. There's no point… no reason, to plot huge amounts of data generally just grab a random sample. Now, numpy has something called “corecoeff()” to get the correlation coefficient between every variable and every other variable, and it returns a matrix. So I can look down here, and so for example, here is the correlation coefficient between variable one, and variable one. Which of course is exactly perfectly 1.0. Right? because variable one is the same as variable one. Here is the small inverse correlation between variable one and variable two, and medium-sized positive correlation between variable one and variable 3 and so forth. This is symmetric about the diagonal because the correlation between variable 1 and variable 8 is the same as the correlation between variable 8 and variable 1. So this is a correlation coefficient matrix. So that's great when we wanted to get a bunch of values all at once. For the Kaggle competition we don't want that. We just want a single correlation number. If we just pass in a pair of variables, we still get a matrix which is kind of weird.. it's kind of… it's not weird, but it's not what we want! So we should grab one of these. So when I want to grab a correlation coefficient, I'll just return the zeroth row, first column. So that's what “core” is. That's going to be our single correlation coefficient. So let's look at the correlation between two things; for example… median income, and median house value: 0.67. Okay? Is that high? medium? low? How big is that? What does it look like? So the main thing we need to understand is: what these things look like. So what I suggest we do is: we're going to take a 10 minute break… nine minute break. We'll come back at half past, and then we're going to look at some examples of correlation coefficients Okay. Welcome back! So what I've done here is… I've created a little function called show correlations, and I'm passing a DataFrame and a couple of columns as strings. I'm going to grab each of those columns as series, do a scatter plot, and then show the correlation. So, we already mentioned “median income” and “median house value” of 0.68, so here it is… here's what .68 looks like. So you know I don't know if you had some intuition about what you expected, but as you can see it's still plenty of variation, even at that reasonably high correlation. Also, you can see here that visualizing your data is very important if you're working with this data set, because you can immediately see all these dots along here. That's clearly truncation right? So this is like, when... it's not until you look at pictures like this, that you're gonna pick stuff like this up. Pictures are great! Oh! little trick: on the scatter plot, I put alpha as 0.5, that creates some transparency. For these kind of scatter plots, that really helps, because it… like… kind of creates darker areas in places where there's lots of dots. So, yeah, alpha in scatter plots is nice. Okay, here's another pair. So this one's gone down from 0.68 to 0.43. Median income versus the number of rooms per house. As you'd expect more rooms… it's more income, but this is a very weird looking thing. Now, you'll find that a lot of these statistical measures like correlation rely on the square of the difference, and when you have big outliers like this, the square of the difference goes crazy, and so this is another place we'd want to look at the data first, and say oh that's… that's going to be a bit of an issue. There's probably"
fast.ai 2022 - Part 1,4,4227,Correlation is sensitive to outliers,"more correlation here, but there's a few examples of some houses with lots and lots of rooms where people that aren't very rich live. Maybe these are some kind of shared… shared accommodation or something? So, “r” is very sensitive to outliers. So let's get rid of the houses… the rooms with 15 rooms… the houses with 15 rooms or more, and now you can see it's gone up from 0.43 to 0.68, even though we probably only got rid of one two three four five six seven… got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row. So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Because 0.34 sounds like quite a good relationship, but you almost can't see it! So this is something I strongly suggest is, if you're working with a new metric, is: Draw some pictures of a few different levels of that metric to kind of try to get a feel for… like… what does it mean? You know, what does 0.6 look like? What does 0.3 look like? And so forth. And here's an example of a correlation of minus 0.2. This very slight, negative slope. Okay, so there's just more of a kind of a general tip, of something I like to do when playing with a new metric, and I recommend you do as well. I think we've now got a sense of what the correlation feels like. Now you can go look up the equation on Wikipedia if you're into that kind of thing. We need to report the correlation after each epoch because we want to know how our training is going. Hugging Face expects you to return a dictionary because it's going to use the keys of the dictionary to like… label… each metric. So here's something that gets the correlation, and returns it as a dictionary with the label “pearson”. Okay, so we've done metrics, we've done our training/ validation split. Oh! we might have actually skipped over the bit where we actually did the split! Did I? I did! So, to actually do the split, because in this Kaggle competition – I've got another notebook, we'll look at later, where we actually split this properly — but here we're just going to do a random split. Just to keep things simple for now, of 25 percent, will be… of the data will be a validation set. So, if we go tok_ds.train_test_split() it returns a data set dict; which has a train, and a test. So that looks a lot like a datasets object in fast.ai. Very similar idea! So this will be the thing that we'll be able to train with, so it's going to train with this data set, and return the metrics on this data set. This is really a validation set but Hugging Face datasets calls it “test”."
fast.ai 2022 - Part 1,4,4440,Training a model,"Okay. We're now ready to train our model. In fast.ai, we use something called a “learner.” The equivalent in Hugging Face transformers is called “trainer”. So we'll bring that in; something we'll learn about quite shortly is the idea of “mini batches” and “batch sizes.” In short, each time we pass some data to our model for training, it's going to return… it's going to send through a few rows at a time to the GPU, so that it can calculate those in parallel. Those… a bunch of rows is called a “batch” or a “mini batch'' and the number of rows is called the “batch size.” So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll get an “out of memory” error on your GPU. So, you know, it's a bit of trial and error to find a batch size that works. “Epochs” we've seen before. Then we've got the “learning rate.” We'll talk in the next lesson – unless we get to this lesson – about a technique to automatically find a… or semi-automatically find a “good” learning rate. We already know what a learning rate is from the last lesson. I've played around and found one that seems to train quite quickly without falling apart, so I just tried a few. Generally, I kind of, you know, if I… if I don't have a so… Hugging Face transformers doesn't have something to help you find the learning rate. This the integration we're doing in fast.ai, will let you do that, but if you're using a framework that doesn't have that, you can just start with a really low learning rate, and then kind of double it, and keep doubling it until it falls apart. Hugging Face transformers uses this thing called “training arguments” which is a class where you just provide all of the kind of configuration… so you have to… tell it what your learning rate is. This stuff here is the same as what we call basically fit_one_cycle() in fast.ai. You always want this to be true, because it's going to be faster… pretty much… and then the… this stuff here, you can probably use exactly the same every time. There's probably a lot of boilerplate compared to fast.ai as you see. This stuff you can probably use the same every time. Okay, so… We now need to create our model. So, the equivalent of the vision learner function that we've used to automatically create a reasonable vision model? In Hugging Face transformers, they've got lots of different ones depending on what you're trying to do. So, we're trying to do classification as we've discussed, of sequences, so if we call AutoModelForSequenceClassification, it will create a model that is appropriate for classifying sequences from a train… pre-trained model, and this is the name of the model that we did earlier the deberta-v3. It has to know when it adds that random matrix to the end, how many outputs it needs to have. So we have one label which is the score. So that's going to create our model, and then this is the equivalent of creating a learner. It contains a model, and the data… the training data, and the test data. Again, there's a lot more boilerplate here than fast.ai, but you can kind of see the same basic steps here. We just have to do a little bit more manually, but it's not… you know, it's nothing too crazy. So, it's going to tokenize it for us using that function, and then these are the metrics… that will print out each time. That's that little function we created which returns a dictionary. At the moment I find Hugging Face transformers very verbose. It spits out lots and lots and lots of text which you can ignore, and we can finally call train, which will spit out much more text again, which you can ignore, and as you can see, as it trains, it's printing out the loss, and here's our Pearson correlation coefficient. So, it's training and we've got a 0.834 correlation, that's pretty cool! Right. I mean it took what …? Oh here we are five minutes to run. Maybe that's five minutes per epoch on Kaggle which doesn't have particularly great GPUs (but good for free) and we've got something that is you know got a very high level of correlation in assessing how similar the two columns are, and the only reason it could do that is because it used a pre-trained model, right. There's no way you could just have that tiny amount of information and figure out whether those two columns are very similar. This pre-trained model already knows a lot about language. It already has a good sense of whether two phrases are similar or not, and we've just fine-tuned it. You can see, given that after one epoch it was already at 0.8. You know we… this was a model that already did something pretty close to what we needed. It didn't really need that much extra tuning for this particular task. We got any questions there John? “Yeah we do! It's actually a bit back on the topic"
fast.ai 2022 - Part 1,4,4760,Question: when is it ok to remove outliers?,"before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how do you decide when it's okay to remove outliers? Like, you… pointed out some in that data set, and clearly your model is going to train a lot better if you clean that up; but I think Kevin's point here is, you know, those kinds of outliers will probably exist in the test set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense.” So, outliers should never just be removed, like, for modeling… So if we take the example of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, “oh that's interesting! it seems like there's a separate group of districts with a different kind of behavior”. Yeah my guess is that they're going to be kind of like dorms or something like that, you know, probably low-income housing and so I would be saying like, “oh clearly, from looking at this dataset, these two different groups can't be treated the same way, they have very different behaviors, and I would probably split them into two separate analyses. You know the... the word outlier... it kind of exists in a statistical sense, right? There can be things that are well outside our normal distribution and mess up our kind of metrics and things. It doesn't exist in a real sense. It doesn't exist in a sense of like... oh... things that we should, like, ignore or throw away. You know, some of the most useful kind of insights I've had in my life in data projects has been by digging into outliers...so-called outliers... and understanding: well, what are they? And where did they come from? …and it's kind of... often in those edge cases that you discover really important things about, like, where processes go wrong – or about, you know, kinds of behaviors you didn't even know existed, or indeed about, you know, kind of labeling problems or process problems which you really want to fix them at the source because otherwise when you go into production you're going to have more of those so-called outliers. So yeah. I'd say never delete outliers without investigating them and having a strategy for ...like... understanding where they came from and ...like... what should you do about them. All right. So now that we've got a trained model, you'll see that it actually behaves,"
fast.ai 2022 - Part 1,4,4930,Predictions,"you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, “oh yeah this looks like stuff I've seen before”, you know, like a bit more wordy and some slight changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're going to pass in our dataset from the Kaggle test file – and so that's going to give us our predictions, which we can cast to float. And here they are. So here are the predictions we made of similarity. Now again, not just for your inputs but also for your outputs, always look at them. Always. Right? And interestingly, I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly all of them had the problem we have right now, which is negative predictions and predictions over one. So I'll be showing you how to fix this in a more proper way, maybe hopefully in the next lesson but for now you know we could at least just round these off ...right? …because we know that none of the scores are going to be bigger than one or smaller than zero, so our correlation coefficient will definitely improve if we at least round this up to zero and round this down to one. As I said, there are better ways to do this but that's certainly better than nothing. So, in Pytorch, you might remember from when we looked at ReLU, there's a thing called clip and that will clip everything under zero to zero and everything over one to one and so now that looks much better. So here's our predictions. So Kaggle expects submissions to generally be in a CSV file and Hugging Face datasets... it kind of looks a lot like pandas, really. We can create our submission file from... with our two columns called dot csv… and there we go. That's basically it. So yeah you know... it's... it's... it's kind of nice to see how... you know... it's a sense how far deep learning has come since we started this course a few years ago. That nowadays you know... there are multiple libraries around to kind of do this… the same thing. We can, you know, use them in multiple application areas. They all look kind of pretty familiar. They're reasonably beginner friendly. And NLP, because it's kind of like the most recent area that's really become effective in the last year or two, is probably where the biggest opportunities are for, you know, big wins both in research and commercialization. And"
fast.ai 2022 - Part 1,4,5130,Opportunities for research and startups,"so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like “oh why now?”.. you know... “why... why would you build this company now?” And of course you know with NLP, the answer is really simple. It's like... it can often be like... “well until last year this wasn't possible” you know, or “it took 10 times more time” or “it took 10 times more money” or whatever. So I think NLP is a huge opportunity area. It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit."
fast.ai 2022 - Part 1,4,5176,Misusing NLP,"Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it. So the question I want you to be thinking about is: What subreddit do you think this comes from? …this debate about military spending? And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model – they're much much better now – so even then you could see these models were generating context appropriate believable prose. You know I would strongly believe that like any of our... kind of like... upper tier of competent fast.ai alumni would be fairly easily able to create a bot which could create context appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side of an argument and you could scale that up such that 99% of twitter was these bots and nobody would know. You know, nobody would know. And that's very worrying to me because a lot of, you know, a lot of...kind of... the way people see the world is now really coming out of their... their social media conversations, which at this point they're controllable. Like... it would not be that hard to create something that's kind of optimized towards moving a point of view amongst a billion people, you know, in a very subtle way, very gradually, over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth. Here is the start of an article in the Guardian which I'll let you read. This article was, you know, quite long. These are just the first few paragraphs and at the end, it explains that this article was written by GPT3. It was given the instruction “please write a short op-ed around 500 words. Keep the language simple and concise. Focus on why humans have nothing to fear from AI.” So GPT3 produced eight outputs and then they say, basically the... the editors at The Guardian did about the same level of editing that they would do for humans. In fact, they found it a bit less editing required than humans. So you know again, like, you can create longer pieces of context appropriate prose designed to argue a particular point of view. What kind of things might this be used for? You know, that we won't know probably for decades if ever but sometimes we get a clue based on older technology. Here's something from back 2017 and the pre ...kind of... deep learning NLP days. There were millions of submissions to the FTC about the net neutrality situation in America. Very very heavily biased towards the point of view of saying “we want to get rid of net neutrality”. An analysis by Jeff Kao showed that something like 99% of them and in particular, nearly all of the ones which were pro removal of net neutrality, were clearly auto generated by basically ...if you look at the green, there's like, selecting from a menu. So we've got... Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire… individuals as opposed to Washington bureaucrats should be blah...blah... people like me as opposed to so-called experts... and you get the idea. Now this is an example of a very very, you know, simple approach to auto-generating huge amounts of text. We don't know for sure but it looks like this might have been successful because this went through. You know, despite what seems to be actually overwhelming disagreement from the public that everybody, almost everybody, likes net neutrality, the FTC got rid of it and this was a big part of the basis. It’s like “oh we got all these comments from the public and everybody said they don't want net neutrality”. So imagine a similar thing where you absolutely couldn't do this. You couldn't figure it out because everyone was really very compelling and very different. That's, you know, it's kind of worrying about how we deal with that. You know, I will say... when I talk about this stuff, often people say “ah no worries we'll build a model to recognize... you know... bot generated content” but, you know, if I put my black hat on, I'm like “no that's not gonna work, right?”. If you told me to build something that beats the bot classifiers, I'd say “no worries, easy. You know, I will take the... the code or the service... or service... whatever that does the bot classifying and I will include beating that in my loss function and I will fine-tune my model until it beats the bot classifier. You know, when I used to run an email company, we had a similar problem with spam prevention and our spammers could always take a spam prevention algorithm and change their emails until it didn't get the spam prevention algorithm anymore, for example. So yes, I... I'm really excited about the opportunities for... for students in this course to build, you know, I think very valuable businesses, really cool research and so forth using these pretty new NLP techniques that are now pretty accessible and I'm also really worried about the things that might go wrong. I do think though that the more people that understand these capabilities the less chance they'll go wrong. John, was there some questions? “Yeah I mean it's a throwback to the... to the workbook that you had before... yeah"
fast.ai 2022 - Part 1,4,5580,Question: isn’t the target categorical in this case?,"that's the one. The question Manikandan is asking... shouldn't num labels be five zero zero point two five zero point five zero point seven five one instead of one? Isn't the target a categorical, or are we considering this as a regression problem?” Yeah it's a good question. So there's one label because there's one column. Even if this was being treated as a categorical problem with five categories, it's still considered one label. In this case though, we're actually treating it as a regression problem. It's one of the things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which is actually why we ended up with predictions that were less than zero and bigger than one. So we'll be learning next time about the use of sigmoid functions to resolve this problem and that should fix it up for us Okay, great. Well thanks everybody. I hope you enjoyed learning about NLP as much as I enjoyed putting this together. I'm really excited about it and can't wait for next week's lesson. See ya!"
fast.ai 2022 - Part 1,5,0,Introduction,okay hi everybody and um welcome to practical deep learning for coders lesson five um we're uh at a stage now where we're going to be getting deeper and deeper into the details of how these networks actually work last week we saw how to use a slightly lower level library than fast ai being hugging face transformers to train a pretty um nice nlp model and today we're going to be going back to tabular data and we're going to be trying to build a tabular model actually from scratch we're going to build a couple of different types of tabular model from scratch so the problem that i'm going to be working through is the uh the titanic problem which if you remember back a couple of weeks is the data set that we looked at um on microsoft excel and it has each row is one passenger on the titanic so this is a real world data set um this direct data set tells you both that passenger survived what class they were on in the ship their sex age how many siblings how many other family members how much they spent the fair and whereabouts they embarked on three different cities and you might remember that we built a linear model we then did the same thing using matrix multiplication and we also created a very very simple neural network um you know excel can do nearly everything we need as you saw to build a neural network but it starts to get um unwieldy and so that's why people don't use excel for neural networks in practice instead we use a programming language like python so what we're going to do today is we're going to do the same thing with python so we're going to start working through the linear model and neural net from scratch notebook which you can find on kaggle um or on the course repository and today what we're going
fast.ai 2022 - Part 1,5,119,Linear model and neural net from scratch,to do is we're going to work through the one in the clean folder so both for fastbook the book and course 22 these lessons the clean folder contains all of our notebooks but without any pros or any outputs so here's what it looks like when i open up the linear model and neural net from scratch and jupiter what i'm using here is paper space gradient which as i mentioned a couple of weeks ago is what i'm going to be doing most things in it looks a little bit different to the normal paper space gradient because the the default view for paper space gradient at least as i do this course is their rather awkward notebook editor which at first glance has the same features as the the real jupiter notebook and jupiter lab environments um but in practice uh actually missing lots of things so this is the the normal paper space so remember you have to click this button okay and um the only reason you might keep this window running is then you might go over here to the machine to remind yourself when you close the other tab to click stop machine if you're using the free one it doesn't matter too much and also when i started i make sure i've got something to set to shut down automatically if case i forget um so other than that we're going to we can stay in this tab and because uh jupiter does this is jupiter lab that that runs um and you can always switch over to classic jupiter notebook if you want to um so given that they've kind of got tabs inside tabs i normally maximize it at this point and it's really good it's really helpful to know the keyboard shortcuts so ctrl shift square bracket right and left switch between tabs that's one of the key things to know about okay so i've opened up the clean version of the um linear model and neural net from scratch notebook and so remember when you go back through the video kind of the second time or through the notebook a second time this is generally what you want to be doing is going through the clean notebook and before you run each cell try to think about like oh what did jeremy say why why are we doing this what output would i expect make sure you get the output you'd expect and if you're not sure why something is the way it is try changing it and see what happens and then if you're still not sure well why did that thing not work the way i expect you know search the forums if anybody's asked that question before and you can ask the question on the forum yourself if you're still not sure um so as i think we've mentioned briefly before i find it really nice to be able to use the same notebook both on kaggle and off kaggle so most of my notebooks start with basically the same cell which is something that just checks whether we're on kaggle so kaggle sets an environment variable so we can just check for it that way we know if we're on kaggle and so then if we are on kaggle you know a notebook that's part of a competition will already have the data downloaded and unzipped for you uh otherwise if i haven't downloaded the data before then i need to download it and unzip it okay so kaggle is a pip installable module so you type pip install kaggle if you're not sure how to do that you should check our deep dive lessons to see exactly the steps but roughly speaking you can use your console pip install and whatever you want to install or as we've seen before you can do it directly in a notebook by putting an exclamation mark at the start so that's going to run not python but a shell command okay so that's enough to ensure that we have the data downloaded and a variable called path that's pointing at it um most of the time we're going to be using at least pi torch and numpy so we import those so they're available to python and when we're working with tabular data as we've talked about before we're generally also going to want to use pandas and it's really important that you're somewhat familiar with the kind of basic api of these three libraries and i've recommended wes mckinney's book before particularly for these ones one thing just by the way is that these things tend to assume you've got a very narrow screen which is really annoying because it always wraps things so if you want to put these three lines as well then it just makes sure that everything is going to use up the screen properly okay so as we've seen before you can read a common separated values file with pandas
fast.ai 2022 - Part 1,5,450,Cleaning the data,and you can take a look at the first few lines in the last few lines and how big it is and so here's the same thing as our spreadsheet okay so there's our data from the spreadsheet and here it is as a data frame so if we go dataframe.is n a that returns a new data frame in which every column it tells us whether or not that particular value is nan so nan is not a number and most written nor the most common reason you get that is because it was missing okay so a missing value is obviously not a number um so we um in the excel version we did something you should never usually do we deleted all the rows with missing data just because in excel it's a little bit harder to work with in pandas it's very easy to work with first of all we can just sum up what i just showed you now if you call sum on a data frame it sums up each column right so you can see that there's kind of some small foundational concepts in pandas which when you put them together take you a long way so one idea is this idea that you can par you can call a method on a data frame and it calls it on every row and then you can call a reduction on that and it reduces each column and so now we've got the total and in python and pandas and numpy and pi torch you can treat a boolean as a number and true will be one false will be zero so this is the number of missing values in each column so we can see that cabin out of 891 rows it's nearly always empty age is empty a bit of the time embarked is almost never empty so if you remember from excel we need to multiply a coefficient by each column that's how we create a linear model so how would you multiply a coefficient by a missing value you can't there's lots of ways of it's called imputing missing values so replacing missing value with a number the easiest which always works is to replace missing values with the mode of a column the mode is the most common value that works both the categorical variables it's the most common category and continuous variables that's the most common number so you can get the mode by calling df.node one thing that's a bit awkward is that if there's a tie for the mode so there's more than one thing that's that's the most common it's going to return multiple rows so i need to return the zeroth row so here is the mode of every column so we can replace the missing values for age with 24 and the missing values for cabin with b96 b98 and embarked with s um i'll just mention in passing i am not going to describe every single method we call in every single function we use and that is not because you're an idiot if you don't already know them nobody knows them all right but i don't know which particular subset of them you don't know right so let's assume just to pick a number at random that the average fastai student knows 80 of the functions we call um then i could tell you what every function is in which case 80 of the time i'm wasting your time because i already know um or i could pick 20 of them at random in which case i'm still not helping because most the time it's not the ones you don't know my approach is that for the ones that are pretty common i'm just not going to mention it at all because i'm assuming that you're google it right so it's really important to know so for example if you don't know what ilock is that's not a problem it doesn't mean you're stupid right it just means you haven't used it yet and you should google it right so i mentioned in this particular case you know this is one of the most important pandas methods because it gives you the row located at this index i for index and lock for location so this is the zeroth row but yeah i do kind of go through things a little bit quickly on the assumption that students fast ai students are you know proactive curious people and if you're not a proactive curious person then you could either decide to become one for the purpose of this course or maybe this course isn't for you um all right so a data frame um has a very convenient method called fill n a and that's going to replace the not a numbers with whatever i put here and the nice thing about pandas is it kind of has this understanding that columns match to columns so it's going to take the the mode from each column and match it to the same column in the data frame and fill in those missing values normally that would return a new data frame many things including this one in pandas have an in-place argument that says actually modify the original one and so if i run that now if i call dot is n a dot sum 0. so that's like the world's simplest way to get rid of missing values okay so why did we do it the world's simplest way because honestly this doesn't make much difference most of the time and so i'm not going to spend time the first time i go through and build a baseline model doing complicated things when i don't necessarily know that i need complicated things and so imputing missing values is an example of something that most of the time this dumb way which always works without even thinking about it will be quite good enough you know for nearly all the time so we keep things simple where we can john question jeremy we've got a question on this topic um javier is sort of commenting on the assumption involved in substituting with the mode and he's asking in your experience what are the pros and cons of doing this versus for example discarding cabin or age as fields that we even train the model yeah so i would certainly never throw them out right um there's just no reason to throw away data and there's lots of reasons to not throw away data so for example when we use the fast ai library which we'll use later one of the things it does which is actually a really good idea is it creates a new column for everything that's got missing values which is boolean which is did that column have a missing value for this row and so maybe it turns out that cabin being empty is a great predictor so yeah i don't throw out rows and i don't throw out columns okay so um it's helpful to understand a bit more about our data set and a really helpful um uh i've already imported this um a really helpful you know quick method and again it's kind of nice to know like a few quick things you can do to get a picture of what's happening in your data is describe and so describe you can say okay describe all the numeric variables and that gives me a quick sense of what's going on here so we can see survived clearly is just zeros and ones because all of the quartiles are zeros and ones looks like p class is one two three um what else do we see fair's an interesting one right lots of smallish numbers and one really big number so probably long tailed um so it's yeah good to have a look at this to see what's what's going on for your numeric variables so as i said fair looks kind of interesting uh to find out what's going on there i would generally go with a histogram so if you can't quite remember what a histogram is again google it but in in short it shows you for each amount of fare how often does that fare appear and it shows me here that the vast majority of fares are less than 50 dollars but there's a few right up here to 500 so this is what we call a long tail distribution a small number of really big values and lots of small ones um there are some types of model which do not like long tail distributions linear models is certainly one of them and neural nets are generally better behaved without them as well luckily there's a almost surefire way to turn a long tail distribution into a more reasonably centered distribution and that is to take the log we use logs a lot in machine learning for those of you that haven't touched them since year 10 math it would be a very good time to like go to khan academy or something and remind yourself about what logs are and what they look like because they're actually really really important um but the basic shape of the log curve causes it to make you know really big numbers less really big and doesn't change really small numbers very much at all so if we take the log now log of 0 is nan so a useful trick is to just do log plus one and in fact there is a log p1 if you want to do that it does the same thing um so if we look at the histogram of that you can see it's much more you know sensible now it's kind of centered and it doesn't have this big long tail so that's pretty good so we'll be using that column in the future as a rule of thumb stuff like money or population things that kind of can grow exponentially you very often want to take the log of so if you have a column with a dollar sign on it that's a good sign it might be something to take the log of so there was another one here which is we had a numeric which actually doesn't look numeric at all it looks like it's actually categories so um pandas gives us a dot unique and so we can see yep they're just one two and three are all the levels of p class that's their first class second class or third class um we can also describe all the non-numeric variables and so we can see here that not surprisingly names are unique because the count of names is the same as count unique there's two sexes 681 different tickets 147 different cabins and three levels of embarked so we cannot multiply the letter s by a coefficient or the word male by a coefficient so what do we do what we do is we create something called dummy variables dummy variables are and we can just go get dummies a column that says for example is sex female is sex male is p class one is p class two is p class three so for every possible level of every possible categorical variable it's a boolean column of did that row have that value of that column um so i think we've briefly talked about this before that there's a couple of different ways we can do this one is that for n an n level categorical variable we could use n minus one levels in which case we also need a constant term in our model um pandas by default shows all n levels although you can pass an argument to change that if you want um oh yeah drop first i kind of like having all of them sometimes because then you don't have to put in a constant term and it's a bit less annoying and it can be a bit easier to interpret but i don't feel strongly about it either way okay so here's a list of all of the columns that pandas added i guess directly speaking i probably should have automated that but never mind i just copied and posted them um and so here are a few examples of the added columns in unix pandas lots of things like that head means the first few rows or the first few loans um so five by default and pandas so here you can see they're never both male and female they're never neither they're always one or the other all right so um with that now we've got numbers which we can multiply by coefficients um it's not going to work for um name obviously because we'd have 891 columns and all of them would be unique so we'll ignore that for now that doesn't mean it's have to always ignore it um and in fact something i did do something i did do on the forum topic because i made a list of some nice titanic notebooks that i found and quite a few of them really go hard on this name column and in fact one of them yeah this one in what i believe is yes christian's first ever kaggle notebook he's now the number one ranked kaggle notebook person in the world um so this is a very good start he got a much better score than any model that we're going to create in this course using only that column name um and basically yeah he came up with this simple little decision tree um by recognizing you know all of the information that's in a name column so yeah we don't have to treat uh you know a big string of letters like this as a random big string of letters we can use our domain expertise to recognize that things like mr have meaning and that people with the same surname might be in the same family um and actually figure out quite a lot from that but that's not something i'm going to do i'll let you look at those notebooks if you're interested in the feature engineering and i do think that they're very interesting so do check them out our focus today is on building a linear model and a neural net from scratch not on tabular feature engineering even though that's also a very important subject okay so we talked about how matrix modification makes linear models much easier um and the other thing we did in excel was element-wise multiplication both of those things are much easier uh if we use pi torch instead of plain python or we could use numpy but i tend to just stick with pi torch when i can because it's easier to learn one library than two so i just do everything in pie torch i almost never touch numpy nowadays they're both great but they do everything each other does except pytorch also does differentiation and gpus so why not just learn pi torch so to turn a column into something that i can do pi torch calculations on i have to turn it into a tensor so a tensor is just it's what num numpy calls an array it's what mathematicians will call either a vector or a matrix or once we go to higher ranks mathematicians and physicists just call them tensors in fact this idea originally in computer science came from a notation developed in the 50s called apl which was turned into a programming language in the 60s by a guy called ken iverson and ken iverson actually came up with this idea um from he said his time doing tensor analysis in physics so there these uh areas are very related so we can turn the survived column into a tensor and we'll call that tensor our dependent variable that's the thing we're trying to predict okay so now we need some independent variables so our independent variables are age siblings that one is oh yeah a number of other family members uh the log of fare that we just created plus all of those dummy columns we added and so we can now um grab those values and turn them into a tensor and we have to make sure they're floats um we want them all to be the same data type and pi torch wants things to be floats if you're going to multiply things together so there we are and so one of the most important attributes of a tensor probably the most important attribute is its shape which is how many rows does it have and how many columns does it have the length of the shape is called its rank that's the rank of the tensor it's the number of dimensions or axes that it has so a vector is length as rank one a matrix is rank two a scalar is rank zero um and so forth um i try not to use too much jargon but there's some pieces of jargon that are really important because you like otherwise you're gonna have to say the length of the shape again and again it's much easier to say rank so we'll say we'll use that word a lot so a table is a rank two tensor okay so we've now got the data in good shape here's our independent variables and
fast.ai 2022 - Part 1,5,1606,Setting up a linear model,we've got our dependent variable so we can now go ahead and do exactly what we did in excel which is to multiply our rows of data buys and coefficients and remember to start with we create random coefficients so we're going to need one coefficient for each column now in excel we also had a constant um but in our case now we've we've got um every column every level in our dummy variable so we don't need a constant so the number of coefficients we need is equal to the shape of the independent variables and it's the index one element that's the number of columns that's how many coefficients we want so we can now crew ask torch pi torch to give us some random numbers and co-f of them uh they're between zero and one so if we subtract a half then they'll be centered and there we go before i do that i set the seed what that means is in computers computers in general cannot create truly random numbers instead they can calculate a sequence of numbers that behave in a random-like way um that's actually good for us because often in my teaching i like to be able to say you know in the pros oh look that was two now it's three or whatever and if i was using really random numbers then i couldn't do that because it'd be different each time so this is makes my results reproducible that means if you run it you'll get the same random numbers as i do by saying start the pseudo random sequence with this number i mentioned in passing a lot of people are very very into reproducible results they think it's really important to always do this i strongly disagree with that in my opinion an important part of understanding your data is understanding how much it varies from run to run so if i'm not teaching and wanting to be able to write things about these pseudo-random numbers i almost never use a manual seat instead i like to run things a few times and get an intuitive sense of like oh this is like very very stable or oh this is all over the place i'm getting an intuitive understanding of how your data behaves and your model behaves is really important now here's one of the coolest lines of code you'll ever see i know it doesn't look like much but think about what it's doing yeah okay so we've multiplied a matrix by a vector now that's pretty interesting now mathematicians amongst you will know that you can certainly do a matrix vector product but that's not what we've done here at all we've used element-wise multiplication so normally if we did the element-wise multiplication of two vectors it would multiply you know element one with element one uh element two with element two and so forth and create a vector of the same size output but here we've done a matrix times a vector how does that work um this is using the incredibly powerful technique of broadcasting and broadcasting again comes from apl a notation invented in the 50s in a programming language developed in the 60s and it's got a number of benefits basically what it's going to do is it's going to take each coefficient and multiply them in turn by every row in our matrix so if you look at the shape of our independent variable and the shape of our coefficients you can see that each one of these coefficients can be multiplied by each of these 891 values in turn and so the reason we call it broadcasting is it's as if this is 891 columns by 12 it rose by 12 columns it's as if this was broadcast 891 times it's as if we had a loop looping 891 times and doing coefficients times row zero coefficients times row one coefficients times zero two and so forth which is exactly what we want now reasons to use broadcasting obviously the code is much more concise it looks more like math rather than clunky programming with lots of boilerplate so that's good also that broadcasting all happened in optimized c code and if in fact it's being done on a gpu it's being done in optimized gpu assembler cuda code it's going to run very very fast indeed and this is a trick of why we can use a so-called slow language like python to do very fast big models is because a single line of code like this can run very quickly on optimized hardware on lots and lots of data the rules of broadcasting are a little bit subtle and important to know and so i would strongly encourage you to google numpy broadcasting rules and see exactly how they work but you know the kind of intuitive understanding of them hopefully you'll get pretty quickly which is generally speaking you can kind of as long as the last axes match it'll broadcast over those axes you can um broadcast a rank three thing with a rank one thing or you know most simple version would be tensor one two three times two so broadcaster scalar over a vector that's exactly what you'd expect so it's copying effectively that two into each of these bots multiplying them together but it does it doesn't use it up any memory to do that it's kind of a virtual copying if you like so this line of code independence by coefficients is very very important and it's the key step that we wanted to take which is now we know exactly how what happens when we multiply the coefficients in and if you remember back to excel um we did that product and then in excel there's a sum product we then added it all together because that's what a linear model is it's the coefficients times the values added together so we're now going to need to add those together but before we do that if we did add up this row you can see that the very first value has a very large magnitude and all the other ones are small same with row two same with row three same with row four what's going on here well what's going on is that the very first column was age and age is much bigger than any of the other columns it's not the end of the world but it's not ideal right because it means that a coefficient of say 0.5 times age means something very different to a coefficient of say 0.5 times log fair right and that means that that random coefficient we start with it's going to mean very different things for different columns and that's going to make it really hard to optimize so we would like all the columns to have about the same range so what we could do as we did in excel is to divide them by the maximum so the maximum so we did it for age and we also did it for fair in this case i didn't use log so we can get the max of each row by calling dot max and you can pass in a dimension do you want the maximum of the rows or the maximum of the columns we want the maximum over the rows so we pass in dimension zero so those um different parts of the shape uh are called either axes or dimensions uh pie torch calls some dimensions so that's going to give us the maximum of each row and if you look at the docs for player torch's max function it'll tell you it returns two things the actual value of each maximum and the index of where where which row it was we want the values so now thanks to broadcasting we can just say take the independent variables and divide them by the vector of values again we've got a matrix and a vector and so this is going to do an element-wise division of each row of this divided by this vector again in a very optimized way so if we now look at our normalized independent variables by the coefficients you can see they're all pretty similar values so that's good there's lots of different ways of normalizing but the main ones you'll come across is either dividing by the maximum or subtracting the mean and dividing by the standard deviation um it normally doesn't matter too much um because i'm lazy i just pick the easier one and being lazy and picking the easier one is a very good plan in my opinion so now that we can see that multiplying them together is working pretty well we can now add them up and now we want to add up over the columns and that would give us predictions now obviously just like in excel when we started out they're not useful predictions because they're random coefficients but they are predictions nonetheless and here's the first 10 of them so then remember uh we want to um use gradient descent to try to make these better so to do gradient descent we need a loss right the loss is the measure of how good or bad are these coefficients um my favorite loss function as i kind of like don't think about it just chuck something out there is the mean absolute value and here it is torch dot absolute value of the error the difference take the mean and often stuff like this you'll see people will use pre-written mean absolute error functions which is also fine but i quite like to write it out because i can see exactly what's going on no confusion no chance of misunderstanding so those are all the steps i'm going to need to create coefficients um run a linear model and get its loss so what i like to do in my notebooks like not just for teaching but all the time is to like do everything step by step manually and then just copy and paste the steps into a function so here's my calc prints function is
fast.ai 2022 - Part 1,5,2328,Creating functions,exactly what i just did right here's my calculus function exactly what i just did and that way you know i a lot of people like go back and delete all their explorations or they like do them in a different notebook or they're like working in an ide they'll go and do it in some you know line oriented rep or whatever but if you you know think about the benefits of keeping it here when you come back to it in six months you'll see exactly why you did what you did and how we got there or if you're showing it to your boss or your colleague you can see you know exactly what's happening what does each step look like i think this is really very helpful indeed uh i know not many people code that way but i feel strongly that it's a huge productivity win to individuals and teams so remember from our um gradient descent
fast.ai 2022 - Part 1,5,2379,Doing a gradient descent step,from scratch that um the one bit we don't want to do from scratch is calculating derivatives because it's just menial and boring so to get pie torch to do it for us you have to say well what things do you want derivatives for and of course we want it for the coefficients so then we have to say requires grid and remember very important in pi torch if there's an underscore at the end that's an in-place operation so this is actually going to change co-fs it also returns them right but it also changes them in place so now we've got exactly the same numbers as before but with requires grad turned on so now when we calculate our loss that doesn't do any other calculations but what it does store is a gradient function it's the function that python has remembered that it would have to do to undo those steps to get back to the gradient and to say oh please actually call that backward gradient function you call backward and at that point it sticks into a dot grad attribute the coefficient the coefficients gradients so this tells us that if we increased the age coefficient the loss would go down so therefore we should do that right so since negative means increasing this would decrease the loss that means we need to if you remember back to the gradient descent from scratch notebook we need to subtract the coefficients times the learning rate so we haven't got any particular ideas yet of how to set the learning rate so for now i just pick just try a few and still find out what works best in this case i found 0.1 worked pretty well so i now subtract so again this is sub underscore so subtract in place from the coefficients their gradient times the learning rate and so the loss has gone down that's great from 0.54 to 0.52 so there is one step so we've now got everything we need to train a linear model um so let's do it now as we discussed last week to see whether your model is any good it's important that you split your data into training and validation
fast.ai 2022 - Part 1,5,2535,Training the linear model,um for the titanic data set it's actually pretty much fine to use a random split because back when my friend market and i actually created this competition for kaggle many years ago that's basically what we did if i remember correctly so we can split them randomly into a training set and a validation set so we're just going to use uh fast di for that there's you know it's very easy to do it manually with numpy or pi torch you can use scikit learns train test split i'm using fast ai's here partly because it's easy just to remember one way to do things and this works everywhere and partly because in the next notebook we're going to be seeing how to do more stuff in fastai so i want to make sure we have exactly the same split so those are a list of the indexes of the rows that will be for example in the validation set that's why i call it validation split so to create the validation independent variables you have to use those to index into the independent variables and ditto for the dependent variables and so now we've got our independent variable training set and our validation set and we've also got the same for the dependent variables so like i said before i normally take stuff that i've already done in a notebook seems to be working and put them into functions so here's the step which actually updates coefficients so let's chuck that into a function and then the steps that go cap last stop backward update coefficients and then print the loss or chuck that in one function so just copying and pasting stuff into cells here and then the bit on the very top of the previous section that got the round of numbers minus 0.5 requires grad chuck that in the function so here we've got something that initializes coefficients something that does one epoch by updating coefficients so we can put that into them together into something that trains the model for n epochs with some learning rate by setting the manual seed initializing the coefficients doing one epoch in a loop and then return the coefficients so let's go ahead and run that function so it's printing at the end of each one the loss and you can see the loss going down from 0.53 down down down down down to a bit under 0.3 so that's good we have successfully built and trained a linear model on a real data set i mean it's a kaggle data set but it's important to like not underestimate how real capital data sets are they're real data and this one's a playground data set so it's not like anybody actually cares about predicting who survived the titanic because we already know but it has all the same features of you know different data types and missing values and normalization and so forth so you know it's a good it's a good playground so it'd be nice to see what the coefficients are attached to each variable so if we just zip together the independent variables and the coefficients and we don't need the regret anymore and create a dict of that there we go so uh it looks like older people had less chance of surviving that makes sense males had less chance of surviving also makes sense so it's good to kind of eyeball these and check that they seem reasonable now the metric for this cargo competition is not mean absolute error it's accuracy now of
fast.ai 2022 - Part 1,5,2765,Measuring accuracy,course we can't use accuracy as a loss function because it doesn't have a sensible gradient really but we should measure accuracy to see how we're doing because that's going to tell us how we're going against the thing that the cackle competition cares about so we can calculate our predictions and we'll just say okay well any times the prediction's over 0.5 uh we'll say that's predicting survival so that's our predictive of survival this is the actual in a validation set so if they're the same then we predicted it correctly so here's are we right or wrong for the first 16 rows we're right more often than not so if we take the mean of those remember true equals one then that's our accuracy so we're right about 79 of the time so it's not bad okay so we've successfully created something that's actually predicting who survived the titanic that's cool from scratch so let's create a function for that an accuracy function that just does what i showed and there it is yeah i say another thing like you know my weird coding thing for me you know where does it not that common is i use less comments than most people because all of my code lives in notebooks and of course in the real version of this notebook is full of pros right so when i've taken people through a whole journey about what i've built here and why i've built it and what intermediate results are and check them along the way the function itself in my you know for me doesn't need extensive comments and i'd rather explain the thinking of how i got there and show examples of how to use it and so forth okay now here's the first few predictions we made
fast.ai 2022 - Part 1,5,2890,Using sigmoid,and some of the time we're predicting negatives for survival and greater than one for survival which doesn't really make much sense right people either survived one or they didn't zero um it would be nice if we had a way to automatically squish everything between zero and one that's gonna make it much easier to optimize um the the optimizer doesn't have to try hard to hit exactly one or hit exactly zero but it can just like try to create a really big number to mean survive to a really small number to mean um perished um here's a great function here's a function that as i increase let's make it even bigger range as my numbers get beyond four or five it's asymptoting to one and on the negative side as they get beyond negative four or five they asymptote to zero or to zoom in a bit but then around about zero it's pretty much a lot a straight line this is actually perfect this is exactly what we want so here is the equation 1 over 1 plus a to the negative minus x and this is called the sigmoid function um by the way if you haven't checked out simpli before definitely do so this is the symbolic python package which can do it's kind of like mathematica or wolfram style symbolic calculations including the ability to plot symbolic expressions which is pretty nice pie torch already has a sigmoid function i mean it just calculates this but it does it in a more optimized way so what if we replaced calc preds remember before cat treads was just this what if we took that and then put it through a sigmoid so cat pretzel now basically the bigger oops the bigger this number is the closer it's going to get to one and the smaller it is the closer it's going to get to zero there should be a much easier thing to optimize and ensures that all of our values are in a sensible range now here's another cool thing about using jupiter plus python python is a dynamic language even if they were called cut preds train model calls one epoch which calls calc loss which calls cap creds i can redefine cockpits now and i don't have to do anything that's now inserted into python's symbol table and that's the cat prints that train model will eventually call so if i now call train model that's actually going to call my new version of cat print so it's a really neat way of doing exploratory programming in python i wouldn't um you know release you know a library that redefines creds multiple times you know when i'm done i would just keep the final version of course but it's a great way to try things as you'll see um and so look what's happened i found i was able to increase the learning rate from point one to two it was much easier to optimize as i guessed and the loss has improved from 0.295 to 0.197 the accuracy has improved from 0.79 to 0.82 from nearly 0.83 so as a rule this is something that we're pretty much always going to do when we have a binary dependent variable so dependent variable that's one or zero is the very last step is chuck it through a sigmoid generally speaking if you're wondering why is my model with a binary dependent variable not training very well this is the thing you want to check oh are you chucking it through a sigmoid or is the thing you're calling chucking it through a sigma mode or not it can be surprisingly hard to find out if that's happening so for example with hugging face transformers i actually found i had to look in their source code to find out and i discovered that something i was doing wasn't um and didn't seem to be documented anywhere but it is important to to find these things out as we'll discuss in the next lesson we'll talk a lot about neural net architecture details but the details we'll focus on are what happens to the inputs at the very first stage and what happens to the outputs at the very last stage we'll talk a bit about what happens in the middle but a lot less and the reason why is it's the things that you put into the inputs that's going to change for every single data set you do and what do you want to happen to the outputs which is going to happen for every different target that you're trying to hit so those are the things that you actually need to know about so for example this thing of like well you need to know about the sigmoid function and you need to know that you need to use it fast ai is very good at handling this for you that's why we haven't had to talk about it much until now if you say oh it's a category block dependent variable you know it's going to use the right kind of thing for you but most things are not so convenient uh john's a question uh yes there is um it's back in the sort of the feature engineering topic um but a couple of people have liked it so i thought we'd put it out there um so shivam says one concern i have while using get dummies right so it's in that get dummies phase is what happens while using test data i have a new category let's say male female and other and this will have an extra column missing from the training data how do you take care of that that's a great question yeah so normally you've got to think about this pretty carefully and check pretty carefully unless you use fast ai so fast ai always creates an extra category called other and at test time inference time if you have some level that didn't exist before we put it into the other category for you um otherwise you basically have to do that yourself um or or at least check you know generally speaking it's pretty likely that otherwise your extra level will be silently ignored you know because it's going to be in the data set but it's not going to be matched to a column so yeah it's a good point and definitely worth checking um for categorical variables with lots of levels i actually normally like to put the less common ones into another category and again that's something that fast ai will do for you automatically but yeah definitely something to keep an eye out for good question okay so before we take our break we'll just do one last thing which is we will submit this to kaggle because i think it's quite cool that we have successfully built a model from scratch
fast.ai 2022 - Part 1,5,3369,Submitting to Kaggle,so kaggle provides us with a test.csv which is exactly the same structure as the training csv except that it doesn't have a survived column now interestingly when i tried to submit to kaggle i got an error in my code uh saying that oh one of my fares is empty so that was interesting because the training set doesn't have any empty fares um so sometimes this will happen that the the training set and the test set have different things to deal with so in this case i just said oh there's only one row i don't care so i just replaced the empty one with the with a zero for fair so then i just copied and pasted the pre-pressings to pre-processing steps from my training data frame and stuck them here for the test data frame and the normalization as well and so now i just call calpretz is it greater than 0.5 turn it into a zero or one because that's what cackle expects and put that into the survived column which previously remember didn't exist so then finally i create a data frame with just the two columns id and survived stick it in a csv file and then i can call the unix command head just to look at the first few rows and if you look at the kaggle competition's data page you'll see this is what the submission file is expected to look like so that made me feel good so i went ahead and submitted it i didn't mention it okay so anyway i submitted it and i remember i got like i i think i was basically right in the middle about 50 you know better than half the people who have entered the competition worse than half the people so you know solid middle of the pack result for a linear model from scratch i think it's a pretty good result so it's a great place to start so let's take a 10 minute break we'll come back at 7 17 and continue on our journey all right welcome back um
fast.ai 2022 - Part 1,5,3505,Using matrix product,you might remember from excel that after we did the some product version um we then replaced it with a matrix multiply uh wait not there must be here here we are where the matrix multiply so let's do that step now so um matrix times vector dot sum over axis equals 1 is the same thing as matrix model play so here is the times dot sum version now we can't use this character for a matrix multiply because it means element-wise operation all of the times plus minus divide then pi torch numpy mean element-wise so corresponding elements so in python instead we use this character as far as i know it's pretty arbitrary it's one of the ones that wasn't used so that is an official python it's a bit unusual it's an official python operator it means matrix model play but python doesn't come with an implementation of it so because we've imported because these are tensors and in pi torch that will use pi torches and as you can see they're exactly the same so we can now just simplify a little bit what we had before calc preds is now torch.sigmoid of the matrix multiply now there is one thing i'd like to move towards now is that we're going to try to create a neural net in a moment and so that means rather than treat this as a matrix times a vector i want to treat this as a matrix times a matrix because we're about to add some more um uh columns of coefficients so um we're going to change in at coeffs so that rather than creating an n coeff vector we're going to create an n cof by 1 matrix so in math we would probably call that a column vector but i think that's a kind of a dumb name in some ways because it's it's a matrix right it's a rank two tensor um so um like the matrix multiplier will work fine either way but the key difference is that if we do it this way then um the result of the matrix multiplier will also be a matrix it'll be again a n rows by one matrix that means when we compare it to the dependent variable we need the dependent variable to be an n rows by one matrix as well so effectively we need to take the n rows long vector and turn it into an n rows by one matrix um so there's some useful very useful and at first maybe a bit weird notation in pi torch numpy for this which is if i take my training dependent variables vector i index into it and colon means every row right so in other words that just means the whole vector right it's the same basically as that and then i index into a second dimension now this doesn't have a second dimension so there's a special thing you can do which is if you index into a second dimension with a special value none it creates that dimension so this has the effect of adding an extra trailing dimension to train dependence so it turns it from a vector to a matrix with one column so if we look at the shape after that as you see it's now got we call this a unit axis it's got a trailing unit axis 713 rows in one column so now if we train our model we'll get coefficients just like before except that it's now a column vector also known as a rank two matrix with a trailing unit axis okay so that hasn't changed anything it's just repeated what we did in the previous section but it's kind of set us up to expand because now that we've done this using matrix multiply we can go crazy and we can go ahead and create a neural network so with our neural network remember
fast.ai 2022 - Part 1,5,3811,A neural network,back to the excel days notice here it's the same thing right we created a column vector but we didn't create a column vector we actually created a matrix with kind of two sets of coefficients so when we did our matrix multiply every row gave us two sets of outputs which we then chat through value right which remember we just used an if statement and we added them together so our co-fs now to make a proper neural net we need one set of co-fs here and so here they are torch.rand and co-f by what well in excel we just did two because i kind of got bored of getting everything working properly but you don't have to worry about feeling right and creating columns and blah blah blah and in pi torch you can create as many as you like so i made it something you can change i call it n hidden number of hidden activations i just set it to 20. and as before we centralize them by making them go from minus 0.5 to 0.5 now when you do stuff by hand everything does get more fiddly if our coefficients aren't if they're too big or too small it's not going to train it at all basically the gradients will kind of vaguely point in the right direction but you'll jump too far or not far enough or whatever so i want my gradients to be about the same as they were before so i divide by n hidden because otherwise at the next step when i add up the the next matrix multiply it's going to be much bigger than it was before so it's all very fiddly so then i want to take so that's going to give me for every row it's going to give me 20 activations 20 values right just like in excel we had two values because we had two sets of coefficients and so to create a neural net i now need to multiply each of those 20 things by a coefficient and this time it's going to be a column vector because i want to create one output predictor of survival so again torch.rand and this time the n hidden will be the number of coefficients by one and again like trying to find something that actually trains properly required me some fiddling around to figure out how much to subtract and i found if i subtract 0.3 i could get it to train and then finally i didn't need a constant term for the first layer as we discussed because our dummy variables have you know n columns rather than n minus one columns but layer two absolutely needs a constant term okay and we could do that as we discussed last time by having a column of ones although in practice i actually find it's just easier just to create a constant term okay so here is a single scalar random number so those are the coefficients we need one set of coefficients to go from input to hidden one goes from hidden to a single output and a constant so they're all going to need grad and so now we can change how we calculate predictions so we're going to pass in all of our coefficients so a nice thing in python is if you've got a list or a tuple of values on the left hand side you can expand them out into variables so this is going to be a list of three things so we'll call them l1 layer 1 layer 2 and the constant term because those are the list of three things we returned so in python if you just chuck things with commas between them like this it creates a tuple a tuple is a list it's an immutable list so now we're going to grab those three things so step one is to do our matrix multiply and as we discussed we then have to replace the negatives with zeros and then we put that through our second matrix model place so our second layer and add the constant term and remember of course at the end chuck it through a sigmoid so here is a neural network now update coeffs previously subtracted the coefficients the gradients times the learning rate from the coefficients but now we've got three sets of those so we have to just chuck that in a for loop so change that as well and now we can go ahead and train our model ta-da we just trained a model and how does that compare so the loss function's a little better than before accuracy exactly the same as before and you know i will say it was very annoying to get to this point trying to get these constants right and find a learning rate that worked like it was super fiddly um but you know we got there we got there it's a very small test set i don't know if this is necessarily better or worse than the linear model but it's certainly fine um and i think that's pretty cool that we were able to build a neural net from scratch that's doing pretty well but i hear that all the cool kids nowadays are doing deep learning
fast.ai 2022 - Part 1,5,4160,Deep learning,not just neural nets so we better make this deep learning so this one only has one hidden layer so let's create one with n hidden layers so for example let's say we want two hidden layers 10 activations in each you can put as many as you like here all right so inner coeffs now is going to have to create a torch.rand for every one of those hidden layers and then another torch.rand for your constant terms stick requires great in all of them and then we can return that so that's how we can just initialize as many layers as we want of coefficients so the first one the first layer so the sizes of each one the first layer will go from ncof to 10 the second matrix will go from 10 to 10 and the third matrix will go from 10 to 1. so it's worth like working through these matrix multipliers on like a spreadsheet or a piece of paper or something to kind of convince yourself that there's a right number of activations at each point and so then we need to update calc threads so that rather than doing each of these steps manually we now need to loop through all the layers do the matrix multiply add the constant and as long as it's not the last layer do the value why not the last layer because remember the last layer has sigmoid so these things about like remember what happens on the last layer this is an important thing you need to know about you need to kind of check if things aren't working what's your this thing here is called the activation function.sigmoid and f.value they're the activation functions for these layers one of the most common mistakes amongst people trying to kind of create their own architectures or kind of variants of architectures is to mess up their final activation function and that makes things very hard to train so make sure we've got a torch.sigmoid at the end and no value at the end so there's our deep learning cut threads and then just one last change is now when we update our coefficients we go through all the layers and all the constants and again there was so much messing around here with trying to find like exact ranges of random numbers that end up training okay but eventually i found some and as you can see it gets to about the same loss and about the same accuracy
fast.ai 2022 - Part 1,5,4330,Linear model final thoughts,this code is worth spending time with and when the codes inside a function it can be a little difficult to experiment with so you know what i would be inclined to do to understand this code is to kind of copy and paste this cell make it so it's not in a function anymore and then use ctrl shift dash to separate these out into separate cells right and then try to kind of set it up so you can run a single layer at a time or a single coefficient like make sure you can see what's going on okay and that's why we use notebooks it's so that we can experiment and it's only through experimenting like that that at least for me i find that i can really understand what's going on nobody can look at this code and immediately say i don't think anybody can i get it that all makes perfect sense but once you try running through it yourself you'll be like oh i see why that's as it is so um you know one thing to point out here is that our neural nets and deep learning models didn't particularly seem to help um so does that mean that deep learning is a waste of time and you just did five lessons that you shouldn't have done no not necessarily um this is a playground competition we're doing it because it's easy to get your head around but for very small data sets like this with very very few columns and the columns are really simple um you know deep learning is not necessarily going to give you the best result in fact as i mentioned um nothing we do is going to be as good as a carefully designed model that uses just the name column so you know i think that's an interesting insight right is that the kind of data types which have a very consistent structure like for example images or natural language text documents quite often you can somewhat brainlessly chuck a deep learning neural net at it and get a great result generally for tabular data i find that's not the case i find i normally have to think pretty long and hard about the feature engineering in order to get good results but once you've got good features you then want a good model and so you you know and generally like the more features you have and the more levels in your categorical features and stuff like that you know the more value you'll get from more sophisticated models um but yeah i definitely would say a an insight here is that you know you want to include simple bass lines as well and we're going to be seeing even more of that um in a couple of notebooks time
fast.ai 2022 - Part 1,5,4530,Why you should use a framework,so we've just seen how you can build stuff from scratch we now say why you shouldn't i mean i say you shouldn't you you should to learn but why you probably won't want to in real life when you're doing stuff in real life you don't want to be fiddling around with all this annoying initialization stuff and learning great stuff and dummy variable stuff and normalization stuff and so forth because um we can do it for you and it's not like everything's so automated that you don't get to make choices but you want like you want to make the choice not to do things the obvious way and have everything else done the obvious way for you so that's why we're going to look at this why you should phrase use a framework notebook and again i'm going to look at the clean version of it and again in the clean version of it step one is to download the data as appropriate for the kaggle or non-cable environment and set the display options and set the random seed and read the data frame all right now um there was so much fussing around with the doing it
fast.ai 2022 - Part 1,5,4593,Prep the data,from scratch version that i did not want to do any feature engineering because every column i added was another thing i had to think about dummy variables and normalization and random coefficient initialization and blah blah blah but um with a framework everything's so easy you can do all the feature engineering you want because this isn't a lesson about feature engineering instead i plagiarized entirely from this fantastic advanced feature engineering tutorial on kaggle and what this tutorial found was that um in addition to the log fare we've already done that you can do cool stuff with the deck with adding up the number of family members whether people are traveling alone how many people are on each ticket and finally we're going to do stuff with the name which is we're going to grab the mr miss mrs master whatever so we're going to create a function to like do some feature engineering and if you want to learn a bit of python pandas here's some great lines of code to step through one by one and again like take this out of a function put them into individual cells run each one look up the uh tutorials what does straw do what does map do what does group buy and transform do what does value accounts do like these are all like part of the reason i put this here was for folks that haven't done much if any pandas to have some you know examples of functions that i think are useful and i actually refactored this code quite a bit to try to show off some features of pandas i think are really nice so we'll do the same random split as before so pasting in the same seed and so now we're going to do the same set of steps that we did manually with fast ai so we want to create a tabular model data set based on a pandas data frame and here is the data frame these are the train versus validation splits i want to use here's a list of all the stuff i want done please uh deal with dummy variables for me deal with deal with missing values for me normalize continuous variables for me i'm going to tell you which ones are the categorical variables so here's for example p class was a number but i'm telling fastai to treat it as categorical here's all the continuous variables here's my dependent variable and the dependent variable is a category so create data loaders from that place and save models right here in this directory that's it that's all the pre-processing i need to do even with all those extra engineered features
fast.ai 2022 - Part 1,5,4778,Train the model,create a learner okay so this remember is something that contains a model and data and i want you to put in two hidden layers with 10 units and 10 units just like we did in our final example uh what learning rate should i use uh make a suggestion for me please so call lr find you can use this for any fasta model now what this does is it starts at a learning rate that's very very small 10 to the negative seven and it puts in one batch of data and it calculates the loss and then it puts through and then it increases the learning rate slightly and puts through another batch of data and it keeps doing that for higher and higher learning rates and it keeps track of the loss as it increases the learning rate just one batch of data at a time and what happens is for the very small learning rates nothing happens but then once you get high enough the loss starts improving and then as it gets higher it improves faster until you make the learning rate so big that it overshoots and then it kills it and so generally somewhere around here is the learning rate you want fast ai has a few different ways of recommending a learning rate you can look up the docs to see what they mean i generally find if you choose slide and valley and pick one between the two you get a pretty good learning rate so here we've got about .01 and about 0.08 so i picked 0.03 so just run a bunch of epochs away it goes this is a bit crazy after all that we've ended up exactly the same accuracy as the last two models that's just a coincidence right i mean there's nothing particularly about that accuracy um and so at this point we can now submit that to kaggle now remember with the linear model we had to repeat
fast.ai 2022 - Part 1,5,4894,Submit to Kaggle,all of the pre-processing steps on the test set in exactly the same way don't have to worry about it with fast ai in fast ai i mean we still have to deal with the fill missing for fare because that's that's that we have to add our feature engineering features but all the pre-processing we just have to use this one function called testdl and that says create a data loader that contains exactly the same pre-processing steps that our learner used and that's it that's all you need so just because you you want to make sure that your inference time transformations pre-processing are exactly the same as a training time so this is the magic method which does that just one line of code and then to get your predictions you just say get reads and pass in that data loader i just built and so then these three lines of code are the same as the previous notebook and we can take a look at the top and you can see there it is so how did that go i don't remember no i didn't say i i think it was again basically middle of the pack if i remember correctly so one of the nice things about um now that it's so easy to like add features and build models is we can experiment with things much more quickly so i'm going to show you how easy it is to experiment with you know what's often considered a fairly advanced idea which is called ensembling
fast.ai 2022 - Part 1,5,5002,Ensembling,there's lots of ways of doing ensembling but basically ensembling is about creating multiple models and combining their predictions and the easiest kind of ensemble to do is just to literally just build multiple models and so each one is going to have a different set of randomly initialized coefficients and therefore each one is going to end up with a different set of predictions so i just create a function called ensembl which creates a learner exactly the same as before fits exactly the same as before and returns the predictions and so we'll just use a list comprehension to do that five times so that's going to create a set of five predictions done so now we can take all those predictions and stack them together and take the main over the rows so that's going to give us the what's actually sorry the mean over the over the first dimension so the mean over the sets of predictions and so that will give us the average prediction of our five models and again we can turn that into a csv and submit it to cattle and that one i think that went a bit better let's check yeah okay so that one actually finally gets into the top 20 25 in the competition so i mean not amazing by any means but you can see that you know this simple step of creating five independently
fast.ai 2022 - Part 1,5,5108,Framework final thoughts,trained models just starting from different starting points in terms of random coefficients actually improved us from top 50 to top 25 john is there an argument because you've got a categorical result zero one effectively is there an argument that you might use the mode of the ensemble rather than the numerical mean i mean yes there's an argument that's been made um and um yeah that's something i would just try i generally find it's less good but not always and i don't feel like i've got a great intuition as to why and i don't feel like i've seen any studies as to why um you could predict like there's a few there's there's at least three things you could do right you could take the is it greater or less than 0.5 ones and zeros and average them or you could take the mode of them or you could take the actual probable probability predictions and take the average of those and then threshold that and i've seen examples where certainly both of the different averaging versions each of them has been better i don't think i've seen one where the mode's better but that was very popular back in the 90s so yeah so beer is so easy to try you may as well give it a go okay we don't have time to finish the next notebook but let's make a start on it
fast.ai 2022 - Part 1,5,5204,How random forests really work,so the next notebook is random forests how random forests really work uh who here has heard of random forests before nearly everybody okay so very popular um developed i think initially in 1999 but you know gradually improved in popularity during the 2000s i was like everybody kind of knew me as mr random forests for for years um i implemented them like a couple of days after the original technical report came out i was such a fan um all of my early kaggle results from random forests i love them um and i think hopefully you'll see why i'm such a fan of them because they're so elegant and they're almost impossible to mess up a lot of people will say like oh why are you using machine learning why don't you use something simple like logistic regression and i think like oh gosh in industry i've seen far more examples of people screwing up logistic regression than successfully using logistic regression because it's very very very very difficult to do correctly you know you've got to make sure you've got the correct transformations and the correct interactions and the correct outlier handling and blah blah blah and anything you get wrong the entire thing falls apart um random forests i it's very rare to that i've seen somebody screw up a random forest in industry they're very hard to screw up because they're they're so resilient and you'll see why so in this notebook just by the way rather than importing numpy and pandas and matplotlib and blah blah blah there's a little handy shortcut which is if you just import everything from fasta to imports that imports all the things that you normally want um so i mean it doesn't do anything special it's just save some messing around so again we've got our cell here to grab the data
fast.ai 2022 - Part 1,5,5337,Data preprocessing,and i'm just going to do some basic pre-processing here with my uh fill in a for the fare only needed for the test set of course um uh grab the modes and do the filling a on the modes take the log fare um and then i've got a couple of new steps here which is um converting embarked insects into categorical variables what does that mean well let's just run this on both the data frame a data frame and the test data frame split things into set into categories and continuous and sex is a categorical variable so let's look at it well that's interesting it looks exactly the same as before male and female but now it's called a category and it's got a list of categories what's happened here well what's happened is pandas has made a list of all of the unique values of this field and behind the scenes if you look at the cat codes you can see behind the scenes it's actually turned them into numbers it looks up this one into this list to get mail looks at this zero into this list to get female so when you print it out it prints out the friendly version but it stores it as numbers now um you'll see in a moment why this is helpful but a key thing to point out is we're not going to have to create any dummy variables and even that first second or third class we're not going to consider that categorical at all and you'll see why in a moment a random forest is an ensemble of trees
fast.ai 2022 - Part 1,5,5456,Binary splits,a tree is an ensemble of binary splits and so we're going to work from the bottom up we're going to first work we're going to first learn about what is a binary split and we're going to do it by looking at example let's consider what would happen if we took all the passengers on the titanic and group them into males and females and let's look at two things the first is let's look at their survival rate so about 20 survival rate for males and about 75 for females and let's look at the histogram how many of them are there about twice as many males as females consider what would happen if you created the world's simplest model which was what sex are they that wouldn't be bad would it because there's a big difference between the males and the females a huge difference in survival rate so if we said oh if you're a man you probably died if you're a woman you probably survived or not just a man or a boy so male or female um that would be a pretty good model because it's done a good job of splitting it into two groups that have very different survival rates um this is called a binary split a binary split is something that splits the rows into two groups tense binary let's talk about another example of a binary split i'm getting ahead of myself before we do that let's look at what would happen if we used this model so if we created a model which just looked at sex how good would it be so to figure that out we first have to split into training and test sets so let's go ahead and do that and then let's convert all of our categorical variables into their codes so we've now got zero one two whatever we don't have male or female there anymore um and let's also create something that returns the independent variables which we'll call the x's and the dependent variable which we'll call y and so we can now get the x's and the y for each of the training set in the validation set and so now let's create some predictions we'll predict that they survived if their sex is zero so if they're female so how good is that model um remember i told you that um to calculate mean absolute error we can get um psychic learn or pie touch whatever do it for us instead of doing it ourselves so just showing you here's how you do it just by importing it directly this is exactly the same as the one we did manually in the last notebook so that's a 21 and a half percent error so that's a pretty good model um could we do better um well here's another example what about fair so fair is different to sex because fair is continuous or log fair i think but we could still split it into two groups so here's for all the people that didn't survive this is their median fare here and then this is their quartiles for big affairs and quartiles for small affairs and here's the median fare for those that survived and their quartiles so you can see the median fare for those that survived is higher than the median fare for those that didn't we can't create a histogram exactly for fair because it's continuous um we could bucket it into groups to create a histogram so i guess we can create a histogram that wasn't true what i should say is we could create something better which is a kernel density plot which is just like a histogram but it's like with infinitely small bins so we can see most people have a log fare of about two so what if we split on about a bit under three you know that seems to be a point at which there's a difference in survival between people that are greater than or less than that amount so here's another model log fare greater than 0.2.7 oh much worse 0.336 versus 0.215 well i don't know maybe there's something better we could create a little interactive tool so what i want is something that can give us a quick score of how good a binary split is and i want it to be able to work regardless of whether we're dealing with categorical or continuous or whatever data so i just came up with a simple little way of scoring which is i said okay if you split your data into two groups a good split would be one in which all of the values of the dependent variable on one side are all pretty much the same and all the dependent variables on the other side are all pretty much the same for example if pretty much all the males had the same survival outcome which is didn't survive and all the females had about the same survival outcome which is they did survive that would be a good split right it doesn't just work for categorical variables it would work if your dependent variable was continuous as well you basically want each of your groups within group to be as similar as possible on the dependent variable and then the other group you want them to be as similar as possible on the dependent variable so how similar is all the things in a group that's a standard deviation so what i want to do is basically add the standard deviations of the two groups of the dependent variable and then if there's a really small standard deviation but it's a really small group that's not very interesting so i'll multiply it by the size right so this is something which says what's the score for one of my groups one of my sides it's the standard deviation multiplied by how many things are in that group so the total score is the score for the left-hand side so all the things in one group plus the score for the right-hand side which is tilde means not so not left-hand side is right-hand side and then we'll just take the average of that so for example if we split by 6 is greater than or less than 0.5 that'll create two groups males and females and that gives us this score and if we do log fare greater than or less than 2.7 it gives us this score and lower score is better so sex is better than log fare so now that we've got that we can use our favorite interact tool to create a little gui and so we can say you know let's try like oh what about this one can we uh oops can we find something that's a bit better 4.8.485 no not very good what about p class 0.468 0.460 so we can fiddle around with these um we could do the same thing for the categorical variables sorry we know that sex we can get to 0.407 what about embarked hmm all right so looks like sex might be our best well that was pretty inefficient right would be nice if we could find some automatic way to do all that well of course we can for example if we wanted to find what's the best split point for age then we just have to create let's do this again if we want to find the best split point for age we could just create a list of all of the unique values of age and try each one in turn and see what score we get if we made a binary split on that level of age so here's a list of all of the possible binary split thresholds for age let's go through all of them for each of them calculate the score and then numpy and pi torch have an arg min function which tells you what index into that list is the smallest so just to show you here's the scores and 0 1 2 3 4 5 six oh here sorry zero one two three four five six so apparently that that value has the smallest score so that tells us that for age the threshold of 6 would be best so here's something that just calculates that for a column it calculates the best split point so here's six right and it also tells us what the score is at that point which is 0.478 so now we can just go through and calculates the score for the best split point for each column and if we do that we find that the lowest score is sex so that is how we calculate the best binary split so we now know that the model that we created earlier this one is the best single binary split model we can find so next week we're going to be we're going to learn how we can recursively do this to create a decision tree and then do that multiple times to create a random forest but before we do
fast.ai 2022 - Part 1,5,6094,Final Roundup,i want to point something out which is this ridiculously simple thing which is find a single binary split in stock is a type of model it has a name it's called 1r and the 1r model it turned out in a review of machine learning methods in the 90s turned out to be one of the best if not the best machine learning classifiers for a wide range of real-world data sets so that is to say don't assume that you have to go complicated it's not a bad idea to always start creating a baseline of one r a decision tree with a single binary split and in fact for the titanic competition that's exactly what we do if we look at the titanic competition on kaggle you'll find that what we did is our sample submission is one that just splits into male versus female all right thanks everybody hope you found that interesting and i will see you next lesson bye
fast.ai 2022 - Part 1,6,0,Review,"Practical Deep Learning for Coders, Lesson 6 OK, so welcome back to lesson 6… not welcome back to, welcome to lesson 6 — first time we've been in lesson 6! Welcome back to Practical Deep Learning for Coders. We just started looking at tabular data last time, and for those of you who've forgotten what we did was: We were looking at the Titanic data set and we were looking at creating binary splits by looking at categorical variables or binary variables like sex and continuous variables, like the log of the fare that they paid, and using those. You know, we also kind of came up with a score which was basically: How good a job did that split do of grouping the survival characteristics into two groups, you know, all of, nearly all of one of whom survived, nearly all of whom the other didn't survive so they had like small standard deviation in each group. And so then we created the world's simplest little UI to allow us to fiddle around and try to find a good binary split. And we did… we did come up with a very good binary split, which was on sex and actually we created this all automated version. And so this is, I think, the first time we can —well not quite the first time is it?—no, this is, this is yet another time, I should say, that we have successfully created a uh, actual machine learning algorithm from scratch, this one is about the world's simplest one, it's “OneR”, creating the single rule which does a good job of splitting your data set into two parts which differ as much as possible on the dependent variable."
fast.ai 2022 - Part 1,6,129,TwoR model,"“OneR” is probably not going to cut it for a lot of things, though, it's surprisingly effective but it's uh maybe we could go a step further. And the other step further we could go is we could create like a “TwoR”, what if we took each of those groups: males and females in the Titanic data set and split each of those into two other groups, so split the males into two groups and split the females into two groups. So, to do that we can repeat the exact same piece of code we just did but let's remove sex from it and then split the data set into males and females and run the same piece of code that we just did before but just for the males. And so this is going to be like a “OneR” rule for how do we predict which males survive the Titanic. And let's have a look: 38, 37, 38, 38, 38. Okay, so it's ‘Age’, were they greater than or less than six. Turns out to be for the males the biggest predictor of whether they were going to survive that shipwreck. And we can do the same thing for females, so for females… there we go, no great surprise, ‘Pclass’. So whether they were in first class or not was the biggest predictor for females of whether they would survive the shipwreck. So that has now given us a decision tree. It is a series of binary splits which will gradually split up our data more and more such that in the end, in these in the leaf nodes —as we call them— we will hopefully get as, you know, much stronger prediction as possible about survival. So we could just repeat this step for each of the four groups we've now created: males, kids and older than six. Females, first class and everybody else, and we can do it again. And then we'd have eight groups. We could do that manually with another couple of lines of code or we can just use a decision tree classifier, which is a class which does exactly that for us."
fast.ai 2022 - Part 1,6,283,How to create a decision tree,"So there's no magic in here, just doing what we've just described. And a decision tree classifier comes from a library called scikit-learn. scikit-learn is a fantastic library that focuses on, kind of, classical non-deep learning ish machine learning methods like decision trees. So we can, so to create the exact same decision tree we can say: please create a decision tree classifier with at most four leaf nodes. And one very nice thing it has is it can draw the tree for us. So here's a tiny little draw_tree function. And you can see here it's going to first of all split on sex, now, it looks a bit weird to say sex is less than or equal to 0.5 but remember what our binary characteristics are coded as zero or one, so that's just how we, you know, easy way to say males versus females. And then here we've got for the females, what class are they in, and for the males what age are they. And here's our four leaf nodes. So for the females in first class 116 of them survived and four of them didn't so… very good idea to be a “well to do” woman on the Titanic. On the other hand, males adults: 68 survived 350 died so, very bad idea to be a male adult on the Titanic. So you can see you can kind of get a quick summary of what's going on and one of the reasons people tend to like decision trees, particularly for exploratory data analysis, is it does allow us to get a quick picture of what are the key driving variables in this data set and how much do they kind of predict what was happening in the data. Okay, so it's around the same splits as us and it's got one additional piece of"
fast.ai 2022 - Part 1,6,422,Gini,"information we haven't seen before, this is something called “Gini”. “Gini” is just another way of measuring how good a split is, and I've put the code to calculate “Gini” here. Here's how you can think of “Gini”: How likely is it that, if you go into that sample and grab one item and then go in again and grab another item, how likely is it that you're going to grab the same item each time. And so, if that, if the entire leaf node is just people who survived or just people who didn't survive the probability would be one, you get the same time, same every time. If it was an exactly equal mix, the probability would be 0.5. So that's why we just, yeah, that's where this formula comes from in the binary case. And in fact you can see it here, right, this group here is pretty much 50-50 so “Gini” is 0.5. Or else this grip here is nearly 100% in one class so “Gini” is nearly zero —so that backwards is one minus. And I think I've written it backwards here as well, so… I've got to fix that. So this decision tree is, you know, we would expect it to be more accurate so we can calculate its mean absolute error. And for the “OneR”, so just doing males versus females, what was our score? Here we go, 0.407. Uh, should we have a, do we have an accuracy score, somewhere, here we are, 0.336. Oh that was for ‘LogFare’ and for ‘Sex’ it was 0.215, okay so 0.215. So that was for the “OneR” version, for the decision tree with four leaf nodes 0.224 so it's actually a little worse, right? And I think it just reflects the fact that this is such a small data set and the “OneR” version was so good we haven't really improved it that much, or not enough to really see it amongst the randomness of such a small validation set. We could go further to 50, a minimum of 50 samples per Leaf node. So that means that in each of these, see how it says “samples”, which in this case is passengers on the Titanic, there's at least... there's 67 people that were... were female first class less than 28. That's how you define that. So this decision tree keeps building, keep splitting until it gets to a point where there's going to be less than 50 at which point it stops splitting that... that leaf. So you can see they're all got at least 50 samples and so here's the decision tree that builds. As you can see, it doesn't have to be like constant depth, right? So this group here, which is males who had cheaper fares and who were older than 20 but younger than 32... yeah actually younger than 24. and actually super cheap fares and so forth right. So it keeps going down until we get to that group. So let's try that decision tree. That decision tree has an absolute error of 0.183 so not surprisingly you know once we get there it's starting to look like it's a little bit better."
fast.ai 2022 - Part 1,6,654,Making a submission,"So there's a model and this is a Kaggle competition so therefore we should submit it to the leaderboard. And. you know.one of the... you know... biggest mistakes I see not just beginners but every level of practitioner make on Kaggle is not to submit to the leaderboard, spend months making some perfect thing right but you actually got to see how you're going and you should try and submit something to the leaderboard every day. So you know, regardless of how rubbish it is because you want to improve every day. So you want to keep iterating. So to submit something to the leaderboard you generally have to provide a CSV file and so we're going to create a CSV file. And we're going to apply the category codes to get the category for each one in our test set. we're going to set the survived column to our predictions and then we're going to send that off to a CSV. And so yeah so I submitted that and I got to score a little bit worse than most of our linear models and neural nets but not terrible, you know. It was... it's... it's just doing an okay job. Now one interesting thing for the decision tree is there was a lot less preprocessing to do, did you notice that? We didn't have to create any dummy variables for... for our categories and like, you certainly can create dummy variables but you often don't have to. So for example you know... for… for “class” you know it's... it's one two or three, you can just split on one, two, or three, you know. Even for like, what was that thing... like the... the “embarkation city code”, like we just convert them kind of arbitrarily to numbers one, two, and three, and you can split on those numbers. So with Random Forest also... not Random Forest, not there yet... decision trees, yeah... you can generally get away with not doing stuff like dummy variables. In fact, even taking the log of “fair” we only did that to make our graph look better but if you think about it, splitting on log fare less than 2.7 is exactly the same as splitting on Fair is less than e to the 2.7 you know or whatever log base we used, I can't remember. So all that a decision tree cares about is the ordering of the data and this is another reason that decision tree based approaches are fantastic because they don't care at all about outliers, you know... long tail distributions, categorical variables whatever. You can throw it all in and it'll do a perfectly fine job. So for tabular data I would always start by using a decision tree based approach and kind of create some baselines and so forth because it's... it's really hard to mess it up and that's important. So yeah... so here for example is “Embarked”, right? It... it was coded originally as the first letter of the city they embarked in, but we turned it into a categorical variable and so pandas for us creates this this vocab this list of all of the possible values and if you look at the codes attribute you can see it's that S is the... zero one two... so S has become 2, C has become zero and so forth, right. So that's how we're converting the categories, the strings into numbers that we can sort and group by. So yeah... so if we wanted to split C into one group and Q and S in the other we can just do, okay, less than a quarter one point 0.5. Now of course if we wanted to split C and S into one group and Q into the other we would need two binary splits first C on one side and Q and S on the other and then Q and S into Q versus S and then the Q and S leaf nodes could get similar predictions. So like you do have, like sometimes it can take a little bit more messing around but most of the time I find categorical variables work fine as numeric in decision tree based approaches and as I say here I tend to use dummy variables only if there's like less than four levels."
fast.ai 2022 - Part 1,6,952,Bagging,"Now, what if we wanted to make this more accurate? could we grow the tree further? I mean, we could, but, you know, there's only 50 samples in these leaves, right? it's not really… you know, if I keep splitting it, the leaf nodes are going to have so little data that's not really going to make very useful predictions. Now there are limitations to how accurate a decision tree can be. So what can we do? We can do something that's actually very, I mean, I find it amazing and fascinating, comes from a guy called Leo Breiman. And Leo Breiman came up with… came up with this idea called bagging. And here's the basic idea of bagging: let's say we've got a model that's not very good because, let's say, it's a decision tree, it's really small, we've hardly used any data for it right, it's not very good so, it's got error, it's got errors on predictions. It's not a systematically biased error, it's not always predicting too high or always predicting too low, I mean, decision trees, you know, on average will predict the average, right? but it has errors. So what I could do is I could build another decision tree in some slightly different way that would have different splits and it would also be not a great model, but it predicts the correct thing on average, it's not completely hopeless and again, you know, some of the errors are a bit too high and some are a bit too low. And I could keep doing this so, if I could create building lots and lots of slightly different decision trees I'm going to end up with say a hundred different models, all of which are unbiased, all of which are better than nothing, and all of which have some errors a bit high, some bit low, whatever. So what would happen if I averaged their predictions? Assuming that the models are not correlated with each other then you're going to end up with errors on either side of the curve: the correct prediction, some are a bit high, some are a bit low, there'll be this kind of distribution of errors, right? And the average of those errors will be zero, and so that means the average of the predictions of these multiple uncorrelated models —each of which is unbiased— will be the correct prediction because they have an error of zero. And this is a mind-blowing insight, it says that if we can generate a whole bunch of uncorrelated, unbiased models, we can average them and get something better than any of the individual models because the average of the error will be zero."
fast.ai 2022 - Part 1,6,1146,Random forest introduction,"So all we need is a way to generate lots of models. Well we already have a great way to build models —which is to create a decision tree— how do we create lots of them? how to create lots of unbiased but different models? Well, let's just grab a different subset of the data each time, let's just grab at random half the rows and build a decision tree, and then grab another half of the rows and build a decision tree, and grab another half the rows and build a decision tree. Each of those decision trees is going to be not great —it's only using half the data— but it will be unbiased, it will be predicting the average on average, it will certainly be better than nothing because it's using, you know, some real data to try and create a real decision tree. They won't be correlated with each other because they're each random subsets. So that meets all of our criteria for bagging. When you do this you create something called a “Random Forest”."
fast.ai 2022 - Part 1,6,1209,Creating a random forest,"So let's create one in four lines of code. So here is a function to create a decision tree, so let's say, what —this is just the proportion of data— so let's say we put 75% of the data in each time —or we could change it to 50%— whatever, so this is the number of samples in this subset —I call it n— and so let's at random choose n times the proportion we requested from the sample and build a decision tree from that. And so now let's, 100 times, get a tree, and stick them all in a list using a list comprehension. And now let's grab the predictions for each one of those trees and then let's stack all those predictions up together and take their mean. And that is a Random Forest. And, what do we get?, one, two, three, four, five, six, seven, eight, that's some, yeah, seven lines of code. So Random Forests are very simple. This is a slight simplification, there's one other difference that Random Forests do which is when they build the decision tree they also randomly select a subset of columns and they select a different random subset of columns each time they do a split. And so the idea is you kind of want it to be as random as possible but also somewhat useful. So we can do that by creating a “RandomForestClassifier”, say how many trees do we want, how many samples per leaf and then fit —that is what we just did— and here's our mean absolute error which… Yeah, again, it's like not as good as our decision tree but it's still pretty good and again it's such a small data set it's hard to tell if that means anything and so we can submit that to Kaggle. So earlier on I created little function to submit to Kaggle, so now I just create some predictions and I submit to Kaggle. And, yeah, looks like it gave nearly identical results to a single tree."
fast.ai 2022 - Part 1,6,1358,Feature importance,"Now to one of my favorite things about Random Forest —and I should say, in… in most real world data sets of reasonable size, Random Forest basically always give you much better results than decision trees, this is just a small data set to show you what to do. One of my favorite things about Random Forests is we can do something quite cool with it, what we can do is we can look at the underlying decision trees they create, so we've now got 100 decision trees, and we can see what columns did it find a split on and so, say here, okay well the first thing it spit on was ‘Sex’, and it improved the Gini from 0.47 to —now just take the weighted average of 0.38 and 0.31 weighted by the samples— so that's probably going to be, I don't know, about 0.33. So I'd say, okay, it's like 0.14 Improvement in Gini thanks to ‘Sex’. And we can do that again: okay, we'll then ‘PClass’, you know, how much did that improve Gini, again, we keep weighting it by the number of samples as well. ‘LogFare’, how much does that improve Gini and we can keep track: for each column of, how much in total did they improve the Gini in this decision tree. And then do that for every decision tree, and then add them up per column and that gives you something called a feature importance plot, and here it is. And a feature importance plot tells you how important is each feature, how often did the trees pick it and how much did it improve the Gini when it did. And so, we can see from the feature importance plot that ‘Sex’ was the most important, and class was the second most important and everything else was a long way back. And this is another reason, by the way, why a Random Forest isn't really particularly helpful: because it's just such a easy split to do, right? basically all that matters is, you know, what class you're in and whether you're male or female. And these feature importance plots, remember: because they're built on Random Forests, and Random Forests don't care about, really, the distribution of your data and they can handle categorical variables and stuff like that, that means that you can basically, any tabular data set you have, you can just plot this, right away, and Random Forests, you know, for most data sets, only take a few seconds to train, you know, really most of a minute or two. And so, if you've got a big data set and, you know, hundreds of columns: do this first and find the 30 columns that might matter, it's such a helpful thing to do. So I've done that, for example, I did some work in credit scoring, so we're trying to find out which things would predict who's going to default on a loan and I was given something like seven thousand columns from the database, now I put it straight into a Random Forest and found, I think, there was about 30 columns that seemed kind of interesting. I did that like two hours after I started the job and I went to the head of marketing and the head of risk and I told them: here's the columns I think that we should focus on, and they were like: “oh my God, we just finished a two-year consulting project with one of the big consultants, paid the millions of dollars, and they came up with a subset of these… «laughs»"
fast.ai 2022 - Part 1,6,1597,Adding trees,"There are other things that you can do with Random Forests along this path, I'll touch on them briefly, and specifically I'm going to look at Chapter 8 of the book, which goes into this in a lot more detail. And particularly interestingly Chapter 8 of the book uses a much bigger and more interesting data set which is auction prices of heavy industrial equipment, I mean, it's less interesting historically but more interestingly numerically. And so, some of the things I did there on this data set — sorry this isn’t from the data set, this is from the scikit-learn documentation— they looked at how, as you increase the number of estimators, so the number of trees, how much does the accuracy improve. So I then did the same thing on our data set, so I actually just added up to 40, more and more and more trees, and you can see that basically —as predicted by that kind of an initial bit of hand wavy theory I gave you— that you'd expect the more trees, the lower the error, because the more things you're averaging and that's exactly what we find: the accuracy improves as we have more trees. John what's up… John: Oh, Víctor… it is possible you might have just answered his question actually as he typed it— but he's asking on the same theme: the number of trees in a Random Forest. Does increasing the number of trees always translate to a better error? Jeremy: yes it does, always, I mean: tiny bumps, right? but yeah, once you smoothed it out. But decreasing returns and… if you end up productionizing a Random Forest, then, of course, every one of these trees, you have to, you know, go through for, at inference time, so it's not that there's no cost, I mean, having said that, zipping through a binary tree is the kind of thing you can really do fast, in fact, it's it's quite easy to like literally spit out C++ code with a bunch of if statements and compile it and get extremely fast performance. I don't often use more than 100 trees, this is a rule of thumb… is that the only one John? okay"
fast.ai 2022 - Part 1,6,1772,What is OOB,"So, then there's another interesting feature of Random Forests which is: remember how in our example we trained with 75% of the data on each tree, so that means for each tree there was 25% of the data we didn't train on. Now this actually means if you don't have much data, in some situations you can get away with not having a validation set, and the reason why is because for each tree we can pick the 25% of rows that weren't in that tree and see how accurate that tree was on those rows and we can average for each row their accuracy on all of the trees in which they were not part of the training, and that is called the Out-of-Bag Error or OOB error. And this is built in, also, scikit-learn you can ask for an OOB prediction… Uhm, John! John: Just before we move on, Zakia has a question about bagging: so we know that bagging is powerful as an ensemble approach to machine learning, would it be advisable to try out bagging being first when approaching a particular, say, tabular task, before deep learning? So that's the first part of the question, and the second part is: could we create a bagging model which includes fast.ai deep learning models? Jeremy: Yes, absolutely. So, to be clear, you know, bagging is kind of like a meta method, it's not a prediction, it's not a method of modeling itself, it's just a method of combining other models. So Random Forests, in particular, as a particular approach to bagging. is a, you know, I would probably always start, personally, a tabular project with a Random Forest because they're nearly impossible to mess up, and they give good insight, and they give a good base case. But yeah, your question then about “can you bag other models?” is a very interesting one, and the answer is: you absolutely can. And people very rarely do, but we will, we will, quite soon, maybe even today."
fast.ai 2022 - Part 1,6,1928,Model interpretation,"So I, you know… you might be getting the impression I'm a bit of a fan of Random Forests, and (~before I was…) before, you know, people thought of me as the Deep Learning guy, people thought of me as the Random Forest guy. I used to go on about Random Forests all the time and one of the reasons I'm so enthused about them isn't just that they're very accurate or (~that they require, you know…) that they're very hard to mess up and require very little (~processing…) pre-processing, but they give you a lot of quick and easy insight. And specifically these are the five things which I think that we're interested in and all of which are things that random for us are good at. They will tell us how confident are we in our predictions on some particular row? So when somebody, you know… when we're giving a loan to somebody we don't necessarily just want to know “How likely are they to repay?” but we'd also like to know “How confident are we that we know?” because if we're… if we're like well we think they'll repay but we're not confident of that we would probably want to give them less of a loan. And another thing that's very important is when we're then making a prediction… so again, for example, for credit… let's say you rejected that person's loan… “Why?...” And a Random Forest will tell us “What what is the… what is the reason that we made a prediction.” And you'll see why, and all these things. Which columns are the strongest predictors - you've already seen that one, right, that's the Feature Importance Plot. Which columns are effectively redundant with each other, i.e. they're basically highly correlated with each other. And then one of the most important ones… As you vary a column how does it vary the predictions? So for example in your credit model, how does your prediction of risk vary as you vary… (well something that probably the regulator would want to know might be some, you know…) some protected variable like you know race or some socio-demographic characteristics that you're not allowed to use in your model. So they might check things like that. For the first thing: “How confident are we in our predictions using a particular row of data?” There's a really simple thing we can do which is… remember how when we calculated our predictions manually we stacked up the predictions together and took their mean? Well what if you took their standard deviation instead? So if you stack up your predictions and take their standard deviation, and if that standard deviation is high, that means all of them (all of the trees) are predicting something different!! And that suggests that we don't really know what we're doing. And so that would happen if different subsets of the data end up getting completely different trees for this particular row. So there is (like…) a really simple thing you can do to get a sense of your prediction confidence. Okay, “Feature Importance” we've already discussed After I do feature importance (you know…) like I said when I had the (what) 7,000 or so columns I got rid of like all-but 30. That doesn't tend to improve the predictions of your Random Forest very much, if at all, but it certainly helps (like, you know…) kind of logistically thinking about cleaning up the data, you can focus on cleaning those 30 columns, and stuff like that. So I tend to remove the low importance variables."
fast.ai 2022 - Part 1,6,2147,Removing the redundant features,"I'm going to skip over this bit about removing redundant features because it's a little bit outside what we're talking about, but definitely check it out in the book, something called a “Dendrogram.” But what I do want to mention is the “Partial Dependence.” This is the thing which says,"
fast.ai 2022 - Part 1,6,2159,What does Partial dependence do,"“What is the relationship between a column and the dependent variable - and so this is something called a “Partial Dependence Plot.” Now this one's actually not specific to Random Forests. A partial dependence plot is something you can do for basically any machine learning model. Let's first of all look at one and then talk about how we make it. So in this dataset we're looking at the relationship… we're looking at the sale price at auction of heavy industrial equipment like bulldozers - this is specifically the blue books for bulldozers Kaggle competition. And a partial dependence plot between the year that the bulldozer -or whatever was made- and the price it was sold for (this is actually the log price) is that it goes up. More recent bulldozers… more recently made bulldozers are more expensive. And as you go back… back to older and older bulldozers, they're less and less expensive, to a point. And maybe these ones are some classic bulldozers you pay a bit extra for. Now you might think that you could easily create this plot by simply looking at your data at each year and taking the average sale price, but that doesn't really work very well. I mean it kind of does, but it kind of doesn't. Let me give an example. It turns out that one of the biggest predictors of sale price for industrial equipment is whether it has air conditioning and so air conditioning is, you know… it's an expensive thing to add and it makes the equipment more expensive to buy. And most things didn't have air conditioning back in the 60s and 70s and most of them do now. So if you plot the relationship between year made and price you're actually going to be seeing a whole bunch of when… you know… “How popular was air conditioning?” right, so you get this this cross-correlation going on. But we just want to know… what's just the impact of the year it was made all else being equal. So there's actually a really easy way to do that, which is: We take our data set ~(we take the we…) and we leave it exactly as it is, so just use the training dataset… but we take every single row - and for the year made column we set it to 1950. And so then we predict for every row what would the sale price of that have been if it was made in 1950, and then we repeat it for 1951, and then repeat it for 1952 and so forth, and then we plot the averages. And that does exactly what I just said, remember I said the special words, “all else being equal…” This is setting everything else equal. It's the… Everything else is the data as it actually occurred and we're only varying YearMade. And that's what a partial dependence plot is! That works just as well for Deep Learning or Gradient Boosting Trees or Logistic Regressions or whatever. It's a really cool thing you can do. And you can do more than one column at a time, you know… You can do two-way partial dependence plots, for example."
fast.ai 2022 - Part 1,6,2362,Can you explain why a particular prediction is made,"Okay so then another one I mentioned was: “Can you describe why a particular prediction was made? So how did you decide for this particular row to predict this particular value?” And this is actually pretty easy to do - there's a thing called Tree Interpreter but we could you could easily create this in about a half a dozen lines of code. All we do is we're saying, okay this customers come in, they've asked her a loan, we put in all of that data through the Random Forest, spat out a prediction… We can actually have a look and say okay, “Well that in Tree #1 what's the path that went down through the tree to get to the leaf node?” And we can say, oh well, first of all it looked at sex, and then it looked at postcode, and then it looked at income, and so we can see exactly in Tree #1 which variables were used and what was the change in ginni for each one. And then we could do the same in Tree #2, same in Tree #3, same in Tree #4… Does this sound familiar? It's basically the same as our Feature Importance Plot, right, but it's just for this one row of data. And so that will tell you basically the Feature Importances for that one particular prediction. And so then we can plot them, like this. So for example, this is an example of an auction price prediction, and according to this plot, you know, so we predicted that the net would be… (oh, this is just a change from…) So I don't actually know what the price is, but this is how much each one impacted the price. So Year Made, I guess this must have been an older tractor - it caused our prediction of the price to go down. But then it must have been a larger machine - the Product Size caused it to go up, a Coupler System made it go up, Model ID made it go up, and so forth, right. So you can see the Reds says this made our prediction go down. Green made our prediction go up. And so overall you can see which things had the biggest impact on the prediction and what was the direction for each one. So it's basically a feature importance plot but just ~(for a single roll) for a single row Any questions John? John: Yeah, there are a couple that have, that have sort of queued up, this is a, this is a good spot to, to jump to them. So, first of all, Andrew is asking, jumping back to the OOB era: “would you ever exclude a tree from a forest you've had if it had a bad Out of Bag Error?” Like if you, if you have a bogus, if you had a particularly bad tree in your ensemble Jeremy: Yeah… John: Might you just drop… Would you delete a tree that was not doing its thing? It's not playing its part. Jeremy: No you wouldn't. If you start deleting trees then you are no longer having an unbiased prediction of the dependent variable. You are biasing it by making a choice so, even the bad ones, will be improving the quality of the overall average. John: All right, thank you. Zakia followed up with the question about bagging and we're just going, you know, layers and layers here, you know: we could go on and create ensembles of bagged models? and, you know, is it reasonable to assume that they would continue… Jeremy: So that's not going to make much difference, right? If they're all like… you could take your 100 trees, split them into groups of ten, create ten bagged ensembles and then average those but the average of an average is the same as the average. You could like have a wider range of other kinds of models, you could have like Neural Nets trained on different subsets as well but again, it's just the average of an average, we'll still give you the average. John: Right, so there's not a lot of value in, kind of, structuring the ensemble you just… Jeremy: I mean, some, some ensembles you can structure, but not bagging. Bagging's the simplest one, it's the one I mainly use, there are more sophisticated approaches but this one is nice and easy. John: All right, and there's one that is a bit specific and it's referencing content you haven't covered but we're here now so... and it's on explainability so, feature importance of Random Forest models sometimes has different results when you compare it to other explainability techniques like SHAP, S-H-A-P, or LIME. And we haven't covered these in the course but Amir is just curious if you've got any thoughts on which is more accurate or reliable: Random Forest feature importance or other techniques Jeremy: I would lean towards, more immediately, trusting Random Forest Feature Importances over other techniques on the whole, on the basis that it's very hard to mess up a Random Forest. So, yeah, I feel like pretty confident that a Random Forest Feature Importance is gonna be pretty reasonable as long as this is the kind of data which a Random Forest is likely to be pretty good at, you know, doing, you know, if it's like a computer vision model Random Forest aren’t particularly good at that and so, one of the things that Breiman talked about a lot was explainability and he's got a great essay called the two cultures of statistics (“Statistical Modeling: The Two Cultures”) in which he talks about —I guess what nowadays call kind of like data scientists, machine learning folks versus classic statisticians—. And he was, you know, definitely a data scientist well before the label existed. And he pointed out, yeah, you know, first and foremost you need a model that's accurate. It needs to make good predictions. A model that makes bad predictions, will also be bad for making explanations because it doesn't actually know what's going on. So if, you know, if you, if you've got a Deep Learning model that's far more accurate than your Random Forest then it's, you know, explainability methods from the Deep Learning model world probably be more useful because it's explaining a model that's actually correct. All right, let's take a 10 minute break and we'll come back at five past seven."
fast.ai 2022 - Part 1,6,2767,Can you overfit a random forest,"Welcome back, one person pointed out I noticed I got the chapter wrong — it's Chapter 9 not Chapter 8 in the book, I guess I can't read. Somebody asked during the break about overfitting. Can you overfit a Random Forest?. Basically no, not really, adding more trees will make it more accurate. It kind of asymptotes so you can't make it infinitely accurate by using infinite trees but certainly you know adding more trees won't make it worse. If you don't have enough trees and you let the trees grow very deep: that could overfit. So you just have to make sure you have enough trees. Radek told me about this experiment he did during… Radek talked to me during the break about an experiment he did, which is something I've done something similar, which is adding lots and lots of randomly generated columns to a data set and try to break the Random Forest. And if you tried it, it basically doesn't work, it's like, it's really hard to confuse a Random Forest by giving it lots of meaningless data, it does an amazingly good job of picking out the useful stuff. As I said, you know, I had 30 useful columns out of 7,000 and it found them perfectly well. And often, you know, when you find those 30 columns, you know, you could go to, you know —I was doing consulting at the time— go back to the client and say like: tell me more about these columns, and say like: “oh! well that one there we've actually got a better version of that now, there's a new system, you know, we should grab that… and, oh, this column actually that was because of this thing that happened last year but we don't do it anymore…” or, you know, like you can really have this kind of discussion about the stuff you've zoomed into. You know, there are other things that you have to think about with lots of kinds of models like, particularly regression models; things like interactions. You don't have to worry about that with Random Forests like, because you split on one column and then split on another column you get interactions for free as well. Normalization you don't have to worry about, you know you don't have to have normally distributed columns. So yeah, definitely worth a try. Now something I haven't gone into… is Gradient Boosting."
fast.ai 2022 - Part 1,6,2943,What is gradient boosting,"But if you go to explain.ai, you'll see that my friend Terence and I have a three-part series about Gradient Boosting, including pictures of golf made by Terence. But to explain Gradient Boosting is a lot like Random Forests but rather than fitting a tree again and again and again on different random subsets of the data… instead, what we do is we fit very, very, very small trees, so hardly ever any splits and we then say: “okay, what's the error”. So, you know, so, imagine the simplest tree would be our “OneR” raw tree of male versus female, say, and then you take what's called the residual: that's the difference between the prediction and the actual, it's the error. And then you create another tree, which attempts to predict that —a very small tree— and then you create another very small tree which tries to predict the error from that and so forth, right? Each one is predicting the residual from all of the previous ones. And so then to calculate a prediction, rather than taking the average of all the trees, you take the sum of all the trees, because each one is predicting the difference between the actual and all of the previous trees. And that's called boosting —versus bagging— so boosting and bagging are two kind of meta ensembling techniques, and when bagging is applied to trees it's called a Random Forest and when boosting is applied to trees it's called a Gradient Boosting Machine or Gradient Boosted Decision Trees. Gradient Boosting is, generally speaking, more accurate than Random Forests, but you can absolutely overfit and so therefore it's not necessarily my first go-to thing. Having said that there are ways to avoid overfitting, but yeah, (it's just, it's not, you know) because it's breakable, it's not my first choice. But yeah, check out our stuff here if you're interested and, you know, there is stuff which largely automates the process, there's lots of hyper parameters you have to select. People generally just, you know, try every combination of hyper parameters and, in the end, you generally should be able to get a more accurate Gradient Boosting model than Random Forest… but not necessarily by much. Okay, so that was the"
fast.ai 2022 - Part 1,6,3116,Introducing walkthrus,"Kaggle notebook on Random Forests: “How random forests really work”. So, what we've been doing is having this daily walk-through where me and —I don't know how many— 20 or 30 folks get together on a Zoom call and chat about, you know, getting through the course, and setting up machines, and stuff like that. And, you know, we've been trying to kind of practice what, you know, things along the way and so, a couple of weeks ago, I wanted to show: what does it look like to pick a Kaggle competition and just like do the normal, sensible, kind of mechanical steps that you would do for any computer vision model. And so the competition I picked was Paddy Disease Classification, which is about recognizing diseases, rice diseases and rice paddies. And yeah I spent —I don't know— a couple of hours, or three —I can't remember—. A few hours, throwing together something and I found that I was number one on the leaderboard and I thought: “oh, that's interesting”, like, because you never quite have a sense of how well these things work. And then I thought: “well there's all these other things we should be doing as well”, and I tried three more things, and each time I tried another thing, I got further ahead at the top of the leaderboard. So I thought it'd be cool to take you through the process. I'm going to do it reasonably quickly because the walkthroughs are all available for you to see the entire thing in, you know, seven hours of detail —or however long we probably were, six to seven hours of conversations— but I want to kind of take you through the basic process that I went through."
fast.ai 2022 - Part 1,6,3268,What does fastkaggle do,"So since I've been studied to do more stuff on Kaggle, you know, I realized there's some kind of manual steps I have to do each time, particularly because I like to run stuff on my own machine and then kind of upload it to Kaggle. So to do, to to make my life easier I created a little module called fastKaggle which you'll see in my notebooks from now on, which you can download from pip or conda, and as you'll see it makes some things a bit easier. For example: downloading the data for the Paddy Disease Classification, if you just run “setup_comp()” and pass in the name of the competition, if you are on Kaggle, it will return a path to that competition data that's already on Kaggle; if you are not on Kaggle and you haven't downloaded it, it will download and unzip the data for you. If you're not on Kaggle and you have downloaded and unzip the data, it will return a path to the one that you've already downloaded. Also if you are on Kaggle you can ask it to make sure that pip things are installed —that might not be up to date otherwise— so that's basically one line of code now gets us all set up and ready to go. So this path… so I ran this particular one on my own machine, so it downloaded and unzipped the data. I've also got links to the six walkthrus so far: these are the videos. Oh yes, and here's my result after these four attempts, that's a few fiddling around at the start. So the overall approach at, is, well and this is not just a Kaggle competition, right? the reason I like looking at Kaggle competitions is you can't hide from the truth in a Kaggle competition, you know, when you're working on some work project or something, you might be able to convince yourself and everybody around you that you've done a fantastic job of not overfitting, and that your model's better than what anybody else could have made, and whatever else; but the brutal assessment of the private leaderboard will tell you the truth. Is your model actually predicting things correctly? and, is it overfit? Until you've been through that process, you know, you're never going to know. And a lot of people don't go through that process because at some level they don't want to know. But it's okay, you know, nobody needs it, you don't have to put your own name there. I always did, right from the very first one I wanted, you know, if I was going to screw up royally I wanted to have the pressure on myself of people seeing me in last place. But you know, it's fine, you can do it all anonymously and you'll actually find as you improve you'll have so much self-confidence, you know. And the stuff we do in a Kaggle competition is indeed a subset of the things we need to do in real life, but it's an important subset, you know. Building a model that actually predicts things correctly and doesn't overfit is important. And furthermore structuring your code and analysis in such a way that you can keep improving over a three-month period without gradually getting into more and more of a tangled mess of impossible to understand code and having no idea what UntitledCopy13 was, and why it was better than 25, right, this is all stuff you want to be practicing, ideally well away from customers or whatever, you know, before you've kind of figured things out. So the things I talk about here about doing things well in this Kaggle competition should work, you know, in other settings as well. And so these are the two focuses that I recommend: Get a really good validation set together - we've talked about that before, right, and in a Kaggle competition (that's like…) it's very rare to see people do well in a Kaggle competition who don't have a good validation set. Sometimes that's easy, and this competition actually it is easy, because the the the test set seems to be a random sample, but most of the time it's not actually, I would say. And then how quickly can you iterate? How quickly can you try things and find out what worked? So obviously you need a good validation set otherwise it's impossible to iterate. And so “quickly iterating” means not saying what is the biggest, you know, OpenAI takes four months on 100 TPUs model that I can train. It's what can I do that's going to train in a minute or so and will quickly give me a sense of… like well I could try this, I could try that, what things going to work, and then try, you know, 80 things. It also doesn't mean that saying like… Oh I heard this is amazing new Bayesian hyper parameter tuning approach, I'm going to spend three months implementing that, because that's gonna, like, give you one thing. But actually to do well in these competitions or in machine learning in general, you actually have to do everything reasonably well. And doing just one thing really well will still put you somewhere about last place. So I actually saw that a couple of years ago, an Aussie guy who's a very very distinguished machine learning practitioner, actually put together a team and entered a Kaggle competition and literally came in last place, because they spent the entire three months trying to build this amazing new fancy thing and never actually, never actually iterated. If you iterate I guarantee you won't be in last place. Okay, so here's how we can grab our data, with FastKaggle, and it gives us (tells us) what path it's in. And then I set my random seed - and I only do this because I'm creating a notebook to share. You know, when I share a notebook I like to be able to say “as you can see this is 0.83 blah blah blah right and know that when you see it, it'll be 0.83 as well. But when I'm doing stuff otherwise, I would never set a random seed. I want to be able to run things multiple times and see how much it changes each time, all right, because that'll give me a sense of like… are the modifications I'm making, changing it because they're improving it or making it worse, or is it just random variation. So if you (or if you) always set a random seed, that's a bad idea because you won't be able to see the random variation. So this is just here for presenting a notebook. Okay, so the data they've given us, as usual, they've got a sample submission, they've got some test set images, they've got some training set images, a CSV file about the training set and then these other two you can ignore because I created them. So let's grab a path to train images… and so do you remember get_image_files()... so that gets us a list of the file names of all the images here recursively, so we could just grab the first one, and take a look - so it's 480 by 640. Now we've got to be careful. This is a pillow image (Python Imaging Library image.) In the imaging world they generally say columns by rows. In the array slash tensor world we always say rows by columns. So if you ask pytorch what the size of this is it'll say 640 by 480, and I guarantee at some point this is going to bite you. So try to recognize it now. Okay so they're kind of taller than they are… at least this one is taller than it is wide."
fast.ai 2022 - Part 1,6,3772,fastcore.parallel,"So I actually like to know were they all this size, because it's really helpful if they all are all the same size, or at least similar. Believe it or not the amount of time it takes to decode a JPEG is actually quite significant and so figuring out what size these things are is actually going to be pretty slow, but my fastcore library has a parallel sub module which can basically do anything that you can do in Python, it can do it in parallel. So in this case we wanted to create a pillow image and get its size. So if we create a function that does that and pass it to parallel, passing in the function and the list of files, it does it in parallel and that actually runs pretty fast. And so here is the answer… (I don't know how this happened…) 10,403 images are indeed 480 by 640 and four of them aren't. So basically what this says to me is that we should pre-process them or you know at some point process them so that they're probably all 480 by 640, or all basically kind of the same size. We'll pretend they're all this size but we can't not do some initial resizing otherwise this is going to screw things up."
fast.ai 2022 - Part 1,6,3852,"item_tfms=Resize(480, method='squish')","So like the probably the easiest way to do things, the most common way to do things, is to either squish or crop every image to be a square. So squishing is when you just… (in this case…) squish the aspect ratio down as opposed to cropping randomly a section out. So if we call Resize() ‘squish’ it will squish it down, and so this is 480 by 480 squared. So this is what it's going to do to all of the images first, on the CPU, that allows them to be all batched together into a single mini batch (everything in a mini batch has to be the same shape otherwise the GPU won't like it,) and then that many batches put through data augmentation, and it will grab a random subset of the image, and make it a 128 by 128 pixels. And here's what that looks like, here's our data. So show_batch() works for pretty much everything, not just in the fast AI library, but even for things like FastAudio which are kind of community based things. You should better use show_batch() on anything and see, or hear, or whatever, what your data looks like. I don't know anything about rice disease but apparently these are various rice diseases and this is what they look like. So I jump into creating models much more quickly than most people, because I find… model, you know, models… are a great way to understand my data, as we've seen before. So I basically build a model as soon as I can, and I want to create a model that's going to let me iterate quickly. So that means that I'm going to need a model that can train quickly. So Thomas Capell and I recently"
fast.ai 2022 - Part 1,6,3980,Fine-tuning project,"did this big project “The Best Vision Models For Fine Tuning” where we looked at nearly a hundred different architectures from Ross Whiteman's Timm Library (Pytorch Image Model Library) and looked at which ones could we fine-tune - which ones had the best transfer learning results. And we tried two different datasets, very different datasets. One is the pets dataset that we've seen before - so trying to predict what breed of pet is from 37 different breeds. And the other was a satellite imagery data set called planet. So very very different data sets in terms of what they contain and also very different sizes. The planet one is a lot smaller the Pets one is a lot bigger. And so the main things"
fast.ai 2022 - Part 1,6,4042,Criteria for evaluating models,"we measured were how much memory did it use, how accurate was it, and how long did it take to fit. And then I created this score which can… which combines the fit, time and error rate together. And so this is a really useful table for picking a model. And now in this case I want to pick something that's really fast, and there's one clear winner on speed, which is resnet26d. And so its accuracy was 6% versus the best was like 4.1% - so okay it's not amazingly accurate, but it's still pretty good, and it's going to be really fast, so that's why I picked “resnet26d. A lot of people think that when they do deep learning they're going to spend all of their time learning about exactly how a resnet26d is made and convolutions and resonant blocks and transformers and blah blah blah. We will cover all that stuff in Part Two and a little bit of it next week but it almost never matters, right, it's just a function, right, and what matters is the inputs to it, and the outputs to it, and how fast it is, and how accurate it is. So let's create a learner which with a resnet26d from our data loaders, and let's run lr_find(). So lr_find() will put through one mini-batch at a time starting at a very very very low learning rate, and gradually increase the learning rate, and track the loss. And initially the loss won't improve because the learning rate is so small it doesn't really do anything, and at some point the learning rate is high enough that the loss will start coming down, and then at some other point the learning rate is so high that it's going to start jumping past the answer and it's going to predict worse. And so somewhere around here is a learning rate we'd want to pick. We've got a couple of different ways of making suggestions. I generally ignore them because these suggestions are specifically designed to be conservative. They're a bit lower than perhaps optimal, in order to make sure we don't recommend something that totally screws up, but I kind of like to say, like well… How far right can I go and still see it, like, clearly really improving quickly, and so I'd pick somewhere around 0.01 for this. So I can now fine-tune our model with a learning rate of 0.01, three epochs, so look!... the whole thing took a minute! That's what we want, right, we want to be able to iterate, rapidly, just a minute or so. So that's enough time for me to go and you know, grab a glass of water, or do some reading, like I’m not going to get too distracted. And what did we do before we submit? Nothing! We submit as soon as we can. Okay,"
fast.ai 2022 - Part 1,6,4222,Should we submit as soon as we can,"let's get our submission in. So we've got a model, let's get it in. So we read in our CSV file of the sample submission and so the CSV file basically looks like we're going to have to have a list of the image file names, in order, and then a column of labels. So we can grab all the image files in the test image —like so— and we can sort them. And so now we want is —what we want is— a dataloader, which is exactly like the dataloader we use to train the model, except pointing at the test set, we want to use exactly the same transformations so there's actually a dls.test_dl() method —which does that— you just pass in the new set of items, so the test set files. So this is a dataloader which we can use for our test set. A test dataloader has a key difference to a normal data loader which is that it does not have any labels. So that's a key distinction. So we can get the predictions for our learner, passing in that dataloader and in the case of a classification problem you can also ask for them to be decoded, decoded means rather than just get returned the probability of every rice disease for every class, it'll tell you what is the index of the most probable rice disease, that's what decoded means. So this is returning the probabilities, targets —which obviously will be empty because it's a test set, so throw them away— and those decoded indexes which look like this: numbers from naught (0) to nine (9) because there's ten possible rice diseases. The Kaggle submission does not expect numbers naught (0) to nine (9), it expects to see strings like these. So, what do those numbers from naught (0) to nine (9) represent? We can look up our vocab to get a list, so that's zero, that's one, et cetera, that's nine. So, I realized later this is a slightly inefficient way to do it, but it does the job I need to be able to map these to strings so, if I enumerate the vocab, that gives me pairs of numbers; zero (0): bacterial leaf blight, one (1): bacterial leaf streak, etc. I can then create a dictionary out of that, and then I can use pandas to look up each thing in a dictionary: they call that map. If you're a pandas user you've probably seen map used before being passed a function —which is really, really slow— but if you pass map a dict it's actually really, really fast, so do it this way if you can. So here's our predictions, so we've got our submission sample: submission file “ss”, so if we replace this column label with our predictions —like so— then we can turn that into a CSV, and remember, this means, this means: run a bash command —a shell command—, head is the first few rows —let's just take a look. That looks reasonable! So we can now submit that to Kaggle. Now, iterating rapidly means everything needs to be fast and easy. Things that are slow and hard don't just take up your time but they take up your mental energy, so even submitting to Kaggle needs to be fast. So I put it into a cell, so I can just run this cell: api.competition_submit_cli(), this CSV file, give it a description, so just run the cell and it submits to Kaggle. And as you can see it says: here we go!, “successfully submitted”. So that submission was terrible: top eighty percent also known as bottom twenty percent, which is not too surprising, right?, I mean, it's one minute of training time. But it's something that we can start with and that would be like: however long it takes to get to this point, that you put in our submission, now you've really started, right? because then tomorrow you can try to make a slightly better one."
fast.ai 2022 - Part 1,6,4515,How to automate the process of sharing kaggle notebooks,"So I'd like to share my notebooks and so, even sharing the notebook, I've automated. So part of fastkaggle is: you can use this thing called push_notebook and that sends it off to Kaggle to create… a notebook on Kaggle, there it is, and there's my score. As you can see it's exactly the same thing. Why would you create public notebooks on Kaggle? Well, it's the same brutality of feedback that you get for entering a competition, but this time rather than finding out, in no uncertain terms, whether you can predict things accurately; this time you can find out —no it's no uncertain terms— whether you can communicate things in the way that people find interesting and useful. And if you get zero votes, you know, so be it, right? that's something to know and then, you know, ideally go and ask some friends, like: what do you think I could do to improve? and if they say: oh, nothing, it's fantastic! you can tell: no, that's not true, I didn't get any votes, I'll try again, this isn't good, how do I make it better, you know, and you can try and improve because if you can create models that predict things well, and you can communicate your results in a way that is clear and compelling, you're a pretty good data scientist, you know, like they're two pretty important things and so here's a great way to test yourself out on those things and improve. Yes John. John: yes Jeremy we have a sort of —I think— a timely question here from Zakia about your iterative approach. And they're asking: “do you create different Kaggle notebooks for each model that you try?” Jeremy: yeah… John: so one Kaggle book for the first one, then separate notebooks subsequently, or do you, do append to the bottom of us — Jeremy: yeah, yeah — but what's your strategy? Jeremy: that's a great question. And I know Zaki is going through the daily walkthroughs but isn't quite caught up yet so, I would say: keep it up because in the six hours of going through this you'll see me create all the notebooks… but, if I go to the actual directory I used, you can see them. So basically: yeah. I started with, you know, what you just saw, a bit messier without the pros, but that same basic thing, I then duplicated it to create the next one —which is here— and because I duplicated it, you know, this stuff which I still need it's still there, right? and so I run it. And I don't always know what I'm doing, you know, and so, at first, if I don't really know what I'm doing next I'm going to duplicate it, it will be called, you know, “first steps in the road to the top part one dash copy one”, you know, and that's okay. As soon as I can I'll try to rename that, once I know what I'm doing, you know, or if it doesn't seem to go anywhere I'll rename it into something like, you know, experiment blah blah blah and I'll put some notes at the bottom and I might put it into a failed folder or something. But yeah, it's like, it's a very low tech approach that I find works really well: which is just duplicating notebooks and editing them and naming them carefully and putting them in order and, you know, put the file name in when you submit as well. And then of course also if you've got things in git, you know, you can have a link to the git commit so you'll know exactly what it is… generally speaking from me, you know, my notebooks will only have one submission in and then I'll move on and create a new notebook so I don't really worry about versioning so much, but you can do that as well if that helps you. Yeah, so that's basically what I do and and I've worked with a lot of people who use much more sophisticated and complex processes and tools and stuff but none of them seem to be able to stay as well organized as I am, I think they kind of get a bit lost in their tools, sometimes. And file systems and file names I think are good. John: oh great thanks so, away from that kind of dev process, more towards the specifics of, you know, finding the best model and all that sort of stuff. We've got a couple of questions that are in the same space, which is, you know, we've got some people here talking about AutoML"
fast.ai 2022 - Part 1,6,4817,AutoML,"Frameworks which you might want to, you know, touch on for people who haven't heard of those. If you've got any particular AutoML Frameworks you think are worth recommending. Or just, more generally, how do you go trying different models. Random Forest, Gradient Boosting, Neural Networks… it just so in that space if you can comment it Jeremy: sure. I use AutoML less than anybody I know, I would guess, which is to say: never. Hyper parameter optimization: never. And the reason why is I like being highly intentional, you know, I like to think more like a scientist and have hypotheses and test them carefully and come up with conclusions —which then I implement, you know—. So for example, in this “best vision models for fine tuning” I didn't try a huge “grid search” of every possible model, every possible learning rate, every possible pre-processing approach blah blah blah right. Instead, step one was to find out: well, which things matter, right? So, for example, “does whether we squish or crop make a difference?” you know, “are some models better with squish and some models better with crop?” and so we just chested that for… and again, not for every possible architecture but for one or two versions of each of the main families, that took: 20 minutes. And the answer was: no, in every single case the same thing was better. So we don't need to do a grid search over that anymore, you know. Or another classic one is like, learning rates, most people do a, kind of, grid search over learning rates or they'll train a thousand models, you know, with different learning rates. But this fantastic researcher named Leslie Smith invented the learning rate finder a few years ago, we implemented it, I think, within days of it (first) coming out as a technical report, and that's what I've used ever since: it works well and runs in a minute or so. Yeah, I mean, then like Neural Nets versus GBMs versus Random Forests, I mean, that's… that shouldn't be too much of a question on the whole, like: they have pretty clear places that they go. Like, if I'm doing computer vision I'm obviously going to use a computer vision Deep Learning model and which one I would use, well, if I'm transfer learning —which hopefully is, always— I would look up the two tables here: this is my table for pets which is, which are the best at fine tuning to very similar things to what they're pre-trained on, and then the same thing for planet is: “which ones are best for fine tuning for two data sets that are very different on what they're trained on”? And it happens in both case they're very similar, in particular “convnext” is right up towards the top in both cases, so I just like to have these rules of thumb and… yeah, my rule of thumb for tabular is: Random Forest is going to be the fastest easiest way to get a pretty good result, GBMs probably going to give me a slightly better result if I need it, and can be bothered fussing around GBM I would probably, yeah, actually I probably would run a hyper parameter sweep because it is fiddly and it's fast, so you may as well."
fast.ai 2022 - Part 1,6,5056,Why the first model run so slow on Kaggle GPUs,"So, yeah, you know, we were able to make a slightly better submission, a slightly better model. And so, I had a couple of thoughts about this. The first thing was: that thing trained in a minute on my home computer and then, when I uploaded it to Kaggle, it took about four minutes per epoch —which was horrifying—. And, Kaggle GPUs are not amazing but they're not that bad so I knew something was up, and what was up is I realized that they only have two virtual CPUs which nowadays is tiny, like, you know, you generally want —as a rule of thumb— about eight physical CPUs per GPU. And so, spending all of its time just reading the damn data. Now, the data was 640 by 480 and we were ending up with only 128 pixel size bits for speed. So there's no point doing that every epoch so, step one was to make my Kaggle iteration faster, as well, and so a very simple thing to do: resize the images, so fastai has a function called resize_images() and you say: “okay, take all the train images and stick them in the destination, making them this size recursively”, and that will recreate the same folder structure over here. And so that's why I call this the training path: because this is now my training data and so, when I then, trained on that, on Kaggle, it went down to four times faster with no loss of accuracy so, that was kind of step one was: to actually get my first iteration working. Now, still, I bet it's a long time and on Kaggle you can actually see this little graph showing: how much the CPU is being used, how much the GPU is being used; on your own home machine you can —there are tools free GPU— you know, free tools to do the same thing. I saw that the GPU was still hardly being used so it's still CPU was being driven pretty hard, I wanted to use a better model anyway —to move up the leaderboard— so I moved from a… Oh, by the way, this graph is very useful, so this is… this is speed versus error rate by family. And so we're about to be looking at these convnext models so we're going to be looking at this one convnext_tiny… Here it is convnext_tiny. So we were looking at resnet26d, which took this long on this data set, but this one here is nearly the best (I think it's third best) but it's still very fast, and so it's the best overall score. So let's use this, particularly because, you know, we're still spending all of our time waiting for the CPU anyway. So it turned out that when I switched my architecture to convnext it basically ran just as fast, on Kaggle, so we can then train that."
fast.ai 2022 - Part 1,6,5273,How much better can a new novel architecture improve the accuracy,"Let me switch to the Kaggle version because my outputs are missing for some reason. So yeah… so I started out by running the resnet26d on the resized images and got similar error rate, but I ran a few more epochs, got 12% error rate. And so then I do exactly the same thing but with convnext_small and 4.5% error rate, so I don't think that different architectures are just tiny little differences - this is over twice as good. And a lot of folks you talk to will never"
fast.ai 2022 - Part 1,6,5313,Convnext,"have heard of this convnext because it's very new and I've noticed a lot of people tend not to keep up to date with new things. They kind of learn something at University and then they stop… stop learning. So if somebody's still just using resnets all the time, you know, you can tell them, we've actually… we've moved on, you know. Resnets is still probably the fastest but for the mix of speed and performance, you know, not so much. Convnext, you know again, you want these rules of thumb, right. If you're not sure what to do, this convnext, okay, and then like most things there's different sizes - there's a tiny, there's a small, there's a base, there's a large, there's an extra large, and you know it's just… well let's look at the picture. This is it here, right, large takes longer but lower error, tiny takes less time but higher error, right, so you pick about your speed versus accuracy trade-off, for you. So for us small is great. And so yeah now we've got a 4.5% error, that's terrific! Now let's iterate! On Kaggle this is taking about a minute per epoch. On my computer it's probably taking about 20 seconds per epoch, so not too bad. So you know, one thing we could try is instead of using squish as our pre-processing, let's try using crop. So that will randomly crop out an area, and that's the default, so if I remove the “method=squish” that will crop. So you see how I've tried to get everything into a single function, right, the single function. I can tell it… (let's go and find the definition…) what architecture do I want to train, how do I want to transform the items, how do I want to transform the batches, and how many epochs do I want to do - that's basically it, right. So this time I want to use the same architecture convnext, I want to resize without cropping, and then use the same data augmentation, and okay, error rate's about the same. So not particularly… it's a tiny bit worse, but not enough to be interesting."
fast.ai 2022 - Part 1,6,5470,How to iterate the model with padding,"Instead of cropping, we can pad. Now padding is interesting… do you see how these are all square, right, but they've got black borders… so padding is interesting because it's the only way of pre-processing images which doesn't distort them and doesn't lose anything. If you crop, you lose things. If you squish, you distort things. This does neither. Now of course the downside is that there's pixels that are literally pointless - they contain zeros. So every way of getting this working has its compromises but this approach of resizing where we pad with zeros is not used enough - and it can actually often work quite well - in this case it was about as good as our best so far, but no, not huge differences yet."
fast.ai 2022 - Part 1,6,5521,What does our data augmentation do to images,"What else could we do? Well, what we could do is… see these pictures? This is all the same picture but it's gone through our data augmentation, so sometimes it's a bit darker, sometimes it's flipped horizontally, sometimes it's slightly rotated, sometimes it's slightly warped, sometimes it's zooming into a slightly different section, but this is all the same picture. Maybe our model would like some of these versions better than others, so what we can do is we can pass all of these to our model, get predictions for all of them, and take the average, right. So it's our own kind of like little mini-bagging approach, and this is called Test Time Augmentation. Fast.ai is very unusual in making that available in a single method. You just pass TTA and it will pass multiple augmented versions of the image and average them for you. And so this is the same model as before, which had a 4.5%, so if instead if we get TTA predictions and then get the error rate, wait why does it say 4.8?... last time I did this it was way better. Well that's messing things up isn't it? So when I did this originally on my home computer it went from like 4.5 to 3.9, so possibly I got a very bad luck this time. So this is the first time I've actually ever seen TTA give a worst result. So that's very weird. I wonder if it's… if I should use something other than the crop-n-padding. All right, I'll have to check that out, and I'll try and come back to you and find out why in this case this one was worse."
fast.ai 2022 - Part 1,6,5652,How to iterate the model with larger images,"Anyway take my word for it every other time I've tried it TTA has been better, so then you know now that we've got a pretty good way of resizing, we've got TTA, we've got a good training process, let's just make bigger images, and something that's really interesting and a lot of people don't realize is your images don't have to be square, they just all have to be the same size, and given that nearly all of our images are 640x480 we can just pick, you know, that aspect ratio (so for example 256x192) and we'll resize everything to the same aspect ratio rectangular, and that should work even better still. So if we do that we'll do 12 epochs… okay now our error rate is down to 2.2% And then we'll do TTA. Okay this time you can see it actually improving, down to under 2% so that's pretty cool, right… we've got our error rate… at the start of this notebook we were at 12% and by the time we've got through our little experiments we're down to under 2%. And nothing about this is in any way specific to rice, or this competition, you know, it's like this is a very mechanistic, you know, standardized approach, which you can use for certainly any kind of this type of computer vision competition - they'd have computer vision data set almost. But you know, it looked very similar for a collaborative filtering model, a tabular model, NLP model, whatever."
fast.ai 2022 - Part 1,6,5768,pandas indexing,"So, of course, again, I want to submit as soon as I can. So, just copy and paste the exact same steps I took last time basically for creating a submission. So, as I said last time, we did it using pandas, but there's actually an easier way. So the step where here I've got the numbers from naught to nine, which is like, which… which rice disease is it? so here's a cute idea; we can take our vocab, and make it an array so that's going to be a list of 10 things and then we can index into that vocab with our indices which is kind of weird this is a list of 10 things this is a list of… I don't know four or five thousand things? so this will give me four or five thousand results which is each vocab item for that thing? So this is another way of doing the same mapping, and I would spend time playing with this code to understand what it does, because it's the kind of like very fast, what, you know, not just in terms of writing, but this this… the… this would optimize… you know on on the CPU very very well.This is the kind of coding you want to get used to, this kind of indexing. Anyway, so then we can submit it just like last time, and when I did that I got in the top 25 percent, and that's… that's where you want to be, right? Like generally speaking, I find in Kaggle competitions the top 25 percent is like, you're kind of like solid competent level? you know? look it's not to say like, it's not easy you've got to know what you're doing, but if you get in the top 25, and I think you can really feel like yeah this is… this is a… you know very reasonable attempt, and so that's I think, this is a very reasonable attempt. Okay before we wrap up, John any last questions? John: Um yeah, there's there's two I think that would be good if we could touch on quickly before"
fast.ai 2022 - Part 1,6,5896,What data-augmentation does tta use?,"you wrap up; one from Victor asking about TTA: “when I use TTA during my training process do I need to do something special during inference or is this something you use only during validate?” Jeremy: Okay so just to explain TTA means “test time augmentation” so, specifically it means inference. I think you mean augmentation during training? so yeah… so during training, you basically always do augmentation which means you're varying each image slightly so that the model never sees the same image exactly the same twice, and so it can't memorize it. On Fast Ai – and as I say I don't think anybody else does this as far as I know – if you call TTA, it will use the exact same augmentation approach on whatever data set you pass it, and average out the prediction, but… but like multiple times on the same image, and will average them out, so you don't have to do anything different, but if you didn't have any data augmentation in training you can't use TTA. It uses the same… by default the same data augmentation you use for training. John: Great! thank you! and the other one is about how you know when you first started this example you squared the models and you the images rather, and you talked about squashing versus cropping versus you know, clipping and scaling, and so on, but then you went on to say that these models can actually take rectangular inputs right? so there's a question that's kind of probing at that… you know, if the…” if the models can take rectangular inputs why would you ever even care as long as they're all the same size?” Jeremy: So, I find most of the time data sets tend to have a wide variety of input sizes and aspect ratios, so you know if there's just as many tall skinny ones as wide short ones you know you doesn't make sense to create a rectangle because some of them… you're going to really destroy them, so a square is the kind of best compromise in some ways there are better things we can do which we don't have any off-the-shelf Library support for yet, and I don't think… I don't know, that anybody else has even published about this, but we've experimented with kind of trying to batch things that are similar aspect ratios together, and use the kind of median rectangle for those and have had some good results with that, but honestly not 89, 99 percent of people, given a wide variety of aspect ratios, chuck everything into a square. John: A follow-up! this is my own interest, “have you ever looked at you know so the issue with with padding as you say is that you're putting you know black pixels there those are not NANs, those are black pixels, that's right, and so there's something problematic to me you know conceptually about that, you know, when you… when you see for example four to three aspect ratio footage presented for broadcast on 16 to nine you get the kind of the Blurred stretch? that kind of stuff? Jeremy: No, we played with that a lot yeah? I used to be really into it actually, and fastai, still by default uses a reflection padding which means if this is… I don't know, let's say this is a 20 pixel wide thing it takes the 20 pixels next to it and flips it over and sticks it here and it looks pretty good you know another one is copy which simply takes the outside pixel and it's a bit more like TV you know um…you know, much too much. Again, it turns out none of them really helped! plus, you know, if anything they make it worse, because in the end the computer wants to know “no this is the end of the image there's nothing else here”, and if you reflect it for example, then you're kind of creating weird spikes that didn't exist, and the computer's got to be like, “oh I wonder what that spike is?”, so yeah it's a great question, and I obviously spent like a couple of years assuming that we should be doing things that look more image like, but actually the computer likes things to be presented to it in as straightforward a way as possible. All right! thanks everybody! and hope to see some of you in the walkthroughs, and otherwise see you next time!"
fast.ai 2022 - Part 1,7,0, Tweaking first and last layers,"All right. Welcome to lesson 7, the penultimate lesson, of Practical Deep Learning for Coders Part 1, and today we're going to be digging into what's inside a neural net. We've already seen what's inside a kind of the most basic possible neural net, which is a sandwich of fully connected layers, or linear layers, and values. And so we built that from scratch, but there's a lot of tweaks that we can do, and so most of the tweaks actually that we probably care about are tweaking the very first layer, or the very last layer. So that's where we'll focus. But over the next couple of weeks we'll look at some of the tricks we can do inside as well. So I'm going to do this through the lens of the Paddy… the Rice Paddy competition we've been talking about, and we got to a point where — let's have a look… So we created a ConvNeXt model, we tried a few different types of basic pre-processing, we added Test time augmentation (TTA) and then we scaled that up to large images, and rectangular images. And that got us into the top 25 percent of the competition, so that's Part 2 of the so-called Road to the Top series, which is increasingly misnamed, since we've been presenting these notebooks, more and more of our students have been passing me on the leaderboard, so, currently first and second place are both people from this class: Kurian and Nick. “Go to hell, you're in my target, and leave my class immediately!” And congratulations, good luck to you. So in Part 3 I'm going to show you a really interesting trick —a very simple trick—"
fast.ai 2022 - Part 1,7,167, What are the benefits of using larger models,"for scaling up these models further. What you'll discover if you've tried to use larger models… so you can replace the word small with the word large in those architectures, and try to train a larger model: a larger model has more parameters, more parameters means it can find more tricky little features, and broadly speaking models with more parameters, therefore, ought to be more accurate. The problem is that those activations — or more specifically — the gradients that have to be calculated chews up memory on your GPU, and your GPU is not as clever as your CPU at kind of sticking stuff it doesn't need right now into virtual memory on the hard drive — when it runs out of memory: it runs out of memory. And it also doesn't do such a good job as your CPU at kind of shuffling things around to try and find memory, it just allocates blocks of memory and it stays allocated until you remove them. So if you try to scale up your models to bigger models, unless you have very expensive GPUs, you will run out of space, and you'll get an error, something like “CUDA out of memory error.” So if that happens, first thing I mentioned is: it's not a bad idea to restart your notebook because they can be a bit tricky to recover from otherwise, and then I'll show you how you can use as large a model as you like. Almost it's, you know, basically you'll be able to use a x-large model on Kaggle. So, let me explain. Now, I want… when you run something on Kaggle — like actually on Kaggle — you're generally going to be on a 16 Gig GPU. And you don't have to run stuff on Kaggle, you can run stuff on your home computer or Paperspace or whatever, but sometimes you'll have — if you want to do Kaggle competition— sometimes you'll have to run stuff on Kaggle because a lot of competitions are what they call “Code Competitions” which is where the only way to submit is from a notebook that you're running on Kaggle, and then a second reason to run stuff on kaggle is that, you know, your notebooks will appear, you know, with the leaderboard score on them, and so people can see which notebooks are actually good. And I kind of like, even in things that aren’t code competitions, I love trying to be the person who's number one on the notebook score leaderboard because that's something which, you know, you can't just work at NVIDIA, and use a thousand GPUs and win a competition through a combination of skill and brute force. Everybody has the same nine hour timeout to work with, so I think it's a good way of keeping the, you know, things a bit more fair."
fast.ai 2022 - Part 1,7,358, Understanding GPU memory usage,"Now… so, my home GPU has 24 Gig so I wanted to find out what can I get away with, you know, in 16 Gig, and the way I did that is, I think, a useful thing to discuss because, again, it's all about fast iteration. So I wanted to really quickly find out how much memory will a model use, so there's a really quick hacky way I can do that which is to say: “okay, for the training set let's not use… (so here's the value counts of labels, so the number of each disease…) let's not look at all the diseases, let's just pick one, the smallest one, right?, and let's make that our training set. Our training set is the “bacterial_panicle_blight” images, and now I can train a model with just 337 images without changing anything else. Not that I care about that model but then I can see how much memory it used. It's important to realize that, you know, each image you pass through is the same size, each batch size is the same size, so training for longer won't use more memory, so that'll tell us how much memory we're going to need. So what I then did was I then tried training different models to see how much memory they… they used up. Now, what happens if we train a model? So obviously Convnext_small doesn't use too much memory. So here's something that reports the amount of GPU memory just by basically printing out cuda's GPU processes, and you can see Convnext_small took up 4GB. And also this might be interesting to you, if you then call Python's garbage collection gc.collect(), and then call Pytorch's empty_cache() that should basically get your GPU back to a clean state of not using any more memory than it needs to, when you can start training the next model without restarting the kernel."
fast.ai 2022 - Part 1,7,484, What is GradientAccumulation?,"So what would happen if we tried to train this little model, and it crashed with a “cuda out of memory error”. What do we do? We can use a cool little trick called gradient accumulation. What's gradient accumulation? So what's gradient accumulation? Well I added this parameter to my train method here. That's my train method, creates my data loaders, creates my learner, and then —depending on whether I'm fine tuning or not— either fits or fine-tunes it. But there's one other thing it does… it does this gradient accumulation thing. What's that about? Well, the key step is here. I set my batch size (so that's the number of images that I pass through to the GPU all at once) to 64, which is my default. Divided by… (slash-slash means integer divide in Python..). divided by this number. So if I pass 2, it's going to use a batch size of 32. If I pass 4, it'll use a batch size of 16. Now that obviously should let me cure any memory problems, use a smaller batch size. But the problem is that now, the dynamics of my training are different, right? The smaller your batch size, the more volatility there is from batch to batch. So now your learning rates are all messed up. You don't want to be messing around with trying to, you know, find a different set of, kind of, optimal parameters for every batch size, for every architecture. So what we want to do is find a way to run just, let's say accum is two, accumulate equals two, let's say we just want to run 32 images at a time through. How do we make it behave as if it was 64 images? Well, the solution to that problem is to consider our training loop. This is the… basically the training loop we used from a couple of lessons ago, the one we created manually. We go through each (x,y) pair in the data loader. We calculate the loss using some coefficients based on that (x,y) pair, and then we call backward() on that loss to calculate the gradients, and then we subtract from the coefficients, the gradients times the learning rate. And then we zero out the gradient. So I've skipped a bit of stuff like the… with torch.no_grad() thing. Actually, no, I don't need that because I've got .data, no that's it. That should all work fine. That skipped out printing the loss, that's about it. So here is a variation of that loop… where I do not always subtract the gradient times the learning rate. Instead I go through each (x,y) pair in the data loader, I calculate the loss, I look at how many images are in this batch. So initially I start at zero, and this count is going to be 32, say if I've divided the batch size by 2. And then if “count” is greater than 64, I do my gradient… my coefficients update. Well it's not, so I skip back to here, and I do this again. And if you remember there was this interesting subtlety in Pytorch, which is if you call backward() again without zeroing out the gradients, then it adds this set of gradients to the old gradients. So by doing these two half size batches without zeroing out the gradients between them, it's adding them up. So I'm going to end up with the total gradient of a 64 image batch size but passing only 32 at a time. If I used accumulate equals four. It would go through this four times, adding them up before it subtracted out the coefficients.grad times learning rate, and zeroed it out. If I put in accum equals 64, it would go through and do a single image one at a time, and after 64 passes through eventually, count would be greater than 64, and we would do the update. So that's gradient accumulation, right? It's… it's a very simple idea, right, which is that you don't have to actually update your weights every loop through, for every minI batch. You can just do it from time to time. But it has quite significant implications, which I find most people seem not to realize, which is if you look on, like, Twitter or Reddit or whatever, people could say “oh I need to buy a bigger GPU to train bigger models”. But they don't. They could just use gradient accumulation, and so given the huge price differential between, say, a RTX3080, and an RTX3090 Ti, huge price differential… the performance is not that different. The big difference is the memory. So what? Just put in a bit smaller batch size, and do gradient accumulation. So there's actually not that much reason to buy giant GPUs. John? JOHN: Are the results with gradient accumulation numerically identical? JEREMY: They're numerically identical for this particular architecture. There is something called batch normalization, which we will look at in Part 2 of the course, which keeps track of the moving average of… standard deviations and averages, and does it in a mathematically slightly incorrect way. As a result of which, if you've got batch normalization, then it could… it basically will introduce more volatility. Which is not necessarily a bad thing, but because it's not mathematically identical, you won't necessarily get the same results. Convnext doesn't use batch normalization, so it is the same. And in fact, a lot of the models people want to use, really big versions of which, is NLP ones, Transformers, tend not to use batch normalization, but instead they use something called layer normalization, which, yeah, doesn't have the same issue. I think that's probably fair to say. I haven't thought about it that deeply. In practice I found adding gradient accumulation for Convnext has not caused any issues for me, and I don't have to change any parameters when I do it. Any other questions on the forum, John? JOHN: Tamori asking shouldn't it be count>=64 if bs=64, I haven't… JEREMY: No, I don't think so, oh yeah, is that… so we start at zero, then it's gonna be 32, then it's gonna be… yeah yeah, probably yeah, you can probably tell I didn't actually run this code. JOHN: Madav is asking: does this mean that lr_find() is based on the batch size set during the data block? JEREMY: Yeah, so lr_find() just uses your data loaders batch size. JOHN: Edward is asking: why do we need gradient accumulation rather than just using a smaller batch size, and follows up with how would we pick a good batch size? JEREMY: Well just, if you use a smaller batch size… here's the thing right, different architectures have different amounts of memory, you know, which they which they take up, and so you'll end up with different batch sizes for different architectures, which is not necessarily a bad thing but each of them is going to then need a different learning rate, and maybe even different weight decay or whatever, like, the kind of… the settings thats working really well for batch size 64 won't necessarily work really well for batch size 32. And you know, you want to be able to experiment as easily, and quickly as possible. I think the second part of the question was “how do you pick a optimal batch size?” Honestly, the standard approach is to pick the largest one you can, just because it's faster that way – you're getting more parallel processing going on. Although to be honest I quite often use batch sizes that are quite a bit smaller than I need because quite often it doesn't make that much difference, but yeah, the rule of thumb would be, you know, pick a batch size that fits in your GPU, and for performance reasons I think it's generally a good idea to have it be a multiple of eight. Everybody seems to always use powers of two, I don't know, like, I don't think it actually matters. JOHN: And look there's one other, just a clarification or a check, if the learning rate should be scaled according to the batch size? JEREMY: Yeah so generally speaking the rule of thumb is that if you divide the batch size by two, you divide the learning rate by two, but unfortunately it's not quite perfect. Did you have a question Nick? If you do you can, okay cool yeah. JOHN: Yeah no, that's us all caught up, thanks Jeremy. JEREMY: Good questions thank you. So gradient accumulation in fastai is very straightforward, you just divide the batch size by however much you want to divide it by, and then add a… you got something called a callback, and a callback is something which changes the way the model trains. This callback is called GradientAccumulation, and you pass in the effective batch size you want, and then you say, when you create the learner you say, these are the callbacks I want, and so it's going to pass in GradientAccumulation callback. So it's going to only update the weights once it's got 64 images. So if we pass in accum=1 it won't do any gradient accumulation, and that uses 4GB. If we use accum=2 about 3GB, accum=4 about 2.5GB, and generally the bigger the model the closer you'll get to a kind of a linear scaling because models have a kind of a bit of overhead that they have anyway. So what I then did, was I just went through all the different models I wanted to try. So I wanted to try convnext_large at a 320 by 240, vit_large, swinv2_large, swin_large, and on each of these I just tried running it with accum=1, and actually every single time for all of these I got an “out of memory” error. And then I tried each of them independently with accum=2, and it turns out that all of these worked with accum=2, and it only took me 12 seconds each time, so that was a very quick thing for me. Then okay, I now know how to train all of these models on a 16GB card. So I can check here, they're all in less than 16GB. So then I just created a little dictionary of all the architectures I wanted, and for each architecture all of the resize methods I wanted, and final sizes I wanted. Now these models vit, swinv2, and swin, are all Transformers models, which means that, well, most Transformers models, nearly all of them, have a fixed size – this one's 224, this one's 192, this one's 224. So I have to make sure that my final sizes are square, of the size required, otherwise I get an error. There are… there is a way of working around this but I haven't experimented with it enough to know when when it works well, and when it doesn't, so we'll probably come back to that in Part 2. So for now we're just going to use the size that they ask us to use. So with this dictionary of architectures, and for each architecture, kind of pre-processing details,"
fast.ai 2022 - Part 1,7,1252, How to run all the models with specifications,"we switch the training path back to using all of our images, and then we can loop through each architecture, and loop through each item transfer– transforms and sizes, and train the model, and then the training script, if you're fine-tuning, returns the tta predictions. So I append all those tta predictions, for each model for each type, into a list, and after each one it's a good idea to do this garbage collection, and empty cache that… because otherwise I find what happens is your GPU memory kind of, I don't know, I think it gets fragmented or something, and after a while it runs out of memory even when you thought it wouldn't. So this way you can really do as much as you like without running out of memory. So they all train, train, train, train, and one key thing to note here, is that in my train script, my data loaders does not have the seed= parameter, so I'm using a different training set every time. So that means that for each of these different runs they're using also different validation sets, so they're not directly comparable, but you can kind of see they're all doing pretty well, 2.1%, 2.3%, 1.7%, and so forth. So why am I using different training and validation sets for each of these? That's because I want to ensemble them, so I'm going to use bagging,"
fast.ai 2022 - Part 1,7,1375, Ensembling,"which is I am going to take the average of their predictions. Now I mean really, when we talked about random forest bagging, we were taking the average of, like, intentionally weak models. These are not intentionally weak models, they're meant to be good models, but they're all different – they're using different architectures, and different pre-processing approaches, and so in general we would hope that these different approaches… some might work well for some images, and some might work well for other images. And so when we average them out, hopefully we'll get a good blend of, kind of, different ideas, which is kind of what you want in bagging. So we can stack up that list of different… of all the different probabilities, and take their mean, and so that's going to give us 3469 predictions, that's our that's our test set size, and each one has 10 probabilities, the probability of each disease. And so then we can use argmax() to find which probability index is the highest, so that's going to give us our list of indexes. So this is basically the same steps as we used before to create our CSV submission file. So at the time of creating this analysis that got me to the top of the leaderboard, and in fact these are my four submissions, and you can see each one got better. Now you're not always going to get this nice monotonic improvement, right, but you want to be trying to submit something every day, to kind of, like, try out something new, right, and the more you practice the more you'll get a good intuition of what's going to help, right. So partly I'm showing you this to say, it's not like purely random as to whether things work or don't, once you've been doing this for a while, you know, you will generally be improving things most of the time. So as you can see from the descriptions my first submission was our convnext_small for 12 epochs with TTA. And then ensemble of convnext, so it's basically this exact same thing but just retraining a few with different training subsets. And then this is the same thing again, this is the thing we just saw, basically the ensemble of large models with TTA. And then the last one was something I skipped over, which was I… the VIT models were the best in my testing, so I basically weighted them as double in the ensemble – pretty unscientific but again it gave it another boost, and so that was that was it. all right, John! JOHN: Yes, thanks Jeremy. So in no particular order Kurian is asking, would trying out cross validation with k-folds with the same architecture makes sense as an ensembling of models? JEREMY: Yeah, so a popular thing is to do k-fold cross-validation. So k-fold cross-validation is something very very similar to what I've done here. So what I've done here is I've trained a bunch of models with different training sets, each one is a different random 80% of the data. Five-fold cross-validation does something as similar, but what it says is rather than picking, like say five samples out with different random subsets, in fact, instead, first like… do all except for the first 20% of the data, and then all but the second 20%, and then all but the third, and so forth, and so you end up with five subsets each of which have non-overlapping validation sets, and then you'll ensemble those. You know in theory maybe that could be slightly better because you're kind of guaranteed that every row is… appears… four times, you know, effectively. It also has a benefit that you could average those five validation sets because there's no kind of overlap between them to get a cross validation. Personally, I generally don't bother, and the reason I don't is because this way I can add and remove models very easily. I don't, you know… I can just, you know… add another architecture and whatever to my ensemble without trying to find a different overlapping non-overlapping subset. So yeah, cross-validation is therefore something that I use probably less than most people or almost… or almost never. JOHN: Awesome, thank you. Are there any… just coming back to gradient accumulation… any other kind of drawbacks or potential gotchas with gradient accumulation? JEREMY: No, not really! Yeah, like, amazingly it doesn't even really slow things down much, you know, going from a batch size of 64 to a batch size of 32. By definition you had to do it because your GPU is full so you're obviously giving a lot of data. So it's probably going to be using its processing speed pretty effectively, so yeah, no it's just… it's just a good technique that… we should all be buying cheaper graphics cards with less memory in them, and using you know, have like… I don't know the prices, I suspect it's like you could probably buy like two 3080s for the price of one 3090Ti or something, that would be a very good deal. JOHN: Yes, clearly you're not on the Nvidia payroll. So look this is a good segue then, we did have a question about sort of GPU recommendations, and there's been a bit of chat on that as well. (JEREMY: I bet) So any… any, you know, commentary… any additional commentary around GPU recommendations. JEREMY: No, not really. I mean obviously at the moment Nvidia is the only game in town, you know. If you buy… if you trying to use a you know Apple M1 or M2 or an AMD card you're basically in for a world of pain in terms of compatibility and stuff, and unoptimized libraries, and whatever. The Nvidia consumer cards, so the ones that start with RTX are much cheaper but are just as good as the expensive enterprise cards. So you might be wondering why anybody would buy the expensive enterprise cards, and the reason is that there's a licensing issue that Nvidia will not allow you to use an RTX consumer card in a data center – which is also why cloud computing is more expensive than they, kind of, ought to be, because everybody selling cloud computing GPUs is selling these cards that are like, I can’t remember, I think they're like three times more expensive for kind of the same features. So yeah, if you do get serious about deep learning to the point that you're prepared to invest, you know, a few days in administering a box, and you know, I guess depending on prices, hopefully will start to come down, but currently a thousand or two thousand dollars, and buying a GPU then you know, that'll probably pay you back pretty quickly. JOHN: Great, thank you. Let's see, another one has come in. If you have a… back on models, not hardware… if you have a well functioning but large model, can it make sense to train a smaller model to produce the same final activations as the larger model? JEREMY: Oh yeah, absolutely. I'm not sure we'll get into that this time around but yeah we'll cover that in Part 2, I think, but yeah basically there's teacher/student models, and model distillation, which broadly speaking there are ways to make inference faster by training small models that work the same way as large models. JOHN: Great thank you, all caught up. JEREMY: All right, so… that is the actual real end of Road to the Top, because beyond that we don't actually cover how to get closer to the top – you'd have to ask Kurian to share his techniques to find out that, or Nick, to get the second place from the top. Part 4 is actually something that I think is very useful to know about for learning, and it's going to teach us a whole lot about how the last layer of a neural net works. And specifically, what we're going to try to do is we're going to try to build a model that doesn't just predict the disease but also predicts the type of rice. So how would you do that? So here's the Data Loader we're going to try to build – it's going to be something that for each image it tells us the disease, and the type of rice. I say disease, sometimes normal, I guess some of them are not diseased. So to build a model that can predict two things, the first thing is you're going to need data loaders that have two dependent variables, and that is shockingly easy to do in fastai thanks to the DataBlock. So we've seen the DataBlock before. We haven't been using it for the Paddy competition so far because we haven't needed it – we could just use ImageDataLoader.from_folder(). So that's like the highest level API, the simplest API. If we go down a level deeper into the DataBlock we have a lot more flexibility. So if you've been following the walkthroughs you'll know that as I built this the first thing I actually did was to simply replicate the previous notebook but replace the ImageDataloader.from_folder() with a DataBlock to try to do, first of all, exactly the same thing, and then I added the second dependent variable. So if we look at the previous ImageDataLoader.from_folder() thingy, here it is. We were passing in some item transforms, and some batch transforms, and we had something saying what percentage should be the validation set So in a DataBlock if you remember we have to pass in a “blocks” argument saying what kind of data is the independent variable, and what is the dependent variable. So to replicate what we had before we would just pass an ImageBlock comma CategoryBlock, because we've got an image as our independent variable, and a category, one type of rice, is the dependent variable. So the new thing I'm going to show you here,is that you don't have to “only put in two things.” You can put in as many as you like, so if you put in three things we're going to generate one image, and two categories. Now fastai, if you're saying I want three things… fastai doesn't know which of those is the independent variable, and which is the dependent variable, so the next thing you have to tell it is how many inputs are there, “number of inputs,” And so here I've said there's one input, so that means this is the input, and therefore by definition two categories will be the output because remember we're trying to predict two things the type of rice, and the disease. Okay this is the same as what we've seen before, to find out… to get our list of items we'll call get_image_files(). Now here is something we haven't seen before – get_y is our labeling function. Normally we pass to get_y a single thing such as the parent_label function which looks at the name of the parent directory, which remembers how these images are structured, and that would tell us the label. But get_y can also take an array, and in this case we want two different labels. One is the name of the parent directory, because that's the disease. The second is the variety. So what is get_variety()? get_variety() is a function. So let me explain how this function works. So we can create a data frame containing our trainings... our training data that came from Kaggle. So for each image it tells us the disease, and the variety. And what I did is something I haven't shown before. In pandas you can set one column to be the index, and when you do that, in this case image_id, it makes this series... this... sorry… this data frame, kind of like a dictionary. I can index into it by saying tell me the row for this image, and to do that you use the “loc” attribute, the location. So we want in the data frame, the location of this image, and then you can also say optionally what column you want – this column. And so here's this image, and here's this column, and as you can see it returns that thing. So hopefully now you can see it's pretty easy for us to create a function that takes a row... sorry, a path, and returns the location in the data frame of the name of that file, because remember these are the names of files, for the variety column. So that's our second get_y okay, and then we've seen this before, randomly split the data into the 20% and 80%. And so we could just switch them all to 192 just for this example, and then use data augmentation to get us down to 128 square images just for this example. And so that's what we get when we say show batch. We get what we just discussed."
fast.ai 2022 - Part 1,7,2271, Multi,"So now we need a model that predicts two things. How do we create a model that predicts two things? Well, the key thing to realize is we never actually had a model that predicts two things. We had a model that predicts ten things, before. The ten things we predicted is the probability of each disease. So we don't actually now want a model that predicts two things, we want a model that predicts 20 things: the probability of each of the 10 diseases, and the probability of each of the 10 varieties. So, how could we do that? Well let's first of all try to just create the same disease model we had before with our new data loader. So this is going to be reasonably straightforward. The key thing to know is that since we told fastai that there's one input, and therefore by definition there's two outputs, it's going to pass to our metrics, and to our loss functions, three things instead of two: the predictions from the model, and the disease, and the variety. So if we're gonna... so we can't just use error rate as our metric anymore because error rate takes two things. Instead we have to create a function that takes three things, and return error rate of the two things we want, which is the predictions from the model, and the disease, okay? So this is predictions of the model, this is the target. So that's actually all we need to do to define a metric that's going to work with our new dataset… with a new dataloader, and this is not going to actually tell us anything about variety, first we're just going to try to replicate something that can do just disease. So when we create our learner we’ll pass in this new disease error function. Okay, so we're halfway there. The other thing we're going to need is to change our loss function. Now we never actually talked about what loss function to use, and that's because vision_learner guessed what loss function to use. vision_learner saw that our dependent variable was a single category, and it knows the best loss function that's probably going to be the case for things with a single category, and it knows how big the category is. So it just didn't bother us at all. It just said okay “I'll figure it out for you”. So the only time we've provided our own loss function is when we were kind of doing linear models and neural nets from scratch, and we did, I think, mean-squared-error. We might also have done mean-absolute-error. Neither of those work when the dependent variable is a category. Now how would you use mean-squared-error or mean-absolute-error to say, how close were these 10 probability predictions to this one correct answer. So in this case we have to use a different loss function. We have to use something called cross entropy loss, and this is actually the loss function that fastai picked for us before without us knowing. But now that we are having to pick it out manually I'm going to explain to you exactly what cross-entropy loss does, okay?"
fast.ai 2022 - Part 1,7,2484, What does `F.cross_entropy` do,"And you know these details are very important indeed. Like... remember I said at the start of this class, the stuff that happens in the middle of the model you're not going to have to care about much in your life, if ever, but the stuff that happens in the first layer, and the last layer including the loss function that sits between the last layer and the loss, you're going to have to care about a lot, right? This stuff comes up all the time, so you definitely want to know about cross-entropy loss, and so I'm going to explain it using a spreadsheet. This spreadsheet's in the course repo, and so let's say you are predicting something like a... kind of a... mini imagenet thing where you're trying to predict whether something... an image, is a cat, a dog, a plane, a fish or a building. So you set up some model, whatever it is, a convnext model, or a just a big bunch of linear layers connected up, or whatever, and initially you've got some random weights, and it spits out at the end, five predictions, right? So remember, to predict something with five categories, your model will spit out five probabilities. Now it doesn't initially spit out probabilities. There's nothing making them probabilities, it just spits out five numbers; could be negative, could be positive, okay? So here's the output of the model. So what we want to do is: we want to convert these into probabilities. And so we do that in two steps. The first thing we do is we go… EXP, that's e-to-the-power-of. We go e-to-the-power-of each of those things, like so... okay? And so here's the mathematical formula we're using. This is called the softmax, what we're working through. We're going to go through each of the categories. So these are our five categories – so here k is five. We've got to go through each of our categories, and we're going to go e-to-the-power-of the output, so zj is the output for the j-th category. So here's that, and then we're going to sum them all together. Here it is... sum up together, okay? So this is the denominator, and then the numerator is just e-to-the-power-of the thing we care about. So this row. So the numerator is e-to-the-power-of cat on this row, e-to-the-power-of dog on this row, and so forth. Now if you think about it, since the denominator adds up all the e-to-the-power-ofs, then when we do each one divided by the sum, that means the sum of these will equal 1 by definition, right? And so now we have things that can be treated as probabilities. They're all numbers between 0 and 1. Numbers that were bigger in the output will be bigger here. But there's something else interesting, which is, because we did e-to-the-power-of, it means that the bigger numbers will be, like, pushed up to numbers closer to one, like we're saying, like, “oh really try to pick one thing” as having most of the probability, because we are trying to predict, you know, one thing. We're trying to predict which one is it, and so this is called softmax. So sometimes you'll see people complaining about the fact that their model, which they said... let's say, is it a teddy bear or a grizzly bear or a black bear, and they feed it a picture of a cat, and they say “oh the model's wrong because it predicted grizzly bear but it's not a grizzly bear“. As you can see there's no way for this to predict anything other than the categories we're giving it – we're forcing it to that. Now we don't... if you want that,"
fast.ai 2022 - Part 1,7,2743, When do you use softmax and when not to?,"like, there's something else you could do which is you could actually have them not add up to one, right? You could instead have something which simply says: what's the probability it's a cat, what's the probability it's a dog, what's the probability it’s a plane, totally separately they could add up to less than one, and in that situation you can cert... you know... or or more than one in which case you could have like more than one thing being true or zero things being true. But in this particular case where we want to predict one and one thing only, we use softmax."
fast.ai 2022 - Part 1,7,2775, Cross_entropy loss,"The first part of the cross entropy formula... The first part of the cross entry formula... in fact let's look it up, nn.CrossEntropyLoss. The first part of what cross-entropy loss in Pytorch does is to calculate the softmax. It's actually the log of the softmax but don't worry about that too much, it's just a... slightly faster to do the log, okay? So now for each one of our five things we've got a probability. The next step is the actual cross-entropy calculation, which is we take our five things, we've got our five probabilities, and then we've got our actuals. Now the truth is the actual, you know, the five things would have indices, right? Zero, one, two, three or four, and the actual turned out to be the number one, but what we tend to do is we think of it as being one-hot-encoded, which is we put a one next to the thing for which it's true, and a zero everywhere else. And so now we can compare these five numbers to these five numbers, and we would expect to have a smaller loss if the softmax was high where the actual is high. And so here's how we calculate... this is the formula... the cross-entropy loss. We sum up… (they switch to M this time for some reason, but the same thing…) we sum up across the five categories so M is 5, and for each one we multiply the actual target value, so that's zero… so here it is here the actual target value, and we multiply that by the log of the predicted probability… the log of (red) the predicted probability. And so, of course, for four of these that value is zero, because see here: y-j equals zero, by definition, for all but one of them, because it’s one hot encoded. So for the one that it's not we've got our actual times the log softmax, okay, and so now actually you can see why pytorch prefers to use log softmax because then it kind of skips over having to do this log at all. So this equation looks slightly frightening but when you think about it all it's actually doing is: it's finding the probability for the one that is one, and taking its log, right. It's kind of weird doing it as a sum but in math it could be a little bit tricky to kind of say, oh look this up in an array, which is basically what it's doing, but yeah basically, at least in this case for a single result with soft max this is all it's doing, it's finding the 0.87 where it's 1 for, and taking the log, and then finally negative. So that is what cross-entropy loss does."
fast.ai 2022 - Part 1,7,2993, How to calculate binary,"We add that together for every row. So here's what it looks like if we add it together over every row, right, so N is the number of rows. And here's a special case, this is called “binary cross-entropy”. What happens if we're not predicting which of five things it is but we're just predicting “is it a cat?” So in that case if you look at this approach you end up with this formula, which it's exactly… this is identical to this formula but in for just two cases, which is you've either: you either are a cat; or you're not a cat, right, and so if you're not-a-cat, it's one minus you-are-a-cat, and same with the probability you've got the probability you-are-a-cat, and then not-a-cat is one minus that. So here's this special case of binary cross entropy, and now our rows represent rows of data, okay, so each one of these is a different image, a different prediction, and so for each one I'm just predicting are-you-a-cat, and this is the actual, and so the actual are-you-not-a-cat is just one minus that. And so then these are the predictions that came out of the model, again we can use soft max or it's binary equivalent, and so that will give you a prediction that you're-a-cat, and the prediction that it's not-a-cat is one minus that. And so here is: each of the part “y-i” times log of “p-y-i”, and here is…(why did I subtract that's weird, oh because I've got minus of both, so I just do it this way, avoids parentheses…) yeah, minus the are-you-not-a-cat times the log of the prediction value not-a-cat, and then we can add those together, and so that would be the binary cross-entropy loss of this dataset of five cat or not-cat images."
fast.ai 2022 - Part 1,7,3139, Two versions of cross,"Now if you've got an eagle eye, you may have noticed that I am currently looking at the documentation for something called “nn.CrossEntropyLoss” but over here I had something called “F.cross_entropy()”. Basically it turns out that all of the loss functions in pytorch have two versions – there's a version which is a class, this is a class, which you can instantiate passing in various tweaks you might want, and there's also a version which is just a function, and so if you don't need any of these tweaks you can just use the function. The functions live in a… I can’t even remember what the sub module called, I think it might be like torch.nn.functional but everybody including the pytorch official docs just calls it capital-F, so that's what this capital-F refers to. So our loss, if we just care about disease (we're going to be passed the three things) we're just going to calculate cross_entropy on our input versus disease. All right so that's all fine… we passed… so now when we create a vision learner you can't rely on fastaI to know what loss function to use, because we've got multiple targets, so you have to say: this is the loss function I want to use, this is the metrics I want to use. And the other thing you can't rely on is that fastaI no longer knows how many activations to create, because again it… there's more than one target, so you have to say the number of outputs to create at the last layer is 10. So this is just saying what's the size of the last matrix. And once we've done that we can train it, and we get, you know, basically the same kind of result as we always get, because this model at this point is identical to our previous convnext_small model – we've just done it in a slightly more roundabout way. So finally, before our break,"
fast.ai 2022 - Part 1,7,3264, How to create a learner for prediction two targets,"I'll show you how to expand this now into a multi-target model. And the trick is actually very simple, and you might have almost got the idea of it when I talked about it earlier – our vision learner now requires twenty outputs – we now need that last matrix to have to produce twenty activations not ten. Ten of those activations are going to predict the disease, and ten of the activations are going to predict the variety. So you might be then asking, like well, how does the model know what it's meant to be predicting, and the answer is with the loss function, you're going to have to tell it. So for example disease_loss – remember it's going to get the input, the disease, and the variety – this is now going to have 20 columns in. So we're just going to decide, all right, we're just going to decide the first 10 columns, we're going to decide are the prediction of what the disease is, which is the probability of each disease. So we can now pass to cross_entropy the first 10 columns, and the disease target. So the way you read this colon means every row, and then colon 10 means every column up to the 10th. So these are the first 10 columns, and that will… that's a loss function that just works on predicting disease using the first ten columns. For variety, we'll use cross_entropy loss with the target of variety, and this time we'll use the second 10 columns, so here's column ten onwards. So then the overall loss function is the sum of those two things disease_loss plus variety_loss. And that's actually it! That's all the model needs to basically… it's now going to… if you kind of think through the manual neural nets we've created, this loss function will be reduced when the first 10 columns are doing good job of predicting the disease probabilities and the second 10 columns are doing a good job of predicting the variety probabilities and therefore the gradients will point in an appropriate direction that the coefficients will ~(be getter and…) get better and better at using those columns for those purposes. It would be nice to see the error rate as well for each of disease and variety, so we can call error_rate passing in the first 10 columns and disease, and then variety, the second 10 columns and variety. And we may as well also add to the metrics the losses. And so now when we create our learner we're going to pass in as the loss function the combined_loss – and as the metrics, our list of all the metrics and n_out=20 and now look what happens when we train! As well as telling us the overall train and valid loss, it also tells us the disease and variety error and the disease and variety loss and you can see our disease error is getting down to a similar level as it was before. It's slightly less good but it's similar. It's not surprising it's slightly less good because we've only given it the same number of epochs and we're now asking it to try to do more stuff, which is to learn to recognize what the rice variety looks like, and also learns to recognize what the disease looks like. Here's the counterintuitive thing though if we train it for longer it may well turn out that this model which is trying to predict two things actually gets better at predicting disease than our disease specific model. Why is that?... like that sounds weird, right, because we're trying to do more stuff, [and] that's the same size model. Well the reason is that quite often it'll turn out that the kinds of features that help you recognize a variety of rice are also useful for recognizing the disease. You know, maybe there are certain textures, right, or maybe some diseases impact different varieties different ways, so it'd be really helpful to know what variety it was. So I haven't tried training this for a long time and I don't know the answer is… In this particular case does a multi-target model do better than a single target model at predicting disease, but I just want to let you know sometimes it does, right. So for example a few years ago there was a Kaggle competition for recognizing the kinds of fish on a boat, and I remember we ended up doing a multi-target model where we tried to predict a second thing… (I can't even remember what it was, maybe it was a type of boat or something…) and it definitely turned out in that Kaggle competition that predicting two things helped you predict the type of fish better than predicting just the type of fish. So there's at least, you know, there's two reasons to learn about multi-target models: one is that sometimes you just want to be able to predict more than one thing, so this is useful; and the second is, sometimes this will actually be better at predicting just one thing, than a just one thing model. And of course the third reason is it really forced us to dig quite deeply into these loss functions and activations in a way we haven't quite done before So it's okay, it's absolutely okay, if this is confusing. The way to make it not confusing is… well… the first thing I do is, like, go back to our earlier models where we did stuff by hand, on like the titanic dataset and built our own architectures, and maybe you could try to build a model that predicts two things in the titanic dataset. Maybe you could try to predict both sex & survival or something like that, or class & survival, because that's going to, kind of, force you to look at it on very small datasets. And then the other thing I'd say is run this notebook and really experiment at trying to see what kind of outputs you get, like actually look at the inputs and look at the outputs and look at the data loaders and so forth. All right let's have a six minute break, so I'll see you back here at ten past seven. Okay, welcome back. Before I continue, I very rudely forgot to mention this very nice equation image here is from an article by Chris Said called “Things that confused me about cross-entropy.” It's a very good article, so I recommend you check it out if you want to go a bit deeper there. There's a link to it inside the spreadsheet. So the next notebook we're going to be looking at is this one called Collaborative Filtering Deep Dive, and this is going to cover our last of the four major application areas,"
fast.ai 2022 - Part 1,7,3720, Collaborative filtering deep dive,"collaborative filtering. And this is actually the first time I'm going to be presenting a chapter of the book largely without variation because this is one where I looked back at the chapter and I was like, “oh I can't think of any way to improve this.” So I thought I'll just leave it as is, but we have put the whole chapter up on Kaggle, so that's for the way I'm going to be showing it to you. And so we're going to be looking at a dataset called the MovieLens dataset, which is a dataset of movie ratings, and we're going to grab a smaller version of it, 100 000 record version of it, and it comes as a csv file which we can read in, well, it's not really a csv file, it's a tsv file, this here means a tab in Python. These are the names of the columns. So here's what it looks like. It's got a user, a movie, a rating, and a timestamp. We're not going to use the timestamp at all. So basically three columns we care about. This is a user id, so maybe 196 is Jeremy, and maybe 186 is Rachel, and 22 is John, I don't know. Maybe this movie is Return Of The Jedi, and this one's Casablanca, this one's LA Confidential. And then this rating says how did Jeremy feel about Return Of The Jedi, he gave it a three out of five. That's how we can read this dataset. This kind of data is very common. Anytime you've got a user and a product or service, and you might not even have ratings, maybe just the fact that they bought that product, you could have a similar table with zeros and ones. So, for example Radek, who's in the audience here, is now at Nvidia doing, like, basically just this, right, recommendation systems. So recommendation systems, you know, it's a huge industry, and so what we're learning today is, you know, a really key foundation of it. Um… So these are the first few rows. This is not a particularly great way to see it. I prefer to kind of cross tabulate it, like that, like this. This is the same information. So for each movie, for each user, here's the rating. So user 212 never watched movie 49. Now if you're wondering, uh… why there's so few empty cells here, I actually grabbed the most watched movies and the most movie watching users for this particular sample matrix. So that's why it's particularly full. So yeah, so this is what kind of a collaborative filtering dataset looks like when we cross tabulate it. So how do we fill in this gap? So maybe user 212 is Nick, and review 49… what's a movie you haven't seen, Nick, and you'd quite like to? Maybe, not sure about it, the new Elvis movie, Bez Luhmann? Good choice. Australian director, filmed in Queensland. Yeah, okay, so that's movie two… that's movie number 49. So is Nick gonna like the new Elvis movie? Well, to figure this out, what we could do, ideally, would like to know: for each movie… what kind of movie is it? Like, what are the kind of features of it. Is it like actiony, science fictiony, dialogue driven, critical acclaimed, you know. So let's say, for example, we were trying to look at The Last Skywalker –maybe that was the movie the… Nick's wondering about watching. And so if we like, had, three categories, being science fiction, action, or kind of classic old movies, would say The Last Skywalker is very science fiction. Let's see this is from like negative one to one, pretty action, definitely not an old classic, or at least not yet. And so then, maybe we then could say, like, okay, well, maybe like Nick’s tastes in movies are that he really likes science fiction, quite likes action movies, and doesn't really like old classics, right. So then we could, kind of, like, match these up to see how much we think this user might like this movie to calculate the match, we could just multiply the corresponding values user1 times The Last Skywalker, and add them up, point nine (0.9) times point nine eight (0.98), plus point eight (0.8) times point nine (0.9), plus negative point six (-0.6) times negative point nine (-0.9). That's going to give us a pretty high number, right, with a maximum of three. So that would suggest Nick probably would like The Last Skywalker. On the other hand, the movie Casablanca we would say definitely not very science fiction, not really very action, definitely very old classic. So then we'd do exactly the same calculation and get this negative result here. So you probably wouldn't like Casablanca. This thing here, when we multiply the corresponding parts of a vector together and add them up, is called a dot product in math. So this is the dot product of the user's preferences and the type of movie. Now the problem is, we weren't given that information. We know nothing about these users or about the movies. So what are we going to do? We want to try to create these factors without knowing ahead of time what they are. We wouldn't even know what factors to create, what are other things that really matters when it says people decide what movies they want to watch?"
fast.ai 2022 - Part 1,7,4135, What are latent factors?,"What we can do is we can create things called latent factors. Latent factors is this weird idea that we can say: I don't know what things about movies matter to people, but there's probably something and let's just try, like, using SGD to find them. And we can do it in everybody's favorite mathematical optimization software: Microsoft Excel. So here is that table. And, what we can do –let's head over here actually– here's that table. So, what we could do is we could say: for each of those movies –so let's say for movie 27– let's assume there are five latent factors –I don't know what they're for–, they're just five latent factors, we'll figure them out later. And for now I certainly don't know what the value of those five latent factors for movie 27, so we're going to just chuck a little random numbers in them, and we're going to do the same thing for movie 49 –pick another five random numbers– and the same thing for movie 57 –pick another five numbers. And you might not be surprised to hear we're going to do the same thing for each user, so for user 14: we're going to pick five random numbers for them, and for user 29: we'll pick five random numbers for them, and so the idea is that this number here 0.19 is saying –if it was true– that user id 14 feels not very strongly about the fact that for movie 27 has a value of 0.71. So therefore in here we do the dot product. The details of why don't matter too much but, well, actually you can figure this out from what we've said so far: if you go back to our definition of matrix product you might notice that the matrix product of a row with a column is the same thing as a dot product and so here in excel I have a row and a column so, therefore I say matrix multiply that by that: that gives us the dot product. So here's the dot product of that by that –or the matrix multiply, given that they're row and column. The only other slight quirk here is that if the actual rating"
fast.ai 2022 - Part 1,7,4288, Dot product model,"is zero –is empty– I'm just going to leave it blank, I'm going to set it to zero actually. So here is everybody's rating, predicted rating of movies. I say predicted –of course these are currently random numbers so they are terrible predictions. But when we have some way to predict things and we start with terrible random predictions, we know how to make them better, don't we?, we use static gradient descent. Now to do that we're going to need a loss function, so that's easy enough, we can just calculate the sum of x minus y squared divided by the count, that is the mean squared error –and if we take the square root, that is the root mean squared error– so here is the root mean squared error, in Excel, between these predictions and these actuals. And so now that we have a loss function, we can optimize it. Data solver, set objective (this one here) by changing cells (these ones here) and (these ones here). Solve. Okay, and initially our loss is 2.81 –so we hope it's going to go down– and as it solves –not a great choice of background color– but it says 0.68 so this number is going down. So this is using… actually an Excel it's not quite using stochastic gradient descent because excel doesn't know how to calculate gradients, there are actually optimization techniques that don't need gradients: they calculate them numerically as they go but that's a minor quirk. One thing you'll notice is it's doing it very, very slowly –there's not much data here and it's still going– one reason for that is that… it's because it's not using gradients it's much slower and the second is Excel is much slower than Pytorch. Anyway, it's come up with an answer, and look at that: it's got to 0.42. So it's got a pretty good prediction and so, we can kind of get a sense of this, for example, looking at the last three, movie user 14 likes, dislikes, likes. Let's see somebody else like that. Here's somebody else, this person likes, dislikes, likes. So, based on our kind of approach we're saying: okay, since they have the same feeling about these three movies, maybe they'll feel the same about these three movies. So this person likes all three of those movies and this person likes two out of three of them so, you know, you kind of… this is the idea, right?, as if somebody says to you: “I like this movie, this movie, this movie” and you're like: “oh, they like those movies too” what other movies do you like? and they'll say: “oh, how about this?” There's a chance, good chance, that you're going to like the same thing. That's the basis of collaborative filtering, okay, it's… and mathematically we call this matrix completion. So this matrix is missing values, we just want to… complete them. So the core of collaborative filtering is, it's a matrix completion exercise. Can you grab a microphone? NICK: Is that better? Okay, my question was, is, with the dot products, right?, so, if we think about the math of that for a minute is, yeah, if we think about the cosine of the angle between the two vectors that's going to roughly approximate the correlation, is that essentially what's going on here in one sense? with… JEREMY: Okay so is the cosine of the angle between the vectors much the same thing as the dot product? The answer is yes, they're the same, once you normalize them so. Is that still on… NICK: It’s correlation what we're doing here at scale, as well? JEREMY: Yeah, you can, yeah, you can think of it that way. NICK: Okay cool… JEREMY: Now, this looks pretty different to how Pytorch looks. Pytorch has things in rows, right?, we've got a user, a movie rating, user movie rating, right? So, how do we do the same kind of thing in Pytorch? So, let's do the same kind of thing in Excel, but using the table in the same format that Pytorch has it. Okay. So to do that in Excel the first thing I'm going to do is I'm going to, see, okay, this… I've got to look at user number 14 and I want to know what index –like how far down this list is 14. Okay, so we'll just match means find the index. So this is user index one. And then what I'm going to do is, I'm going to say the… these five numbers is, basically I want to find row one over here, and in excel that's called OFFSET. So we're going to offset from here by one row, and so you can see here it is 0.19, 0.63, 0.19, 0.63 et cetera, right? So here's the second user: 0.25, 0.83 etc. And we can do the same thing for movies, right? So movie 417 is index 14, that's going to be: 0.75, 0.47 et cetera. And so same thing, right?, but now we're going to offset from here, by 14, to get this row which is 0.75, 0.47 et cetera. And so the prediction now is… the dot product is called SUMPRODUCT in excel, this is a SUMPRODUCT of those two things. So this is exactly the same as we had before, right?, but when we kind of put everything next to each other we have to, like manually, look up the index. And so then for each one we can calculate the error squared prediction minus rating squared and then we could add those all up and –if you remember– this is actually the same root mean squared error we had before –we optimized before. 2.81 because we've got the same numbers as before and so this is mathematically identical. So what's this weird word up here: “embedding”. You've probably heard it before"
fast.ai 2022 - Part 1,7,4717, What is embedding,"and you might have come across the impression it's some very complex fancy mathematical thing, but actually, it turns out, that it is just looking something up in an array: that is what an embedding is. So we call this an “embedding matrix”, and these are our “user embeddings” and our “movie embeddings”. So let's take a look at that in Pytorch, and, you know, at this point if you've heard about embeddings before you might be thinking: that can't be it. And yeah, it's just as complex as the rectified linear unit which turned out to be: replace negatives with zeros. Embedding actually means: “look something up in an array”. So there's a lot of things that we use, as deep learning practitioners, to try to make you as intimidated as possible so that you don't wander into our territory and start winning our Kaggle competitions. And unfortunately, once you discover the simplicity of it, you might start to think that you can do it yourself and then it turns out you can. So yeah, that's what basically, it turns out pretty much all of this jargon turns out to be. So we're going to try to learn these latent factors, which is exactly what we just did in Excel, we just learned the latent factors. All right, so, if we're going to learn things in Pytorch we're going to need data loaders. One thing I did is, there is actually a movies table as well, with the names of the movies, so I merged that together with the ratings so that then we've now got the user id and the actual name of the movie –we don't need that, obviously, for the model, but it's just going to make it a bit more fun to interpret later–. So this is called: ratings. We have something called CollabDataLoaders –so collaborative filtering data loaders– and we can get that from a data frame, by passing in the data frame, and it expects a user column and an item column. So the user column is what it sounds like: the person that is rating this thing, and the item column is the product or service that they're rating. In our case the user column is called “user” –so we don't have to pass that in– and the item column is called “title” –so we do have to pass this in– because by default the user column should be called “user” and the item column will be called “item”. Give it a batch size, and as usual we can call show batch, and so, here's our data loaders… a batch of data loaders –or at least a bit of it–. And so now, since we told it the names, we actually get to see the names which is nice. All right, so, now we're going to create the user_factors and movie_factors -ie- this one and this one. So the number of rows of the movie factors will be equal to the number of movies and the number of rows of the user factors will be equal to the number of users. And the number of columns will be whatever we want: however many factors we want to create. John! JOHN: This might be a pertinent time to jump in with a question: any comments about choosing the number of factors"
fast.ai 2022 - Part 1,7,4938, How do you choose the number of latent factors,"JEREMY: Uhm… not really. We have defaults that we use –for embeddings– in fastai. It's a very obscure formula and people often ask me for like the mathematical derivation of where it came from, but what actually happened is, it's, I wrote down how many factors I think is appropriate for different size categories on a piece of paper in a table –well actually in Excel– and then I fitted a function to that and that's the function. So it's basically a mathematical function that fits my intuition about what works well. But it seems to work pretty well, I've seen it’s used in lots of other places now, lots of papers will be like: “using fastai's rule of thumb for embedding sizes… here's the formula…” JOHN: Cool, thank you. JEREMY: It's pretty fast to train these things so you can try a few. So we're going to create… so the number of users is just the length of how many users there are, number of movies is the length of how many titles there are; so create a matrix of random numbers of users by five and movies: of movies by five. And now we need to look up the index of the movie in our movie latent factor matrix. The thing is, when we've learned about deep learning, we learnt that we do matrix multiplications, not lock-something-up in a matrix –in an array. So in Excel we were saying OFFSET, which is to say, find element number 14 in the table; which, that's not a matrix multiply, how does that work? Well actually it is, it actually is for the same reason that we talked about here, which is: we can represent –find– the element number-one-thing –in this list– is actually the same as multiplying by a one hot encoded matrix. So remember how, if we –let's just take off the log for a moment. Look, this has returned 0.87 –and particularly if I take the negative off here, if I add this up– this is 0.87, which is the result of finding the index number-one-thing in this list. But we didn't do it that way, we did this by taking the dot product of this (sorry) of this and this, but that's actually the same thing, right? taking the dot product of a one hot encoded vector with something is the same as looking up this index in the vector. So that means that this exercise here of looking up the 14th thing is the same as doing a matrix multiply with a one-hot-encoded vector. And we can see that here: this is how we create a one hot encoded vector of length and users in which the third element is set to one and everything else is zero. And if we multiply that –so “at” (@) means, to remember, matrix multiply in python– so if we multiply that by our user_factors, we get back this answer. And if we just ask for user_factors number three we get back the exact same answer –they're the same thing. So you can think of “an embedding” as being a computational shortcut for multiplying something by a one-hot-encoded vector. And so if you think back to what we did with dummy variables, right, this basically means embeddings are like a cool math trick for speeding up doing matrix multipliers with dummy variables. Not just speeding up. We never even have to create the dummy variables. We never have to create the one-hot-encoded vectors. We can just look up in an array. All right, so we're now ready to build a collaborative filter… a collaborative filtering model and we're going to create one from scratch."
fast.ai 2022 - Part 1,7,5233, How to build a collaborative filtering model from scratch,"And as we've discussed before, in Pytorch a model is a class. And so, we briefly touched on this, but I've got to touch on it again. This is how we create a class in Python. You give it a name, and then you say how to initialize it, how to construct it. So in Python, remember they call these things “dunder whatever”. This is dunder init, these are magic methods that Python will call for you at certain times. The method called dunder init is called when you create an object of this class. So we could pass it a value, and so now we set the attribute called “a equal to that value”, and so then later on we could call a method called “say”, that will say hello to whatever you passed in here. And this is what it will say. So, for example, if you construct an object of type Example(), passing in Sylvain, self.a now equals Sylvain. So if you say use the dot method, the say method “nice to meet you”, x is now “nice to meet you”. So it will say hello Sylvain, nice to meet you. So that's… that's kind of all you need to know about object-oriented programming in Pytorch to create a mode. Oh, there's one more thing we need to know, sorry, which is you can put something in parentheses after your class name, and that's called the superclass. It's basically going to give you some stuff for free, give you some functionality for free. And if you create a model in Pytorch, you have to make Module your super class. This is actually fastai's version of Module, but it's nearly the same as Pytorch’s. So when we create this dot product object, it's going to call dunder init, and we say well how many users are going to be in our model, and how many movies, and how many factors, and so we can now create an embedding of users by factors for users, and an embedding of movies by vectors for movies, and so then"
fast.ai 2022 - Part 1,7,5397, How to understand the `forward` function,"Pytorch does something quite magic, which is that if you create a dot product object like so, it then… you can treat it like a function. You can call it up and calculate values on it. And when you do that, this is really important to know, Pytorch is going to call a method called “forward” in your class. So this is where you put your calculation of your model. It has to be called “forward”, and it's going to be passed the object itself, and the thing you're calculating on. In this case, the user and movie for a batch. So this is your batch of data, each row will be one user and movie combination, and the columns will be users and movies. So we can grab the first column, right, so this is every row of the first column, and look it up in the user factors embedding, to get our users embeddings. So that is the same as doing this. Let's say this is one mini batch. And then we do exactly the same thing for the second column, passing it into our movie factors to look up the movie embeddings, and then take the dot product. dim equals one because we're summing across the columns for each row. We're calculating a prediction for each row so once we've got that we can pass it to a learner, passing in our data loaders, and our model, and our loss function mean squared error, and we can call fit. And the way it goes. And this by the way is running on cpu. Now these are very fast to run. So this is doing 100 000 rows in 10 seconds, which is a whole lot faster than our few dozen rows in Excel. and so you can see the loss going down. And so we've trained a model. Um… it's not going to be a great model, and one of the problems is that, let's see if we can see this in our Excel one… Look at this one here. This prediction is bigger than five."
fast.ai 2022 - Part 1,7,5567, Adding a bias term,"But nothing's bigger than five. So that seems like a problem. We're predicting things that are bigger than the highest possible number. And in fact these are very much movie enthusiasts that… nobody gave anything a one. Yeah nobody even gave anything a one here. So… do you remember when we learned about Sigmoid. The idea of squishing things between zero and one. We could do stuff still without a Sigmoid, but when we added a Sigmoid, it trained better because the model didn't have to work so hard to get it, kind of, into the right zone. Now if you think about it, if you take something and put it through a Sigmoid, and then multiply it by five, now you've got something that's going to be between zero and five. We used to have something which is between zero and one, So we could do that in fact we could do that in Excel. I'll leave that as an exercise to the reader. Let's do it over here in Pytorch. So if we take the exact same class as before and this time we call sigmoid_range and so sigmoid_range is something which will take our prediction and then squash it into our range and by default we'll use a range of zero through to 5.5. so it can't be smaller than zero, can't be bigger than 5.5. Why didn't I use five? That's because a sigmoid can never hit one, right? and a sigmoid times five can never hit five but some people do give things... movies a five so you want to make it a bit bigger than our highest."
fast.ai 2022 - Part 1,7,5669, Model interpretation,"So this one got a loss of 0.8628 86 oh it's not better. Isn't that always the way? All right, didn't actually help; doesn't always. So be it. Let's keep trying to improve it. Let me show you something I noticed. Some of the users, like this one... this person here just loves movies. They give nearly everything a four or five. Their worst score is a three, all right? This person... oh, here's a one. This person's got much more range. Some things are twos, some ones, some fives. This person doesn't seem to like movies very much considering how many they watch. Nothing gets a five. They've got discerning tastes, I guess. At the moment we don't have any way in our kind of formulation of this model to say this user tends to give low scores and this user tends to give high scores. There's just nothing like that, right? But that would be very easy to add. Let's add one more number to our five factors, just here, for each user and now, rather than doing just the matrix multiply let's add... Oh it's actually the top one. Let's add this number to it h19 and so for this one let's add i19 to it. Yeah so I've got it wrong. This one here, so this... this row here... We're going to add to each rating and then we're going to do the same thing here. Each movie's now got an extra number here that again we're going to add a 26. So it's our matrix multiplication plus, we call it, the bias. The user bias plus the movie bias so effectively that's like making it so we don't have an intercept of zero anymore. And so if we now train this model... Data -> Solver -> Solve. So previously we got to 0.42, okay? and so we're going to let that go along for a while and then let's also go back and look at the Pytorch version. So for Pytorch, now we're going to have a user_bias which is an embedding of n_users by 1, right? Remember there was just one number for each user and movie bias is an embedding of n_movies also by 1 and so we can now look up the user embedding the movie embedding, do the dot product and then look up the user_bias and the movie_bias and add them. Chuck that through the sigmoid. Let's train that, see if we beat 0.865. Wow we're not training very well, are we? Still not too great. 0.894. I think Excel normally does do better though. Let's see. Okay Excel... oh Excel's done a lot better. It's gone from 0.42 to 0.35. Okay so what happened here? Why did it get worse? Well, look at this the valid loss got better and then it started getting worse again. So we think we might be overfitting, which you know we have got a lot of parameters in our embeddings. So how do we avoid overfitting? So a classic way to avoid overfitting is to use something called weight decay."
fast.ai 2022 - Part 1,7,5946, What is weight decay and How does it help,"Also known as L2 regularization, which sounds much more fancy. What we're going to do is when we can compute the gradients, we're going to first add to our loss function, the sum of the weights squared. This is something you should go back and add to your titanic model, not that it's overfitting, but just to try it, right? So previously our gradients have just been and our loss function has just been about the difference between our predictions and our actuals, right? And so our gradients were based on the derivative of that with respect to the derivative of that with respect to the coefficients but we're saying now “let's add the sum of the square of the weights times some small number”. So what would make that loss function go down? That loss function would go down if we reduce our weights. For example if we reduce all of our weights to zero... I should say we reduce the magnitude of our weights. If we reduce them all to zero, that part of the loss function will be zero because the sum of zero squared is zero. Now, problem is if our weights are all zero, our model doesn't do anything, right? So we'd have crappy predictions. So it would want to increase the weights so that's actually predicting something useful. But if it increases the weights too much then it starts overfitting. So how is it going to actually get the lowest possible value of the loss function? By finding the right mix. Weights not too high, right? But high enough to be useful at predicting. If there's some parameter that's not useful, for example, say we asked for five factors and we only need four, it can just set the weights for the fifth factor to zero, right? And then problem solved, right? It won't be used to predict anything but it also won't contribute to our weight decay part. So previously we had something calculating the loss function. So now we're going to do exactly the same thing but we're going to square the parameters, we're going to sum them up, and we're going to multiply them by some small number like 0.01 or 0.001. And in fact we don't even need to do this because remember the whole purpose of the loss is to take its gradient, right? And to print it out. The gradient of parameters squared is two times parameters. It's okay if you don't remember that from high school but you can take my word for it. The gradient of y equals x squared is 2x. So actually all we need to do is take our gradient and add the weight decay coefficient 0.01 or whatever times two times parameters. And given this is just number... some number we get to pick, we might as well pull the 2 into it and just get rid of it. So when you call fit you can pass in a wd parameter which does... adds this times the parameters to the gradient for you. And so that's going to ask the model...it's going to save the model... “please don't make the weights any bigger than they have to be”. And yay! Finally our loss actually improved, okay? And you can see it getting better and better. In fastai applications like vision we try to set this for you appropriately and we generally do a reasonably good job. Just the defaults are normally fine. But in things like tabular and collaborative filtering, we don't really know enough about your data to know what to use here. So you should just try a few things. Let’s... try a few multiples of 10. Start at 0.1 and then divide by 10 a few times, you know, and just see which one gives you the best result."
fast.ai 2022 - Part 1,7,6227, What is regularization,"So this is called regularization. So regularization is about making your bottle... model no more complex than it has to be, right? It has a lower capacity and so the higher the weights, the more they're moving the model around, right? So we want to keep the weights down but not so far down that they don't make good predictions and so the value of this if it's higher, will keep the weights down more, it will reduce overfitting but it will also reduce the capacity of your model to make good predictions and if it's lower it increases the capacity of model and increases overfitting. All right, I'm going to take this bit for next time. Before we wrap up, John, are there any more questions? JOHN: Yeah there are. The... there's some from... from back at the start of the collaborative filtering. So we had a bit of a conversation a while back about this... the size of the embedding vectors and you talked about your fastai rule of thumb. So there was a question if anyone has ever done a kind of a hyperparameter search, an exploration for… JEREMY: I mean people often will do a hyperparameter search, for sure. JOHN: I beg your pardon. JEREMY: People will often do a hyperparameter search for their model but I haven't seen a... I haven't seen any other rules other than my rule of thumb. JOHN: Right, so not not productively to your knowledge? JEREMY: Oh productively for an individual model that somebody's built. JOHN: And then there's a... there's a question here from Zakia which I didn't quite wrap my head around so Zakia if you want to maybe clarify in the... in the chat as well but “can recommendation systems be built based on average ratings of users experience rather than collaborative filtering?” JEREMY: Not really, right? I mean if you've got lots of metadata, you could, right? So if you've got, you know, like lots of information about demographic data about where the user's from and you know what loyalty scheme results they've had and blah blah blah and then for products there's metadata about that as well then sure averages would be fine but if all you've got is kind of purchasing history then you really want the granular data otherwise how could you say like... they like this movie, this movie, and this movie therefore they might also like that movie or you've got... it's like oh they kind of like movies. There's just not enough information there. JOHN: Yeah great. That's about it. thanks okay great alright thanks everybody see you next time for our last lesson"
fast.ai 2022 - Part 1,8,0, Neural net from scratch,"Practical Deep Learning for Coders, Lesson 8 So welcome to the last lesson of Part One of Practical Deep Learning for Coders. It's been a really fun time doing this course, and depending on when you're watching and listening to this you may want to check the forums or the fast.ai website to see whether we have a Part Two planned, which is going to be some time towards the end of 2022. Or if you're if it's already past that then maybe there's even a Part Two already on the website. So Part Two goes a lot deeper than Part One technically, in terms of getting to the point that you should be able to read and implement research papers and deploy models in a very kind of real life situation So yeah last lesson we started on the Collaborative Filtering notebook and we were looking at Collaborative Filtering and this is where we got to, which is creating your own embedding module, and this is a very cool… this is a very cool place to start the lesson because you're going to learn a lot about what's really going on and it's really important before you dig into this to make sure that you're really comfortable with the 05-liner-model-and-neural-net-from-scratch notebook. So if parts of this are not totally clear, put it aside and redo this notebook because what we're looking at from here are kind of the abstractions that Pytorch and fast.ai add on top of… functionality that we've built ourselves from scratch. So if you remember in the neural network from scratch we built, we initialized a number of coefficients, a couple of different layers, you know, and a bias term and then during as the model trained we updated those coefficients by going through each layer of them and subtracting out the gradients by the learning rate… In… you probably noticed that in Pytorch we don't have to go to all that trouble, and I wanted to show you how Pytorch does this. Pytorch… we don't have to keep track of what our coefficients, or parameters, or weights are - Pytorch does that for us. And the way it does that is it looks inside our module and it tries to find anything that looks like a neural network parameter, or a tensor of neural network parameters, and it keeps track of them. And so here is a class we've created called “T” which is a subclass of module and I've created one thing inside it which is something with the attribute “a”, so this is “a” in the “T” module and it just contains three ones. And so the idea is, you know, maybe we're creating a module and this is we're initializing some parameter that we want to train. Now, we can find out what trainable parameters – or just what parameters in genera l – Pytorch knows about in our model by instantiating our model and then asking for the parameters – which you then have to turn that into a list… or in fastcore we have a thing called capital “L” which is like a fancy list which prints out the number of items in the list and shows you those items. Now in this case, when we create our object of type “T” and ask for its parameters we get told there are zero tensors of parameters and a list with nothing in it. Now why is that? We actually said we wanted to create three… a tensor with three ones in it. How would we make those parameters? Well, the answer is that the way you create… the way you tell Pytorch what your parameters are is: you actually just have to put them inside a special object called an “nn.Parameter”. This"
fast.ai 2022 - Part 1,8,286, Parameters in PyTorch,"thing almost doesn't really do anything. In fact last time I checked it really quite literally had almost no code in it... (sometimes these things change, but let's take a look)... Yeah okay so it's about a dozen lines of code or 20 lines of code, which does almost nothing. It's got a way of being copied, it's got a way of printing itself, it's got a way of saving itself, and it's got a way of being initialized. So Parameter hardly does anything. The key thing is though that when Pytorch checks to see which parameters should it update when it optimizes, it just looks for anything that's been wrapped in this Parameter class. So, if we do exactly the same thing as before, which is to set an attribute containing a tensor with three ones… in it but this case we wrap it in a Parameter. We now get told: okay, there's one parameter tensor in this model and it contains a tensor with three ones. And you can see it also actually, by default, assumes that we're going to want –require– a gradient. It's assuming that anything that's a parameter is something that you want to calculate gradients for. Now, most of the time we don't have to do this because Pytorch provides lots of convenient things for us such as what you've seen before “nn.Linear”, which is something that also (contains) creates a tensor. So this would (contain) create a tensor of one by three without a bias term in it. This has not been wrapped in a “nn.Parameter” – but that's okay. Pytorch knows that anything which is basically a layer in a neural net is going to be a parameter, so it automatically considers this a parameter. So here's exactly the same thing again, I construct my object of type “T”, I'll check for its parameters and I can see there's (three of…) one tensor of parameters and there's our three things. And you'll notice that it's also automatically, randomly, initialized them, which again is generally what we want. So Pytorch does go to some effort to, yeah, try to make things easy for you. So the, this attribute “a” is a… is a linear layer and it's got a bunch of things in it. One of the things in it is the weights, and that's where you'll actually find the parameters, that is, of type “Parameter”. So a linear layer is something that contains attributes of type “Parameter”."
fast.ai 2022 - Part 1,8,462, Embedding from scratch,"Okay so what we want to do is, we want to create something that works just like this did: which is something that creates a matrix which will be trained as we train the model… Okay so, an embedding is something which, yeah, it's going to create a matrix of this by this, and it will be a parameter and it's something that, yeah, we need to be able to index into, as we did here. And so how, yeah, what is… what is happening behind the scenes, you know, in Pytorch? It's nice to create these things ourselves from scratch because it means we really understand it. And so, let's create that exact same module that we did last time but this time we're going to use a function I've created called “create_params()”. You pass in a size, so… such as… in this case – n uses by n factors. And it's going to call “torch.zeros()” to create a tensor of zeros of the size that you request, and then it's going to do a normal random distribution… so a gaussian distribution… of mean zero standard deviation 0.01 to randomly initialize those, and it'll put the whole thing into an “nn.Parameter”. So that… so this here is going to create an attribute called “user_factors” which will be a parameter containing some tensor of normally distributed random numbers of this size. Excuse me. And because it's a parameter that's going to be stored inside… that's going to be available as in parameters in the “Module”... (Oh why am I sneezing) So “user_bias” will be a vector of parameters, “user_factors” will be a matrix of parameters, “movie_factors” will be a matrix “n_movies” by “n_factors”, “movie_bias” will be a vector of “n_movies” and this is the same as before. So now in the forward() we can do exactly what we did before. The thing is, when you put a tensor inside a “Parameter” it has all the exact same features that a tensor has. So, for example, we can index into it. So this whole thing is identical to what we had before and so that's actually, believe it or not, all that's required to replicate Pytorch's embedding layer from scratch. So let's run those and see if it works. And there it is: it's training. So we'll be able to have a look when this is done, at for example… ((let's have a look)) model.move_bias. And here it is, right?, it's a “Parameter” containing a bunch of numbers that have been trained. And as we'd expect it's got 1665 things in, because that's how many movies we have. So a question from Jona Raphael was: “does ‘torch.zeros’ not produce all zeros? Yes “torch.zeros” does produce all zeros. But remember, a method that ends in underscore changes in-place the tensor it's being applied to. And so, if you look up “pytorch normal_” you'll see it fills itself with elements sampled from the normal distribution. So this is actually modifying this tensor in place, and so that's why we end up with something which isn't just zeros. Now this is a bit I find really fun, is, we train this model, but what did it do?"
fast.ai 2022 - Part 1,8,741, Embedding interpretation,"How is it going about predicting who's going to like what movie? What, well, one of the things that's happened is we've created this “movie_bias” parameter which has been optimized and what we could do is we could find which Movie-IDs have the highest numbers here; and the lowest numbers, (so I think this is going to start lowest) and then we can print out… we can look inside our data loaders and grab the names of those movies for each of those five lowest numbers. And what's happened here? Well, we can see broadly speaking that it has printed out some pretty crappy movies. And, why is that? Well, that's because when it does that matrix product that we saw in the excel spreadsheet last week, it's trying to figure out who's going to like what movie based on previous movies people have enjoyed or not, and then it adds “movie_bias”, which can be positive or negative, that's a different number for each movie. So in order to do a good job of predicting whether you're going to like a movie or not, it has to know which movies are crap, and so the crap movies are going to end up with a very low “movie_bias” parameter, and so we can actually find out which movies, do people… not only which movies do people really not like, but which movies do people like less than one would expect given the kind of movie that it is. So “Lawnmower Man 2”, for example, not only apparently is it a crappy movie but, based on the kind of movie it is (you know, it's kind of like a high-tech pop kind of sci-fi movie…) …people who like those kinds of movies still don't like “Lawnmower Man 2”, so that's what this is meaning. So it's kind of nice that we can, like, use a model not just to predict things but to understand things about the data. So if we sort by descending, it'll give us the exact opposite. So here are movies that people enjoy even when they don't normally enjoy that kind of movie. So for example “L.A. Confidential”, classic kind of film noir detective movie with the aussie Guy Pierce, even if you don't really like film noir detective movies, you might like this one. You know, “Silence of the Lambs”, classic kind of… I guess you'd say like, horror kind of… not horror is it, a suspense movie… even people who don't normally like kind-of serial killer suspense movies tend to like this one. Now, the other thing we can do is not just look at what's happening in the bias… oh and by the way, we could do the same thing with users and find out, like, which user just loves movies, even the crappy ones, you know. Dislikes all movies and vice versa. But what about the other thing – we didn't just have bias – we also had movie factors. Which has got the number of movies as one axis and the number of factors as the other and we passed in 50 – what's in that huge matrix? Well, pretty hard to visualize such a huge matrix and we're not going to talk about the details, but you can do something called PCA, which stands for Principal Component Analysis, and that basically tries to compress those 50 columns down into three columns… and then we can draw a chart of the top two. And so this is PCA component number one and this is PCA component number two, and here's a bunch of movies and this is a compressed view of these latent factors that it created. And you can see that they obviously have some kind of meaning, right? So over here towards the right, we've got kind of, you know, very pop mainstream kind of movies. And over here on the left, we've got more of the kind of critically acclaimed gritty kind of movies. And then towards the top we've got very kind of action-oriented and sci-fi movies and then down towards the bottom we've got very dialogue driven movies. So remember, we didn't program in any of these things and we don't have any data at all about what movie is what kind of movie but thanks to the magic of SGD, we just told it to please try and optimize these parameters and the way it was able to predict who would like what movie was it had to figure out what kinds of movies are there or what kind of taste is there for each movie. So I think that's pretty interesting. So this is called “visualizing embeddings” and then this is “visualizing the bias”."
fast.ai 2022 - Part 1,8,1086, Collab filtering in fastai,"We… We obviously would rather not do everything both by hand, like this, or even like this, and fast.ai provides an application for collaborative learner [“collab_learner”]. And so we can create one, and this is going to look much the same as what we just had, we're going to say how many latent factors we want and what the “y_range” is: to do the sigmoid and the multiply; and then we can do “fit”, and away it goes… So let's see how it does. All right, so, it's done a bit better than our manual one. Let's take a look at the model it created. The model looks very similar to what we created in terms of the parameters. You can see here, these are the two embeddings and these are the two biases, and we can do exactly the same thing… we can look in that model and we can find the – you'll see it's not called movies it's “i” for items, it uses an items… this is the item_bias. So we can look at the item_bias, grab the weights, sort, and we get a very similar result. In this case it's very… even more confident that “L.A. Confidential” is a movie that you should probably try watching even if you don't like those kind of movies. And “Titanic” is right up there as well. Even if you don't really like romancy kind of movies, you might like this one. Even if you don't like classic detective, you might like this one [Pointing “Rear Window”] You know, we can have a look at the source code for “collab_learner” and we can see that… Let's see, “use_nn” is false by default, so where our model is going to be of this type… “EmbeddingDotBias”... so we can take a look at that… Here it is, and look!, this does look very similar, okay? It's creating an embedding, using the size we requested for each of users-by-factors and items-by-factors and users and items. And then it's wrapping each thing from the embedding in the “forward”, and it's doing the multiply, and it's adding it up, and it's doing the sigmoid. So yeah, it looks exactly the same, isn't that neat?. So you can see that what's actually happening in real models is not, yeah, it's not… it's not that weird or magic. So Kurian is asking: “is PCA useful in any other areas?” And the answer is: absolutely! And what I suggest you do –if you're interested– is check out our (~contra) “Computational Linear Algebra” course. It's five years old now but it... I mean… this is stuff which hasn't changed for decades really and this will teach you all about things like PCA and stuff like that. It's not nearly as directly practical as “Practical Deep Learning for Coders” but it's definitely, like, very interesting and it's the kind of thing which, if you want to go deeper, you know, it can become pretty useful later along your path."
fast.ai 2022 - Part 1,8,1331, Embedding distance,"Okay, so here's something else interesting we can do: let's grab the “movie_factors”. So that's in our model, it's the item weights [“i_weights”] and it's the “weight” attribute that Pytorch creates. Okay, and now we can convert the movie “Silence of the Lambs” into its “class_id” and we can do that with object-to-id “o2i”, for the titles. And so that's the movie index of “Silence of the Lambs”. And what we can do now is we can look through all of the movies in our latent factors and calculate how far apart (~the) each vector is, each embedding vector is from this one. And this “CosineSimilarity” is very similar to basically the “Euclidean Distance”, you know, the kind of the Root Sum Squared of the… of the differences, but it normalizes it. So it's basically the angle between the vectors. So this is going to calculate how similar each movie is to the “Silence of the Lambs” on, based on these latent factors. And so then we can find which ID is the closest. Yeah, so based on this embedding distance, the closest is “Dial M for Murder”… which makes a lot of sense. I'm not going to discuss it today but in the book there's also some discussion about what's called the Bootstrapping Problem, which is the question of, like, if you've got a new company, or a new product, how would you get started with making recommendations given that you don't have any previous history with which to make recommendations and that's a very interesting problem that you can read about in the book."
fast.ai 2022 - Part 1,8,1462, Collab filtering with DL,"Now… that's one way to do Collaborative Filtering, which is where we create that… do that matrix completion exercise using all those dot products. There's a different way however, which is we can use Deep Learning. And to do it with Deep Learning, what we could do is, we can… we could basically create our user and item embeddings as per usual and then we could create a sequential model. So a sequential model is just layers of a Deep Learning Neural Network in order. And, what we could do is we could just concatenate… so in “forward” we could just concatenate the user and item embeddings together, and then do a reLU. So this is… this is basically a single Hidden Layer Neural Network, and then a linear layer at the end to create a single output. So this is the very, you know, world's most simple neural net exactly the same as the style that we created back here in our “...neural net from scratch.” This is exactly the same, but we're using Pytorch's functionality to do it more easily. So, in the “forward” here we're gonna, in the same… exactly the same way as we have before, we'll look up the user embeddings and we'll look up the item embeddings and then this is new, this is where we concatenate those two things together and put it through our Neural Network and then finally do our sigmoid. Now, one thing different this time is that we're going to ask fast.ai to figure out how big our embeddings should be, and so fast.ai has something called get embedding sizes [“get_emb_sz”]. And it just uses a rule of thumb that says that: for 944 users we recommend 74 factor embeddings and for 1,665 movies (or is it the other way around, I can't remember) we recommend 102 factors for your embeddings – so that's what those sizes are. So now we can create that model and we can pop it into a learner and fit in the usual way. And so, rather than doing all that from scratch, what you can do is you can do exactly the same thing that we've done before: which is to call collaborative learner [“collab_learner”], but you can pass in the parameter use neural network [“use_nn”] equals True, and you can then say how big do you want each layer. So this is going to create a two hidden layer Deep Learning Neural Net: the first will have (~1500) [100] and the second will have 50. And then you can say “fit” and away it goes. Okay so here is our… oh we got 0.87… so these are doing less well than our dot product version which is not too surprising because kind of the dot product version is really trying to take advantage of our understanding of the problem domain. In practice, nowadays, a lot of companies kind of combine…they kind of create a combined model that have a has a Dot Product component and also has a Neural Net component. The Neural Net component is particularly helpful if you've got metadata, for example information about your users, like: when did they sign up; how old are they; what sex are they; you know, where are they from. And then those are all things that you could concatenate in, with your embeddings, and ditto with metadata about the movie: how old is it; what genre is it; and so forth. All right, so, we've got a question from Jona which I think is interesting. And the question is: “is there an issue where the bias components are overwhelmingly determined by the non-experts in a genre?” In general, actually there's a more general issue which is, in Collaborative Filtering Recommendation Systems, very often a small number of users or a small number of movies overwhelm everybody else and the classic one is Anime. A relatively small number of people watch Anime and those group of people watch a lot of Anime. So in movie recommendations, like, there's a classic problem which is every time people try to make a list of well-loved movies all the top ones seem to be anime. And so you can imagine what's happening in the matrix completion exercise is that there are... yeah some… some users that just, you know, really watch this one genre of movie and they watch an awful lot of them. So in general you actually do have to be pretty careful about the, you know, these subtlety kind-of issues. And yeah I won't go into details about how to deal with them but they generally involve taking various kinds of ratios, or normalizing things or so forth."
fast.ai 2022 - Part 1,8,1825, Embeddings for NLP,"All right, so, that's Collaborative Filtering, and, I wanted to show you something interesting then about embeddings, which is that embeddings are not just for Collaborative Filtering. And in fact, if you've heard about embeddings before you've probably heard about them in the context of Natural Language Processing. So you might have been wondering, back when we did the Hugging Face transformers stuff, how did we go about, you know, using text as inputs to models. And we talked about how you can turn words into integers… we make a list… So here's… here's the movie (sorry...) here's the the poem “I am Sam”: I am Daniel / I am Sam / Sam I am / That Sam-I-am, et cetera, et cetera. We can find a list of all the unique words in that poem and make this list here. And then we can give each of those words a unique id, just arbitrarily, well actually in this case it's alphabetical order, but it doesn't have to be. And so we kind of talked about that and that's what we do with categories in general, but how do we turn those into like, you know, lists of random numbers. And you might not be surprised to hear, what we do is: we create an embedding matrix. So here's an embedding matrix containing four latent factors for each word in the vocab. So here's each word in the vocab and here's the embedding matrix. So if we then want to present this poem to a Neural Net… then what we do is we list out our poem – I do not like that, Sam-I-am / Do you like Green eggs and ham, etc. Then for each word we look it up… so in Excel, for example, we use “MATCH”, so that will find this word over here and find it is word id 8 and then we will find the 8th word and the first embedding. And so that gives us… (that's not right, eight?)... Oh no, that is right, sorry. Here it is, it's just weird column widths. So it's going to be 0.22, then 0.1, 0.01 and here it is 0.22, 0.1, 0.01 etc. So this is the embedding matrix we end up with for this poem. And so if you wanted to train, or use a trained neural network on this poem, you basically turn it into this matrix of numbers. And so, this is what an embedding matrix looks like in an NLP model and it works exactly the same way, as you can see. And then you can do exactly the same things in terms of interpretation of an NLP model by looking at both the bias factors and the latent factors in a word embedding matrix. So hopefully you're getting the idea here that… our, you know, our different models, you know, the inputs to them… they're based on a relatively small number of, kind of, basic principles. And these principles are generally things like: look up something in array. And then we know, inside the model, we're basically multiplying things together, adding them up and replacing the negatives with zeros. So hopefully you're getting the idea that what's going on inside a Neural Network is generally not that complicated, but it happens very quickly and at scale. Now, it's not just Collaborative Filtering and NLP but also Tabular Analysis."
fast.ai 2022 - Part 1,8,2096, Embeddings for tabular,"So, in Chapter 9 of the book we've talked about how Random Forests can be used for this, which was for… this is for the thing where we're predicting the auction sale price of industrial heavy equipment like bulldozers. Instead of using a Random Forest, we can use a Neural Net. Now, in this data set, there are some continuous columns and there are some categorical columns. Now, I'm not going to go into the details too much, but in short, we can separate out the continuous columns and categorical (~problem) columns using “cont_cat_split” and that will automatically find which is which based on their data types. And so in this case it looks like… okay so continuous columns, the elapsed sale date, so I think it's the number of seconds, or years, or something since the start of the data set, is a continuous variable. And then here are the “cat”, the categorical variables. So for example there are six different product sizes and two coupler systems, 5059 model descriptions, 6 enclosures, 17 tire sizes and so forth. So, we can use fast.ai, basically, to say: okay, we'll take that data frame and pass in the categorical and continuous variables, and create some random splits, and what's the dependent variable, and we can create data loaders from that. And from that we can create a “tabular_learner”; and basically what that's going to do is… it's going to create a pretty regular, multi-layer neural network, not that different to this one, that we created by hand. And each of the categorical variables, it's going to create an embedding for it. And so I can actually show you this, right? So we're going to use “tabular_learner” to create the “learner” and so “tabular_learner” is one, two, three, four, five, six, seven, eight, nine lines of code. And basically the main thing it does is create a “TabularModel”. And so then “TabularModel”, you're not going to understand all of it, but you might be surprised at how much. So a “TabularModel” is a module, we're going to be passing in how big is each embedding going to be, and “tabular_learner”... what's that passing in? It's gonna call get embedding sizes [“get_emb_sz”] just like we did manually before –automatically. So that's how it gets its embedding sizes and then it's going to create an “Embedding” for each of those embedding sizes [“emb_szs”], from number of inputs, to number of factors. “Dropout” we're going to come back to later. “BatchNorm” we won’t do until part two. So then it's going to create a layer, for each of the layers we want, which is going to contain a linear layer, followed by batch norm, followed by drop out [“LinBnDrop”]. It's going to add the “SigmoidRange” we've talked about at the very end. And so the “forward()”, this is the entire thing! If there's some embeddings, it'll go through and get each of the embeddings using the same indexing approach we've used before. It'll concatenate them all together, and then it'll run it through the layers of the neural net, which are these. So yeah, we don't know all of those details yet, but we know quite a few of them. So that's encouraging, hopefully. And once we've got that, we can do the standard “lr_find” and “fit”. Now, this exact dataset was used in a Kaggle competition… this dataset was in a Kaggle competition… and the third place getter published a paper about their technique and it's basically the exact, almost the exact one, I'm showing you here. So it wasn't this, sorry, it wasn't this data set, it was a dataset, it was a different one, it was about predicting the amount of sales in different stores… But they, they used this basic kind of technique. And one of the interesting things is that they used a lot less manual feature engineering than the other high placed entries. Like they had a much simpler approach, and one of the interesting things they published a paper about their approach. So they published a paper about their approach. So this is the team, from this company, and they basically describe here exactly what I just showed you – these different embedding layers being concatenated together and then going through a couple of layers of a neural network, and it's showing here… it points out in the paper exactly what we learned in the last lesson, which is, embedding layers are exactly equivalent to linear layers on top of a one-hot encoded input. And, yeah, they found that their technique worked really well. One of the interesting things they also showed is that you can take, you can create your Neural Net, get your trained embeddings, and then you can put those embeddings into a Random Forest or Gradient Booster Tree and your Mean Average Percent Error will dramatically improve. So you can actually combine Random Forests and embeddings or Gradient Boosted Trees and embeddings, which is really interesting. Now, what I really wanted to show you though is, what they then did… so, as I said this was a thing about the predicted amount that different products would sell for at different shops around Germany. And what they did was they had a… so, one of their embedding matrices was embeddings by region, and then they did… I think this is a PCA (Principal Component Analysis) of the embeddings for their German regions and when they create a chart of them, you can see that the locations that close together in the embedding matrix are the same locations that are close together in Germany. So you can see here's the blue ones and here's the blue ones. And again, it's important to recognize that the data that they used had no information about the location of these places, the fact that they are close together geographically is something that was figured out as being something that actually helped it to predict sales. And so, in fact, they then did a plot showing: each of these dots is a shop (a store) and it's showing: for each pair of stores, how far away is it in real life (in metric space) and then how far away is it in embedding space. And there's this very strong correlation, right? So, it's, you know, it's kind of reconstructed somehow (~this kind of) the kind of the geography of Germany, by figuring out how people shop. And similar for days of the week, so there was no information, really, about days of the week but when they put it on the embedding matrix the days of the week: Monday, Tuesday, Wednesday – close to each other. Thursday, Friday – close to each other. As you can see, Saturday and Sunday – close to each other. And ditto for months of the year: January, February, March, April, May, June. So yeah, really interesting, cool stuff, I think, what's actually going on inside a Neural Network. All right, let's take a 10 minute break and I will see you back here at 7:10."
fast.ai 2022 - Part 1,8,2673, Convolutions,"All right folks, this is something I think is really fun, which is: we're going to… we've looked at what goes into the start of a model “the input”. We've learned about how they can be categories, or embeddings, and embeddings are basically kind of one-hot-encoded (~category) categories with a little compute trick, or they can just be continuous numbers. We've learned about what comes (~the other) at the other side, which is a bunch of activations, so just a bunch of tensors of numbers which we can use things like softmax to constrain them to add up to one and so forth. And we've looked at what can go in the middle, which is the… matrix multiplies, sandwiched together with, you know, as rectified linear units. And I mentioned that there are other things that can go in the middle as well but we haven't really talked about what those other things are. So I thought we might look at one of the most important and interesting version of things that can go in the middle, but what you'll see is it turns out it's actually just another kind of matrix multiplication. Which might not be obvious at first but I'll explain. We're going to look at something called a convolution – and convolutions are at the heart of a convolutional neural network. So, the first thing to realize is a convolutional neural network is very, very, very similar to the neural networks we've seen so far – it's got inputs, it's got things that are a lot like, or actually are a form of, matrix multiplication sandwiched with activation functions which can be rectified linear. But there's a particular thing which makes them very useful for computer vision. And I'm going to show you using this excel spreadsheet that's in our repo called “conv-example”. And we're going to look at it using an image from MNIST. So MNIST is kind of the world's most famous computer vision dataset (I think) because it was like the first one, really, which really showed image recognition being… being cracked. It's pretty small by today's standards, it's a dataset of handwritten digits, each one is 28 by 28 pixels. But it, yeah, you know, back in the mid 90s Yann LeCun showed, you know, really practically useful performance on this dataset and as a result ended up with convnets being used in the american banking system for reading checks. So here's an example of one of those digits, this is a seven that somebody drew – it's one of those ones with a stroke through it – and this is what it looks like. This is… this is the image. And so I got it from MN… this is just one of the images from MNIST which I put into Excel. And what you see in the… in the next column is a version of the image where the horizontal lines are being recognized and another one where the vertical lines are being recognized. And if you think back to that Zeiler and Fergus paper that talked about what the layers of a neural net does, this is absolutely an example of something that we know that the first layer of a neural network tends to learn how to do. Now, how did I do this? I did this using something called a convolution and so, what we're going to do now is, we're going to zoom in to this excel notebook. We're going to keep zooming in. We're going to keep zooming in. So take a look.. keep an eye on this… on this image, and you'll see that once we zoom in enough, it's actually just made of numbers which, as we discussed in the very first… in the very first lesson, we saw how images are made of numbers. So here they are, right? Here are the numbers, between zero and one. And what I just did is, I just used a little trick, I used Microsoft Excel's conditional formatting to basically make things: the higher numbers more red. So that's how I turn this Excel sheet and I've just rounded it off to the nearest decimal, but it's actually, they're actually bigger than that. So yeah, so here is the image as numbers. And so let me show you how we went about creating this top edge detector. What we did was we created this formula… (don't worry about the MAX…) let's focus on this... what it's doing is… have a look at the colored in areas… it's taking each of these cells and multiplying them by each of these cells, and then adding them up. And then we do the rectified linear part which is, if that ends up less than zero, then make it zero. So this is a, this is like a rectified linear unit but it's not doing the normal matrix product, it's doing the equivalent of a dot product but just on these nine cells, and with just these nine weights. So you might not be surprised to hear that if I move now one to the right, then now it's using the next nine cells, all right? So if I move, like, to the right quite a bit and down quite a bit to here, it's using these nine cells. So it's still doing a dot product, right? …which as we know is a form of matrix multiplication, but it's doing it in this way where it's kind of taking advantage of the… of the geometry of this situation that the things that are close to each other are being multiplied by this consistent group of the same nine weights each time, because there's actually 28 by 28 numbers here, right? Which I think is 768... 28 times 28. That’s close enough, 784, but (~we don't want, we're not) we don't have 784 parameters, we only have nine parameters. And so, this is called a convolution. So a convolution is where you basically slide this, kind of, little 3 by 3 matrix across a bigger matrix and at each location you do a dot product of the corresponding elements of that 3 by 3 with the corresponding elements of this 3 by 3 matrix of coefficients. Now, why does that create something that finds, as you see, top edges. Well, it's because of the particular way I constructed this 3 by 3 matrix. What I said was that all of the rows, just above, (so these ones) are going to get a one, and all of the ones just below are going to get a minus one, and all of the ones in the middle are going to get a zero. So let's think about what happens somewhere like here, right? That is (let's try to find the right one… here it is…) …so here we're going to get (1 x 1) + (1 x 1) + (1 x 1) - (1 x 1) - (1 x 1) - (1 x 1). We're going to get 0. But what about up here. Here we're going to get (1 x 1) + (1 x 1) + (1 + 1). These do nothing because they're ( x 0), minus (1 x 0). So we're going to get 3. So we're only going to get 3 (the highest possible number) in the situation where these are all as black as possible, or in this case as red as possible, and these are all white. And so that's only going to happen at a horizontal edge. So the one underneath it does exactly the same thing, exactly the same formulas (oopsie daisy.) The one underneath are exactly the same formulas, a 3 by 3 sliding thing here, but this time we've got a different mat… different little mini matrix of coefficients which is all ones going down and all minus ones going down. And so, for exactly the same reason, this will only be three in situations where they're all one here and they're all zero here. So, you can think of a convolution as being a sliding window of little mini dot products of these little 3 by 3 matrices. And they don't have to be 3 by 3, right? You could have, we could just have easily done 5 by 5 and then we'd have a 5 by 5 matrix of coefficients –or whatever, whatever size you like. So the size of this is called its kernel size. This is a 3 by 3 kernel for this convolution. So then, because this is deep learning, we just repeat the… we just repeat these steps again, and again and again. So this is… this layer I'm calling “Conv1” – it's the first convolutional layer. So “Conv2” is going to be a little bit different because on “Conv1” we only had a single channel input: it's just black and white or, you know, yeah, black and white, grayscale, one channel. But now we've got two channels. We've got the (let's make it a little smaller so we can see better). We've got the horizontal edges channel and the vertical edges channel. And would have a similar thing in the first layer of its color, we'd have a red channel, a green channel and blue channel. So now our filter (this is called the filter, this little mini matrix is called the filter), our filter… our filter now contains a 3 x 3 x depth 2 (or if you want to think of another way) two 3 x 3 kernels, or one 3 x 3 x 2 kernel. And we basically do exactly the same thing, which is we're going to multiply each of these, by each of these, and sum them up. But then we do it for the second bit as well, we multiply each of these, by each of these, and sum them up. And so that gives us… and I think I just picked some random numbers here, right?. So this is going to now be something which can combine… oh sorry, the set… the second one, the second set… so it's, sorry… each of the red ones, by each of the blue ones (that's here) plus each of the green ones, times each of the mauve ones (that's here). So this first filter is being applied to the horizontal edge detector and the second filter is being applied to the vertical edge detector and as a result, we can end up with something that combines features of the two things. And so then we can have a second channel over here, which is just a different bunch of convolutions for each of the two channels, this one times this one, again, you can see the colors."
fast.ai 2022 - Part 1,8,3427, Optimizing convulutions,"So, what we could do is if, you know, once we kind of get to the end we'll end up (as I'll show you how in a moment) we'll end up with a single set of ten activations, one per num-digit we're recognizing – zero to nine. Or, in this case I think we could just create one, you know, maybe we're just trying to recognize nothing but the number seven, or not the number seven. So we could just have one activation. And then we would back propagate through this using SGD in the usual way. And that is going to end up optimizing these numbers. So in this case I manually put in the numbers I knew would create edge detectors. In real life you start with random numbers and then you use SGD to optimize these parameters."
fast.ai 2022 - Part 1,8,3480, Pooling,"Okay, so there's a few things we can do next and I'm gonna, I'm gonna show you the way that was more common a few years ago and then I'll explain some changes that have been made more recently. What happened a few years ago was we would then take these activations, which as you can see these activations now are kind of in a grid pattern, and we would do something called Max Pooling. And Max Pooling is kind of like a convolution (it's a sliding window) but this time as the sliding window goes across (so here, we're up to here) we don't do a dot product over a filter, but instead we just take a maximum (see here), just this is the maximum of these four numbers, and if we go across a little bit this is the maximum of these four numbers, go across a bit, go across a bit, and so forth (oh that goes off the edge). And you can see what happens when this is called a 2 by 2 Max Pooling. So, you can see what happens, with the 2 by 2 max pooling we end up losing half of our activations on each dimension, so we're going to end up with only one quarter of the number of activations we used to have. And that's actually a good thing because if we keep on doing convolution, max pool, convolution, max pool, we're going to get fewer and fewer and fewer activations, until eventually, we'll just have one left, which is what we want. That's effectively what we used to do, but, the other thing I mentioned is we didn't normally keep going until there's only one left. What we used to then do is we'd basically say: okay, at some point, we're going to take all of the activations that are left and we're going to basically just do a dot product of those with a bunch of coefficients, not as a convolution but just as a normal linear layer (and this is called the Dense Layer) and then we would add them all up. So we basically end up with a final big dot product of all of the max pooled activations by all of the weights, and would do that for each channel and so that would give us our final activation. And as I say here, MNIST would actually have 10 activations so you'd have a separate set of weights for each of the digits you're predicting and then softmax after that. Okay, nowadays we do things very slightly differently, nowadays we normally don't have Max Pool layers but instead, what we normally do is, when we do our sliding window (like this one here) we don't normally… let's go back to see… so when I go one to the right… so currently we're starting in cell column “G”, if I go one to the right the next one is column “H”, and if I go one to the right the next one starts in column “I”. So you can see it's sliding the window over every 3 by 3. Nowadays what we tend to do instead is we generally skip one, so we would normally only look at every second. So we would after doing column “I”, we would skip columns “J” and would go straight to column “K” and that's called a Stride 2 Convolution. We do that both across the rows and down the columns. And what that means is every time we do a convolution, we reduce our effective kind of feature size (grid size) by two on each axis, so it reduces it by four in total. So that's basically, instead of doing Max Pooling. And then the other thing that we do differently is nowadays we don't normally have a single dense layer at the end (a single matrix multiply at the end) but instead what we do… we generally keep doing stride 2 convolutions so each one is going to reduce the grid size by 2 x 2, we keep going down until we've got about a 7 by 7 grid and then we do a single pooling at the end. And we don't normally do max pool nowadays – instead we do an average pool. So we average the activations of each one of the 7 by 7 features. This is actually quite important to know because if you think about what that means, it means that something like an ImageNet style image detector is going to end up with a 7 by 7 grid – let's say it's trying to say, is this a bear? – and in each of the parts of the 7 by 7 grid it's basically saying: is there a bear in this part of the photo?, is there a bear in this part of the photo? is there a bear in this part of the photo? And then it takes the average of those 49 (7 x 7) predictions to decide whether there's a bear in the photo. That works very well if it's basically a photo of a bear, right, because most, you know, if it's… if the bear is big and takes up most of the frame then most of those 7 by 7 bits are bits of a bear. On the other hand, if it's a teeny tiny bear in the corner, then potentially only one of those 49 squares has a bear in it, and even worse if it's like a picture of lots and lots of different things, only one of which is a bear, it could end up not being a great bear detector. And so this is where, like, the details of how we construct our model turn out to be important. And so, if you're trying to find, like, just one part of a photo that has a small bear in it, you might decide to use (~average pool… sorry) maximum pooling instead of average pooling because max-pooling will just say: I think this is a picture of a bear if any one of those 49 bits of my grid has something that looks like a bear in it. So these are, you know, these are potentially important details which often get hand waved over. Although, you know, again like, the key thing here is that… this is happening right at the very end, right, that max pool or that average pool, and actually fast.ai handles this for you – we do a special thing which we, kind of, independently invented (I think we did it first) which is, we do both max pool and average pool and we concatenate them together. We call that Concat Pooling and that has since been reinvented in at least one paper. And so that means that you don't have to think too much about it because we're going to try both for you, basically."
fast.ai 2022 - Part 1,8,3912, Convolutions as matrix products,"So I mentioned that this is actually really just matrix multiplication and to show you that, I'm going to show you some images created by a guy called Matt Kleinsmith who did this… actually, I think this was in our very first ever course (might have been the Part 2, first Part 2 course) …and he basically pointed out that in a certain way of thinking about it, it turns out that convolution is the same thing as a matrix multiplier. So I want to show you how he shows this. He basically says, okay, let's take this 3 by 3 image and a 2 by 2 kernel containing the coefficients alpha, beta, gamma, delta. And so, in this… as we slide the window over, each of the colors… each of the colors are multiplied together: red by red, plus green by green, plus (what is that?) orange by orange, plus blue blue, gives you this. And so to put it another way algebraically P = alpha x A + beta x B… et cetera. And so then, as we slide to this part, we're multiplying again: red by red, green by green and so forth, so we can say Q = alpha x B + beta x C etc. And so this is how we calculate a convolution using the approach we just described, as a sliding window. But here's another way of thinking about it. We could say: okay, we've got all these different things: a, b, c, d, e, f, g, h, j. Let's put them all into a single vector and then let's create a single matrix that has alpha, alpha, alpha, alpha; beta, beta, beta, beta, et cetera. And then if we do this matrix multiplied by this vector we get this. With these grey zeros in the appropriate places, which gives us this, which is the same as this. And so this shows that a convolution is actually a special kind of matrix multiplication: it's a matrix multiplication where there are some zeros that are fixed and some numbers that are forced to be the same. Now, in practice, it's going to be faster to do it this way, but it's a useful kind of thing to think about, I think that just to realize like: oh! it's just another of these special types of matrix multiplications."
fast.ai 2022 - Part 1,8,4101, Dropout,"Okay, I think… well, let's look at one more thing. Because there was one other thing that we saw, and I mentioned we would look at, in the tabular model, which is called Dropout, and I actually have this in my excel spreadsheet. If you go to the “conv-example (dropout)” page … you'll see we've actually got a little bit more stuff here. We've got the same input as before and the same first convolution as before and the same second convolution as before. And then we've got a bunch of random numbers. They're showing as between 0 and 1, but they're actually… (that's just because they're rounding off…) they're actually random numbers between, you know, that are floats between 0 and 1. Over here we're then saying… if… let's have a look… so way up here (I'll zoom in a bit) I've got a dropout factor – let's change this, say to 0.5, there we go. So over here this is something that says: if the random number in the equivalent place is greater than 0.5, then 1, otherwise 0. And so here's a whole bunch of 1 and 0. Now this thing here is called a dropout mask. Now what happens is: we multiply (over here), we multiply the dropout mask and we multiply it by our filtered image. And what that means is we end up with exactly the same image we started with… (here's the image we started with…) but it's corrupted. Random bits of it have been deleted, and based on the amount of dropout we use – so if we change it to, say, 0.2 – not very much of it's deleted at all, so it's still very easy to recognize. Or else if we use lots of dropouts, say 0.8, it's almost impossible to see what the number was. And then we use this as the input to the next layer. So, that seems weird, why would we delete some data, at random, from our processed image, from our activations after a layer of the convolutions? Well, the reason is that a human is able to look at this corrupted image and still recognize it's a seven, and the idea is that a computer should be able to as well, and if we randomly delete different bits of the activations, each time, then the computer is forced to learn the underlying real representation rather than overfitting. You can think of this as data augmentation, but it's data augmentation not for the inputs, but data augmentation for the activations. So this is called a Dropout Layer. And so Dropout Layers are really helpful for avoiding overfitting. And you can decide how much you want to compromise between good generalization – so lack of… so good, you know, avoiding overfitting – versus getting something that works really well on the training data. And so, the more dropout you use, the less good it's going to be on the training data, but the better it ought to generalize. And so this comes from a paper by Geoffrey Hinton's group, quite a few years ago now, Ruslan's now at Apple, I think. And then Krizhevsky and Hinton went on to found Google Brain. And so you can see here they've got this picture of a, like, fully connected neural network, two layers, just like the one we built, and here, look, they're kind of randomly deleting some of the activations, and all that's left is these connections. And so, there's a different bunch that's going to be deleted each, each, each batch. I thought this was an interesting point. So Dropout, which is super important, was actually developed in a master's thesis and it was rejected from the main neural networks conference, then called NIPS, now called NeurIPS. So it ended up being disseminated through… arXiv, which is a preprint server and yes, it's just been pointed out on our chat that Ilya was one of the founders of OpenAI. I don't know what happened to Nitish, I think he went to Google Brain as well, maybe. Yeah so, you know, peer review is a very fallible thing, in both directions, and it's great that we have preprint servers so we can read stuff like this, even if reviewers decide it's not worthy, it's been one of the most important papers ever."
fast.ai 2022 - Part 1,8,4467, Activation functions,"Okay, now… I think that's given us a good tour now, we've really seen quite a few ways of dealing with input to a neural network, quite a few of the things that can happen in the middle of a neural network. We've already talked about “Rectified Linear Units”, which is… this one here: 0 if “x” is less than 0 or “x” otherwise. These are some of the other activations you can use. Don't use this one, of course, because you end up with a linear model. But they're all just different functions. I should mention, like, it turns out these don't matter very much. Basically pretty much any non-linearity works fine, so we don't spend much time talking about activation functions, even in Part 2 of the course, just a little bit. So yeah, so we understand there are… there's our inputs, they can be one-hot encoded, or embeddings, which is a compute/computational shortcut. There are sandwich layers of matrix multipliers and activation functions. The matrix multipliers can sometimes be special cases, such as the convolutions or the embeddings. The output can go through some tweaking, such as the Softmax, and then, of course, you've got the loss function such as cross-entropy loss, or mean squared error or mean absolute error. But, you know, it's not… there's nothing too crazy going on in there. So I feel like we've got a good sense now of, like, what goes inside, you know, a wide range of neural nets, you're not going to see anything too weird from here. And we've also seen a wide range of applications. So before you come back to do Part 2, you know, what now? And we're going to have a little AMA session here and, in fact, one of the questions was: what now? So this is quite… quite good One thing I strongly suggest is, if you've got this far, it's probably worth you investing your time in reading Radek’s book, which is “Meta Learning.” And so Meta Learning is very heavily based on the kind of teachings of fast.ai over the last few years and is all about how to learn deep learning and learn pretty much anything. Yeah, because, you know, you've got to this point, you may as well know how to get to the next point as well as possible. And… The main thing you'll see that Radek talks about (or one of the main things) is practicing and writing. So if you've kind of zipped through the videos on, you know, 2x and haven't done any exercises, you know, go back and watch the videos again. You know a lot of the best students end up watching them two or three times – probably more, like three times. And actually go through and code as you watch, you know, and experiment. You know, write posts, blog posts, about what you're doing. Spend time on the forum both helping others and seeing other people's answers to questions. Read the success stories on the forum and of people's projects to get inspiration for things you could try. One of the most important things to do is to get together with other people, for example, you can do, you know, a Zoom study group. In fact on our discord, which you can find through our forum, there's always study groups going on, or you can create your own. You know a study group to go through the book together. Yeah, and of course, you know, build stuff. And sometimes it's tricky to always be able to build stuff for work because maybe there isn't… you're not quite in the right area, or they're not quite ready to try out Deep Learning yet, but that's okay. You know, build some hobby projects, build some stuff just for fun, or build some stuff that you're passionate about. Yes, it's really important to… to not just put the videos away, and go away and do something else because you'll forget everything you've learnt and you won't have practiced. So one of our community members went on to create an activation function, for example, which is Mish, which is now, as Tanishq has just reminded me on our forums, is now used in many of the state-of-the-art networks around the world, which is pretty cool… this is… and, he's now at Mila, I think, a research… one of the top research labs in the world. I wonder how that's doing? Let's have a look… go to Google Scholar… nice, 486 citations. They're doing great. All right, let's have a look at how our AMA topic is going and pick out"
fast.ai 2022 - Part 1,8,4841, Jeremy AMA,"some of the highest ranked AMA’s. Okay. So, the first one is from Lucas and… actually maybe I should… actually let's switch our view here. So, our first AMA is from Lucas. And Lucas asks: “How do you stay motivated? I often find myself overwhelmed in this field, there are so many new things coming up that I feel like I have to put so much energy just to keep my head above the waterline…” Yeah, that's a very interesting question, I mean, I think Lucas… the important thing is to realize: you don't have to know everything, you know. In fact nobody knows everything, and that's okay. What people do is they take an interest in some… some area… and they follow that and they try and do their best, the best job they can of keeping up with some little sub area. And if your little sub area is too much to keep up on, pick a sub sub area. Yeah, there's nothing like… there's no need for it to be demotivating that there's a lot of people doing a lot of interesting work and a lot of different subfields. That's cool, you know. It used to be kind of dull with, you know, there were only basically five labs in the world working on neural nets. And yeah, from time to time, you know, take a dip into other areas that maybe you're not following as closely. But when you're… but when you're just starting out you'll find that things are not changing that fast at all really, it can kind of look that way because people are always putting out press releases about their new tweaks, but fundamentally the stuff that is in the course now is not that different to what was in the course five years ago: the foundations haven't changed. And it's not that different, in fact, to the Convolutional Neural Network that Yann LeCun used on MNIST back in 1996. It's, you know, the basic ideas I've described are forever, you know, the way the inputs work and the sandwiches of matrix multipliers and activation functions and the stuff you do to the final layer, you know. Everything else is tweaks. And the more you learn about those basic ideas, the more you'll recognize those tweaks as simple little tricks that you'll be able to quickly get your head around. So then Lucas goes on to ask… or to comment… “Another thing that constantly bothers me is I feel the field is getting more and more skewed towards bigger and more computationally expensive models and huge amounts of data. I keep wondering if in some years from now I would still be able to train reasonable models with a single GPU or if everything is going to require a compute cluster.” Yeah, that's a great question, I get that a lot. But interestingly, you know, I've been teaching people machine learning and data science stuff for nearly 30 years and I've had a variation of this question throughout. And the reason is that engineers always want to push the envelope in, like, the… on the biggest computers they can find, you know. That's just this like fun thing engineers love to do. And, by definition, they're going to get slightly better results than people doing exactly the same thing on smaller computers. So it always looks like: “oh you need big computers to be state-of-the-art.” But that's actually never true, right, because there's always smarter ways to do things, not just bigger ways to do things. And so, you know, when you look at fastai's DAWNBench success when we trained ImageNet faster than anybody had trained it before, on standard GPU’s, you know, me and a bunch of students, that was not meant to happen, you know, Google was working very hard with their TPU introduction to try to show how good they were, Intel was using like 256 PC’s in parallel, or something… but yeah, you know, we used common sense and smarts and showed what can be done. You know, it's also a case of picking the problems you solve. So I would not be probably doing like, going head-to-head up against Codex and trying to create code from english descriptions, you know, because that's a problem that does probably require very large neural nets and very large amounts of data. But if you pick areas in different domains, you know, there's still huge areas where much smaller models are still going to be state-of-the-art. So hopefully that helped answer your question. All right, let's see what else we got here. So Daniel has alway been following my journey with teaching my daughter math, yeah he's so: “I homeschool my daughter.” And Daniel asks: “how do you homeschool young children science in general and math in particular? Would you share your experiences by blogging or in lectures someday.” Yeah, I could do that. So I actually spent quite a few months just reading research papers about education recently. So I do probably have a lot I probably need to talk about at some stage. But yeah, broadly speaking, I lean into using computers and tablets, a lot more than most people, because actually there's an awful lot of really great apps that are super compelling – they're adaptive, so they go at the right speed for the student, and they're fun, and I really like my daughter to have fun, you know, I really don't like to force her to do things. And for example there's a really cool app called DragonBox Algebra 5+ which teaches algebra to five-year-olds, by using a really fun computer game involving helping dragon eggs to hatch. And it turns out that yeah, algebra, the basic ideas of algebra, are no more complex than the basic ideas that we do in other kindergarten math and all the parents I know of who have given their kids DragonBox Algebra 5+, their kids have successfully learned algebra. So that would be an example, but yeah, we should talk about this more at some point. All right, let's see what else we've got here. So Farah says: “The walk-thrus have been a game changer for me. The knowledge and tips you shared in those sessions are skills required to become an effective machine learning practitioner and utilize fastai more effectively. Have you considered making the walk-thrus a more formal part of the course, doing a separate software engineering course or continuing live-coding sessions between Part 1 & 2?” So yes, I am going to keep doing live-coding sessions. At the moment we've switched to those, specifically to focusing on APL, and then in a couple of weeks they're going to be going to fast.ai study groups, and then after that they'll gradually turn back into more live-coding sessions. But yeah, the thing I try to do in my live-coding, or study groups, whatever, is yeah, definitely try to show the foundational techniques that just make life easier as a coder or a data scientist. When I say foundational I mean, yeah this, the stuff which you can reuse again and again and again, like learning regular expressions really well, or knowing how to use VIM, or understanding how to use the terminal and command line, you know, all that kind of stuff. Never goes out of style, it never gets old. And yeah, I do plan to, at some point, hopefully, actually do a course really all about that stuff specifically, but yeah, for now the, for now the best approach is follow along with the live-coding and stuff. Okay wgpubs, which is Wade, asks: “How do you turn a model into a business? Specifically, how does a coder with little or no startup experience turn an ML based gradio prototype into a legitimate business venture?”. Okay, I plan to do a course about this, at some point, as well. So, you know, obviously there isn't a two-minute version to this, but the key thing with creating a legitimate business venture is to solve a legitimate problem, you know, a problem that people need solved/solving. And which they will pay you to solve. And so, it's important not to start with your fun gradio prototype as the basis your business, but instead, start with: here's a problem I want to solve. And generally speaking you should try to pick a problem that you understand better than most people. So it's either a problem that you face day to day in your work, or in some hobby or passion that you have, or that, you know, your club has or your local school has, or your… your spouse deals with in their workplace, you know. It's something where you understand that there's a… something that doesn't work as well as it ought to. Particularly something where you think to yourself: “you know, if they just used Deep Learning here, or some algorithm here, or some better compute here, that problem would go away.” And that's, that's the start of a business. And so then, my friend Eric Ries wrote a book called “The Lean Startup” where he describes what you do next, which is basically, you fake it. You create, so he calls it, the “minimum viable product”. You create something that solves that problem, it takes you as little time as possible to create. It could be very manual, it can be loss making, it's fine. You know, even the bit in the middle where you're like: “oh there's going to be a neural net here…” it's fine to, like, launch without the neural net and do everything by hand. You're just trying to find out: “are people going to pay for this?” and is this actually useful? And then once you have, you know, hopefully confirmed that the need is real, and that people will pay for it, and you can solve the need, you can gradually make it less and less of a fake, you know, and to you know more and more getting the product to where you want it to be. Okay, I don't know how to pronounce the name m-i-w-o-j-c, m-i-w-o-j-c says: “Jeremy, can you share some of your productivity hacks? From the content you produce that may seem you work 24 hours a day?” Okay, I certainly don't do that. I think one of my main productivity hacks actually is not to work too hard. Or at least… no, not to work too hard… not to work too much. I spend, probably, less hours a day working than most people, I would guess, but I think I do a couple of things differently when I'm working. One is I've spent half, at least half, of every working day, since I was about 18, learning or practicing something new. Could be a new language, it could be a new algorithm, it could be something I read about. And, nearly all of that time, therefore, I've been doing that thing more slowly than I would if I just used something I already knew – which often drives my co-workers crazy because they're like, you know: “why aren't you focusing on getting that thing done”. But in the other 50% of the time I'm constantly, you know, building up this kind of exponentially improving base of expertise, in a wide range of areas. And so now I do find, you know, I can do things, often, orders of magnitude faster than people around me or certainly many multiples faster than people around me because I, you know, know a whole bunch of tools and skills and ideas which (yeah/no) other people don't necessarily know. So like I think that's one thing that's been helpful. And then another is, yeah, like trying to really not overdo things, like get good sleep, and eat well, and exercise well. And also I think it's a case of like, tenacity, you know, I've noticed a lot of people give up much earlier than I do. So yeah, if you, if you just keep going until something's actually finished then that's going to put you in a small minority, to be honest, most people don't do that. When I say finish, like finish something really nicely. And I try to make it like… so I particularly like coding and so I try to do a lot of coding related stuff. So I create things like nbdev, and nbdev makes it much much easier for me to finish something nicely, you know. So, in my kind of chosen area I've spent quite a bit of time trying to make sure it's really easy for me to like, get out a blog post, get out a Python library, get out a notebook analysis, whatever. So yeah, trying to make these things I want to do easier and so then I'll do them more. So, well, thank you everybody. That's been a lot of fun. Really appreciate you taking the time to go through this course with me. Yeah, if you enjoyed it, it would really help if you would give a like on youtube, because it really helps other people find the course… goes into the youtube recommendation system. And please do come and help other beginners on forums.fast.ai. It's a great way to learn yourself… is to try to teach other people. And yeah, I hope you'll join us in Part 2. Thanks very much… thanks everybody, very much. I really enjoyed this process and I hope to…, I get to meet more of you in person in the future. Bye."
