{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from course_copilot.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fsdl_2022_course_project\n",
    "\n",
    "> Course Co-pilot helps course creators create lesson summaries and chapter markers using ML.\n",
    "\n",
    "\"- order: 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project is to create an augmented ML approach course creators can use to streamline the generation of lecture summaries and chapter markers based on lesson videos.\n",
    "\n",
    "::: {.callout-tip appearance=\\\"simple\\\"}\n",
    "\n",
    "Do checkout out [video demo showcasing Course Co-pilot](https://www.loom.com/share/ca26fff6b911478c879da679495e8f67).\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Functionalities of Course Co-pilot](https://user-images.githubusercontent.com/24592806/195798582-4ef1cd04-24d9-427e-bb0f-3fbb41e1a0e3.png)\n",
    "\n",
    "The basic workflow is:\n",
    "\n",
    "1. User opens a link to a YouTube video lecture in our application and asks Course Co-Pilot to process it\n",
    "2. User can view status of requests via the “Get Predictions” button.\n",
    "3. User can view predicted topic boundaries, headlines, & content summaries for processed videos.\n",
    "4. User can correct and save generated content (planned later to use in data flywheel)\n",
    "5. User will be able to export results as chapter markers to use in YouTube(planned later)\n",
    "6. User will be able to export results in a quarto friendly format for posting to a web page or blog.(planned later)\n",
    "\n",
    "\n",
    "## Why\n",
    "\n",
    "\n",
    "In our own experience, we have noticed that such content either doesn’t get done, is time consuming, and/or requires work from outside parties. In particular, we noted in the below courses we’ve been a part of:\n",
    "\n",
    "1. Fast.ai course - During the course students manually create [youtube chapter markers](https://forums.fast.ai/t/help-wanted-youtube-chapter-markers/96306), [lesson transcripts](https://forums.fast.ai/t/help-wanted-transcriptions/96307), and summaries on the forums.\n",
    "\n",
    "2. FSDL course - The chapter markers and [lesson notes](https://fullstackdeeplearning.com/course/2022/lecture-2-development-infrastructure-and-tooling/) are later created manually and then shared on the FSDL website usually 1 week after the each lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How our application is structured?\n",
    "\n",
    "![System Diagram](https://user-images.githubusercontent.com/24592806/195852655-3e22b972-d09f-4646-8f75-70a04bda2081.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we done so far?\n",
    "\n",
    "Let’s look at the dataset, ML library, API, and web application we created for our prototype system\n",
    "\n",
    "### Dataset \n",
    "\n",
    "Since we had to train summarization models and topic segmentation models, we manually created our dataset from a bunch of youtube videos ranging from videos from fastai lessons, FSDL lesson to random videos teaching something.\n",
    "\n",
    "[Dataset Link](https://huggingface.co/datasets/kurianbenoy/Course_summaries_dataset)\n",
    "\n",
    "![Dataset Schema](https://user-images.githubusercontent.com/24592806/195852870-b9f2acb2-99a7-44a4-92f1-5f465cd1a45b.png)\n",
    "\n",
    "### ML library: course_copilot\n",
    "\n",
    "We leveraged nbdev framework to create a python package which acted as our framework for Model training and model serving. We integrated Wandb for experiment tracking and fine tuning models with sweeps. We created Model trainers for task of topic segmentation and summarization.\n",
    "The timing of our project coincided with release whisper which we used for creating transcription of youtube video URL you are passing. This helps to provide the required data for creating topic segments and summaries.\n",
    "\n",
    "[fsdl_2022_course_project](https://github.com/ohmeow/fsdl_2022_course_project)\n",
    "\n",
    "![nbdev based Model Trainer for Topic Segmentation, Experiment tracking with W&B](https://user-images.githubusercontent.com/24592806/195852790-fef77960-1066-414d-8568-a177f4d1159e.png)\n",
    "\n",
    "### Backend API\n",
    "\n",
    "For the backend, we used FastAPI for creating APIs. Our API is leveraging dagster as the workflow engine to create tasks for running inference jobs from creating transcripts of video with whisper, running topic segmentation and running the summarization models.\n",
    "\n",
    "[fsdl-2022-group-007-app](https://github.com/suvash/fsdl-2022-group-007-app)\n",
    "\n",
    "![Course Copilot APIs](https://user-images.githubusercontent.com/24592806/195852828-52217828-cab7-4b7f-b6e6-227a5cb27abe.png)\n",
    "\n",
    "### Web Application\n",
    "\n",
    "We created our front-end web application using Vue3 and Quasar. It is deployed to github pages from our repo.\n",
    "\n",
    "[fsdl-2022-group-007-web](https://github.com/ohmeow/fsdl-2022-group-007-web)\n",
    "\n",
    "\n",
    "![Topic summaries and chapter summaries generated](https://user-images.githubusercontent.com/24592806/195858845-4ba257ea-935f-4e57-8650-732e493aa7b3.png)\n",
    "\n",
    "\n",
    "\n",
    "## Future Plans\n",
    "\n",
    "- Improve quality of training data\n",
    "- Allow users to save their corrected headlines and summaries\n",
    "- Add ability for users to update topic spans\n",
    "- Implement data flywheel\n",
    "- Implement chapter marker and quarto export features\n",
    "- Add authentication/authorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install course_copilot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your development environment\n",
    "\n",
    "Please take some time reading up on nbdev ... how it works, [directives](https://nbdev.fast.ai/explanations/directives.html), etc... by checking out [the walk-thrus](https://nbdev.fast.ai/tutorials/tutorial.html) and [tutorials](https://nbdev.fast.ai/tutorials/) on the nbdev [website](https://nbdev.fast.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create conda environment\n",
    "\n",
    "After cloning the repo, create a conda environment. This will install nbdev alongside other libraries likely required for this project.\n",
    "\n",
    "`mamba env create -f environment.yml`\n",
    "\n",
    "\n",
    "## Step 2: Install Quarto:\n",
    "\n",
    "`nbdev_install_quarto`\n",
    "\n",
    "\n",
    "## Step 3: Install hooks \n",
    "\n",
    "`nbdev_install_hooks`\n",
    "\n",
    "\n",
    "## Step 4: Add pre-commit hooks (optional)\n",
    "If using VSCode, you can install pre-commit hooks “to catch and fix uncleaned and unexported notebooks” before pushing to get.  See the instructions in the nbdev documentation if you want to use this feature. https://nbdev.fast.ai/tutorials/pre_commit.html\n",
    "\n",
    "\n",
    "## Step 5: Install our library\n",
    "\n",
    "`pip install -e '.[dev]'`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
