{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp tasks\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tasks\n",
    "\n",
    "Code for running experiments and hyper parameters optimization from the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse, json, os, warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from transformers import logging as hf_logging\n",
    "import wandb\n",
    "\n",
    "from course_copilot import utils, training, topic_segmentation, summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import pdb\n",
    "\n",
    "from fastcore.test import *\n",
    "import nbdev\n",
    "\n",
    "from blurr.utils import print_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.12.1+cu102\n",
      "fastai: 2.7.9\n",
      "transformers: 4.22.0\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def build_train_config(\n",
    "    # Our task's `TrainConfig`\n",
    "    train_config_cls: type,\n",
    "    # The arguments (name and values) we want to update our `TrainConfig` with\n",
    "    args,\n",
    ") -> training.TrainConfig:\n",
    "    train_config = train_config_cls()\n",
    "\n",
    "    for arg in vars(args):\n",
    "        if hasattr(train_config, arg):\n",
    "            setattr(train_config, arg, getattr(args, arg))\n",
    "\n",
    "    return train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def run_experiment(\n",
    "    # The ML task to run (e.g., topic_segmentation, summarization)\n",
    "    task: str,\n",
    "    # The name of your experiment (e.g., deberta_v3_large). This value is used in conjunction with `task` when\n",
    "    # logging information with W&B or else saving data releveant to training/evaluation runs\n",
    "    experiment_name: str,\n",
    "    # Where the project's data is stored\n",
    "    data_path: str = \"data\",\n",
    "    # Where exported Learners and other models should stored\n",
    "    model_output_path: str = \"models\",\n",
    "    # Where any logged data should be stored\n",
    "    log_output_path: str = \"logs\",\n",
    "    # Whether predictions should be logged\n",
    "    log_preds: bool = False,\n",
    "    # The number of predictions that should be logged. It is left to each subclass to define what that means\n",
    "    log_n_preds: int = None,\n",
    "    # Whether or not to log experiments and sweeps to W&B\n",
    "    use_wandb: bool = False,\n",
    "    # Whether or not you want to have printed out everything during a training/evaulation run\n",
    "    verbose: bool = False,\n",
    "    # Any args/values we want to use to update our `TrainConfig` with\n",
    "    args=None,\n",
    "):\n",
    "    if task == \"topic_segmentation\":\n",
    "        train_config = build_train_config(topic_segmentation.TopicSegmentationConfig, args)\n",
    "\n",
    "        trainer = topic_segmentation.TopicSegmentationModelTrainer(\n",
    "            experiment_name=experiment_name,\n",
    "            train_config=train_config,\n",
    "            data_path=data_path,\n",
    "            model_output_path=model_output_path,\n",
    "            log_output_path=log_output_path,\n",
    "            log_preds=log_preds,\n",
    "            log_n_preds=log_n_preds,\n",
    "            use_wandb=use_wandb,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    elif task == \"headline_summarization\":\n",
    "        train_config = build_train_config(summarization.SummarizationConfig, args)\n",
    "\n",
    "        trainer = summarization.SummarizationModelTrainer(\n",
    "            experiment_name=experiment_name,\n",
    "            train_config=train_config,\n",
    "            data_path=data_path,\n",
    "            model_output_path=model_output_path,\n",
    "            log_output_path=log_output_path,\n",
    "            log_preds=log_preds,\n",
    "            log_n_preds=log_n_preds,\n",
    "            use_wandb=use_wandb,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    elif task == \"content_summarization\":\n",
    "        train_config = build_train_config(summarization.SummarizationConfig, args)\n",
    "\n",
    "        trainer = summarization.SummarizationModelTrainer(\n",
    "            experiment_name=experiment_name,\n",
    "            train_config=train_config,\n",
    "            data_path=data_path,\n",
    "            model_output_path=model_output_path,\n",
    "            log_output_path=log_output_path,\n",
    "            log_preds=log_preds,\n",
    "            log_n_preds=log_n_preds,\n",
    "            use_wandb=use_wandb,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    # run training\n",
    "    results_df, raw_df, train_df, train_val_idxs = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def prepare_tuning(\n",
    "    # The ML task to run (e.g., topic_segmentation, summarization)\n",
    "    task: str,\n",
    "    # The name of your experiment (e.g., deberta_v3_large). This value is used in conjunction with `task` when\n",
    "    # logging information with W&B or else saving data releveant to training/evaluation runs\n",
    "    experiment_name: str,\n",
    "    # Where the project's data is stored\n",
    "    data_path: str,\n",
    "    # Where exported Learners and other models should stored\n",
    "    model_output_path: str,\n",
    "    # Where any logged data should be stored\n",
    "    log_output_path: str,\n",
    "    # Whether predictions should be logged\n",
    "    log_preds: bool,\n",
    "    # The number of predictions that should be logged. It is left to each subclass to define what that means\n",
    "    log_n_preds: int,\n",
    "    # Whether or not to log experiments and sweeps to W&B\n",
    "    use_wandb: bool,\n",
    "    # Whether or not you want to have printed out everything during a training/evaulation run\n",
    "    verbose: bool,\n",
    "    # Any args/values we want to use to update our `TrainConfig` with\n",
    "    args=None,\n",
    "):\n",
    "    if task == \"topic_segmentation\":\n",
    "        train_config = build_train_config(topic_segmentation.TopicSegmentationConfig, args)\n",
    "\n",
    "        trainer = topic_segmentation.TopicSegmentationModelTrainer(\n",
    "            experiment_name=experiment_name,\n",
    "            train_config=train_config,\n",
    "            data_path=data_path,\n",
    "            model_output_path=model_output_path,\n",
    "            log_output_path=log_output_path,\n",
    "            log_preds=log_preds,\n",
    "            log_n_preds=log_n_preds,\n",
    "            use_wandb=use_wandb,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    elif task == \"headline_summarization\":\n",
    "        pass\n",
    "    elif task == \"content_summarization\":\n",
    "        pass\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_required_args(parser):\n",
    "    # define other `ModelTrainer` args\n",
    "    parser.add_argument(\"--task\", type=str, default=\"train\", help=\"Options: train | tune\")\n",
    "    parser.add_argument(\"--experiment\", type=str, default=\"test_experiment\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./data\")\n",
    "    parser.add_argument(\"--models_path\", type=str, default=\"./models\")\n",
    "    parser.add_argument(\"--logs_path\", type=str, default=\"./logs\")\n",
    "    parser.add_argument(\"--log_preds\", type=bool, default=False)\n",
    "    parser.add_argument(\"--log_n_preds\", type=int, default=10)\n",
    "    parser.add_argument(\"--use_wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--sweep_config_fpath\", type=str, default=None)\n",
    "    parser.add_argument(\"--sweep_trials\", type=int, default=20)\n",
    "    parser.add_argument(\"--verbose\", type=bool, default=False)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    train_config_attrs = [(k, type(v), v) for k, v in vars(training.TrainConfig).items() if not k.startswith(\"_\")]\n",
    "\n",
    "    headline_summarization_train_config_attrs = train_config_attrs\n",
    "    content_summarization_train_config_attrs = train_config_attrs\n",
    "\n",
    "    # instantiate argparser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    subparsers = parser.add_subparsers(dest=\"subcommand\", help=\"Sub-commands help\")\n",
    "\n",
    "    # instantiate argparser for topic segmentation\n",
    "    parser_topic_segmentation = subparsers.add_parser(\"topic_segmentation\", help=\"Topic segmentation\")\n",
    "    add_required_args(parser_topic_segmentation)\n",
    "    train_config_attrs += [\n",
    "        (k, type(v), v) for k, v in vars(topic_segmentation.TopicSegmentationConfig).items() if not k.startswith(\"_\")\n",
    "    ]\n",
    "    for attr in train_config_attrs:\n",
    "        parser_topic_segmentation.add_argument(f\"--{attr[0]}\", type=attr[1], default=attr[2])\n",
    "\n",
    "    # # instantiate argparser for headline summarization\n",
    "    parser_headline_summarization = subparsers.add_parser(\"headline_summarization\", help=\"Headling summarization\")\n",
    "    add_required_args(parser_headline_summarization)\n",
    "    headline_summarization_train_config_attrs += [\n",
    "        (k, type(v), v) for k, v in vars(summarization.HeadlineSummarizationConfig).items() if not k.startswith(\"_\")\n",
    "    ]\n",
    "\n",
    "    for attr in headline_summarization_train_config_attrs:\n",
    "        parser_headline_summarization.add_argument(f\"--{attr[0]}\", type=attr[1], default=attr[2])\n",
    "\n",
    "    #     # # instantiate argparser for content summarization\n",
    "    #     parser_content_summarization = subparsers.add_parser(\"content_summarization\", help=\"Content summarization\")\n",
    "    #     add_required_args(parser_content_summarization)\n",
    "    #     content_summarization_train_config_attrs += [(k, type(v), v) for k, v in vars(summarization.ContentSummarizationConfig).items() if not k.startswith(\"_\")]\n",
    "    #     for attr in content_summarization_train_config_attrs:\n",
    "    #         parser_content_summarization.add_argument(f\"--{attr[0]}\", type=attr[1], default=attr[2])\n",
    "\n",
    "    # get the arg values\n",
    "    args = parser.parse_args()\n",
    "    task = args.subcommand\n",
    "\n",
    "    # run the specific task task\n",
    "    if task == \"train\":\n",
    "        run_experiment(\n",
    "            task,\n",
    "            experiment_name=args.experiment,\n",
    "            data_path=args.data_path,\n",
    "            model_output_path=args.models_path,\n",
    "            log_output_path=args.logs_path,\n",
    "            log_preds=args.log_preds,\n",
    "            log_n_preds=args.log_n_preds,\n",
    "            use_wandb=args.use_wandb,\n",
    "            verbose=args.verbose,\n",
    "            args=args,\n",
    "        )\n",
    "    elif task == \"tune\":\n",
    "        trainer = prepare_tuning(\n",
    "            task,\n",
    "            experiment_name=args.experiment,\n",
    "            data_path=args.data_path,\n",
    "            model_output_path=args.models_path,\n",
    "            log_output_path=args.logs_path,\n",
    "            log_preds=args.log_preds,\n",
    "            log_n_preds=args.log_n_preds,\n",
    "            use_wandb=args.use_wandb,\n",
    "            verbose=args.verbose,\n",
    "            args=args,\n",
    "        )\n",
    "\n",
    "        sweep_config_fpath = (\n",
    "            f\"./sweep_configs/{task}/default.json\" if args.sweep_config_fpath is None else args.sweep_config_fpath\n",
    "        )\n",
    "        with open(sweep_config_fpath, \"r\") as f:\n",
    "            sweep_config_d = json.load(f)\n",
    "\n",
    "        # https://docs.wandb.ai/guides/sweeps/faq#how-can-i-change-the-directory-my-sweep-logs-to-locally\n",
    "        os.environ[\"WANDB_DIR\"] = args.logs_path\n",
    "\n",
    "        # begin the sweep\n",
    "        sweep_id = trainer.configure_sweep(sweep_config=sweep_config_d)\n",
    "        wandb.agent(sweep_id, function=partial(trainer.train, sweep_config=sweep_config_d), count=args.sweep_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fsdl_2022_course_project')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
