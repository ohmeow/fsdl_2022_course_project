{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp tasks\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tasks\n",
    "\n",
    "Code for running experiments and hyper parameters optimization from the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse, os, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "from course_copilot import utils, training, topic_segmentation, summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import pdb\n",
    "\n",
    "from fastcore.test import *\n",
    "import nbdev\n",
    "\n",
    "from blurr.utils import print_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.12.1+cu102\n",
      "fastai: 2.7.9\n",
      "transformers: 4.22.0\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def build_train_config(train_config_cls, args):\n",
    "    train_config = train_config_cls()\n",
    "\n",
    "    for arg in vars(args):\n",
    "        if hasattr(train_config, arg):\n",
    "            setattr(train_config, arg, getattr(args, arg))\n",
    "\n",
    "    return train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def run_experiment(\n",
    "    experiment_name: str,\n",
    "    data_path: str,\n",
    "    model_output_path: str,\n",
    "    log_output_path: str,\n",
    "    log_preds: bool,\n",
    "    log_n_preds: int,\n",
    "    use_wandb: bool,\n",
    "    verbose: bool,\n",
    "    args=None,\n",
    "):\n",
    "    if experiment_name == \"topic_segmentation\":\n",
    "        train_config = build_train_config(topic_segmentation.TopicSegmentationConfig, args)\n",
    "\n",
    "        trainer = topic_segmentation.TopicSegmentationModelTrainer(\n",
    "            experiment_name=experiment_name,\n",
    "            train_config=train_config,\n",
    "            data_path=data_path,\n",
    "            model_output_path=model_output_path,\n",
    "            log_output_path=log_output_path,\n",
    "            log_preds=log_preds,\n",
    "            log_n_preds=log_n_preds,\n",
    "            use_wandb=use_wandb,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    elif experiment_name == \"headline_summarization\":\n",
    "        pass\n",
    "    elif experiment_name == \"content_summarization\":\n",
    "        pass\n",
    "\n",
    "    # run training\n",
    "    results_df, raw_df, train_df, train_val_idxs = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def run_tuning():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_required_args(parser):\n",
    "    # define other `ModelTrainer` args\n",
    "    parser.add_argument(\"--task\", type=str, default=\"train\", help=\"Options: train | tune\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"data\")\n",
    "    parser.add_argument(\"--models_path\", type=str, default=\"models\")\n",
    "    parser.add_argument(\"--logs_path\", type=str, default=\"logs\")\n",
    "    parser.add_argument(\"--log_preds\", type=bool, default=False)\n",
    "    parser.add_argument(\"--log_n_preds\", type=int, default=10)\n",
    "    parser.add_argument(\"--use_wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--verbose\", type=bool, default=False)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    train_config_attrs = [(k, type(v), v) for k, v in vars(training.TrainConfig).items() if not k.startswith(\"_\")]\n",
    "\n",
    "    # instantiate argparser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    subparsers = parser.add_subparsers(dest=\"subcommand\", help=\"Sub-commands help\")\n",
    "\n",
    "    # instantiate argparser for topic segmentation\n",
    "    parser_topic_segmentation = subparsers.add_parser(\"topic_segmentation\", help=\"Topic segmentation\")\n",
    "    add_required_args(parser_topic_segmentation)\n",
    "    train_config_attrs += [\n",
    "        (k, type(v), v) for k, v in vars(topic_segmentation.TopicSegmentationConfig).items() if not k.startswith(\"_\")\n",
    "    ]\n",
    "    for attr in train_config_attrs:\n",
    "        parser_topic_segmentation.add_argument(f\"--{attr[0]}\", type=attr[1], default=attr[2])\n",
    "\n",
    "    # # instantiate argparser for headline summarization\n",
    "    # parser_headline_summarization = subparsers.add_parser(\"headline_summarization\", help=\"Headling summarization\")\n",
    "    # add_required_args(parser_headline_summarization)\n",
    "    # train_config_attrs += [(k, type(v), v) for k, v in vars(summarization.HeadlineSummarizationConfig).items() if not k.startswith(\"_\")]\n",
    "    # for attr in train_config_attrs:\n",
    "    #     parser_headline_summarization.add_argument(f\"--{attr[0]}\", type=attr[1], default=attr[2])\n",
    "\n",
    "    # # instantiate argparser for headline summarization\n",
    "    # parser_content_summarization = subparsers.add_parser(\"content_summarization\", help=\"Content summarization\")\n",
    "    # add_required_args(parser_content_summarization)\n",
    "    # train_config_attrs += [(k, type(v), v) for k, v in vars(summarization.ContentSummarizationConfig).items() if not k.startswith(\"_\")]\n",
    "    # for attr in train_config_attrs:\n",
    "    #     parser_content_summarization.add_argument(f\"--{attr[0]}\", type=attr[1], default=attr[2])\n",
    "\n",
    "    # get the arg values\n",
    "    args = parser.parse_args()\n",
    "    experiment = args.subcommand\n",
    "\n",
    "    # run the specific task task\n",
    "    if args.task == \"train\":\n",
    "        run_experiment(\n",
    "            experiment_name=experiment,\n",
    "            data_path=args.data_path,\n",
    "            model_output_path=args.models_path,\n",
    "            log_output_path=args.logs_path,\n",
    "            log_preds=args.log_preds,\n",
    "            log_n_preds=args.log_n_preds,\n",
    "            use_wandb=args.use_wandb,\n",
    "            verbose=args.verbose,\n",
    "            args=args,\n",
    "        )\n",
    "\n",
    "    elif args.task == \"tune\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fsdl_2022_course_project')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
