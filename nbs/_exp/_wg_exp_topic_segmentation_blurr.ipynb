{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, os, gc, pdb, random, time\n",
    "\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, IndexSplitter, RegressionBlock\n",
    "from fastai.imports import *\n",
    "from fastai.layers import SigmoidRange\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat, MSELossFlat, LabelSmoothingCrossEntropyFlat\n",
    "from fastai.optimizer import Adam\n",
    "from fastai.metrics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastcore.transform import Transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForNextSentencePrediction,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DebertaV2Model,\n",
    "    logging,\n",
    ")\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout\n",
    "\n",
    "from blurr.callbacks import GradientCheckpointing\n",
    "from blurr.text.data.core import TextBlock, BatchTokenizeTransform\n",
    "from blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\n",
    "from blurr.text.utils import get_hf_objects\n",
    "from blurr.utils import PreCalculatedCrossEntropyLoss, PreCalculatedMSELoss, set_seed\n",
    "\n",
    "# silence all the HF warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "gpu_num = 0\n",
    "\n",
    "torch.cuda.set_device(gpu_num)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2022\n",
    "val_pct = 0.25\n",
    "bsz = 8\n",
    "data_subset_pct = 1.0\n",
    "\n",
    "# HF objects\n",
    "model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "# model_cls = AutoModelForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25383\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>course_title</th>\n",
       "      <th>lesson_num</th>\n",
       "      <th>topic</th>\n",
       "      <th>seq</th>\n",
       "      <th>next_seq</th>\n",
       "      <th>is_topic_end</th>\n",
       "      <th>next_topic_begin_seq</th>\n",
       "      <th>other_topic_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C-Squared Podcast</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro</td>\n",
       "      <td>[Music] welcome everybody to episode one of a</td>\n",
       "      <td>chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[christian well not so much fabi uh it's first of all great um to finally start a, podcast the chess podcast i know that um there's a lot of podcasts out there but, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C-Squared Podcast</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro</td>\n",
       "      <td>chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up</td>\n",
       "      <td>christian well not so much fabi uh it's first of all great um to finally start a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Music] welcome everybody to episode one of a, podcast the chess podcast i know that um there's a lot of podcasts out there but, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location because, those ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C-Squared Podcast</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro</td>\n",
       "      <td>christian well not so much fabi uh it's first of all great um to finally start a</td>\n",
       "      <td>podcast the chess podcast i know that um there's a lot of podcasts out there but</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C-Squared Podcast</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro</td>\n",
       "      <td>podcast the chess podcast i know that um there's a lot of podcasts out there but</td>\n",
       "      <td>i wanted to bring our own tune to the mix and i think uh yeah i'm</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, christian well not so much fabi uh it's first of all great um to finally start a, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C-Squared Podcast</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro</td>\n",
       "      <td>i wanted to bring our own tune to the mix and i think uh yeah i'm</td>\n",
       "      <td>excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, christian well not so much fabi uh it's first of all great um to finally start a, podcast the chess podcast i know that um there's a lot of podcasts out there but, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location because, those uh cra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       course_title lesson_num  topic  \\\n",
       "0      0  C-Squared Podcast          1  Intro   \n",
       "1      1  C-Squared Podcast          1  Intro   \n",
       "2      2  C-Squared Podcast          1  Intro   \n",
       "3      3  C-Squared Podcast          1  Intro   \n",
       "4      4  C-Squared Podcast          1  Intro   \n",
       "\n",
       "                                                                                           seq  \\\n",
       "0                                                [Music] welcome everybody to episode one of a   \n",
       "1  chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up   \n",
       "2             christian well not so much fabi uh it's first of all great um to finally start a   \n",
       "3             podcast the chess podcast i know that um there's a lot of podcasts out there but   \n",
       "4                            i wanted to bring our own tune to the mix and i think uh yeah i'm   \n",
       "\n",
       "                                                                                                           next_seq  \\\n",
       "0                       chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up   \n",
       "1                                  christian well not so much fabi uh it's first of all great um to finally start a   \n",
       "2                                  podcast the chess podcast i know that um there's a lot of podcasts out there but   \n",
       "3                                                 i wanted to bring our own tune to the mix and i think uh yeah i'm   \n",
       "4  excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's   \n",
       "\n",
       "   is_topic_end next_topic_begin_seq  \\\n",
       "0         False                  NaN   \n",
       "1         False                  NaN   \n",
       "2         False                  NaN   \n",
       "3         False                  NaN   \n",
       "4         False                  NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          other_topic_seqs  \n",
       "0  [christian well not so much fabi uh it's first of all great um to finally start a, podcast the chess podcast i know that um there's a lot of podcasts out there but, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to m...  \n",
       "1  [[Music] welcome everybody to episode one of a, podcast the chess podcast i know that um there's a lot of podcasts out there but, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location because, those ...  \n",
       "2  [[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location beca...  \n",
       "3  [[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, christian well not so much fabi uh it's first of all great um to finally start a, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention th...  \n",
       "4  [[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, christian well not so much fabi uh it's first of all great um to finally start a, podcast the chess podcast i know that um there's a lot of podcasts out there but, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location because, those uh cra...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_df = pd.read_csv(\"../../data/clean/segmentation_train.csv\", index_col=None)\n",
    "raw_train_df[\"other_topic_seqs\"] = raw_train_df[\"other_topic_seqs\"].apply(ast.literal_eval)\n",
    "raw_train_df.reset_index(inplace=True)\n",
    "\n",
    "print(len(raw_train_df))\n",
    "raw_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"one starting point is you can what's called canary the model which means that instead of instead of\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_df[\"seq\"].sample(n=1).values[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Eval split\n",
    "\n",
    "For training we need to remove sequences for which there is not a \"next_seq\" (e.g., we are at end of a topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24786\n"
     ]
    }
   ],
   "source": [
    "train_df = raw_train_df[raw_train_df[\"is_topic_end\"] == False].copy()\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set should ideally ***not*** include courses in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we were just concerned with creating a validation set and nothing else\n",
    "# _, val_idxs = train_test_split(range(len(train_df)), test_size=val_pct, random_state=random_seed)\n",
    "\n",
    "\n",
    "# shuffle dataset - optional subset for faster iteration\n",
    "train_df = train_df.sample(frac=data_subset_pct, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "courses = train_df[\"course_title\"].unique()\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(courses)\n",
    "\n",
    "val_sz = int(len(courses) * val_pct)\n",
    "val_courses = courses[:val_sz]\n",
    "\n",
    "is_val = np.isin(train_df[\"course_title\"], val_courses)\n",
    "\n",
    "idxs = np.arange(len(train_df))\n",
    "val_idxs = idxs[is_val]\n",
    "trn_idxs = idxs[~is_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_seed:\n",
    "    set_seed(random_seed)\n",
    "\n",
    "    # need to create configuration object separately because we may be adding new attributes (e.g., cls_dropout)\n",
    "    hf_config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    hf_config.update({})\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        model_checkpoint,\n",
    "        model_cls=model_cls,\n",
    "        config=hf_config,\n",
    "        tokenizer_kwargs={},\n",
    "        model_kwargs={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBatchTokenizeTransform(BatchTokenizeTransform):\n",
    "    def __init__(self, use_next_pos_prob=0.75, use_adjacent_neg_prob=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_next_pos_prob = use_next_pos_prob\n",
    "        self.use_adjacent_neg_prob = use_adjacent_neg_prob\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        # our positive example\n",
    "        pos_ex_idx = 0 if random.uniform(0, 1) < self.use_next_pos_prob else 1\n",
    "        updated_samples1, inputs1 = super().encodes(\n",
    "            [(s[0][pos_ex_idx] if s[0][pos_ex_idx] != \"xxNONExx\" else s[0][0], *s[2:], *s[2:]) for s in samples],\n",
    "            return_batch_encoding=True,\n",
    "        )\n",
    "\n",
    "        # our negative example (sometimes the adjacent will be \"\"; if that is the case use the other topic negative example which is at idx=1)\n",
    "        neg_ex_idx = 0 if random.uniform(0, 1) < self.use_adjacent_neg_prob and pos_ex_idx == 0 else 1\n",
    "        updated_samples2, inputs2 = super().encodes(\n",
    "            [(s[1][neg_ex_idx] if s[1][neg_ex_idx] != \"xxNONExx\" else s[1][1], *s[2:]) for s in samples],\n",
    "            return_batch_encoding=True,\n",
    "        )\n",
    "\n",
    "        # if there are no targets (e.g., when used for inference)\n",
    "        if len(samples[0]) == 2:\n",
    "            return [(inps1[0], inps2[0]) for inps1, inps2 in zip(updated_samples1, updated_samples2)]\n",
    "\n",
    "        return [(inps1[0], inps2[0], inps1[-1]) for inps1, inps2 in zip(updated_samples1, updated_samples2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos_inputs(example, tok_sep_token=\"[SEP]\", lower_case=True):\n",
    "    seq_text = example[\"seq\"].strip().lower() if lower_case else example[\"seq\"].strip()\n",
    "    next_seq_text = example[\"next_seq\"].strip().lower() if lower_case else example[\"next_seq\"].strip()\n",
    "\n",
    "    non_adjacent_text = (\n",
    "        random.choice(example[\"other_topic_seqs\"]).strip() if len(example[\"other_topic_seqs\"]) > 0 else None\n",
    "    )\n",
    "    if lower_case and non_adjacent_text:\n",
    "        non_adjacent_text = non_adjacent_text.lower()\n",
    "\n",
    "    if example[\"is_topic_end\"] and example[\"next_topic_begin_seq\"] and non_adjacent_text:\n",
    "        # this is the last sequence in the topic so the only thing that will work here is to pair it with another non-adjacent seq in the same topic\n",
    "        # and therefore we just duplicate it here.\n",
    "        next_topic_begin_seq = (\n",
    "            example[\"next_topic_begin_seq\"].strip().lower() if lower_case else example[\"next_topic_begin_seq\"].strip()\n",
    "        )\n",
    "        inp = (f\"{seq_text}{tok_sep_token}{non_adjacent_text}\", f\"{seq_text}{tok_sep_token}{non_adjacent_text}\")\n",
    "    else:\n",
    "        # the positive pair will be a seq + the next seq -or- the seq + a non-adjacent seq in the same topic\n",
    "        inp = (\n",
    "            f\"{seq_text}{tok_sep_token}{next_seq_text}\",\n",
    "            f\"{seq_text}{tok_sep_token}{non_adjacent_text}\" if non_adjacent_text else \"xxNONExx\",\n",
    "        )\n",
    "\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neg_inputs(example, tok_sep_token=\"[SEP]\", lower_case=True):\n",
    "    seq_text = example[\"seq\"].strip()\n",
    "\n",
    "    # if at the last sequence for a topic, set the negative pair = seq + first sequence in next topic,\n",
    "    # else get a sequence that is not adjacent but in same topic\n",
    "    if example[\"is_topic_end\"] and example[\"next_topic_begin_seq\"]:\n",
    "        neg_seq_non_adjacent_text = example[\"next_topic_begin_seq\"].strip()\n",
    "    elif len(example[\"other_topic_seqs\"]) > 0:\n",
    "        neg_seq_non_adjacent_text = random.choice(example[\"other_topic_seqs\"]).strip()\n",
    "    else:\n",
    "        neg_seq_non_adjacent_text = \"xxNONExx\"\n",
    "\n",
    "    # get a sequence that is in an entirely different topic\n",
    "    # option 1: can be in same lesson but different topic or in a different course entirely\n",
    "    # neg_seq_other_topic_text = (\n",
    "    #     train_df[\"seq\"][\n",
    "    #         (train_df[\"course_title\"] != example[\"course_title\"]) | (train_df[\"lesson_num\"] != example[\"lesson_num\"])\n",
    "    #     ]\n",
    "    #     .sample(n=1)\n",
    "    #     .values[0]\n",
    "    #     .strip()\n",
    "    # )\n",
    "\n",
    "    # option 2: sample from a different course entirely\n",
    "    neg_seq_other_topic_text = (\n",
    "        train_df[\"seq\"][(train_df[\"course_title\"] != example[\"course_title\"])].sample(n=1).values[0].strip()\n",
    "    )\n",
    "\n",
    "    if lower_case:\n",
    "        seq_text = seq_text.lower()\n",
    "        neg_seq_non_adjacent_text = neg_seq_non_adjacent_text.lower()\n",
    "        neg_seq_other_topic_text = neg_seq_other_topic_text.lower()\n",
    "\n",
    "    # our SiameseBatchTokenizeTransform will choose which one to use each time the item is fetched\n",
    "    inp = (\n",
    "        f\"{seq_text}{tok_sep_token}{neg_seq_non_adjacent_text}\",\n",
    "        f\"{seq_text}{tok_sep_token}{neg_seq_other_topic_text}\",\n",
    "    )\n",
    "\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(example):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model, val_idxs, get_x=None):\n",
    "    # define validation set\n",
    "    splitter = IndexSplitter(val_idxs)\n",
    "\n",
    "    if random_seed:\n",
    "        set_seed(random_seed)\n",
    "\n",
    "    batch_tokenize_tfm = SiameseBatchTokenizeTransform(\n",
    "        use_adjacent_neg_prob=0.75,\n",
    "        hf_arch=hf_arch,\n",
    "        hf_config=hf_config,\n",
    "        hf_tokenizer=hf_tokenizer,\n",
    "        hf_model=hf_model,\n",
    "        include_labels=False,\n",
    "        max_length=True,\n",
    "        truncation=True,\n",
    "        tok_kwargs={},\n",
    "    )\n",
    "\n",
    "    blocks = (TextBlock(batch_tokenize_tfm=batch_tokenize_tfm), noop, CategoryBlock)\n",
    "\n",
    "    get_pos_x = partial(build_pos_inputs, tok_sep_token=hf_tokenizer.sep_token, lower_case=True)\n",
    "    get_neg_x = partial(build_neg_inputs, tok_sep_token=hf_tokenizer.sep_token, lower_case=True)\n",
    "    get_y = partial(build_targets)\n",
    "\n",
    "    dblock = DataBlock(\n",
    "        blocks=blocks,\n",
    "        get_x=[get_pos_x, get_neg_x],\n",
    "        get_y=get_y,\n",
    "        splitter=splitter,\n",
    "        n_inp=2,\n",
    "    )\n",
    "\n",
    "    if random_seed:\n",
    "        set_seed(random_seed)\n",
    "\n",
    "    return dblock.dataloaders(df, bs=bsz, val_bs=bsz * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(train_df, hf_arch, hf_config, hf_tokenizer, hf_model, val_idxs=val_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] and in fact in this case they actually have scores, it's either going to be, basically, zero (0), zero point two five (0.25), point five (0.5), point seven five (0.75), or one (1), of,[SEP] like, how similar is it. but it's basically a classification task, when you think of it that way. so yeah, you can have a look at the data and, next week, we're going to go through step[SEP][PAD][PAD][PAD][PAD][PAD][PAD]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.decode(b[0][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] and in fact in this case they actually have scores, it's either going to be, basically, zero (0), zero point two five (0.25), point five (0.5), point seven five (0.75), or one (1), of,[SEP] all right, thanks, everybody. i'll see you next week. bye.[SEP]\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.decode(b[1][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorCategory([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blurr_splitter_with_head(m: Module):\n",
    "    \"\"\"Simply adds an additional layer group to the classification head\"\"\"\n",
    "    base_param_groups = blurr_splitter(m)\n",
    "\n",
    "    added_groups = L([m for m_name, m in list(m.named_children()) if m_name != \"hf_model\"])\n",
    "    added_param_groups = added_groups.map(params).filter(lambda el: len(el) > 0)\n",
    "\n",
    "    return base_param_groups + added_param_groups\n",
    "\n",
    "\n",
    "def blurr_splitter_on_backbone(m: Module):\n",
    "    \"\"\"Creates two layer groups: One for the backbone and one for the pooler/classification head\"\"\"\n",
    "    root_modules = list(m.named_children())\n",
    "    top_module_name, top_module = root_modules[0]\n",
    "\n",
    "    groups = L(top_module)\n",
    "    groups += L([m for m_name, m in root_modules[1:]])\n",
    "\n",
    "    return groups.map(params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Review PyTorch docs (https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html); consider changing\n",
    "def MarginRankingLoss(pos_neg_scores, targs):\n",
    "    margin = 1\n",
    "    p_scores, n_scores = pos_neg_scores\n",
    "\n",
    "    scores = margin - p_scores + n_scores\n",
    "    scores = scores.clamp(min=0)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(inps, targs):\n",
    "    labels = []\n",
    "    all_pos_scores, all_neg_scores = inps[0], inps[1]\n",
    "\n",
    "    for i in range(len(all_pos_scores)):\n",
    "        if all_pos_scores[i] > all_neg_scores[i]:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "    return sum(labels) / float(len(all_pos_scores))\n",
    "\n",
    "\n",
    "_f1_score = AvgMetric(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicSegmentationModelWrapper(BaseModelWrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_config,\n",
    "        hf_model,\n",
    "        dropout_cls=nn.Dropout,\n",
    "        p=0.1,\n",
    "        hf_model_kwargs={},\n",
    "    ):\n",
    "        super().__init__(hf_model=hf_model, output_hidden_states=True, hf_model_kwargs=hf_model_kwargs)\n",
    "        store_attr()\n",
    "\n",
    "        self.coherence_prediction_dec = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(hf_config.hidden_size, hf_config.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                dropout_cls(p=p),\n",
    "                nn.Linear(hf_config.hidden_size, 2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        # sequence 1 (pos examples)\n",
    "        inputs1_res = super().forward(inputs1)\n",
    "        pos_scores = inputs1_res.hidden_states[-1][:, 0, :]\n",
    "        pos_scores = self.coherence_prediction_dec(pos_scores)\n",
    "\n",
    "        # sequence 2 (neg examples)\n",
    "        inputs2_res = super().forward(inputs2)\n",
    "        neg_scores = inputs2_res.hidden_states[-1][:, 0, :]\n",
    "        neg_scores = self.coherence_prediction_dec(neg_scores)\n",
    "\n",
    "        return pos_scores[:, 0], neg_scores[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_seed:\n",
    "    set_seed(random_seed)\n",
    "\n",
    "learn_cbs = []\n",
    "fit_cbs = [GradientClip(max_norm=1.0)]\n",
    "\n",
    "blurr_model_wrapper = TopicSegmentationModelWrapper(hf_config=hf_config, hf_model=hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    blurr_model_wrapper,\n",
    "    loss_func=MarginRankingLoss,\n",
    "    metrics=[_f1_score],\n",
    "    cbs=learn_cbs,\n",
    "    splitter=blurr_splitter_on_backbone,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(minimum=0.0033113110810518267, steep=0.02290867641568184, valley=0.010964781977236271, slide=0.0014454397605732083)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.420839</td>\n",
       "      <td>0.587762</td>\n",
       "      <td>0.759843</td>\n",
       "      <td>06:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325482</td>\n",
       "      <td>0.472662</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>06:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.278993</td>\n",
       "      <td>0.411808</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>06:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.266935</td>\n",
       "      <td>0.388746</td>\n",
       "      <td>0.840858</td>\n",
       "      <td>06:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if random_seed:\n",
    "    set_seed(random_seed)\n",
    "\n",
    "learn.fit_one_cycle(4, slice(1e-5, 1e-3), cbs=fit_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#5) ['epoch','train_loss','valid_loss','f1_score','time'],\n",
       " [(#3) [0.4208391308784485,0.5877618193626404,0.7598433312719027],\n",
       "  (#3) [0.325481653213501,0.4726616442203522,0.8080808080808081],\n",
       "  (#3) [0.27899280190467834,0.41180819272994995,0.8253968253968254],\n",
       "  (#3) [0.26693519949913025,0.38874563574790955,0.8408575551432694]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = learn.recorder\n",
    "r.metric_names, r.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(\"topic_segmentation_learner.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C-Squared Podcast',\n",
       " 'Full Stack Deep Learning - Spring 2021',\n",
       " 'ali - how to start a business',\n",
       " 'cc - how to invest in stocks',\n",
       " 'dr berg - adrenal body type',\n",
       " 'dr berg - cereal vid',\n",
       " 'dr berg - diabetes myth',\n",
       " 'dr berg - how to fast',\n",
       " 'dr berg - what happens when you fast',\n",
       " 'fast.ai 2022 - Part 1',\n",
       " 'markowskyart - begginer drawing course',\n",
       " 'parker - learn photography']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_df.course_title.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['markowskyart - begginer drawing course',\n",
       "       'cc - how to invest in stocks', 'dr berg - cereal vid'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[val_idxs][\"course_title\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_course_title = random.choice(train_df.iloc[val_idxs][\"course_title\"].unique().tolist())\n",
    "# print(val_course_title)\n",
    "# val_lesson_num = \"1\"\n",
    "\n",
    "val_course_title = \"fast.ai 2022 - Part 1\"\n",
    "val_lesson_num = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>course_title</th>\n",
       "      <th>lesson_num</th>\n",
       "      <th>topic</th>\n",
       "      <th>seq</th>\n",
       "      <th>next_seq</th>\n",
       "      <th>is_topic_end</th>\n",
       "      <th>next_topic_begin_seq</th>\n",
       "      <th>other_topic_seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15307</td>\n",
       "      <td>15307</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think</td>\n",
       "      <td>is the lesson that a lot of the regulars in the community have been most excited about,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15308</td>\n",
       "      <td>15308</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>is the lesson that a lot of the regulars in the community have been most excited about,</td>\n",
       "      <td>because it's where we're gonna get some totally new material  totally new topic, we've</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15309</td>\n",
       "      <td>15309</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>because it's where we're gonna get some totally new material  totally new topic, we've</td>\n",
       "      <td>never covered before. We're going to cover natural language processing (NLP), and you'll</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15310</td>\n",
       "      <td>15310</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>never covered before. We're going to cover natural language processing (NLP), and you'll</td>\n",
       "      <td>find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-trained NLP model using a library call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15311</td>\n",
       "      <td>15311</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the</td>\n",
       "      <td>fast.ai library, using recurrent neural networks (RNNs).</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-traine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index           course_title lesson_num              topic  \\\n",
       "0    15307  15307  fast.ai 2022 - Part 1          4  Using Huggingface   \n",
       "1    15308  15308  fast.ai 2022 - Part 1          4  Using Huggingface   \n",
       "2    15309  15309  fast.ai 2022 - Part 1          4  Using Huggingface   \n",
       "3    15310  15310  fast.ai 2022 - Part 1          4  Using Huggingface   \n",
       "4    15311  15311  fast.ai 2022 - Part 1          4  Using Huggingface   \n",
       "\n",
       "                                                                                                                                                                               seq  \\\n",
       "0                                                                                       Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think   \n",
       "1                                                                                          is the lesson that a lot of the regulars in the community have been most excited about,   \n",
       "2                                                                                          because it's where we're gonna get some totally new material  totally new topic, we've   \n",
       "3                                                                                         never covered before. We're going to cover natural language processing (NLP), and you'll   \n",
       "4  find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the   \n",
       "\n",
       "                                                                                                                                                                          next_seq  \\\n",
       "0                                                                                          is the lesson that a lot of the regulars in the community have been most excited about,   \n",
       "1                                                                                          because it's where we're gonna get some totally new material  totally new topic, we've   \n",
       "2                                                                                         never covered before. We're going to cover natural language processing (NLP), and you'll   \n",
       "3  find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the   \n",
       "4                                                                                                                         fast.ai library, using recurrent neural networks (RNNs).   \n",
       "\n",
       "   is_topic_end next_topic_begin_seq  \\\n",
       "0         False                  NaN   \n",
       "1         False                  NaN   \n",
       "2         False                  NaN   \n",
       "3         False                  NaN   \n",
       "4         False                  NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          other_topic_seqs  \n",
       "0  [because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doi...  \n",
       "1  [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, ...  \n",
       "2  [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, d...  \n",
       "3  [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-trained NLP model using a library call...  \n",
       "4  [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-traine...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_df = raw_train_df[\n",
    "    (raw_train_df[\"course_title\"] == val_course_title) & (raw_train_df[\"lesson_num\"] == val_lesson_num)\n",
    "].copy()\n",
    "inf_df.reset_index(inplace=True)\n",
    "\n",
    "print(len(inf_df))\n",
    "inf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(\"topic_segmentation_learner.pkl\")\n",
    "inf_learn.model.hf_model = inf_learn.model.hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pairs = L()\n",
    "\n",
    "for i in range(len(inf_df) - 1):\n",
    "    seq_a = inf_df.iloc[i][\"seq\"].strip().lower()\n",
    "    seq_b = inf_df.iloc[i + 1][\"seq\"].strip().lower()\n",
    "\n",
    "    seq_pairs.append((seq_a, seq_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(0, len(seq_pairs), 16):\n",
    "    # print(i)\n",
    "    batch = seq_pairs[i : i + 16]\n",
    "    inputs = hf_tokenizer(\n",
    "        list(batch.itemgot(0)), list(batch.itemgot(1)), padding=True, max_length=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    batch_scores = inf_learn.model.hf_model(**inputs)\n",
    "    scores += batch_scores[0][:, 0].detach().cpu()[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.sigmoid(torch.concat(scores)).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics, segeval\n",
    "from sklearn.metrics import mean_absolute_error, f1_score, recall_score, precision_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_score_cal(scores):\n",
    "    output_scores = []\n",
    "    for i in range(len(scores)):\n",
    "        lflag = scores[i]\n",
    "        rflag = scores[i]\n",
    "        if i == 0:\n",
    "            hl = scores[i]\n",
    "            for r in range(i + 1, len(scores)):\n",
    "                if rflag <= scores[r]:\n",
    "                    rflag = scores[r]\n",
    "                else:\n",
    "                    break\n",
    "        elif i == len(scores):\n",
    "            hr = scores[i]\n",
    "            for l in range(i - 1, -1, -1):\n",
    "                if lflag <= scores[l]:\n",
    "                    lflag = scores[l]\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            for r in range(i + 1, len(scores)):\n",
    "                if rflag <= scores[r]:\n",
    "                    rflag = scores[r]\n",
    "                else:\n",
    "                    break\n",
    "            for l in range(i - 1, -1, -1):\n",
    "                if lflag <= scores[l]:\n",
    "                    lflag = scores[l]\n",
    "                else:\n",
    "                    break\n",
    "        depth_score = 0.5 * (lflag + rflag - 2 * scores[i])\n",
    "        output_scores.append(depth_score)\n",
    "\n",
    "    return output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001102358102798462, 0.001096278429031372, 0.0, 0.0017909705638885498, 0.00334717333316803, 0.0, 0.0002167820930480957, 0.0027249157428741455, 0.0008751749992370605, 0.0]\n"
     ]
    }
   ],
   "source": [
    "depth_scores = depth_score_cal(scores)\n",
    "print(depth_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018276203486878994\n",
      "0.0002057399657654498\n"
     ]
    }
   ],
   "source": [
    "print(sum(depth_scores) / (len(depth_scores)))\n",
    "print(statistics.stdev(depth_scores) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004913719835169646\n"
     ]
    }
   ],
   "source": [
    "threshold = sum(depth_scores) / (len(depth_scores)) + (statistics.stdev(depth_scores) * 1.5)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(depth_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_indice = []\n",
    "seg_p_labels = [0] * (len(depth_scores) + 1)\n",
    "\n",
    "for i in range(len(depth_scores)):\n",
    "    if depth_scores[i] > threshold:\n",
    "        boundary_indice.append(i)\n",
    "\n",
    "for i in boundary_indice:\n",
    "    seg_p_labels[i] = 1\n",
    "\n",
    "\n",
    "tmp = 0\n",
    "seg_p = []\n",
    "for idx, fake in enumerate(seg_p_labels):\n",
    "    if fake == 1 and idx != 0:\n",
    "        # tmp += 1\n",
    "        seg_p.append(tmp)\n",
    "        tmp = 1\n",
    "    else:\n",
    "        tmp += 1\n",
    "seg_p.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "68\n",
      "[24, 23, 7, 13, 2, 21, 2, 9, 4, 6, 29, 4, 4, 4, 2, 5, 11, 1, 47, 28, 3, 15, 5, 36, 28, 37, 6, 2, 3, 4, 1, 1, 4, 1, 6, 28, 7, 9, 3, 15, 2, 6, 4, 13, 23, 14, 3, 17, 21, 9, 4, 15, 2, 3, 2, 6, 4, 24, 18, 5, 4, 7, 8, 7, 9, 2, 3, 14]\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "print(len(seg_p_labels))\n",
    "print(seg_p_labels[:20])\n",
    "print(\"\")\n",
    "\n",
    "total = 0\n",
    "for el in seg_p:\n",
    "    total += el\n",
    "\n",
    "print(len(seg_p))\n",
    "print(seg_p)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_r_labels = []\n",
    "seg_r = []\n",
    "tmp = 1\n",
    "\n",
    "for r_idx, r in inf_df.iterrows():\n",
    "    current_topic = r[\"topic\"]\n",
    "    if r_idx == 0:\n",
    "        last_seen_topic = r[\"topic\"]\n",
    "\n",
    "    if last_seen_topic != current_topic:\n",
    "        last_seen_topic = current_topic\n",
    "        seg_r_labels.append(1)\n",
    "        seg_r.append(tmp)\n",
    "        tmp = 1\n",
    "    else:\n",
    "        seg_r_labels.append(0)\n",
    "        tmp += 1 if r_idx != 0 else 0\n",
    "\n",
    "seg_r.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "[27, 15, 29, 12, 31, 11, 37, 28, 34, 31, 25, 18, 18, 16, 17, 37, 12, 38, 16, 17, 20, 47, 27, 39, 19, 24, 5, 47, 12]\n",
      "[27, 15, 29, 12, 31, 11, 37, 28, 34, 31, 25, 18, 18, 16, 17, 37, 12, 38, 16, 17, 20, 47, 27, 39, 19, 24, 5, 47, 12]\n",
      "\n",
      "709\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "print(len(seg_r))\n",
    "print(seg_r[:30])\n",
    "print(seg_r[-30:])\n",
    "print(\"\")\n",
    "print(len(seg_r_labels))\n",
    "print(seg_r_labels[:30])\n",
    "print(seg_r_labels[-30:])\n",
    "print(\"\")\n",
    "total = 0\n",
    "for el in seg_r:\n",
    "    total += el\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 68\n",
      "0.6829268292682926829268292683\n",
      "0.5423242467718794835007173601\n"
     ]
    }
   ],
   "source": [
    "print(len(seg_r), len(seg_p))\n",
    "\n",
    "score_wd = segeval.window_diff(seg_p, seg_r)\n",
    "print(score_wd)\n",
    "\n",
    "score_pk = segeval.pk(seg_p, seg_r)\n",
    "print(score_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709 709\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(seg_r_labels), len(seg_p_labels))\n",
    "print(seg_r_labels[:20])\n",
    "print(seg_p_labels[:20])\n",
    "print(\"\")\n",
    "print(seg_r_labels[-20:])\n",
    "print(seg_p_labels[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709 709\n",
      "87\n",
      "0.5092254445637904\n",
      "0.5148677043189036\n",
      "0.51115915748361\n",
      "0.5251730648206419\n"
     ]
    }
   ],
   "source": [
    "print(len(seg_r_labels), len(seg_p_labels))\n",
    "\n",
    "score_mae = sum(list(map(abs, np.array(seg_r_labels) - np.array(seg_p_labels))))\n",
    "print(score_mae)\n",
    "\n",
    "print(f1_score(seg_r_labels, seg_p_labels, average=\"macro\"))\n",
    "print(fbeta_score(seg_r_labels, seg_p_labels, beta=2, average=\"macro\"))\n",
    "print(precision_score(seg_r_labels, seg_p_labels, average=\"macro\"))\n",
    "print(recall_score(seg_r_labels, seg_p_labels, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "[0, 24, 47, 54, 67, 69, 90, 92, 101, 105, 111, 140, 144, 148, 152, 154, 159, 170, 171, 218, 246, 249, 264, 269, 305, 333, 370, 376, 378, 381, 385, 386, 387, 391, 392, 398, 426, 433, 442, 445]\n"
     ]
    }
   ],
   "source": [
    "seg_idxs = [seg_idx for seg_idx, v in enumerate(seg_p_labels) if v == 1 or seg_idx == 0]\n",
    "print(len(seg_idxs))\n",
    "print(seg_idxs[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df[\"pred_start\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.loc[seg_idxs, \"pred_start\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>course_title</th>\n",
       "      <th>lesson_num</th>\n",
       "      <th>topic</th>\n",
       "      <th>seq</th>\n",
       "      <th>next_seq</th>\n",
       "      <th>is_topic_end</th>\n",
       "      <th>next_topic_begin_seq</th>\n",
       "      <th>other_topic_seqs</th>\n",
       "      <th>pred_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15307</td>\n",
       "      <td>15307</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think</td>\n",
       "      <td>is the lesson that a lot of the regulars in the community have been most excited about,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15308</td>\n",
       "      <td>15308</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>is the lesson that a lot of the regulars in the community have been most excited about,</td>\n",
       "      <td>because it's where we're gonna get some totally new material  totally new topic, we've</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15309</td>\n",
       "      <td>15309</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>because it's where we're gonna get some totally new material  totally new topic, we've</td>\n",
       "      <td>never covered before. We're going to cover natural language processing (NLP), and you'll</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15310</td>\n",
       "      <td>15310</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>never covered before. We're going to cover natural language processing (NLP), and you'll</td>\n",
       "      <td>find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-trained NLP model using a library call...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15311</td>\n",
       "      <td>15311</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the</td>\n",
       "      <td>fast.ai library, using recurrent neural networks (RNNs).</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-traine...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15312</td>\n",
       "      <td>15312</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>fast.ai library, using recurrent neural networks (RNNs).</td>\n",
       "      <td>Today we're going to do something else, which is we're going to do Transformers, and we're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, not even going to use the fast.ai library at all in fact. ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15313</td>\n",
       "      <td>15313</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Today we're going to do something else, which is we're going to do Transformers, and we're</td>\n",
       "      <td>not even going to use the fast.ai library at all in fact. So, what we're going to be</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15314</td>\n",
       "      <td>15314</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>not even going to use the fast.ai library at all in fact. So, what we're going to be</td>\n",
       "      <td>doing today is we're going to be fine-tuning a pre-trained NLP model using a library called</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15315</td>\n",
       "      <td>15315</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>doing today is we're going to be fine-tuning a pre-trained NLP model using a library called</td>\n",
       "      <td>Hugging Face Transformers. Now given this is the fast.ai course, you might be wondering</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15316</td>\n",
       "      <td>15316</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Hugging Face Transformers. Now given this is the fast.ai course, you might be wondering</td>\n",
       "      <td>why we'd be using a different library other than fast.ai.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15317</td>\n",
       "      <td>15317</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>why we'd be using a different library other than fast.ai.</td>\n",
       "      <td>The reason is that I think that It's really useful for everybody to have experience and practice of using more than one library.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15318</td>\n",
       "      <td>15318</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>The reason is that I think that It's really useful for everybody to have experience and practice of using more than one library.</td>\n",
       "      <td>Because you'll get to see the same concepts applied in different ways, and I think that's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15319</td>\n",
       "      <td>15319</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Because you'll get to see the same concepts applied in different ways, and I think that's</td>\n",
       "      <td>great for your understanding of what these concepts are. Also, I really like the Hugging</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15320</td>\n",
       "      <td>15320</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>great for your understanding of what these concepts are. Also, I really like the Hugging</td>\n",
       "      <td>Face Transformers library. It's absolutely the state of the art in NLP, and it's well</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15321</td>\n",
       "      <td>15321</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Face Transformers library. It's absolutely the state of the art in NLP, and it's well</td>\n",
       "      <td>worth knowing. If you're watching this on video, by the time you're watching it, we will probably have</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15322</td>\n",
       "      <td>15322</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>worth knowing. If you're watching this on video, by the time you're watching it, we will probably have</td>\n",
       "      <td>completed our integration of the Transformers library into fast.ai. So it's in the process of becoming the main NLP (kind of) foundation for fast.ai. So you'll be able to combine</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15323</td>\n",
       "      <td>15323</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>completed our integration of the Transformers library into fast.ai. So it's in the process of becoming the main NLP (kind of) foundation for fast.ai. So you'll be able to combine</td>\n",
       "      <td>Transformers and fast.ai together. Yeah, so I think there's a lot of benefits to this,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15324</td>\n",
       "      <td>15324</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>Transformers and fast.ai together. Yeah, so I think there's a lot of benefits to this,</td>\n",
       "      <td>and in the end you're going to know how to do NLP, you know, in a really fantastic library. Now the other thing is, Hugging Face Transformers doesn't have the same layered architecture</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15325</td>\n",
       "      <td>15325</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>and in the end you're going to know how to do NLP, you know, in a really fantastic library. Now the other thing is, Hugging Face Transformers doesn't have the same layered architecture</td>\n",
       "      <td>that fast.ai has, which means particularly for beginners, the kind of high level, height</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15326</td>\n",
       "      <td>15326</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>that fast.ai has, which means particularly for beginners, the kind of high level, height</td>\n",
       "      <td>you know top-tier API that you'll be using most of the time, is not as (kind of) ready</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15327</td>\n",
       "      <td>15327</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>you know top-tier API that you'll be using most of the time, is not as (kind of) ready</td>\n",
       "      <td>to go for beginners, as you're used to from fast.ai. And so that's actually, I think,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15328</td>\n",
       "      <td>15328</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>to go for beginners, as you're used to from fast.ai. And so that's actually, I think,</td>\n",
       "      <td>a good thing. You're up to Lesson Four, you know the basic idea now of how gradient descent works, and and you know, how parameters are learned as part of a flexible function,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15329</td>\n",
       "      <td>15329</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>a good thing. You're up to Lesson Four, you know the basic idea now of how gradient descent works, and and you know, how parameters are learned as part of a flexible function,</td>\n",
       "      <td>I think you're ready to try using a somewhat lower level library that does a little bit</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15330</td>\n",
       "      <td>15330</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>I think you're ready to try using a somewhat lower level library that does a little bit</td>\n",
       "      <td>less for you. So it's going to be, you know, a little bit more work. It's still it's a very well designed library, and it's still reasonably high level, but you're going to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15331</td>\n",
       "      <td>15331</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>less for you. So it's going to be, you know, a little bit more work. It's still it's a very well designed library, and it's still reasonably high level, but you're going to</td>\n",
       "      <td>learn to go a little bit deeper. And that's kind of how the rest of the course in general is going to be. On the whole, is, we're going to get a bit deeper, and a bit deeper, and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15332</td>\n",
       "      <td>15332</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>learn to go a little bit deeper. And that's kind of how the rest of the course in general is going to be. On the whole, is, we're going to get a bit deeper, and a bit deeper, and</td>\n",
       "      <td>a bit deeper. Now, so first of all, let's talk about what we're going to be doing with</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15333</td>\n",
       "      <td>15333</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Using Huggingface</td>\n",
       "      <td>a bit deeper. Now, so first of all, let's talk about what we're going to be doing with</td>\n",
       "      <td>fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now</td>\n",
       "      <td>True</td>\n",
       "      <td>fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now</td>\n",
       "      <td>[Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15334</td>\n",
       "      <td>15334</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now</td>\n",
       "      <td>you do. You played with these sliders last week, and hopefully you've all actually gone</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you said, Oh! actually slider A, that, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because, you could immediately start focusing on the one we have...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15335</td>\n",
       "      <td>15335</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>you do. You played with these sliders last week, and hopefully you've all actually gone</td>\n",
       "      <td>into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you said, Oh! actually slider A, that, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because, you could immediately start focusing on the one we...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15336</td>\n",
       "      <td>15336</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So</td>\n",
       "      <td>imagine that your job was to move these sliders, to get this as nice as possible, but when</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, it was given to you, the person who gave it to you said, Oh! actually slider A, that, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because, you could immediately start focusing on the one we ha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15337</td>\n",
       "      <td>15337</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>imagine that your job was to move these sliders, to get this as nice as possible, but when</td>\n",
       "      <td>it was given to you, the person who gave it to you said, Oh! actually slider A, that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>15338</td>\n",
       "      <td>15338</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>it was given to you, the person who gave it to you said, Oh! actually slider A, that</td>\n",
       "      <td>should be on 2.0, we know for sure. And slider B, we think it's like around two and a half.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, Slider C, we've got no idea. Now that'd be pretty ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15339</td>\n",
       "      <td>15339</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>should be on 2.0, we know for sure. And slider B, we think it's like around two and a half.</td>\n",
       "      <td>Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15340</td>\n",
       "      <td>15340</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because</td>\n",
       "      <td>you could immediately start focusing on the one we have no idea about, get that in roughly the right spot, and then the one you kind of got a vague idea about, you could just</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>15341</td>\n",
       "      <td>15341</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>you could immediately start focusing on the one we have no idea about, get that in roughly the right spot, and then the one you kind of got a vague idea about, you could just</td>\n",
       "      <td>tune it a little bit, and the one that they said was totally confident, you wouldn't move at all. You would probably tune these sliders really quickly. That's what a pre-trained</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>15342</td>\n",
       "      <td>15342</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>tune it a little bit, and the one that they said was totally confident, you wouldn't move at all. You would probably tune these sliders really quickly. That's what a pre-trained</td>\n",
       "      <td>model is. A pre-trained model is a bunch of parameters that have already been fitted,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>15343</td>\n",
       "      <td>15343</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>model is. A pre-trained model is a bunch of parameters that have already been fitted,</td>\n",
       "      <td>where some of them were already pretty confident of what they should be, and some</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>15344</td>\n",
       "      <td>15344</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>where some of them were already pretty confident of what they should be, and some</td>\n",
       "      <td>of them we really have no idea at all. And so fine-tuning is the process of taking those</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>15345</td>\n",
       "      <td>15345</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>of them we really have no idea at all. And so fine-tuning is the process of taking those</td>\n",
       "      <td>ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15346</td>\n",
       "      <td>15346</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit.</td>\n",
       "      <td>The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>15347</td>\n",
       "      <td>15347</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm</td>\n",
       "      <td>called ULMFiT which was first presented actually in a fast.ai course, I think the very first</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15348</td>\n",
       "      <td>15348</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finetuning pretrained model</td>\n",
       "      <td>called ULMFiT which was first presented actually in a fast.ai course, I think the very first</td>\n",
       "      <td>fast.ai course. It was later turned into an academic paper by me in conjunction with a</td>\n",
       "      <td>True</td>\n",
       "      <td>fast.ai course. It was later turned into an academic paper by me in conjunction with a</td>\n",
       "      <td>[fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>15349</td>\n",
       "      <td>15349</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>fast.ai course. It was later turned into an academic paper by me in conjunction with a</td>\n",
       "      <td>then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 1...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15350</td>\n",
       "      <td>15350</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers</td>\n",
       "      <td>and went on to help inspire a huge change, you know, huge kind of step improvement in</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15351</td>\n",
       "      <td>15351</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>and went on to help inspire a huge change, you know, huge kind of step improvement in</td>\n",
       "      <td>NLP capabilities around the world, along with a number of other important innovations at</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>15352</td>\n",
       "      <td>15352</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>NLP capabilities around the world, along with a number of other important innovations at</td>\n",
       "      <td>the time. This is the basic process that ULMFiT described. Step One was to build something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 17th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>15353</td>\n",
       "      <td>15353</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>the time. This is the basic process that ULMFiT described. Step One was to build something</td>\n",
       "      <td>called a language model using basically nearly all of Wikipedia and what the language model</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 17th pr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15354</td>\n",
       "      <td>15354</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>called a language model using basically nearly all of Wikipedia and what the language model</td>\n",
       "      <td>did was it tried to predict the next word of a Wikipedia article. In fact every next</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 1...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15355</td>\n",
       "      <td>15355</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>did was it tried to predict the next word of a Wikipedia article. In fact every next</td>\n",
       "      <td>word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, articles which would say things like, you know the 17...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15356</td>\n",
       "      <td>15356</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia</td>\n",
       "      <td>articles which would say things like, you know the 17th prime number is dot dot</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>15357</td>\n",
       "      <td>15357</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>articles which would say things like, you know the 17th prime number is dot dot</td>\n",
       "      <td>dot or the 40th president of the United States, blah, said at his residence, blah</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>15358</td>\n",
       "      <td>15358</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>dot or the 40th president of the United States, blah, said at his residence, blah</td>\n",
       "      <td>that. You know, filling in these kinds of things requires understanding a lot about</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>15359</td>\n",
       "      <td>15359</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>that. You know, filling in these kinds of things requires understanding a lot about</td>\n",
       "      <td>how language is structured, and about the world, and about math, and so forth. So to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>15360</td>\n",
       "      <td>15360</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>how language is structured, and about the world, and about math, and so forth. So to</td>\n",
       "      <td>get good at being a language model a neural network has to get good at a lot of things.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15361</td>\n",
       "      <td>15361</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>get good at being a language model a neural network has to get good at a lot of things.</td>\n",
       "      <td>It has to understand how language works at a reasonably good level and it needs to understand</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>15362</td>\n",
       "      <td>15362</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>It has to understand how language works at a reasonably good level and it needs to understand</td>\n",
       "      <td>what it's actually talking about, and what is actually true, and what is actually not true, and the different ways in which things are expressed, and so forth. So this was trained</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>15363</td>\n",
       "      <td>15363</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>what it's actually talking about, and what is actually true, and what is actually not true, and the different ways in which things are expressed, and so forth. So this was trained</td>\n",
       "      <td>using a very similar approach to what we'll be looking at for fine-tuning but it started with random weights and at the end of it there was a model that could predict more than 30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>15364</td>\n",
       "      <td>15364</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>using a very similar approach to what we'll be looking at for fine-tuning but it started with random weights and at the end of it there was a model that could predict more than 30</td>\n",
       "      <td>percent of the time correctly what the next word of a Wikipedia article would be. So in</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>15365</td>\n",
       "      <td>15365</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>percent of the time correctly what the next word of a Wikipedia article would be. So in</td>\n",
       "      <td>this particular case, for the ULMFiT paper, we then took that and we were trying to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>15366</td>\n",
       "      <td>15366</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>this particular case, for the ULMFiT paper, we then took that and we were trying to</td>\n",
       "      <td>the first task I did actually, for the fast.ai course, back when I invented this, was to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>15367</td>\n",
       "      <td>15367</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>the first task I did actually, for the fast.ai course, back when I invented this, was to</td>\n",
       "      <td>try and figure out whether IMDb movie reviews were positive or negative sentiment: Did the person like the movie or not? So what I did was I created a second language model so again</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>15368</td>\n",
       "      <td>15368</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>try and figure out whether IMDb movie reviews were positive or negative sentiment: Did the person like the movie or not? So what I did was I created a second language model so again</td>\n",
       "      <td>the language model here is something that predicts the next word of a sentence but rather than using Wikipedia I took this pre-trained model that was trained on Wikipedia and I</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>15369</td>\n",
       "      <td>15369</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>the language model here is something that predicts the next word of a sentence but rather than using Wikipedia I took this pre-trained model that was trained on Wikipedia and I</td>\n",
       "      <td>ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>15370</td>\n",
       "      <td>15370</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next</td>\n",
       "      <td>word of an IMDb movie review. And then finally I took those weights and I fine-tuned them</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>15371</td>\n",
       "      <td>15371</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>word of an IMDb movie review. And then finally I took those weights and I fine-tuned them</td>\n",
       "      <td>for the task of predicting whether or not a movie review was positive or negative sentiment.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>15372</td>\n",
       "      <td>15372</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>for the task of predicting whether or not a movie review was positive or negative sentiment.</td>\n",
       "      <td>So those were the three steps. This is a particularly interesting approach because this very first model, in fact the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>15373</td>\n",
       "      <td>15373</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>So those were the three steps. This is a particularly interesting approach because this very first model, in fact the</td>\n",
       "      <td>first two models, if you think about it they don't require any label. I didn't have to collect any kind of document categories, or do any kind of surveys, or collect anything.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>15374</td>\n",
       "      <td>15374</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>first two models, if you think about it they don't require any label. I didn't have to collect any kind of document categories, or do any kind of surveys, or collect anything.</td>\n",
       "      <td>All I needed was the actual text of Wikipedia and movie reviews themselves because the labels</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>15375</td>\n",
       "      <td>15375</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>All I needed was the actual text of Wikipedia and movie reviews themselves because the labels</td>\n",
       "      <td>was: whats the next word of a sentence?. Now, since we built ULMFiT, and we used RNNs (recurrent neural networks) for this, at about</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>15376</td>\n",
       "      <td>15376</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>was: whats the next word of a sentence?. Now, since we built ULMFiT, and we used RNNs (recurrent neural networks) for this, at about</td>\n",
       "      <td>the same time-ish that we released this, a new kind of architecture particularly useful for NLP at the time was developed called Transformers. And Transformers were particularly built because</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>15377</td>\n",
       "      <td>15377</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit</td>\n",
       "      <td>the same time-ish that we released this, a new kind of architecture particularly useful for NLP at the time was developed called Transformers. And Transformers were particularly built because</td>\n",
       "      <td>they can take really good advantage of modern accelerators like, like Google's TPUs.</td>\n",
       "      <td>True</td>\n",
       "      <td>they can take really good advantage of modern accelerators like, like Google's TPUs.</td>\n",
       "      <td>[fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>15378</td>\n",
       "      <td>15378</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>they can take really good advantage of modern accelerators like, like Google's TPUs.</td>\n",
       "      <td>They didn't really, kind of, allow you to predict the next word of a sentence. It's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were deleted, essentially. So it's a, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>15379</td>\n",
       "      <td>15379</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>They didn't really, kind of, allow you to predict the next word of a sentence. It's</td>\n",
       "      <td>just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were deleted, essentially. So it's a, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>15380</td>\n",
       "      <td>15380</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,</td>\n",
       "      <td>instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, the model to predict which/what were the words that were deleted, essentially. So it's a, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the, same. So today we're going to be looking at models using what's bec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>15381</td>\n",
       "      <td>15381</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked</td>\n",
       "      <td>the model to predict which/what were the words that were deleted, essentially. So it's a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>15382</td>\n",
       "      <td>15382</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>the model to predict which/what were the words that were deleted, essentially. So it's a</td>\n",
       "      <td>pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, our RNN approach with a Transformer model, they replaced ou...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>15383</td>\n",
       "      <td>15383</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced</td>\n",
       "      <td>our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>15384</td>\n",
       "      <td>15384</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the</td>\n",
       "      <td>same. So today we're going to be looking at models using what's become the, you know,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>15385</td>\n",
       "      <td>15385</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>same. So today we're going to be looking at models using what's become the, you know,</td>\n",
       "      <td>much more popular approach than ULMFiT which is this Transformers masked language model approach. Okay, John do we have any questions? And I should mention we do have a professor</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>15386</td>\n",
       "      <td>15386</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>much more popular approach than ULMFiT which is this Transformers masked language model approach. Okay, John do we have any questions? And I should mention we do have a professor</td>\n",
       "      <td>from University of Queensland, John Williams, joining us, who will be asking the highest</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>15387</td>\n",
       "      <td>15387</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>from University of Queensland, John Williams, joining us, who will be asking the highest</td>\n",
       "      <td>voted questions from the community. What have you got, John? Yeah thanks Jeremy. Look, and we might be jumping the gun here, I suspect this is where</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>15388</td>\n",
       "      <td>15388</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>voted questions from the community. What have you got, John? Yeah thanks Jeremy. Look, and we might be jumping the gun here, I suspect this is where</td>\n",
       "      <td>you're going tonight but we've got a good question here on the forum which is: How do you go from a model that's trained to predict the next word, to a model that can be used</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>15389</td>\n",
       "      <td>15389</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>you're going tonight but we've got a good question here on the forum which is: How do you go from a model that's trained to predict the next word, to a model that can be used</td>\n",
       "      <td>for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place</td>\n",
       "      <td>True</td>\n",
       "      <td>for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place</td>\n",
       "      <td>[they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>15390</td>\n",
       "      <td>15390</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place</td>\n",
       "      <td>to start would be the next slide, kind of give you a sense of this. You might remember</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>15391</td>\n",
       "      <td>15391</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>to start would be the next slide, kind of give you a sense of this. You might remember</td>\n",
       "      <td>in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>15392</td>\n",
       "      <td>15392</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at</td>\n",
       "      <td>visualizations of the first layer of a imagenet classification model and Layer One had sets</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then set...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15393</td>\n",
       "      <td>15393</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>visualizations of the first layer of a imagenet classification model and Layer One had sets</td>\n",
       "      <td>of weights that found diagonal edges, and here are some examples of bits of photos that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>15394</td>\n",
       "      <td>15394</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>of weights that found diagonal edges, and here are some examples of bits of photos that</td>\n",
       "      <td>successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>15395</td>\n",
       "      <td>15395</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's</td>\n",
       "      <td>some examples of bits of pictures that matched, and then Layer Two combined those and now</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of thos...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>15396</td>\n",
       "      <td>15396</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>some examples of bits of pictures that matched, and then Layer Two combined those and now</td>\n",
       "      <td>you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified linear units, the outputs of those, they're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, called activations, where then ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>15397</td>\n",
       "      <td>15397</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified linear units, the outputs of those, they're</td>\n",
       "      <td>called activations, where then themselves run through a matrix multiply, a rectified linear unit, added together, so that now you don't just have to have edge detectors, but</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>15398</td>\n",
       "      <td>15398</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>called activations, where then themselves run through a matrix multiply, a rectified linear unit, added together, so that now you don't just have to have edge detectors, but</td>\n",
       "      <td>Layer Two had corner detectors. And here's some examples of some corners that that corner detector successfully found. And remember, these were not engineered in any way, they</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>15399</td>\n",
       "      <td>15399</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>Layer Two had corner detectors. And here's some examples of some corners that that corner detector successfully found. And remember, these were not engineered in any way, they</td>\n",
       "      <td>just evolved from the gradient descent training process. Layer Two had examples of circle</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>15400</td>\n",
       "      <td>15400</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>just evolved from the gradient descent training process. Layer Two had examples of circle</td>\n",
       "      <td>detectors as it turns out, and skipping a bit, by the time we got to Layer Five we had</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>15401</td>\n",
       "      <td>15401</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>detectors as it turns out, and skipping a bit, by the time we got to Layer Five we had</td>\n",
       "      <td>bird and lizard eyeball detectors, and dog face detectors, and flower detectors and so</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>15402</td>\n",
       "      <td>15402</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>bird and lizard eyeball detectors, and dog face detectors, and flower detectors and so</td>\n",
       "      <td>forth. Now, you know, nowadays you'd have something like a resnet50 would be something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15403</td>\n",
       "      <td>15403</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>forth. Now, you know, nowadays you'd have something like a resnet50 would be something</td>\n",
       "      <td>you'd probably be training pretty regularly in this course so that, you know, you've got 50-layers, not just 5-layers. Now the later layers do things that are much more specific</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>15404</td>\n",
       "      <td>15404</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>you'd probably be training pretty regularly in this course so that, you know, you've got 50-layers, not just 5-layers. Now the later layers do things that are much more specific</td>\n",
       "      <td>to the training task which is, like, actually predicting really, what is it that we're looking</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15405</td>\n",
       "      <td>15405</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>to the training task which is, like, actually predicting really, what is it that we're looking</td>\n",
       "      <td>at? The early layers, pretty unlikely you're going to need to change them much, as long</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>15406</td>\n",
       "      <td>15406</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>at? The early layers, pretty unlikely you're going to need to change them much, as long</td>\n",
       "      <td>as you're looking at, like, some kind of natural photos, right? You're going to need edge detectors and gradient detectors.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>15407</td>\n",
       "      <td>15407</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>as you're looking at, like, some kind of natural photos, right? You're going to need edge detectors and gradient detectors.</td>\n",
       "      <td>So what we do, in the fine-tuning process, is there's actually one extra layer after</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>15408</td>\n",
       "      <td>15408</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>So what we do, in the fine-tuning process, is there's actually one extra layer after</td>\n",
       "      <td>this, which is the layer that actually says: What is this?. You know, it's, it's a dog or a cat or whatever. We actually delete that, we throw it away. So now that last matrix</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>15409</td>\n",
       "      <td>15409</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>this, which is the layer that actually says: What is this?. You know, it's, it's a dog or a cat or whatever. We actually delete that, we throw it away. So now that last matrix</td>\n",
       "      <td>multiply has one output, or one output per category you're predicting. We throw that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>15410</td>\n",
       "      <td>15410</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>multiply has one output, or one output per category you're predicting. We throw that</td>\n",
       "      <td>away, so the model now has that last matrix that's spitting out, you know, depends, but</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>15411</td>\n",
       "      <td>15411</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>away, so the model now has that last matrix that's spitting out, you know, depends, but</td>\n",
       "      <td>generally a few hundred activations, and what we do is, as we'll learn more shortly in the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>15412</td>\n",
       "      <td>15412</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>generally a few hundred activations, and what we do is, as we'll learn more shortly in the</td>\n",
       "      <td>coming lesson, we just stick a new random matrix on the end of that. And that's what</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>15413</td>\n",
       "      <td>15413</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>coming lesson, we just stick a new random matrix on the end of that. And that's what</td>\n",
       "      <td>we initially train, so it learns to use these kinds of features to predict whatever it is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>15414</td>\n",
       "      <td>15414</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>we initially train, so it learns to use these kinds of features to predict whatever it is</td>\n",
       "      <td>you're trying to predict. And then we gradually train all of those layers. So that's basically</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>15415</td>\n",
       "      <td>15415</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>you're trying to predict. And then we gradually train all of those layers. So that's basically</td>\n",
       "      <td>how it's done and so it's a bit hand wavy but we'll, particularly in part two, actually</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>15416</td>\n",
       "      <td>15416</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>how it's done and so it's a bit hand wavy but we'll, particularly in part two, actually</td>\n",
       "      <td>build that from scratch ourselves. And in fact in this lesson, time permitting, we're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>15417</td>\n",
       "      <td>15417</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>build that from scratch ourselves. And in fact in this lesson, time permitting, we're</td>\n",
       "      <td>actually going to start going down the process of actually building a real-world deep neural</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>15418</td>\n",
       "      <td>15418</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>actually going to start going down the process of actually building a real-world deep neural</td>\n",
       "      <td>net in python, so we'll be starting to actually make some progress towards that goal. Okay</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>15419</td>\n",
       "      <td>15419</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>net in python, so we'll be starting to actually make some progress towards that goal. Okay</td>\n",
       "      <td>so let's jump into the notebook. So we're going to look at a Kaggle competition that's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>15420</td>\n",
       "      <td>15420</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Zeiler &amp; Fergus</td>\n",
       "      <td>so let's jump into the notebook. So we're going to look at a Kaggle competition that's</td>\n",
       "      <td>actually on as I speak, and I created this notebook called Getting started with NLP</td>\n",
       "      <td>True</td>\n",
       "      <td>actually on as I speak, and I created this notebook called Getting started with NLP</td>\n",
       "      <td>[for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>15421</td>\n",
       "      <td>15421</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>actually on as I speak, and I created this notebook called Getting started with NLP</td>\n",
       "      <td>for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model....</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>15422</td>\n",
       "      <td>15422</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition.</td>\n",
       "      <td>And, so I'm going to take you through, you know, a complete submission to this competition.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, y...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>15423</td>\n",
       "      <td>15423</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>And, so I'm going to take you through, you know, a complete submission to this competition.</td>\n",
       "      <td>And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data, about a real problem tha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>15424</td>\n",
       "      <td>15424</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this</td>\n",
       "      <td>is an actual project, that an actual organization, is prepared to invest money in getting solved,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data, about a real problem that real...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>15425</td>\n",
       "      <td>15425</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>is an actual project, that an actual organization, is prepared to invest money in getting solved,</td>\n",
       "      <td>using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>15426</td>\n",
       "      <td>15426</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions</td>\n",
       "      <td>as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, about a real pr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>15427</td>\n",
       "      <td>15427</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data</td>\n",
       "      <td>about a real problem that real organizations really care about, and a very direct way to measure the, you know, accuracy of your solution, you can't really get better than this. Okay</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>15428</td>\n",
       "      <td>15428</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>about a real problem that real organizations really care about, and a very direct way to measure the, you know, accuracy of your solution, you can't really get better than this. Okay</td>\n",
       "      <td>so this is a good place, a good competition to experiment with for trying NLP. Now, as</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>15429</td>\n",
       "      <td>15429</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>so this is a good place, a good competition to experiment with for trying NLP. Now, as</td>\n",
       "      <td>I mentioned here, probably the most widely useful application for NLP is classification</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>15430</td>\n",
       "      <td>15430</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>I mentioned here, probably the most widely useful application for NLP is classification</td>\n",
       "      <td>and as we've discussed in computer vision, classification refers to taking an object</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>15431</td>\n",
       "      <td>15431</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>US Patent Phrase to Phase Matching Kaggle competition</td>\n",
       "      <td>and as we've discussed in computer vision, classification refers to taking an object</td>\n",
       "      <td>and trying to identify a category that object belongs to. So, previously we've mainly been</td>\n",
       "      <td>True</td>\n",
       "      <td>and trying to identify a category that object belongs to. So, previously we've mainly been</td>\n",
       "      <td>[actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>15432</td>\n",
       "      <td>15432</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>and trying to identify a category that object belongs to. So, previously we've mainly been</td>\n",
       "      <td>looking at images. Today we're going to be looking at documents. Now, in NLP when we</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identificat...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>15433</td>\n",
       "      <td>15433</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>looking at images. Today we're going to be looking at documents. Now, in NLP when we</td>\n",
       "      <td>say document, we don't specifically mean, you know, a 20 page long, you know, essay.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author ident...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>15434</td>\n",
       "      <td>15434</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>say document, we don't specifically mean, you know, a 20 page long, you know, essay.</td>\n",
       "      <td>A document could be three or four words, or a document could be the entire encyclopedia.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identific...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>15435</td>\n",
       "      <td>15435</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>A document could be three or four words, or a document could be the entire encyclopedia.</td>\n",
       "      <td>So a document is just an input to an NLP model that contains text. Now, classifying a document,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identification would...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>15436</td>\n",
       "      <td>15436</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>So a document is just an input to an NLP model that contains text. Now, classifying a document,</td>\n",
       "      <td>so deciding what category a document belongs to, is a surprisingly rich thing to do. There's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identification would be ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>15437</td>\n",
       "      <td>15437</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>so deciding what category a document belongs to, is a surprisingly rich thing to do. There's</td>\n",
       "      <td>all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, positive or negative sentiment. Author identification would be taking a document and trying to find, the category of author. Legal discovery wou...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>15438</td>\n",
       "      <td>15438</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:</td>\n",
       "      <td>positive or negative sentiment. Author identification would be taking a document and trying to find</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, the category of author. Legal discovery would be t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>15439</td>\n",
       "      <td>15439</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>positive or negative sentiment. Author identification would be taking a document and trying to find</td>\n",
       "      <td>the category of author. Legal discovery would be taking documents and putting them into</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>15440</td>\n",
       "      <td>15440</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>the category of author. Legal discovery would be taking documents and putting them into</td>\n",
       "      <td>categories according to in- or out-of-scope for a court case. Triaging inbound emails</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>15441</td>\n",
       "      <td>15441</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>categories according to in- or out-of-scope for a court case. Triaging inbound emails</td>\n",
       "      <td>would be putting them into categories of, you know, throw away, send to customer service,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>15442</td>\n",
       "      <td>15442</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>would be putting them into categories of, you know, throw away, send to customer service,</td>\n",
       "      <td>send to sales, etc. Right? So classification is a very, very rich area, and for people</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>15443</td>\n",
       "      <td>15443</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>send to sales, etc. Right? So classification is a very, very rich area, and for people</td>\n",
       "      <td>interested in trying out NLP in real life, I would suggest classification would be the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>15444</td>\n",
       "      <td>15444</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>interested in trying out NLP in real life, I would suggest classification would be the</td>\n",
       "      <td>place I would start, for looking for, kind of, accessible, real world, useful problems</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>15445</td>\n",
       "      <td>15445</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>place I would start, for looking for, kind of, accessible, real world, useful problems</td>\n",
       "      <td>you can solve right away. Now, the Kaggle competition does not immediately look like a classification competition. What</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>15446</td>\n",
       "      <td>15446</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>you can solve right away. Now, the Kaggle competition does not immediately look like a classification competition. What</td>\n",
       "      <td>it contains Let me show you some data</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>15447</td>\n",
       "      <td>15447</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>it contains Let me show you some data</td>\n",
       "      <td>What it contains is data that looks like this. It has a thing that they call anchor, a thing they call target, a thing they call context, and a score. Now these</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>15448</td>\n",
       "      <td>15448</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>What it contains is data that looks like this. It has a thing that they call anchor, a thing they call target, a thing they call context, and a score. Now these</td>\n",
       "      <td>are.. I can't remember exact details but I think these are from patents, and I think</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>15449</td>\n",
       "      <td>15449</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>are.. I can't remember exact details but I think these are from patents, and I think</td>\n",
       "      <td>on the patents there are various, like, things they have to fill in in the patent, and one</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>15450</td>\n",
       "      <td>15450</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>on the patents there are various, like, things they have to fill in in the patent, and one</td>\n",
       "      <td>of those things is called anchor, one of those things is called target and in the competition the goal is to come up with a model that automatically determines</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>15451</td>\n",
       "      <td>15451</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>of those things is called anchor, one of those things is called target and in the competition the goal is to come up with a model that automatically determines</td>\n",
       "      <td>which anchor and target pairs are talking about the same thing. So a score of one here</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>15452</td>\n",
       "      <td>15452</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>which anchor and target pairs are talking about the same thing. So a score of one here</td>\n",
       "      <td>wood article and wooden article obviously talking about the same thing. A</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>15453</td>\n",
       "      <td>15453</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>wood article and wooden article obviously talking about the same thing. A</td>\n",
       "      <td>score of zero here abatement and forest region not talking about the same thing. So the basic idea is we're trying to guess the score. And it's kind of a classification</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>15454</td>\n",
       "      <td>15454</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>score of zero here abatement and forest region not talking about the same thing. So the basic idea is we're trying to guess the score. And it's kind of a classification</td>\n",
       "      <td>problem, kind of not. We're basically trying to classify things into either these two things are the same or these two things aren't the same. It's kind of not because</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>15455</td>\n",
       "      <td>15455</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>problem, kind of not. We're basically trying to classify things into either these two things are the same or these two things aren't the same. It's kind of not because</td>\n",
       "      <td>we have not just 1 and 0 but also 0.25, 0.5 and 0.75. There's also a column called context,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>15456</td>\n",
       "      <td>15456</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>we have not just 1 and 0 but also 0.25, 0.5 and 0.75. There's also a column called context,</td>\n",
       "      <td>which is, I believe, is like the category that this patent was filed in and my understanding</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>15457</td>\n",
       "      <td>15457</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>which is, I believe, is like the category that this patent was filed in and my understanding</td>\n",
       "      <td>is that whether the anchor and the target count as similar or not depends on, you know,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>15458</td>\n",
       "      <td>15458</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>is that whether the anchor and the target count as similar or not depends on, you know,</td>\n",
       "      <td>what the patent was filed under. So how would we take this and turn it into something like</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>15459</td>\n",
       "      <td>15459</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>what the patent was filed under. So how would we take this and turn it into something like</td>\n",
       "      <td>a classification problem? So the suggestion I make here is that we could basically say, okay, let's put the, you know,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>15460</td>\n",
       "      <td>15460</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>a classification problem? So the suggestion I make here is that we could basically say, okay, let's put the, you know,</td>\n",
       "      <td>some constant string like TEXT1 or FIELD1 before the first column and then something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>15461</td>\n",
       "      <td>15461</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>some constant string like TEXT1 or FIELD1 before the first column and then something</td>\n",
       "      <td>else like TEXT2 before the second column. Oh, and maybe, also the context, I should</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>15462</td>\n",
       "      <td>15462</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>else like TEXT2 before the second column. Oh, and maybe, also the context, I should</td>\n",
       "      <td>have as well TEXT3 in the context, and then try to choose a category of meaning similarity: Different Similar or Identical. So you can basically concatenate those three</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>15463</td>\n",
       "      <td>15463</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>have as well TEXT3 in the context, and then try to choose a category of meaning similarity: Different Similar or Identical. So you can basically concatenate those three</td>\n",
       "      <td>pieces together, call that a document and then try to train a model that can predict</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>15464</td>\n",
       "      <td>15464</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>pieces together, call that a document and then try to train a model that can predict</td>\n",
       "      <td>these categories. That would be an example of how we can take this, basically, similarity</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>15465</td>\n",
       "      <td>15465</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>these categories. That would be an example of how we can take this, basically, similarity</td>\n",
       "      <td>problem, and turn it into something that looks like a classification problem. And we tend to do this a lot in deep learning, is we kind of take problems that look a bit novel and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>15466</td>\n",
       "      <td>15466</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>problem, and turn it into something that looks like a classification problem. And we tend to do this a lot in deep learning, is we kind of take problems that look a bit novel and</td>\n",
       "      <td>different, and turn them into a problem that looks like something we recognize. All right,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>15467</td>\n",
       "      <td>15467</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>different, and turn them into a problem that looks like something we recognize. All right,</td>\n",
       "      <td>so on Kaggle this is a, you know, larger data set that you're going to need a GPU to run.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>15468</td>\n",
       "      <td>15468</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>NLP Classification</td>\n",
       "      <td>so on Kaggle this is a, you know, larger data set that you're going to need a GPU to run.</td>\n",
       "      <td>So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically.</td>\n",
       "      <td>True</td>\n",
       "      <td>So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically.</td>\n",
       "      <td>[and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>15469</td>\n",
       "      <td>15469</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically.</td>\n",
       "      <td>Personally, you know, I like using things like Paperspace generally better than Kaggle,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if you want to. So I basically create, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd, say if iskaggle and put those changes., So here, you can see h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>15470</td>\n",
       "      <td>15470</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>Personally, you know, I like using things like Paperspace generally better than Kaggle,</td>\n",
       "      <td>like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if you want to. So I basically create, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd, say if iskaggle and put those changes., So here, yo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>15471</td>\n",
       "      <td>15471</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's</td>\n",
       "      <td>some information here, I won't go through but it basically describes how you can download</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, stuff to Paperspace or your own computer as well if you want to. So I basically create, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd, say if iskaggle and put those changes., So here, you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>15472</td>\n",
       "      <td>15472</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>some information here, I won't go through but it basically describes how you can download</td>\n",
       "      <td>stuff to Paperspace or your own computer as well if you want to. So I basically create</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any li...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>15473</td>\n",
       "      <td>15473</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>stuff to Paperspace or your own computer as well if you want to. So I basically create</td>\n",
       "      <td>this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, say if iskaggle and put those changes., So here, y...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>15474</td>\n",
       "      <td>15474</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd</td>\n",
       "      <td>say if iskaggle and put those changes.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>15475</td>\n",
       "      <td>15475</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>say if iskaggle and put those changes.</td>\n",
       "      <td>So here, you can see here, if I'm not on Kaggle and I don't have the data yet, then download</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>15476</td>\n",
       "      <td>15476</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>So here, you can see here, if I'm not on Kaggle and I don't have the data yet, then download</td>\n",
       "      <td>it. And Kaggle has a little API which is quite handy for doing stuff like downloading data and uploading notebooks and stuff like that, submitting to competitions.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>15477</td>\n",
       "      <td>15477</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>it. And Kaggle has a little API which is quite handy for doing stuff like downloading data and uploading notebooks and stuff like that, submitting to competitions.</td>\n",
       "      <td>If we are on Kaggle then the data's already going to be there for us which is actually a good reason for beginners to use Kaggle as you don't have to worry about grabbing</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>15478</td>\n",
       "      <td>15478</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>If we are on Kaggle then the data's already going to be there for us which is actually a good reason for beginners to use Kaggle as you don't have to worry about grabbing</td>\n",
       "      <td>the data at all  it's sitting there for you as soon as you open the notebook.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>15479</td>\n",
       "      <td>15479</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>the data at all  it's sitting there for you as soon as you open the notebook.</td>\n",
       "      <td>Kaggle has a lot of python packages installed, but not necessarily all the ones you want,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>15480</td>\n",
       "      <td>15480</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>Kaggle has a lot of python packages installed, but not necessarily all the ones you want,</td>\n",
       "      <td>and at the point I wrote this they didn't have the Hugging Faces datasets package, for some reason, so you can always just install stuff. So you might remember the exclamation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>15481</td>\n",
       "      <td>15481</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>and at the point I wrote this they didn't have the Hugging Faces datasets package, for some reason, so you can always just install stuff. So you might remember the exclamation</td>\n",
       "      <td>mark means this is not a python command, but a shell command, a bash command. But it's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>15482</td>\n",
       "      <td>15482</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>mark means this is not a python command, but a shell command, a bash command. But it's</td>\n",
       "      <td>quite neat you can even put bash commands inside python conditionals so that's a pretty</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>15483</td>\n",
       "      <td>15483</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>quite neat you can even put bash commands inside python conditionals so that's a pretty</td>\n",
       "      <td>cool little trick in notebooks.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>15484</td>\n",
       "      <td>15484</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>cool little trick in notebooks.</td>\n",
       "      <td>Another cool little trick in notebooks is that if you do use a bash command like ls</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>15485</td>\n",
       "      <td>15485</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>Another cool little trick in notebooks is that if you do use a bash command like ls</td>\n",
       "      <td>but you then want to insert the contents of a python variable, just chuck it in parentheses. So, I've got a python variable called path and I can go ls {path} in parentheses</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>15486</td>\n",
       "      <td>15486</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>but you then want to insert the contents of a python variable, just chuck it in parentheses. So, I've got a python variable called path and I can go ls {path} in parentheses</td>\n",
       "      <td>and that will ls the contents of the python variable path. So there's another little trick for you.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>15487</td>\n",
       "      <td>15487</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>and that will ls the contents of the python variable path. So there's another little trick for you.</td>\n",
       "      <td>All right, so when we ls that we can see that there's some CSV files. So what I'm</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>15488</td>\n",
       "      <td>15488</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>All right, so when we ls that we can see that there's some CSV files. So what I'm</td>\n",
       "      <td>going to do is, kind of, take you through, roughly the process, the kind of process I, you know, went through as, you know when I first look at a competition. So the first</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>15489</td>\n",
       "      <td>15489</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>going to do is, kind of, take you through, roughly the process, the kind of process I, you know, went through as, you know when I first look at a competition. So the first</td>\n",
       "      <td>thing is like, already a data set, indeed, what's in it? Okay, so it's got some CSV files.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>15490</td>\n",
       "      <td>15490</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>thing is like, already a data set, indeed, what's in it? Okay, so it's got some CSV files.</td>\n",
       "      <td>You know, as well as looking at it here, the other thing I would do is I would go to the competition website</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>15491</td>\n",
       "      <td>15491</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>You know, as well as looking at it here, the other thing I would do is I would go to the competition website</td>\n",
       "      <td>and if you go to Data A lot of people skip over this, which is a terrible idea, because it actually tells you</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>15492</td>\n",
       "      <td>15492</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>and if you go to Data A lot of people skip over this, which is a terrible idea, because it actually tells you</td>\n",
       "      <td>what the dependent variable means, what the different files are, what the columns are, and so forth. So don't just rely on looking at the data itself but look at the information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>15493</td>\n",
       "      <td>15493</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>what the dependent variable means, what the different files are, what the columns are, and so forth. So don't just rely on looking at the data itself but look at the information</td>\n",
       "      <td>that you're given about the data. So, for CSV files, so CSV files are comma separated values, so they're just text files</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>15494</td>\n",
       "      <td>15494</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>that you're given about the data. So, for CSV files, so CSV files are comma separated values, so they're just text files</td>\n",
       "      <td>with a comma between each field, and we can read them using pandas, which for some reason</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>15495</td>\n",
       "      <td>15495</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>with a comma between each field, and we can read them using pandas, which for some reason</td>\n",
       "      <td>is always called pd. Pandas is one of, I guess, like, (I'm trying to think) probably</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>15496</td>\n",
       "      <td>15496</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kaggle configs, insert python in bash, read competition website</td>\n",
       "      <td>is always called pd. Pandas is one of, I guess, like, (I'm trying to think) probably</td>\n",
       "      <td>like four key libraries that you have to know to do data science in python.</td>\n",
       "      <td>True</td>\n",
       "      <td>like four key libraries that you have to know to do data science in python.</td>\n",
       "      <td>[So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>15497</td>\n",
       "      <td>15497</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>like four key libraries that you have to know to do data science in python.</td>\n",
       "      <td>And specifically, those four libraries are: numpy matplotlib pandas and pytorch.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>15498</td>\n",
       "      <td>15498</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>And specifically, those four libraries are: numpy matplotlib pandas and pytorch.</td>\n",
       "      <td>So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and an...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>15499</td>\n",
       "      <td>15499</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for</td>\n",
       "      <td>plotting; pandas we use for tables of data; and pytorch we use for deep learning.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>15500</td>\n",
       "      <td>15500</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>plotting; pandas we use for tables of data; and pytorch we use for deep learning.</td>\n",
       "      <td>Those are all covered in a fantastic book by the author of pandas which, the new version</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>15501</td>\n",
       "      <td>15501</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>Those are all covered in a fantastic book by the author of pandas which, the new version</td>\n",
       "      <td>is actually available for free, I believe. Python for data analysis. So if you're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and an...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>15502</td>\n",
       "      <td>15502</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>is actually available for free, I believe. Python for data analysis. So if you're</td>\n",
       "      <td>not familiar with these libraries just read the whole book, it doesn't take too long to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>15503</td>\n",
       "      <td>15503</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>not familiar with these libraries just read the whole book, it doesn't take too long to</td>\n",
       "      <td>get through, and it's got lots of cool tips and it's very readable. I do find a lot of</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, people doing this course often I see people kind of, trying to jump ahead, and and,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>15504</td>\n",
       "      <td>15504</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>get through, and it's got lots of cool tips and it's very readable. I do find a lot of</td>\n",
       "      <td>people doing this course often I see people kind of, trying to jump ahead, and and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>15505</td>\n",
       "      <td>15505</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>people doing this course often I see people kind of, trying to jump ahead, and and</td>\n",
       "      <td>want to be like: Oh I want to know how to, like, create a new architecture or Build a speech recognition system or whatever. But it then turns out that they</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>15506</td>\n",
       "      <td>15506</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>want to be like: Oh I want to know how to, like, create a new architecture or Build a speech recognition system or whatever. But it then turns out that they</td>\n",
       "      <td>don't know how to use these fundamental libraries. So it's always good to be bold and be trying to build things, but do also take the time to, you know, make sure you finish reading</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>15507</td>\n",
       "      <td>15507</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>don't know how to use these fundamental libraries. So it's always good to be bold and be trying to build things, but do also take the time to, you know, make sure you finish reading</td>\n",
       "      <td>the fast.ai book and read at least Wes McKinney's book. That would be enough to really give</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>15508</td>\n",
       "      <td>15508</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>the fast.ai book and read at least Wes McKinney's book. That would be enough to really give</td>\n",
       "      <td>you all the basic knowledge you need, I think. So, with pandas we can read a CSV file and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>15509</td>\n",
       "      <td>15509</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>you all the basic knowledge you need, I think. So, with pandas we can read a CSV file and</td>\n",
       "      <td>that creates something called a DataFrame, which is just a table of data, as you see.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>15510</td>\n",
       "      <td>15510</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>that creates something called a DataFrame, which is just a table of data, as you see.</td>\n",
       "      <td>So, now that we've got a DataFrame, we can see what we're working with, and when we ask</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>15511</td>\n",
       "      <td>15511</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>So, now that we've got a DataFrame, we can see what we're working with, and when we ask</td>\n",
       "      <td>when in jupyter we just put the name of a variable containing a DataFrame, we get the first five rows, the last five rows, and the size. So we've got 36,473 rows. Okay, so other</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>15512</td>\n",
       "      <td>15512</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>when in jupyter we just put the name of a variable containing a DataFrame, we get the first five rows, the last five rows, and the size. So we've got 36,473 rows. Okay, so other</td>\n",
       "      <td>things I like to use for understanding a DataFrame is the describe method.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>15513</td>\n",
       "      <td>15513</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>things I like to use for understanding a DataFrame is the describe method.</td>\n",
       "      <td>If you pass include equals object that will describe, that will describe, basically all the kind of the string fields, the non-numeric fields. So, in this case there's four of those,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>15514</td>\n",
       "      <td>15514</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>If you pass include equals object that will describe, that will describe, basically all the kind of the string fields, the non-numeric fields. So, in this case there's four of those,</td>\n",
       "      <td>and so you can see here that, that anchor field we looked at, there's actually only 733 unique values, okay, so this thing, you can see that there's lots of repetition out</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>15515</td>\n",
       "      <td>15515</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>and so you can see here that, that anchor field we looked at, there's actually only 733 unique values, okay, so this thing, you can see that there's lots of repetition out</td>\n",
       "      <td>of 36,000. So there's lots of repetition. This is the most common one: it appears 152</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>15516</td>\n",
       "      <td>15516</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>of 36,000. So there's lots of repetition. This is the most common one: it appears 152</td>\n",
       "      <td>times. And then context, we also see lots of repetition  there's 106 of those contexts. So, this is a nice little method, we can see a lot about the data in a glance.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>15517</td>\n",
       "      <td>15517</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>times. And then context, we also see lots of repetition  there's 106 of those contexts. So, this is a nice little method, we can see a lot about the data in a glance.</td>\n",
       "      <td>And when I first saw this in this competition I thought: well this is actually not that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>15518</td>\n",
       "      <td>15518</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>And when I first saw this in this competition I thought: well this is actually not that</td>\n",
       "      <td>much language data, when you think about it. The, you know Each document is very short,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>15519</td>\n",
       "      <td>15519</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>much language data, when you think about it. The, you know Each document is very short,</td>\n",
       "      <td>you know, three or four words really, and lots of it is repeated. So that's like</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>15520</td>\n",
       "      <td>15520</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>you know, three or four words really, and lots of it is repeated. So that's like</td>\n",
       "      <td>as I'm looking through it I'm thinking, like, what are some key features of this data set?. And that would be something, I'd be thinking, well, that's, you know, we've</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>15521</td>\n",
       "      <td>15521</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>as I'm looking through it I'm thinking, like, what are some key features of this data set?. And that would be something, I'd be thinking, well, that's, you know, we've</td>\n",
       "      <td>got to do a lot with not very much unique data here.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>15522</td>\n",
       "      <td>15522</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>got to do a lot with not very much unique data here.</td>\n",
       "      <td>So here's how we can just go ahead and create a single string like I described which contains,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>15523</td>\n",
       "      <td>15523</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>So here's how we can just go ahead and create a single string like I described which contains,</td>\n",
       "      <td>you know, some kind of field separator, plus the context, the target and the anchor. So</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>15524</td>\n",
       "      <td>15524</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>you know, some kind of field separator, plus the context, the target and the anchor. So</td>\n",
       "      <td>we're going to pop that into a field called input. Something slightly weird in pandas</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>15525</td>\n",
       "      <td>15525</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>we're going to pop that into a field called input. Something slightly weird in pandas</td>\n",
       "      <td>is there's two ways of referring to a column. You can use square brackets and a string to get the input column or you can just treat it as an attribute. When you're setting it,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>15526</td>\n",
       "      <td>15526</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>is there's two ways of referring to a column. You can use square brackets and a string to get the input column or you can just treat it as an attribute. When you're setting it,</td>\n",
       "      <td>you should always use the form seen here (... df [input]= ) When reading it you can use either. I tend to use this one because it's less typing.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>15527</td>\n",
       "      <td>15527</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>you should always use the form seen here (... df [input]= ) When reading it you can use either. I tend to use this one because it's less typing.</td>\n",
       "      <td>So you can see now we've got this/these concatenated rows. So, head() is the first few rows.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>15528</td>\n",
       "      <td>15528</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>So you can see now we've got this/these concatenated rows. So, head() is the first few rows.</td>\n",
       "      <td>So we've now got some documents to do NLP with. Now, the problem is, as you know from</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>15529</td>\n",
       "      <td>15529</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>So we've now got some documents to do NLP with. Now, the problem is, as you know from</td>\n",
       "      <td>the last lesson, neural networks work with numbers. All right, we're going to take some</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>15530</td>\n",
       "      <td>15530</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pandas, numpy, matplotlib, &amp; pytorch</td>\n",
       "      <td>the last lesson, neural networks work with numbers. All right, we're going to take some</td>\n",
       "      <td>numbers and we're going to multiply them by matrices, we're going to replace the negatives</td>\n",
       "      <td>True</td>\n",
       "      <td>numbers and we're going to multiply them by matrices, we're going to replace the negatives</td>\n",
       "      <td>[like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>15531</td>\n",
       "      <td>15531</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>numbers and we're going to multiply them by matrices, we're going to replace the negatives</td>\n",
       "      <td>with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even, the pieces are not next to each other. Another reason is that, what we're going t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that</td>\n",
       "      <td>for these strings? So there's basically two steps we're going to take.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even, the pieces are not next to each other. Another reason is that...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>15533</td>\n",
       "      <td>15533</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>for these strings? So there's basically two steps we're going to take.</td>\n",
       "      <td>The first step is to split each of these into tokens. Tokens are basically words. We're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>15534</td>\n",
       "      <td>15534</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>The first step is to split each of these into tokens. Tokens are basically words. We're</td>\n",
       "      <td>going to split it into words. There's a few problems with splitting things into words,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>15535</td>\n",
       "      <td>15535</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>going to split it into words. There's a few problems with splitting things into words,</td>\n",
       "      <td>though. The first is that some languages like chinese don't have words, right, or at least</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>15536</td>\n",
       "      <td>15536</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>though. The first is that some languages like chinese don't have words, right, or at least</td>\n",
       "      <td>certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, the pieces are not next to each other. Another reason is that, what we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>15537</td>\n",
       "      <td>15537</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even</td>\n",
       "      <td>the pieces are not next to each other. Another reason is that, what we're going to be doing is, after we've split it into words, or something like words, we're going to be getting a list</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>15538</td>\n",
       "      <td>15538</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>the pieces are not next to each other. Another reason is that, what we're going to be doing is, after we've split it into words, or something like words, we're going to be getting a list</td>\n",
       "      <td>of all of the unique words that appear, which is called the vocabulary, and every one of those unique words is going to get a number. As you'll see later on the bigger the vocabulary,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>15539</td>\n",
       "      <td>15539</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>of all of the unique words that appear, which is called the vocabulary, and every one of those unique words is going to get a number. As you'll see later on the bigger the vocabulary,</td>\n",
       "      <td>the more memory is going to get used, the more data we'll need to train. In general</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>15540</td>\n",
       "      <td>15540</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>the more memory is going to get used, the more data we'll need to train. In general</td>\n",
       "      <td>we don't want a vocabulary to be too big.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>15541</td>\n",
       "      <td>15541</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>we don't want a vocabulary to be too big.</td>\n",
       "      <td>So instead, nowadays, people tend to tokenize into something called subwords which is pieces</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>15542</td>\n",
       "      <td>15542</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>So instead, nowadays, people tend to tokenize into something called subwords which is pieces</td>\n",
       "      <td>of words  so I'll show you what it looks like. So the process of turning it into smaller</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>15543</td>\n",
       "      <td>15543</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>of words  so I'll show you what it looks like. So the process of turning it into smaller</td>\n",
       "      <td>units like words, it's called tokenization  and we call them tokens instead of words. The token is just like the more general concept of, like, whatever we're putting it into.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>15544</td>\n",
       "      <td>15544</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>units like words, it's called tokenization  and we call them tokens instead of words. The token is just like the more general concept of, like, whatever we're putting it into.</td>\n",
       "      <td>So we're going to get Hugging Face transformers and Hugging Face datasets doing our work for</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>15545</td>\n",
       "      <td>15545</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>So we're going to get Hugging Face transformers and Hugging Face datasets doing our work for</td>\n",
       "      <td>us, and so, what we're going to do is we're going to turn our pandas DataFrame into a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>15546</td>\n",
       "      <td>15546</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>us, and so, what we're going to do is we're going to turn our pandas DataFrame into a</td>\n",
       "      <td>Hugging Face datasets Dataset. It's a bit confusing: pytorch has a class called</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>15547</td>\n",
       "      <td>15547</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>Hugging Face datasets Dataset. It's a bit confusing: pytorch has a class called</td>\n",
       "      <td>Dataset and Hugging Face has a class called Dataset and they're different things, okay, so this is a Hugging Face Dataset. Hugging Face datasets dataset. So we can turn a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>15548</td>\n",
       "      <td>15548</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>Dataset and Hugging Face has a class called Dataset and they're different things, okay, so this is a Hugging Face Dataset. Hugging Face datasets dataset. So we can turn a</td>\n",
       "      <td>DataFrame into a Dataset just using the from_pandas method and so we've now got a Dataset. So,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>15549</td>\n",
       "      <td>15549</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>DataFrame into a Dataset just using the from_pandas method and so we've now got a Dataset. So,</td>\n",
       "      <td>if we take a look it just tells us: all right it's got these features, okay? And remember</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>15550</td>\n",
       "      <td>15550</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>if we take a look it just tells us: all right it's got these features, okay? And remember</td>\n",
       "      <td>input is the one we just created with the concatenated strings and here's those 36,000 rows.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>15551</td>\n",
       "      <td>15551</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>input is the one we just created with the concatenated strings and here's those 36,000 rows.</td>\n",
       "      <td>Okay, so now we're going to do these two things. Tokenization, which is to split each text</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>15552</td>\n",
       "      <td>15552</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>Okay, so now we're going to do these two things. Tokenization, which is to split each text</td>\n",
       "      <td>up into tokens, and the numericalization, which is to turn each token into its unique</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>15553</td>\n",
       "      <td>15553</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>up into tokens, and the numericalization, which is to turn each token into its unique</td>\n",
       "      <td>id based on where it is in the vocabulary. The vocabulary, remember, being the unique,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>15554</td>\n",
       "      <td>15554</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>id based on where it is in the vocabulary. The vocabulary, remember, being the unique,</td>\n",
       "      <td>the list of unique tokens. Now, particularly in this stage: tokenization, there's a lot</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>15555</td>\n",
       "      <td>15555</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>the list of unique tokens. Now, particularly in this stage: tokenization, there's a lot</td>\n",
       "      <td>of little decisions that have to be made. The good news is you don't have to make them</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>15556</td>\n",
       "      <td>15556</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>of little decisions that have to be made. The good news is you don't have to make them</td>\n",
       "      <td>because whatever pre-trained model you used the people that pre-trained it made some decisions,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>15557</td>\n",
       "      <td>15557</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>because whatever pre-trained model you used the people that pre-trained it made some decisions,</td>\n",
       "      <td>and you're going to have to do exactly the same thing, otherwise you'll end up with a different vocabulary to them and that's going to mess everything up. So that means before</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>15558</td>\n",
       "      <td>15558</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>and you're going to have to do exactly the same thing, otherwise you'll end up with a different vocabulary to them and that's going to mess everything up. So that means before</td>\n",
       "      <td>you start tokenizing you have to decide on what model to use. Hugging Face transformers</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>15559</td>\n",
       "      <td>15559</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>you start tokenizing you have to decide on what model to use. Hugging Face transformers</td>\n",
       "      <td>is a lot like timm. It has a library of, I believe, hundreds of models.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>15560</td>\n",
       "      <td>15560</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>is a lot like timm. It has a library of, I believe, hundreds of models.</td>\n",
       "      <td>I guess I shouldn't say Hugging Face transformers. It's really the Hugging Face model hub. 44,000</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>15561</td>\n",
       "      <td>15561</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tokenization</td>\n",
       "      <td>I guess I shouldn't say Hugging Face transformers. It's really the Hugging Face model hub. 44,000</td>\n",
       "      <td>models, so even many more even than timm's image models. And so, these models, they vary</td>\n",
       "      <td>True</td>\n",
       "      <td>models, so even many more even than timm's image models. And so, these models, they vary</td>\n",
       "      <td>[numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>15562</td>\n",
       "      <td>15562</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>models, so even many more even than timm's image models. And so, these models, they vary</td>\n",
       "      <td>in a couple of ways. There's a variety of different architectures, just like in timm</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>15563</td>\n",
       "      <td>15563</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>in a couple of ways. There's a variety of different architectures, just like in timm</td>\n",
       "      <td>but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there are some just generally pretty good models that work for a lot of, things a lot of the ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>15564</td>\n",
       "      <td>15564</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could</td>\n",
       "      <td>type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there are some just generally pretty good models that work for a lot of, things a lot of the time, a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>15565</td>\n",
       "      <td>15565</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,</td>\n",
       "      <td>there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there are s...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>15566</td>\n",
       "      <td>15566</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,</td>\n",
       "      <td>thanks to the Hugging Face model hub, you can start your pre-trained model with something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>15567</td>\n",
       "      <td>15567</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>thanks to the Hugging Face model hub, you can start your pre-trained model with something</td>\n",
       "      <td>that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, Having said that, there are some just generally pretty go...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>15568</td>\n",
       "      <td>15568</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents.</td>\n",
       "      <td>Having said that, there are some just generally pretty good models that work for a lot of</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>15569</td>\n",
       "      <td>15569</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>Having said that, there are some just generally pretty good models that work for a lot of</td>\n",
       "      <td>things a lot of the time, and deberta-v3 is certainly one of those. This is a very new</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>15570</td>\n",
       "      <td>15570</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>things a lot of the time, and deberta-v3 is certainly one of those. This is a very new</td>\n",
       "      <td>area. NLP has been, like, practically, really effective for, you know, general users, for</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>15571</td>\n",
       "      <td>15571</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>area. NLP has been, like, practically, really effective for, you know, general users, for</td>\n",
       "      <td>only a year or two, whereas for computer vision it's been quite a while. So you'll see, you'll</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>15572</td>\n",
       "      <td>15572</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>only a year or two, whereas for computer vision it's been quite a while. So you'll see, you'll</td>\n",
       "      <td>find that a lot of things aren't quite as well bedded down. I don't have a picture to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>15573</td>\n",
       "      <td>15573</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>find that a lot of things aren't quite as well bedded down. I don't have a picture to</td>\n",
       "      <td>show you of which models are the best or the fastest and the most accurate and whatever,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>15574</td>\n",
       "      <td>15574</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>show you of which models are the best or the fastest and the most accurate and whatever,</td>\n",
       "      <td>right? This, a lot of this stuff is, like stuff that we're figuring out as a community</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>15575</td>\n",
       "      <td>15575</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>right? This, a lot of this stuff is, like stuff that we're figuring out as a community</td>\n",
       "      <td>using competitions like this, in fact. And this is one of the first NLP competitions,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>15576</td>\n",
       "      <td>15576</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>using competitions like this, in fact. And this is one of the first NLP competitions,</td>\n",
       "      <td>actually, in the kind of modern NLP era. So, you know, we've been studying these competitions</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>15577</td>\n",
       "      <td>15577</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>actually, in the kind of modern NLP era. So, you know, we've been studying these competitions</td>\n",
       "      <td>closely and yes, I can tell you that deberta-v3 is actually a really good starting point for</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>15578</td>\n",
       "      <td>15578</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>closely and yes, I can tell you that deberta-v3 is actually a really good starting point for</td>\n",
       "      <td>a lot of things so that's why we've picked it. So we pick our model and just like in timm for image, you know, models there's often going to be a small, a medium, a large</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>15579</td>\n",
       "      <td>15579</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>a lot of things so that's why we've picked it. So we pick our model and just like in timm for image, you know, models there's often going to be a small, a medium, a large</td>\n",
       "      <td>and of course we should start with small, right, because small is going to be faster to train we're going to be able to do more iterations.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>15580</td>\n",
       "      <td>15580</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>and of course we should start with small, right, because small is going to be faster to train we're going to be able to do more iterations.</td>\n",
       "      <td>and so forth. Okay.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>15581</td>\n",
       "      <td>15581</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>and so forth. Okay.</td>\n",
       "      <td>So at this point remember the only reason we picked our model is because we have to make sure we tokenize in the same way.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>15582</td>\n",
       "      <td>15582</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>So at this point remember the only reason we picked our model is because we have to make sure we tokenize in the same way.</td>\n",
       "      <td>To tell transformers that we want to tokenize the same way that the people that built a model did, we use something called AutoTokenizer. It's nothing fancy, it's basically just a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>15583</td>\n",
       "      <td>15583</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>To tell transformers that we want to tokenize the same way that the people that built a model did, we use something called AutoTokenizer. It's nothing fancy, it's basically just a</td>\n",
       "      <td>dictionary which says: oh, which model uses which tokenizer?. So when we say AutoTokenizer.from_pretrained</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>15584</td>\n",
       "      <td>15584</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>dictionary which says: oh, which model uses which tokenizer?. So when we say AutoTokenizer.from_pretrained</td>\n",
       "      <td>it will download the vocabulary and the details about how this particular model tokenized</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>15585</td>\n",
       "      <td>15585</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>it will download the vocabulary and the details about how this particular model tokenized</td>\n",
       "      <td>the dataset. So, at this point we can now take that tokenizer and pass a string to it.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>15586</td>\n",
       "      <td>15586</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Huggingface model hub</td>\n",
       "      <td>the dataset. So, at this point we can now take that tokenizer and pass a string to it.</td>\n",
       "      <td>So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's</td>\n",
       "      <td>True</td>\n",
       "      <td>So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's</td>\n",
       "      <td>[models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>15587</td>\n",
       "      <td>15587</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's</td>\n",
       "      <td>kind of putting it into words, kind of not. So if you've ever wondered whether g'day</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>15588</td>\n",
       "      <td>15588</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>kind of putting it into words, kind of not. So if you've ever wondered whether g'day</td>\n",
       "      <td>is one word or two you know it's actually three tokens according to this tokenizer.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 devel...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>15589</td>\n",
       "      <td>15589</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>is one word or two you know it's actually three tokens according to this tokenizer.</td>\n",
       "      <td>And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 develo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>15590</td>\n",
       "      <td>15590</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token.</td>\n",
       "      <td>And so, you kind of get the idea. These underscores here? That represents the start of a word,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 developers used.,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>15591</td>\n",
       "      <td>15591</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>And so, you kind of get the idea. These underscores here? That represents the start of a word,</td>\n",
       "      <td>right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 developers used., So here's a less common (unless you're a big platypus fan like me), less common se...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>15592</td>\n",
       "      <td>15592</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the</td>\n",
       "      <td>start of a word, that kind of means a different thing. So this is what happens when we tokenize</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, this sentence using the tokenizer that the deberta-v3 developers used., So here's a less common (unless you're a big platypus fan like me), less common sen...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>15593</td>\n",
       "      <td>15593</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>start of a word, that kind of means a different thing. So this is what happens when we tokenize</td>\n",
       "      <td>this sentence using the tokenizer that the deberta-v3 developers used.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>15594</td>\n",
       "      <td>15594</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>this sentence using the tokenizer that the deberta-v3 developers used.</td>\n",
       "      <td>So here's a less common (unless you're a big platypus fan like me), less common sentence.:</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>15595</td>\n",
       "      <td>15595</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>So here's a less common (unless you're a big platypus fan like me), less common sentence.:</td>\n",
       "      <td>A platypus is an ornithorhynchus anatinus. So okay, in this particular vocabulary platypus</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>15596</td>\n",
       "      <td>15596</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>A platypus is an ornithorhynchus anatinus. So okay, in this particular vocabulary platypus</td>\n",
       "      <td>got its own word, its own token, but ornithorhynchus didn't. And so I still remember grade one,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>15597</td>\n",
       "      <td>15597</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>got its own word, its own token, but ornithorhynchus didn't. And so I still remember grade one,</td>\n",
       "      <td>for some reason our teacher got us all to learn how to spell ornithorhynchus, so, one of my favorite words. So you can see here it's been split into _or, ni,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>15598</td>\n",
       "      <td>15598</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>for some reason our teacher got us all to learn how to spell ornithorhynchus, so, one of my favorite words. So you can see here it's been split into _or, ni,</td>\n",
       "      <td>tho, rhynch, us. So every one of these tokens you see here is going to be in the vocabulary, right? The</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>15599</td>\n",
       "      <td>15599</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>tho, rhynch, us. So every one of these tokens you see here is going to be in the vocabulary, right? The</td>\n",
       "      <td>list of unique tokens that was created when this, when this particular model, this pre-trained</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>15600</td>\n",
       "      <td>15600</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>list of unique tokens that was created when this, when this particular model, this pre-trained</td>\n",
       "      <td>model, was first trained. So somewhere in that list we'll find _A (underscore</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>15601</td>\n",
       "      <td>15601</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>model, was first trained. So somewhere in that list we'll find _A (underscore</td>\n",
       "      <td>capital A), and it'll have a number and so that's how we'll be able to turn these</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>15602</td>\n",
       "      <td>15602</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>capital A), and it'll have a number and so that's how we'll be able to turn these</td>\n",
       "      <td>into numbers. So this first process is called tokenization and then the thing where we take</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>15603</td>\n",
       "      <td>15603</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>into numbers. So this first process is called tokenization and then the thing where we take</td>\n",
       "      <td>these tokens and turn them into numbers is called numericalization. So, our data set, remember we put our string into the input field so here's a function</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>15604</td>\n",
       "      <td>15604</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Examples of tokenized sentences</td>\n",
       "      <td>these tokens and turn them into numbers is called numericalization. So, our data set, remember we put our string into the input field so here's a function</td>\n",
       "      <td>that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our</td>\n",
       "      <td>True</td>\n",
       "      <td>that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our</td>\n",
       "      <td>[So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>15605</td>\n",
       "      <td>15605</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our</td>\n",
       "      <td>tokenization function. Tokenization can take a minute or two so we may as well get all</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ~...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>15606</td>\n",
       "      <td>15606</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>tokenization function. Tokenization can take a minute or two so we may as well get all</td>\n",
       "      <td>of our processes used doing it at the same time to save some time. So if you use the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>15607</td>\n",
       "      <td>15607</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>of our processes used doing it at the same time to save some time. So if you use the</td>\n",
       "      <td>dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going, to contain exactly the same as our original data set.) No sorry, it...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>15608</td>\n",
       "      <td>15608</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes</td>\n",
       "      <td>this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going, to contain exactly the same as our original data set.) No sorry, it's not going to take, exactly the same as the original data set, it's going to contain exa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>15609</td>\n",
       "      <td>15609</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with</td>\n",
       "      <td>batched=True it'll be able to do more stuff at once. So look it only took six seconds,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going, to contain exactly the same as our original data set.) No sorry, it's not goin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>15610</td>\n",
       "      <td>15610</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>batched=True it'll be able to do more stuff at once. So look it only took six seconds,</td>\n",
       "      <td>so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>15611</td>\n",
       "      <td>15611</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going</td>\n",
       "      <td>to contain exactly the same as our original data set.) No sorry, it's not going to take</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>15612</td>\n",
       "      <td>15612</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>to contain exactly the same as our original data set.) No sorry, it's not going to take</td>\n",
       "      <td>exactly the same as the original data set, it's going to contain exactly the same input as our original data set and it's also going to contain a bunch of numbers. These numbers</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>15613</td>\n",
       "      <td>15613</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>exactly the same as the original data set, it's going to contain exactly the same input as our original data set and it's also going to contain a bunch of numbers. These numbers</td>\n",
       "      <td>are the position in the vocabulary of each of the tokens in the string, so we've now</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>15614</td>\n",
       "      <td>15614</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>are the position in the vocabulary of each of the tokens in the string, so we've now</td>\n",
       "      <td>successfully turned a string into a list of numbers. That is a great first step. We can</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>15615</td>\n",
       "      <td>15615</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>successfully turned a string into a list of numbers. That is a great first step. We can</td>\n",
       "      <td>see how this works, we can see for example that we've got of at this a separate</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>15616</td>\n",
       "      <td>15616</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>see how this works, we can see for example that we've got of at this a separate</td>\n",
       "      <td>word, so that's going to be an _of in the vocabulary we can grab the vocabulary,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>15617</td>\n",
       "      <td>15617</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>word, so that's going to be an _of in the vocabulary we can grab the vocabulary,</td>\n",
       "      <td>look up _of, find that it's 265 and check here: yep here it is 265. Okay, so it's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>15618</td>\n",
       "      <td>15618</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>look up _of, find that it's 265 and check here: yep here it is 265. Okay, so it's</td>\n",
       "      <td>not rocket science right? It's just looking stuff up in a dictionary to get the numbers.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>15619</td>\n",
       "      <td>15619</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>not rocket science right? It's just looking stuff up in a dictionary to get the numbers.</td>\n",
       "      <td>Okay, so that is the tokenization and numericalization necessary in NLP to turn our documents into</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>15620</td>\n",
       "      <td>15620</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>Okay, so that is the tokenization and numericalization necessary in NLP to turn our documents into</td>\n",
       "      <td>numbers to allow us to put it into our model. Any questions so far John?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>15621</td>\n",
       "      <td>15621</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>numbers to allow us to put it into our model. Any questions so far John?</td>\n",
       "      <td>Yeah, thanks Jeremy so there's a couple and this seems like a good time to throw them out  and it's related to how you've formatted your input data into these sentences that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>15622</td>\n",
       "      <td>15622</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Numericalization</td>\n",
       "      <td>Yeah, thanks Jeremy so there's a couple and this seems like a good time to throw them out  and it's related to how you've formatted your input data into these sentences that</td>\n",
       "      <td>you've just tokenized. So one question was really about: How you choose those keywords</td>\n",
       "      <td>True</td>\n",
       "      <td>you've just tokenized. So one question was really about: How you choose those keywords</td>\n",
       "      <td>[that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>15623</td>\n",
       "      <td>15623</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>you've just tokenized. So one question was really about: How you choose those keywords</td>\n",
       "      <td>and the order of the fields that you know, so I guess just interested in an explanation,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, infor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>15624</td>\n",
       "      <td>15624</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>and the order of the fields that you know, so I guess just interested in an explanation,</td>\n",
       "      <td>is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, information, as long as it's there, right? I could h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>15625</td>\n",
       "      <td>15625</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,</td>\n",
       "      <td>you know, doesn't matter! We just want some way, something that it can learn from, right?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, information, as long as it's there, right? I could ha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>15626</td>\n",
       "      <td>15626</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>you know, doesn't matter! We just want some way, something that it can learn from, right?</td>\n",
       "      <td>So if I just concatenated it without these headers before each one, it wouldn't know</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, info...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>15627</td>\n",
       "      <td>15627</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>So if I just concatenated it without these headers before each one, it wouldn't know</td>\n",
       "      <td>where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, information, as long as it's there, right? I could have used punctuation, I could ha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>15628</td>\n",
       "      <td>15628</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're</td>\n",
       "      <td>so flexible As long as you give it the information somehow, it doesn't really matter how you give it the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>15629</td>\n",
       "      <td>15629</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>so flexible As long as you give it the information somehow, it doesn't really matter how you give it the</td>\n",
       "      <td>information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't know, one semicolon here, and two here, and three here. Yeah it's not</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>15630</td>\n",
       "      <td>15630</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't know, one semicolon here, and two here, and three here. Yeah it's not</td>\n",
       "      <td>a big deal. At the level where you're, like, trying to get an extra half a percent to get</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>15631</td>\n",
       "      <td>15631</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>a big deal. At the level where you're, like, trying to get an extra half a percent to get</td>\n",
       "      <td>up the leaderboard of a Kaggle competition you may find tweaking these things makes tiny differences, but in practice you won't generally find it matters too much.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>15632</td>\n",
       "      <td>15632</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>up the leaderboard of a Kaggle competition you may find tweaking these things makes tiny differences, but in practice you won't generally find it matters too much.</td>\n",
       "      <td>Right, thank you. And I guess the second part of that, somebody's asking: If one of their</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>15633</td>\n",
       "      <td>15633</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>Right, thank you. And I guess the second part of that, somebody's asking: If one of their</td>\n",
       "      <td>fields was particularly long, say it was a thousand characters, is there any special</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>15634</td>\n",
       "      <td>15634</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>fields was particularly long, say it was a thousand characters, is there any special</td>\n",
       "      <td>handling required there? Do you need to re-inject those kinds of special marker tokens? Does</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>15635</td>\n",
       "      <td>15635</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>handling required there? Do you need to re-inject those kinds of special marker tokens? Does</td>\n",
       "      <td>it change if you've got much bigger fields that you're trying to learn and query?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>15636</td>\n",
       "      <td>15636</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>it change if you've got much bigger fields that you're trying to learn and query?</td>\n",
       "      <td>Yes. Long documents and ULMFiT require no special consideration. IMDb in fact has multi</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>15637</td>\n",
       "      <td>15637</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>Yes. Long documents and ULMFiT require no special consideration. IMDb in fact has multi</td>\n",
       "      <td>thousand word movie reviews, and it works great. To this day, ULMFiT is probably the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>15638</td>\n",
       "      <td>15638</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: rationale behind how input data was formatted</td>\n",
       "      <td>thousand word movie reviews, and it works great. To this day, ULMFiT is probably the</td>\n",
       "      <td>best approach for reasonably quickly and easily using large documents. Otherwise, if you use</td>\n",
       "      <td>True</td>\n",
       "      <td>best approach for reasonably quickly and easily using large documents. Otherwise, if you use</td>\n",
       "      <td>[you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>15639</td>\n",
       "      <td>15639</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>best approach for reasonably quickly and easily using large documents. Otherwise, if you use</td>\n",
       "      <td>transformer-based approaches, large documents are challenging. Specifically, transformers</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got noth...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>15640</td>\n",
       "      <td>15640</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>transformer-based approaches, large documents are challenging. Specifically, transformers</td>\n",
       "      <td>basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory., So, Hugging Face transformer...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>15641</td>\n",
       "      <td>15641</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with</td>\n",
       "      <td>large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory., So, Hugging Face transformers has these, you know As I say it right now, I find them, somewhat obscure and not particularl...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>15642</td>\n",
       "      <td>15642</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with</td>\n",
       "      <td>documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory., So, Hugging Face transformers has...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>15643</td>\n",
       "      <td>15643</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if</td>\n",
       "      <td>it works for you, but you know I'd certainly try both. For under 2,000 words, you know,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, transformers should be fine unless you've go...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>15644</td>\n",
       "      <td>15644</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>it works for you, but you know I'd certainly try both. For under 2,000 words, you know,</td>\n",
       "      <td>transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>15645</td>\n",
       "      <td>15645</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory.</td>\n",
       "      <td>So, Hugging Face transformers has these, you know As I say it right now, I find them</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>15646</td>\n",
       "      <td>15646</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>So, Hugging Face transformers has these, you know As I say it right now, I find them</td>\n",
       "      <td>somewhat obscure and not particularly well documented expectations about your data, that you kind of have to figure out, and one of those is that it expects that your target</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>15647</td>\n",
       "      <td>15647</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>somewhat obscure and not particularly well documented expectations about your data, that you kind of have to figure out, and one of those is that it expects that your target</td>\n",
       "      <td>is a column called labels. So once I figured that out, I just went, got our tokenized</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>15648</td>\n",
       "      <td>15648</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>is a column called labels. So once I figured that out, I just went, got our tokenized</td>\n",
       "      <td>DataSet, and renamed our score column to labels, and everything started working. I don't know if at some point they'll make this a bit more flexible, but its probably</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>15649</td>\n",
       "      <td>15649</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>DataSet, and renamed our score column to labels, and everything started working. I don't know if at some point they'll make this a bit more flexible, but its probably</td>\n",
       "      <td>best to just call your target labels and life will be easy. You might have seen back when I went ls {path} that there was another data set</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>15650</td>\n",
       "      <td>15650</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>best to just call your target labels and life will be easy. You might have seen back when I went ls {path} that there was another data set</td>\n",
       "      <td>there, called test.csv. And if you look at it, it looks a lot like our training set,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>15651</td>\n",
       "      <td>15651</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>there, called test.csv. And if you look at it, it looks a lot like our training set,</td>\n",
       "      <td>that's our other CSV that we've been working with, but it's missing the score. The labels.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>15652</td>\n",
       "      <td>15652</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>that's our other CSV that we've been working with, but it's missing the score. The labels.</td>\n",
       "      <td>This is called a test set  and so we're going to talk a little bit about that now</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>15653</td>\n",
       "      <td>15653</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>This is called a test set  and so we're going to talk a little bit about that now</td>\n",
       "      <td>because my claim here is that perhaps the most important idea in machine learning is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>15654</td>\n",
       "      <td>15654</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>because my claim here is that perhaps the most important idea in machine learning is</td>\n",
       "      <td>the idea of having separate training, validation and test data sets.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>15655</td>\n",
       "      <td>15655</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>ULMFit fits large documents easily</td>\n",
       "      <td>the idea of having separate training, validation and test data sets.</td>\n",
       "      <td>Test and validation sets are all about identifying and controlling for something called overfitting</td>\n",
       "      <td>True</td>\n",
       "      <td>Test and validation sets are all about identifying and controlling for something called overfitting</td>\n",
       "      <td>[best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>15656</td>\n",
       "      <td>15656</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>Test and validation sets are all about identifying and controlling for something called overfitting</td>\n",
       "      <td>and we're going to try and learn about this through example. This is the same information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, those of you that remember, a f...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>15657</td>\n",
       "      <td>15657</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>and we're going to try and learn about this through example. This is the same information</td>\n",
       "      <td>that's in that Kaggle notebook  I've just put it on some slides here.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>15658</td>\n",
       "      <td>15658</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>that's in that Kaggle notebook  I've just put it on some slides here.</td>\n",
       "      <td>So I'm going to create a function here called plot_poly and I'm actually going to use the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>15659</td>\n",
       "      <td>15659</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>So I'm going to create a function here called plot_poly and I'm actually going to use the</td>\n",
       "      <td>same data that, I don't know if you remember, we used it earlier for trying to fit this</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, those of you that...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>15660</td>\n",
       "      <td>15660</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>same data that, I don't know if you remember, we used it earlier for trying to fit this</td>\n",
       "      <td>quadratic. We created some x and some y data. This is the data we're going</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, those ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>15661</td>\n",
       "      <td>15661</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>quadratic. We created some x and some y data. This is the data we're going</td>\n",
       "      <td>to use and we're going to use this to look at overfitting.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>15662</td>\n",
       "      <td>15662</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>to use and we're going to use this to look at overfitting.</td>\n",
       "      <td>The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, those of you that remember, a first degree polynomial is just a line,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>15663</td>\n",
       "      <td>15663</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for</td>\n",
       "      <td>those of you that remember, a first degree polynomial is just a line, it's y = a x.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., A second ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>15664</td>\n",
       "      <td>15664</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>those of you that remember, a first degree polynomial is just a line, it's y = a x.</td>\n",
       "      <td>A second degree polynomial will be y = ax^2 + bx + c, third degree polynomial we'll</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>15665</td>\n",
       "      <td>15665</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>A second degree polynomial will be y = ax^2 + bx + c, third degree polynomial we'll</td>\n",
       "      <td>have a cubic, fourth degree you know quartic, and so forth. And what I've done here is I've</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>15666</td>\n",
       "      <td>15666</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>have a cubic, fourth degree you know quartic, and so forth. And what I've done here is I've</td>\n",
       "      <td>plotted what happens if we try to fit a line to our data. It doesn't fit very well. So</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>15667</td>\n",
       "      <td>15667</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>plotted what happens if we try to fit a line to our data. It doesn't fit very well. So</td>\n",
       "      <td>what happened here is we we did a linear regression and what we're using here is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>15668</td>\n",
       "      <td>15668</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>what happened here is we we did a linear regression and what we're using here is</td>\n",
       "      <td>a very cool library called scikit-learn. scikit-learn is something that, you know, I think it'd be fair to say it's mainly designed for kind of classic machine learning methods like,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>15669</td>\n",
       "      <td>15669</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>a very cool library called scikit-learn. scikit-learn is something that, you know, I think it'd be fair to say it's mainly designed for kind of classic machine learning methods like,</td>\n",
       "      <td>kind of linear regression and stuff like that  I mean, very advanced versions of these things, but it's also great for doing these quick and dirty things. So in this case I</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>15670</td>\n",
       "      <td>15670</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>kind of linear regression and stuff like that  I mean, very advanced versions of these things, but it's also great for doing these quick and dirty things. So in this case I</td>\n",
       "      <td>wanted to do a what's called a polynomial regression which is fitting the polynomial to data and it's just these two lines of code. It's a super nice library. So in this case,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>15671</td>\n",
       "      <td>15671</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>wanted to do a what's called a polynomial regression which is fitting the polynomial to data and it's just these two lines of code. It's a super nice library. So in this case,</td>\n",
       "      <td>a degree one polynomial is just a line, so I fit it, and then I show it with the data,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>15672</td>\n",
       "      <td>15672</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>a degree one polynomial is just a line, so I fit it, and then I show it with the data,</td>\n",
       "      <td>and there it is. Now that's what we call underfit, which is to say there's not enough, kind of,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>15673</td>\n",
       "      <td>15673</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>and there it is. Now that's what we call underfit, which is to say there's not enough, kind of,</td>\n",
       "      <td>complexity in this model I fit, to match the data that's there.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>15674</td>\n",
       "      <td>15674</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>complexity in this model I fit, to match the data that's there.</td>\n",
       "      <td>So an underfit model is a problem. It's got to be systematically biased, you know; all the stuff up here, we're going to be predicting too low; all the stuff down here, we're predicting</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>15675</td>\n",
       "      <td>15675</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>So an underfit model is a problem. It's got to be systematically biased, you know; all the stuff up here, we're going to be predicting too low; all the stuff down here, we're predicting</td>\n",
       "      <td>too low; all the stuff in the middle, well be predicting too high. A common misunderstanding</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>15676</td>\n",
       "      <td>15676</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>too low; all the stuff in the middle, well be predicting too high. A common misunderstanding</td>\n",
       "      <td>is that simpler models are kind of more reliable in some way, but models that are too simple</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>15677</td>\n",
       "      <td>15677</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>is that simpler models are kind of more reliable in some way, but models that are too simple</td>\n",
       "      <td>will be systematically incorrect as you see here.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>15678</td>\n",
       "      <td>15678</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>will be systematically incorrect as you see here.</td>\n",
       "      <td>What happens if we fit a 10 degree polynomial?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>15679</td>\n",
       "      <td>15679</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>What happens if we fit a 10 degree polynomial?</td>\n",
       "      <td>That's not great either! In this case it's not really showing us what the actual Remember</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>15680</td>\n",
       "      <td>15680</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>That's not great either! In this case it's not really showing us what the actual Remember</td>\n",
       "      <td>this was originally a quadratic. This is meant to match, right? And particularly at the ends here, it's predicting things that are way above what we would expect in real life right?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>15681</td>\n",
       "      <td>15681</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>this was originally a quadratic. This is meant to match, right? And particularly at the ends here, it's predicting things that are way above what we would expect in real life right?</td>\n",
       "      <td>And it's trying to get really it's trying really hard to get through this point, but clearly this point was just some noise, right? So this is what we call overfit. It's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>15682</td>\n",
       "      <td>15682</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>And it's trying to get really it's trying really hard to get through this point, but clearly this point was just some noise, right? So this is what we call overfit. It's</td>\n",
       "      <td>done a good job of fitting to our exact data points, but if we sample some more data points</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>15683</td>\n",
       "      <td>15683</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>done a good job of fitting to our exact data points, but if we sample some more data points</td>\n",
       "      <td>from this distribution, honestly we probably would suspect they're not going to be very close to this, particularly if they're a bit beyond the edges. So that's what overfitting</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>15684</td>\n",
       "      <td>15684</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>from this distribution, honestly we probably would suspect they're not going to be very close to this, particularly if they're a bit beyond the edges. So that's what overfitting</td>\n",
       "      <td>looks like. We don't want underfitting or overfitting. Now underfitting is actually pretty easy to recognize, because we can actually look at</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>15685</td>\n",
       "      <td>15685</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>looks like. We don't want underfitting or overfitting. Now underfitting is actually pretty easy to recognize, because we can actually look at</td>\n",
       "      <td>our training data and see that it's not very close. Overfitting is a bit harder to recognize</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>15686</td>\n",
       "      <td>15686</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>our training data and see that it's not very close. Overfitting is a bit harder to recognize</td>\n",
       "      <td>because the training data is actually very close.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>15687</td>\n",
       "      <td>15687</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>because the training data is actually very close.</td>\n",
       "      <td>Now on the other hand, here's what happens if we fit a quadratic. And here I've got both</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>15688</td>\n",
       "      <td>15688</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>Now on the other hand, here's what happens if we fit a quadratic. And here I've got both</td>\n",
       "      <td>the real-line and the fit-line and you can see they're pretty close, and that's of course</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>15689</td>\n",
       "      <td>15689</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>the real-line and the fit-line and you can see they're pretty close, and that's of course</td>\n",
       "      <td>what we actually want.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>15690</td>\n",
       "      <td>15690</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>what we actually want.</td>\n",
       "      <td>So how do we tell whether we have something more like this, or something more like this.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>15691</td>\n",
       "      <td>15691</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>So how do we tell whether we have something more like this, or something more like this.</td>\n",
       "      <td>Well what we do is we do something pretty straightforward is we take our original data set, these points, and we remove a few of them, so let's say 20% of them.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>15692</td>\n",
       "      <td>15692</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overfitting &amp; underfitting</td>\n",
       "      <td>Well what we do is we do something pretty straightforward is we take our original data set, these points, and we remove a few of them, so let's say 20% of them.</td>\n",
       "      <td>We then fit our model using only those points we haven't removed, and then we measure how</td>\n",
       "      <td>True</td>\n",
       "      <td>We then fit our model using only those points we haven't removed, and then we measure how</td>\n",
       "      <td>[Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>15693</td>\n",
       "      <td>15693</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>We then fit our model using only those points we haven't removed, and then we measure how</td>\n",
       "      <td>good it is by looking at only the points we removed. So in this case let's say we had</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>15694</td>\n",
       "      <td>15694</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>good it is by looking at only the points we removed. So in this case let's say we had</td>\n",
       "      <td>removed (I'm just trying to think) If I had removed this point here right, then it might have</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only on t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>15695</td>\n",
       "      <td>15695</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>removed (I'm just trying to think) If I had removed this point here right, then it might have</td>\n",
       "      <td>kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only on the validation set. This is really...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>15696</td>\n",
       "      <td>15696</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away.</td>\n",
       "      <td>The model the data that we take away and don't let the model see it when it's training</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only on the validation set. This is...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>15697</td>\n",
       "      <td>15697</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>The model the data that we take away and don't let the model see it when it's training</td>\n",
       "      <td>is called the validation set. So in fast.ai we've seen splitters before, right</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measure...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>15698</td>\n",
       "      <td>15698</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>is called the validation set. So in fast.ai we've seen splitters before, right</td>\n",
       "      <td>The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, things like accuracy, measured only on the validation set. This is really unusual. Most, libraries make it reall...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>15699</td>\n",
       "      <td>15699</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so</td>\n",
       "      <td>things like accuracy, measured only on the validation set. This is really unusual. Most</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, libraries make it really easy...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>15700</td>\n",
       "      <td>15700</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>things like accuracy, measured only on the validation set. This is really unusual. Most</td>\n",
       "      <td>libraries make it really easy to shoot yourself in the foot, by not having a validation set,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>15701</td>\n",
       "      <td>15701</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>libraries make it really easy to shoot yourself in the foot, by not having a validation set,</td>\n",
       "      <td>or accidentally not using it correctly. So fast.ai won't even let you do that. So you've got to be particularly careful when using other libraries. HuggingFace transformers</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>15702</td>\n",
       "      <td>15702</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>or accidentally not using it correctly. So fast.ai won't even let you do that. So you've got to be particularly careful when using other libraries. HuggingFace transformers</td>\n",
       "      <td>is good about this, so they make sure that they do show you your metrics on a validation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15703</td>\n",
       "      <td>15703</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>is good about this, so they make sure that they do show you your metrics on a validation</td>\n",
       "      <td>set. Now creating a good validation set is not generally as simple as just randomly pulling</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15704</td>\n",
       "      <td>15704</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Splitting the dataset</td>\n",
       "      <td>set. Now creating a good validation set is not generally as simple as just randomly pulling</td>\n",
       "      <td>some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was</td>\n",
       "      <td>True</td>\n",
       "      <td>some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was</td>\n",
       "      <td>[We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15705</td>\n",
       "      <td>15705</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was</td>\n",
       "      <td>the data you were trying to fit something to (okay) and you randomly remove some, so</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate, and remove the last couple of weeks. So if this was your validation set and this is your, training set, that's going to be actually testing whet...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>15706</td>\n",
       "      <td>15706</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>the data you were trying to fit something to (okay) and you randomly remove some, so</td>\n",
       "      <td>it looks like this. That looks very easy doesn't it, because you've kind of like, still got</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate, and remove the last couple of weeks. So if this was your validation set and this is your, trai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>15707</td>\n",
       "      <td>15707</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>it looks like this. That looks very easy doesn't it, because you've kind of like, still got</td>\n",
       "      <td>all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate, and remove the last couple of weeks. So if this was your validation set and this is your, training set, that's going to be actually testing whether you can use this to predict, the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>15708</td>\n",
       "      <td>15708</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So</td>\n",
       "      <td>if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, and remove the last couple of weeks. So if this was your validation set and this is your, training set, that's going to be actually testing whether you can use this to predict, the future, rather than using it to predict the past., Kaggle competitions are a fantastic way t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>15709</td>\n",
       "      <td>15709</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate</td>\n",
       "      <td>and remove the last couple of weeks. So if this was your validation set and this is your</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, training set, that's going to be actually testing whether you can use this to predict, the future, ra...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>15710</td>\n",
       "      <td>15710</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>and remove the last couple of weeks. So if this was your validation set and this is your</td>\n",
       "      <td>training set, that's going to be actually testing whether you can use this to predict</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>15711</td>\n",
       "      <td>15711</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>training set, that's going to be actually testing whether you can use this to predict</td>\n",
       "      <td>the future, rather than using it to predict the past.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>15712</td>\n",
       "      <td>15712</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>the future, rather than using it to predict the past.</td>\n",
       "      <td>Kaggle competitions are a fantastic way to test your ability to create a good validation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>15713</td>\n",
       "      <td>15713</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>Kaggle competitions are a fantastic way to test your ability to create a good validation</td>\n",
       "      <td>set, because Kaggle competitions only allow you to submit, generally, a couple of times</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>15714</td>\n",
       "      <td>15714</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>set, because Kaggle competitions only allow you to submit, generally, a couple of times</td>\n",
       "      <td>a day. The dataset that you are scored on in the leaderboard during that time is actually</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>15715</td>\n",
       "      <td>15715</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>a day. The dataset that you are scored on in the leaderboard during that time is actually</td>\n",
       "      <td>only a small subset in fact it's a totally separate subset to the one you'll be scored on, on the end of the competition. And so most beginners on Kaggle overfit. And it's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>15716</td>\n",
       "      <td>15716</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>only a small subset in fact it's a totally separate subset to the one you'll be scored on, on the end of the competition. And so most beginners on Kaggle overfit. And it's</td>\n",
       "      <td>not until you've done it that you'll get that visceral feeling of like: oh my god, I</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>15717</td>\n",
       "      <td>15717</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>not until you've done it that you'll get that visceral feeling of like: oh my god, I</td>\n",
       "      <td>overfit. In the real world outside of Kaggle you will often not even know that you overfit</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>15718</td>\n",
       "      <td>15718</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>overfit. In the real world outside of Kaggle you will often not even know that you overfit</td>\n",
       "      <td> you just destroy value for your organization silently. So it's a really good idea to do</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>15719</td>\n",
       "      <td>15719</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td> you just destroy value for your organization silently. So it's a really good idea to do</td>\n",
       "      <td>this kind of stuff on Kaggle a few times first, in real competitions, to really make sure that you are confident you know how to avoid overfitting  how to find a good validation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>15720</td>\n",
       "      <td>15720</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>this kind of stuff on Kaggle a few times first, in real competitions, to really make sure that you are confident you know how to avoid overfitting  how to find a good validation</td>\n",
       "      <td>set and how to interpret it correctly. And you really don't get that until you screw it up a few times.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>15721</td>\n",
       "      <td>15721</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>set and how to interpret it correctly. And you really don't get that until you screw it up a few times.</td>\n",
       "      <td>A good example of this was there was a distracted driver competition on Kaggle  there</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>15722</td>\n",
       "      <td>15722</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>A good example of this was there was a distracted driver competition on Kaggle  there</td>\n",
       "      <td>are these kind of pictures from inside a car, and the idea was that you had to try and predict</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>15723</td>\n",
       "      <td>15723</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>are these kind of pictures from inside a car, and the idea was that you had to try and predict</td>\n",
       "      <td>whether somebody was driving in a distracted way or not, and on Kaggle they did something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>15724</td>\n",
       "      <td>15724</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>whether somebody was driving in a distracted way or not, and on Kaggle they did something</td>\n",
       "      <td>pretty smart the test set, so the thing that they scored you on the leaderboard, contained people that didn't exist, at all, in the competition data that you train the model with. So if</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>15725</td>\n",
       "      <td>15725</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>pretty smart the test set, so the thing that they scored you on the leaderboard, contained people that didn't exist, at all, in the competition data that you train the model with. So if</td>\n",
       "      <td>you wanted to create an effective validation set in this competition, you would have to make sure that you separated the photos, so that your validation set contained photos</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>15726</td>\n",
       "      <td>15726</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>you wanted to create an effective validation set in this competition, you would have to make sure that you separated the photos, so that your validation set contained photos</td>\n",
       "      <td>of people that aren't in the data you're training your model on.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>15727</td>\n",
       "      <td>15727</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>of people that aren't in the data you're training your model on.</td>\n",
       "      <td>There's another one like that, the Kaggle fisheries competition, which had boats that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>15728</td>\n",
       "      <td>15728</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>There's another one like that, the Kaggle fisheries competition, which had boats that</td>\n",
       "      <td>didn't appear so they were basically pictures of boats and you meant to try to guess/predict what fish were in the pictures. And it turned out that a lot of people accidentally figured</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>15729</td>\n",
       "      <td>15729</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>didn't appear so they were basically pictures of boats and you meant to try to guess/predict what fish were in the pictures. And it turned out that a lot of people accidentally figured</td>\n",
       "      <td>out what the fish were by looking at the boat, because certain boats tended to catch certain</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>15730</td>\n",
       "      <td>15730</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>out what the fish were by looking at the boat, because certain boats tended to catch certain</td>\n",
       "      <td>kinds of fish. And so by messing up their validation set, they were really overconfident</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>15731</td>\n",
       "      <td>15731</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>kinds of fish. And so by messing up their validation set, they were really overconfident</td>\n",
       "      <td>of the accuracy of their model. I'll mention in passing, if you've been around Kaggle a bit, you'll see people talk about</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>15732</td>\n",
       "      <td>15732</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>of the accuracy of their model. I'll mention in passing, if you've been around Kaggle a bit, you'll see people talk about</td>\n",
       "      <td>cross-validation a lot. I'm just going to mention, be very very careful. Cross-validation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>15733</td>\n",
       "      <td>15733</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>cross-validation a lot. I'm just going to mention, be very very careful. Cross-validation</td>\n",
       "      <td>is explicitly not about building a good validation set, so you've got to be super super careful</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>15734</td>\n",
       "      <td>15734</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>is explicitly not about building a good validation set, so you've got to be super super careful</td>\n",
       "      <td>if you ever do that.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>15735</td>\n",
       "      <td>15735</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>if you ever do that.</td>\n",
       "      <td>Another thing I'll mention, is that scikit-learn conveniently offers something called train_test_split,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>15736</td>\n",
       "      <td>15736</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>Another thing I'll mention, is that scikit-learn conveniently offers something called train_test_split,</td>\n",
       "      <td>as does Hugging Face datasets, as does fast.ai  we have something called random splitter.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>15737</td>\n",
       "      <td>15737</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>as does Hugging Face datasets, as does fast.ai  we have something called random splitter.</td>\n",
       "      <td>It can be encouraging it can almost feel like it's encouraging you to use a randomized</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>15738</td>\n",
       "      <td>15738</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>It can be encouraging it can almost feel like it's encouraging you to use a randomized</td>\n",
       "      <td>validation set because there are these methods that do it for you. But yeah, be very very</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>15739</td>\n",
       "      <td>15739</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>validation set because there are these methods that do it for you. But yeah, be very very</td>\n",
       "      <td>careful, because very very often that's not what you want, okay. So we've learned what</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>15740</td>\n",
       "      <td>15740</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>careful, because very very often that's not what you want, okay. So we've learned what</td>\n",
       "      <td>a validation set is, so that's the bit that you pull out of your data that you don't train with, but you do measure your accuracy with. So what's a test set? It's basically another</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>15741</td>\n",
       "      <td>15741</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>a validation set is, so that's the bit that you pull out of your data that you don't train with, but you do measure your accuracy with. So what's a test set? It's basically another</td>\n",
       "      <td>validation set, but you don't even use it for tracking your accuracy while you build</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>15742</td>\n",
       "      <td>15742</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Creating a good validation set</td>\n",
       "      <td>validation set, but you don't even use it for tracking your accuracy while you build</td>\n",
       "      <td>your model. Why not? Well imagine you tried two new models every day for three months</td>\n",
       "      <td>True</td>\n",
       "      <td>your model. Why not? Well imagine you tried two new models every day for three months</td>\n",
       "      <td>[some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>15743</td>\n",
       "      <td>15743</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>your model. Why not? Well imagine you tried two new models every day for three months</td>\n",
       "      <td>(that's how long a Kaggle competition goes for.) So you would have tried 180 models,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard du...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>15744</td>\n",
       "      <td>15744</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>(that's how long a Kaggle competition goes for.) So you would have tried 180 models,</td>\n",
       "      <td>and then you look at the accuracy on the validation set for each one. Some of those models you</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>15745</td>\n",
       "      <td>15745</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>and then you look at the accuracy on the validation set for each one. Some of those models you</td>\n",
       "      <td>would have got a good accuracy on the validation set, potentially because of pure chance, just</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the competitio...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>15746</td>\n",
       "      <td>15746</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>would have got a good accuracy on the validation set, potentially because of pure chance, just</td>\n",
       "      <td>a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the competition and a second test set which, you don't get to see until after the competition is...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>15747</td>\n",
       "      <td>15747</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually</td>\n",
       "      <td>overfit using the validation set. So you actually want to know whether you've really found a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the competition and a second test set which, you don't get to see until after the competition ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>15748</td>\n",
       "      <td>15748</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>overfit using the validation set. So you actually want to know whether you've really found a</td>\n",
       "      <td>good model or not. So in fact on Kaggle they have two test sets. They've got the one that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, gives you feedback on the leaderboard during the com...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>15749</td>\n",
       "      <td>15749</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>good model or not. So in fact on Kaggle they have two test sets. They've got the one that</td>\n",
       "      <td>gives you feedback on the leaderboard during the competition and a second test set which</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>15750</td>\n",
       "      <td>15750</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>gives you feedback on the leaderboard during the competition and a second test set which</td>\n",
       "      <td>you don't get to see until after the competition is finished. So in real life you've got to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>15751</td>\n",
       "      <td>15751</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>you don't get to see until after the competition is finished. So in real life you've got to</td>\n",
       "      <td>be very careful about this, not to try so many models during your model building process</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>15752</td>\n",
       "      <td>15752</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>be very careful about this, not to try so many models during your model building process</td>\n",
       "      <td>that you accidentally find one that's good by coincidence. And only if you have a test</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>15753</td>\n",
       "      <td>15753</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>that you accidentally find one that's good by coincidence. And only if you have a test</td>\n",
       "      <td>set that you've held out, or you know that. Now that leads to the obvious question which is very challenging, is you spent three months working on a model, worked well on your validation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>15754</td>\n",
       "      <td>15754</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>set that you've held out, or you know that. Now that leads to the obvious question which is very challenging, is you spent three months working on a model, worked well on your validation</td>\n",
       "      <td>set, you did a good job of locking that test set away in a safe so you weren't allowed to use it, and at the end of the three months you finally checked it on the test set, and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>15755</td>\n",
       "      <td>15755</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>set, you did a good job of locking that test set away in a safe so you weren't allowed to use it, and at the end of the three months you finally checked it on the test set, and</td>\n",
       "      <td>it's terrible. What do you do? Honestly you have to go back to square one. You know there</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>15756</td>\n",
       "      <td>15756</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>it's terrible. What do you do? Honestly you have to go back to square one. You know there</td>\n",
       "      <td>really isn't any choice other than starting again. So this is tough, but it's better to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>15757</td>\n",
       "      <td>15757</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>really isn't any choice other than starting again. So this is tough, but it's better to</td>\n",
       "      <td>know, right. Better to know than to not know, so that's what a test set is for.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>15758</td>\n",
       "      <td>15758</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Test set</td>\n",
       "      <td>know, right. Better to know than to not know, so that's what a test set is for.</td>\n",
       "      <td>So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something</td>\n",
       "      <td>True</td>\n",
       "      <td>So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something</td>\n",
       "      <td>[your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>15759</td>\n",
       "      <td>15759</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something</td>\n",
       "      <td>like accuracy. It's a number that tells you: How good is your model? Now on Kaggle</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For example,, consider accuracy. Now, if we were using accuracy to calculate our derivative and get ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>15760</td>\n",
       "      <td>15760</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>like accuracy. It's a number that tells you: How good is your model? Now on Kaggle</td>\n",
       "      <td>this is very easy. What metric should we use? Well they tell us go to overview, click</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For exam...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>15761</td>\n",
       "      <td>15761</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>this is very easy. What metric should we use? Well they tell us go to overview, click</td>\n",
       "      <td>on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For example,,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>15762</td>\n",
       "      <td>15762</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation</td>\n",
       "      <td>Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For example,, consider accuracy. Now, if we were using accuracy to calcula...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>15763</td>\n",
       "      <td>15763</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that</td>\n",
       "      <td>we will take the derivative of, and find the gradient, and use that to improve our parameters</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, during training? And the answer is: maybe, sometimes, but probably not. For example,, consider accuracy. Now, if we were using accuracy to calculate o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>15764</td>\n",
       "      <td>15764</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>we will take the derivative of, and find the gradient, and use that to improve our parameters</td>\n",
       "      <td>during training? And the answer is: maybe, sometimes, but probably not. For example,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>15765</td>\n",
       "      <td>15765</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>during training? And the answer is: maybe, sometimes, but probably not. For example,</td>\n",
       "      <td>consider accuracy. Now, if we were using accuracy to calculate our derivative and get the gradient,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>15766</td>\n",
       "      <td>15766</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>consider accuracy. Now, if we were using accuracy to calculate our derivative and get the gradient,</td>\n",
       "      <td>you could have a model that's actually slightly better, you know, it's slightly like it's doing a better job of recognizing dogs and cats, but not so much better that it's actually</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>15767</td>\n",
       "      <td>15767</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>you could have a model that's actually slightly better, you know, it's slightly like it's doing a better job of recognizing dogs and cats, but not so much better that it's actually</td>\n",
       "      <td>caused any incorrectly classified cat to become a dog. So the accuracy doesn't change at all.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>15768</td>\n",
       "      <td>15768</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>caused any incorrectly classified cat to become a dog. So the accuracy doesn't change at all.</td>\n",
       "      <td>So the gradient is zero. You don't want stuff like that. You don't want bumpy functions</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>15769</td>\n",
       "      <td>15769</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>So the gradient is zero. You don't want stuff like that. You don't want bumpy functions</td>\n",
       "      <td>because they don't have nice gradients  often they don't have gradients at all, they're basically zero nearly everywhere. You want a function that's nice and smooth. Something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>15770</td>\n",
       "      <td>15770</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>because they don't have nice gradients  often they don't have gradients at all, they're basically zero nearly everywhere. You want a function that's nice and smooth. Something</td>\n",
       "      <td>like, for instance, the average absolute error, mean absolute error, which we've used before.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>15771</td>\n",
       "      <td>15771</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>like, for instance, the average absolute error, mean absolute error, which we've used before.</td>\n",
       "      <td>So that's the difference between your metrics and your loss. Now be careful, right, because when you're training your model's spending all of its time trying to improve the loss</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>15772</td>\n",
       "      <td>15772</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>So that's the difference between your metrics and your loss. Now be careful, right, because when you're training your model's spending all of its time trying to improve the loss</td>\n",
       "      <td>and most of the time that's not the same as a thing you actually care about, which is your metric. So you've got to keep those two different things in mind.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>15773</td>\n",
       "      <td>15773</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>and most of the time that's not the same as a thing you actually care about, which is your metric. So you've got to keep those two different things in mind.</td>\n",
       "      <td>The other thing to keep in mind is that in real life you can't go to a website and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>15774</td>\n",
       "      <td>15774</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>The other thing to keep in mind is that in real life you can't go to a website and</td>\n",
       "      <td>be told what metric to use. In real life the model that you choose, there isn't one</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>15775</td>\n",
       "      <td>15775</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Metric vs loss</td>\n",
       "      <td>be told what metric to use. In real life the model that you choose, there isn't one</td>\n",
       "      <td>number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex</td>\n",
       "      <td>True</td>\n",
       "      <td>number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex</td>\n",
       "      <td>[So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>15776</td>\n",
       "      <td>15776</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex</td>\n",
       "      <td>process often involving humans, both as users and customers and as people, you know, involved</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found people's lives turned upside, down because of how badly they get screwed up by models that ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>15777</td>\n",
       "      <td>15777</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>process often involving humans, both as users and customers and as people, you know, involved</td>\n",
       "      <td>in as part of the process. There's all kinds of things that are changing over time</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found p...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>15778</td>\n",
       "      <td>15778</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>in as part of the process. There's all kinds of things that are changing over time</td>\n",
       "      <td>and there's lots and lots of outcomes of decisions that are made. One metric is not enough to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found p...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>15779</td>\n",
       "      <td>15779</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>and there's lots and lots of outcomes of decisions that are made. One metric is not enough to</td>\n",
       "      <td>capture all of that. Unfortunately, because it's so convenient to pick one metric and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found peo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>15780</td>\n",
       "      <td>15780</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>capture all of that. Unfortunately, because it's so convenient to pick one metric and</td>\n",
       "      <td>use that to say: I've got a good model, that very often finds its way into industry, into</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>15781</td>\n",
       "      <td>15781</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>use that to say: I've got a good model, that very often finds its way into industry, into</td>\n",
       "      <td>government where people roll out these things that are good on the one metric that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, happened to be easy to measure. And again and again we fou...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>15782</td>\n",
       "      <td>15782</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>government where people roll out these things that are good on the one metric that</td>\n",
       "      <td>happened to be easy to measure. And again and again we found people's lives turned upside</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>15783</td>\n",
       "      <td>15783</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>happened to be easy to measure. And again and again we found people's lives turned upside</td>\n",
       "      <td>down because of how badly they get screwed up by models that have been incorrectly measured</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15784</td>\n",
       "      <td>15784</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>down because of how badly they get screwed up by models that have been incorrectly measured</td>\n",
       "      <td>using a single metric. So my partner Rachel Thomas has written this article which I recommend</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>15785</td>\n",
       "      <td>15785</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>using a single metric. So my partner Rachel Thomas has written this article which I recommend</td>\n",
       "      <td>you read about The problem with metrics is a big problem for AI</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>15786</td>\n",
       "      <td>15786</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>you read about The problem with metrics is a big problem for AI</td>\n",
       "      <td>It's not just an AI thing! There's actually this thing called Goodharts Law that states when a measure becomes a target, it ceases to be a good measure. The thing</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>15787</td>\n",
       "      <td>15787</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>It's not just an AI thing! There's actually this thing called Goodharts Law that states when a measure becomes a target, it ceases to be a good measure. The thing</td>\n",
       "      <td>is so when I was a management consultant, you know, 20 years ago, we were always kind</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>15788</td>\n",
       "      <td>15788</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>is so when I was a management consultant, you know, 20 years ago, we were always kind</td>\n",
       "      <td>of part of these strategic things trying to like: find key performance indicators and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>15789</td>\n",
       "      <td>15789</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>of part of these strategic things trying to like: find key performance indicators and</td>\n",
       "      <td>ways to kind of, you know, set commission rates for sales people and we were really doing a lot of this, like, stuff which is basically about picking metrics and, you know,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>15790</td>\n",
       "      <td>15790</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>ways to kind of, you know, set commission rates for sales people and we were really doing a lot of this, like, stuff which is basically about picking metrics and, you know,</td>\n",
       "      <td>we see that happen go wrong in industry all the time. AI is dramatically worse because</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>15791</td>\n",
       "      <td>15791</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>we see that happen go wrong in industry all the time. AI is dramatically worse because</td>\n",
       "      <td>AI is so good at optimizing metrics, and so that's why you have to be extra, extra, extra</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>15792</td>\n",
       "      <td>15792</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>AI is so good at optimizing metrics, and so that's why you have to be extra, extra, extra</td>\n",
       "      <td>careful about metrics, when you are trying to use a model in real life. Anyway, as I said in Kaggle, we don't have to worry about any of that, we're just going</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>15793</td>\n",
       "      <td>15793</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>careful about metrics, when you are trying to use a model in real life. Anyway, as I said in Kaggle, we don't have to worry about any of that, we're just going</td>\n",
       "      <td>to use the Pearson correlation coefficient which is all very well as long as you know what the hell the Pearson correlation coefficient is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>15794</td>\n",
       "      <td>15794</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>to use the Pearson correlation coefficient which is all very well as long as you know what the hell the Pearson correlation coefficient is</td>\n",
       "      <td>If you don't, let's learn about it. So Pearson correlation coefficient is usually abbreviated</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>15795</td>\n",
       "      <td>15795</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>The problem with metrics</td>\n",
       "      <td>If you don't, let's learn about it. So Pearson correlation coefficient is usually abbreviated</td>\n",
       "      <td>using letter r and it's the most widely used measure of how similar two variables</td>\n",
       "      <td>True</td>\n",
       "      <td>using letter r and it's the most widely used measure of how similar two variables</td>\n",
       "      <td>[number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>using letter r and it's the most widely used measure of how similar two variables</td>\n",
       "      <td>are. And so, if your predictions are very similar to the real values then the Pearson</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pears...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>15797</td>\n",
       "      <td>15797</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>are. And so, if your predictions are very similar to the real values then the Pearson</td>\n",
       "      <td>correlation coefficient will be high, and that's what you want. r can be between</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pears...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>15798</td>\n",
       "      <td>15798</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>correlation coefficient will be high, and that's what you want. r can be between</td>\n",
       "      <td>minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pearson Cor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>15799</td>\n",
       "      <td>15799</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle</td>\n",
       "      <td>competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>15800</td>\n",
       "      <td>15800</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct.</td>\n",
       "      <td>Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical function, but how, it behaves; and I find most people, even who work in data science, have...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>15801</td>\n",
       "      <td>15801</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function.</td>\n",
       "      <td>I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical function, but how</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., it behaves; and I find most people, even who work in data science, have not actually...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>15802</td>\n",
       "      <td>15802</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical function, but how</td>\n",
       "      <td>it behaves; and I find most people, even who work in data science, have not actually looked</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>15803</td>\n",
       "      <td>15803</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>it behaves; and I find most people, even who work in data science, have not actually looked</td>\n",
       "      <td>at a bunch of data sets to understand how r behaves. So let's do that right now</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>15804</td>\n",
       "      <td>15804</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>at a bunch of data sets to understand how r behaves. So let's do that right now</td>\n",
       "      <td>so that you're not one of those people. The best way I find to understand how data behaves in real life, is to look at real-life</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>15805</td>\n",
       "      <td>15805</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>so that you're not one of those people. The best way I find to understand how data behaves in real life, is to look at real-life</td>\n",
       "      <td>data so there's a data set scikit-learn comes with a number of data sets, and one of them is called California housing and it's a data set where each row is a district</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>15806</td>\n",
       "      <td>15806</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>data so there's a data set scikit-learn comes with a number of data sets, and one of them is called California housing and it's a data set where each row is a district</td>\n",
       "      <td>and, it's kind of demographic, sorry it's information some demographic information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>15807</td>\n",
       "      <td>15807</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>and, it's kind of demographic, sorry it's information some demographic information</td>\n",
       "      <td>about different districts, and about the value of houses in that district.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>15808</td>\n",
       "      <td>15808</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>about different districts, and about the value of houses in that district.</td>\n",
       "      <td>Im not going to try to plot the whole thing, it's too big, and this is a very common question</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>15809</td>\n",
       "      <td>15809</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>Im not going to try to plot the whole thing, it's too big, and this is a very common question</td>\n",
       "      <td>I have from people is: how do I plot data sets with far too many points? The answer</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>15810</td>\n",
       "      <td>15810</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>I have from people is: how do I plot data sets with far too many points? The answer</td>\n",
       "      <td>is very simple: get less points. So I just randomly grab a thousand points. Whatever</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>15811</td>\n",
       "      <td>15811</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>is very simple: get less points. So I just randomly grab a thousand points. Whatever</td>\n",
       "      <td>you see with a thousand points, is going to be the same as what you see with a million points. There's no point no reason, to plot huge amounts of data generally just grab</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>15812</td>\n",
       "      <td>15812</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>you see with a thousand points, is going to be the same as what you see with a million points. There's no point no reason, to plot huge amounts of data generally just grab</td>\n",
       "      <td>a random sample. Now, numpy has something called corecoeff() to get the correlation coefficient between</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>15813</td>\n",
       "      <td>15813</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>a random sample. Now, numpy has something called corecoeff() to get the correlation coefficient between</td>\n",
       "      <td>every variable and every other variable, and it returns a matrix. So I can look down here,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>15814</td>\n",
       "      <td>15814</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>every variable and every other variable, and it returns a matrix. So I can look down here,</td>\n",
       "      <td>and so for example, here is the correlation coefficient between variable one, and variable one. Which of course is exactly perfectly 1.0. Right? because variable one is the same</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>15815</td>\n",
       "      <td>15815</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>and so for example, here is the correlation coefficient between variable one, and variable one. Which of course is exactly perfectly 1.0. Right? because variable one is the same</td>\n",
       "      <td>as variable one. Here is the small inverse correlation between variable one and variable</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>15816</td>\n",
       "      <td>15816</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>as variable one. Here is the small inverse correlation between variable one and variable</td>\n",
       "      <td>two, and medium-sized positive correlation between variable one and variable 3 and so</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>15817</td>\n",
       "      <td>15817</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>two, and medium-sized positive correlation between variable one and variable 3 and so</td>\n",
       "      <td>forth. This is symmetric about the diagonal because the correlation between variable 1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>15818</td>\n",
       "      <td>15818</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>forth. This is symmetric about the diagonal because the correlation between variable 1</td>\n",
       "      <td>and variable 8 is the same as the correlation between variable 8 and variable 1. So this</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>15819</td>\n",
       "      <td>15819</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>and variable 8 is the same as the correlation between variable 8 and variable 1. So this</td>\n",
       "      <td>is a correlation coefficient matrix.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>15820</td>\n",
       "      <td>15820</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>is a correlation coefficient matrix.</td>\n",
       "      <td>So that's great when we wanted to get a bunch of values all at once. For the Kaggle competition we don't want that. We just want a single correlation number. If we just pass in a pair</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>15821</td>\n",
       "      <td>15821</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>So that's great when we wanted to get a bunch of values all at once. For the Kaggle competition we don't want that. We just want a single correlation number. If we just pass in a pair</td>\n",
       "      <td>of variables, we still get a matrix which is kind of weird.. it's kind of it's not</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>15822</td>\n",
       "      <td>15822</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>of variables, we still get a matrix which is kind of weird.. it's kind of it's not</td>\n",
       "      <td>weird, but it's not what we want! So we should grab one of these. So when I want to grab a correlation coefficient, I'll just return the zeroth row, first column. So that's what</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>15823</td>\n",
       "      <td>15823</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>weird, but it's not what we want! So we should grab one of these. So when I want to grab a correlation coefficient, I'll just return the zeroth row, first column. So that's what</td>\n",
       "      <td>core is. That's going to be our single correlation coefficient. So let's look at the correlation between two things; for example</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>15824</td>\n",
       "      <td>15824</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>core is. That's going to be our single correlation coefficient. So let's look at the correlation between two things; for example</td>\n",
       "      <td>median income, and median house value: 0.67. Okay? Is that high? medium? low? How big is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>15825</td>\n",
       "      <td>15825</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>median income, and median house value: 0.67. Okay? Is that high? medium? low? How big is</td>\n",
       "      <td>that? What does it look like? So the main thing we need to understand is: what these things look like.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>15826</td>\n",
       "      <td>15826</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>that? What does it look like? So the main thing we need to understand is: what these things look like.</td>\n",
       "      <td>So what I suggest we do is: we're going to take a 10 minute break nine minute break. We'll come back at half past, and then we're going to look at some examples of correlation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>15827</td>\n",
       "      <td>15827</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>So what I suggest we do is: we're going to take a 10 minute break nine minute break. We'll come back at half past, and then we're going to look at some examples of correlation</td>\n",
       "      <td>coefficients Okay. Welcome back! So what I've done here is I've created a little function called</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>15828</td>\n",
       "      <td>15828</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>coefficients Okay. Welcome back! So what I've done here is I've created a little function called</td>\n",
       "      <td>show correlations, and I'm passing a DataFrame and a couple of columns as strings. I'm going</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>15829</td>\n",
       "      <td>15829</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>show correlations, and I'm passing a DataFrame and a couple of columns as strings. I'm going</td>\n",
       "      <td>to grab each of those columns as series, do a scatter plot, and then show the correlation.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>15830</td>\n",
       "      <td>15830</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>to grab each of those columns as series, do a scatter plot, and then show the correlation.</td>\n",
       "      <td>So, we already mentioned median income and median house value of 0.68, so here</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>15831</td>\n",
       "      <td>15831</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>So, we already mentioned median income and median house value of 0.68, so here</td>\n",
       "      <td>it is here's what .68 looks like. So you know I don't know if you had some intuition about what you expected, but as you can see it's still plenty of variation, even at that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>15832</td>\n",
       "      <td>15832</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>it is here's what .68 looks like. So you know I don't know if you had some intuition about what you expected, but as you can see it's still plenty of variation, even at that</td>\n",
       "      <td>reasonably high correlation. Also, you can see here that visualizing your data is very important if you're working with</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>15833</td>\n",
       "      <td>15833</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>reasonably high correlation. Also, you can see here that visualizing your data is very important if you're working with</td>\n",
       "      <td>this data set, because you can immediately see all these dots along here. That's clearly</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>15834</td>\n",
       "      <td>15834</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>this data set, because you can immediately see all these dots along here. That's clearly</td>\n",
       "      <td>truncation right? So this is like, when... it's not until you look at pictures like this, that you're gonna pick stuff like this up. Pictures are great! Oh! little trick: on the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>15835</td>\n",
       "      <td>15835</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>truncation right? So this is like, when... it's not until you look at pictures like this, that you're gonna pick stuff like this up. Pictures are great! Oh! little trick: on the</td>\n",
       "      <td>scatter plot, I put alpha as 0.5, that creates some transparency. For these kind of scatter</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>15836</td>\n",
       "      <td>15836</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>scatter plot, I put alpha as 0.5, that creates some transparency. For these kind of scatter</td>\n",
       "      <td>plots, that really helps, because it like kind of creates darker areas in places where</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>15837</td>\n",
       "      <td>15837</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>plots, that really helps, because it like kind of creates darker areas in places where</td>\n",
       "      <td>there's lots of dots. So, yeah, alpha in scatter plots is nice. Okay, here's another pair. So this one's gone down from 0.68 to 0.43. Median income versus</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>15838</td>\n",
       "      <td>15838</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>there's lots of dots. So, yeah, alpha in scatter plots is nice. Okay, here's another pair. So this one's gone down from 0.68 to 0.43. Median income versus</td>\n",
       "      <td>the number of rooms per house. As you'd expect more rooms it's more income, but this is</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>15839</td>\n",
       "      <td>15839</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>the number of rooms per house. As you'd expect more rooms it's more income, but this is</td>\n",
       "      <td>a very weird looking thing. Now, you'll find that a lot of these statistical measures like</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>15840</td>\n",
       "      <td>15840</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>a very weird looking thing. Now, you'll find that a lot of these statistical measures like</td>\n",
       "      <td>correlation rely on the square of the difference, and when you have big outliers like this,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>15841</td>\n",
       "      <td>15841</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>correlation rely on the square of the difference, and when you have big outliers like this,</td>\n",
       "      <td>the square of the difference goes crazy, and so this is another place we'd want to look at the data first, and say oh that's that's going to be a bit of an issue. There's probably</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>15842</td>\n",
       "      <td>15842</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pearson correlation</td>\n",
       "      <td>the square of the difference goes crazy, and so this is another place we'd want to look at the data first, and say oh that's that's going to be a bit of an issue. There's probably</td>\n",
       "      <td>more correlation here, but there's a few examples of some houses with lots and lots of rooms</td>\n",
       "      <td>True</td>\n",
       "      <td>more correlation here, but there's a few examples of some houses with lots and lots of rooms</td>\n",
       "      <td>[using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>15843</td>\n",
       "      <td>15843</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>more correlation here, but there's a few examples of some houses with lots and lots of rooms</td>\n",
       "      <td>where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>15844</td>\n",
       "      <td>15844</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?</td>\n",
       "      <td>So, r is very sensitive to outliers. So let's get rid of the houses the rooms</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pret...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>15845</td>\n",
       "      <td>15845</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>So, r is very sensitive to outliers. So let's get rid of the houses the rooms</td>\n",
       "      <td>with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>15846</td>\n",
       "      <td>15846</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up</td>\n",
       "      <td>from 0.43 to 0.68, even though we probably only got rid of one two three four five six</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to mak...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>15847</td>\n",
       "      <td>15847</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>from 0.43 to 0.68, even though we probably only got rid of one two three four five six</td>\n",
       "      <td>seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row., So there's what a correlation of 0.68 loo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>15848</td>\n",
       "      <td>15848</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,</td>\n",
       "      <td>and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Becau...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>15849</td>\n",
       "      <td>15849</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row.</td>\n",
       "      <td>So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Because</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>15850</td>\n",
       "      <td>15850</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Because</td>\n",
       "      <td>0.34 sounds like quite a good relationship, but you almost can't see it! So this is something</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>15851</td>\n",
       "      <td>15851</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>0.34 sounds like quite a good relationship, but you almost can't see it! So this is something</td>\n",
       "      <td>I strongly suggest is, if you're working with a new metric, is: Draw some pictures of a</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>15852</td>\n",
       "      <td>15852</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>I strongly suggest is, if you're working with a new metric, is: Draw some pictures of a</td>\n",
       "      <td>few different levels of that metric to kind of try to get a feel for like what does it mean? You know, what does 0.6 look like? What does 0.3 look like? And so forth.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>15853</td>\n",
       "      <td>15853</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>few different levels of that metric to kind of try to get a feel for like what does it mean? You know, what does 0.6 look like? What does 0.3 look like? And so forth.</td>\n",
       "      <td>And here's an example of a correlation of minus 0.2. This very slight, negative slope.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>15854</td>\n",
       "      <td>15854</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>And here's an example of a correlation of minus 0.2. This very slight, negative slope.</td>\n",
       "      <td>Okay, so there's just more of a kind of a general tip, of something I like to do when playing with a new metric, and I recommend you do as well. I think we've now got a sense</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>15855</td>\n",
       "      <td>15855</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>Okay, so there's just more of a kind of a general tip, of something I like to do when playing with a new metric, and I recommend you do as well. I think we've now got a sense</td>\n",
       "      <td>of what the correlation feels like. Now you can go look up the equation on Wikipedia if you're into that kind of thing.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>15856</td>\n",
       "      <td>15856</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>of what the correlation feels like. Now you can go look up the equation on Wikipedia if you're into that kind of thing.</td>\n",
       "      <td>We need to report the correlation after each epoch because we want to know how our training</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>15857</td>\n",
       "      <td>15857</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>We need to report the correlation after each epoch because we want to know how our training</td>\n",
       "      <td>is going. Hugging Face expects you to return a dictionary because it's going to use the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>15858</td>\n",
       "      <td>15858</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>is going. Hugging Face expects you to return a dictionary because it's going to use the</td>\n",
       "      <td>keys of the dictionary to like label each metric. So here's something that gets</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>15859</td>\n",
       "      <td>15859</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>keys of the dictionary to like label each metric. So here's something that gets</td>\n",
       "      <td>the correlation, and returns it as a dictionary with the label pearson.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>15860</td>\n",
       "      <td>15860</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>the correlation, and returns it as a dictionary with the label pearson.</td>\n",
       "      <td>Okay, so we've done metrics, we've done our training/ validation split.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>15861</td>\n",
       "      <td>15861</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>Okay, so we've done metrics, we've done our training/ validation split.</td>\n",
       "      <td>Oh! we might have actually skipped over the bit where we actually did the split! Did I?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>15862</td>\n",
       "      <td>15862</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>Oh! we might have actually skipped over the bit where we actually did the split! Did I?</td>\n",
       "      <td>I did! So, to actually do the split, because in this Kaggle competition  I've got another notebook,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>15863</td>\n",
       "      <td>15863</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>I did! So, to actually do the split, because in this Kaggle competition  I've got another notebook,</td>\n",
       "      <td>we'll look at later, where we actually split this properly  but here we're just going to do a random split. Just to keep things simple for now, of 25 percent, will be</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>15864</td>\n",
       "      <td>15864</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>we'll look at later, where we actually split this properly  but here we're just going to do a random split. Just to keep things simple for now, of 25 percent, will be</td>\n",
       "      <td>of the data will be a validation set. So, if we go tok_ds.train_test_split() it returns</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>15865</td>\n",
       "      <td>15865</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>of the data will be a validation set. So, if we go tok_ds.train_test_split() it returns</td>\n",
       "      <td>a data set dict; which has a train, and a test. So that looks a lot like a datasets</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>15866</td>\n",
       "      <td>15866</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>a data set dict; which has a train, and a test. So that looks a lot like a datasets</td>\n",
       "      <td>object in fast.ai. Very similar idea!</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>15867</td>\n",
       "      <td>15867</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>object in fast.ai. Very similar idea!</td>\n",
       "      <td>So this will be the thing that we'll be able to train with, so it's going to train with this data set, and return the metrics on this data set. This is really a validation set</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>15868</td>\n",
       "      <td>15868</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>So this will be the thing that we'll be able to train with, so it's going to train with this data set, and return the metrics on this data set. This is really a validation set</td>\n",
       "      <td>but Hugging Face datasets calls it test.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15869</td>\n",
       "      <td>15869</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Correlation is sensitive to outliers</td>\n",
       "      <td>but Hugging Face datasets calls it test.</td>\n",
       "      <td>Okay. We're now ready to train our model. In fast.ai, we use something called a learner.</td>\n",
       "      <td>True</td>\n",
       "      <td>Okay. We're now ready to train our model. In fast.ai, we use something called a learner.</td>\n",
       "      <td>[more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>15870</td>\n",
       "      <td>15870</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>Okay. We're now ready to train our model. In fast.ai, we use something called a learner.</td>\n",
       "      <td>The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll, get an out of memory error on your GPU. So, you know, it's a bit of trial and err...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>15871</td>\n",
       "      <td>15871</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch</td>\n",
       "      <td>sizes. In short, each time we pass some data to our model for training, it's going</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll, get an out of memory error on your GPU. So, you know, it's a bit of trial ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>15872</td>\n",
       "      <td>15872</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>sizes. In short, each time we pass some data to our model for training, it's going</td>\n",
       "      <td>to return it's going to send through a few rows at a time to the GPU, so that it</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make it to...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>15873</td>\n",
       "      <td>15873</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>to return it's going to send through a few rows at a time to the GPU, so that it</td>\n",
       "      <td>can calculate those in parallel. Those a bunch of rows is called a batch or</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>15874</td>\n",
       "      <td>15874</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>can calculate those in parallel. Those a bunch of rows is called a batch or</td>\n",
       "      <td>a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll, get an out of memory error on your GPU. So, you know, it's a bit of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>15875</td>\n",
       "      <td>15875</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the</td>\n",
       "      <td>more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, get an out of memory error on your GPU. So, you know, it's a bit of trial and er...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>15876</td>\n",
       "      <td>15876</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll</td>\n",
       "      <td>get an out of memory error on your GPU. So, you know, it's a bit of trial and error</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>15877</td>\n",
       "      <td>15877</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>get an out of memory error on your GPU. So, you know, it's a bit of trial and error</td>\n",
       "      <td>to find a batch size that works. Epochs we've seen before. Then we've got the learning</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>15878</td>\n",
       "      <td>15878</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>to find a batch size that works. Epochs we've seen before. Then we've got the learning</td>\n",
       "      <td>rate. We'll talk in the next lesson  unless we get to this lesson  about a technique</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>15879</td>\n",
       "      <td>15879</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>rate. We'll talk in the next lesson  unless we get to this lesson  about a technique</td>\n",
       "      <td>to automatically find a or semi-automatically find a good learning rate. We already know what a learning rate is from the last lesson. I've played around and found one that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>15880</td>\n",
       "      <td>15880</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>to automatically find a or semi-automatically find a good learning rate. We already know what a learning rate is from the last lesson. I've played around and found one that</td>\n",
       "      <td>seems to train quite quickly without falling apart, so I just tried a few. Generally, I</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>15881</td>\n",
       "      <td>15881</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>seems to train quite quickly without falling apart, so I just tried a few. Generally, I</td>\n",
       "      <td>kind of, you know, if I if I don't have a so Hugging Face transformers doesn't</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>15882</td>\n",
       "      <td>15882</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>kind of, you know, if I if I don't have a so Hugging Face transformers doesn't</td>\n",
       "      <td>have something to help you find the learning rate. This the integration we're doing in fast.ai, will let you do that, but if you're using a framework that doesn't have that,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>15883</td>\n",
       "      <td>15883</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>have something to help you find the learning rate. This the integration we're doing in fast.ai, will let you do that, but if you're using a framework that doesn't have that,</td>\n",
       "      <td>you can just start with a really low learning rate, and then kind of double it, and keep doubling it until it falls apart.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>15884</td>\n",
       "      <td>15884</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>you can just start with a really low learning rate, and then kind of double it, and keep doubling it until it falls apart.</td>\n",
       "      <td>Hugging Face transformers uses this thing called training arguments which is a class where you just provide all of the kind of configuration so you have to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>15885</td>\n",
       "      <td>15885</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>Hugging Face transformers uses this thing called training arguments which is a class where you just provide all of the kind of configuration so you have to</td>\n",
       "      <td>tell it what your learning rate is. This stuff here is the same as what we call basically</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>15886</td>\n",
       "      <td>15886</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>tell it what your learning rate is. This stuff here is the same as what we call basically</td>\n",
       "      <td>fit_one_cycle() in fast.ai. You always want this to be true, because it's going to be faster pretty much</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>15887</td>\n",
       "      <td>15887</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>fit_one_cycle() in fast.ai. You always want this to be true, because it's going to be faster pretty much</td>\n",
       "      <td>and then the this stuff here, you can probably use exactly the same every time. There's probably a lot of boilerplate compared to fast.ai as you see. This stuff you can probably use the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>15888</td>\n",
       "      <td>15888</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>and then the this stuff here, you can probably use exactly the same every time. There's probably a lot of boilerplate compared to fast.ai as you see. This stuff you can probably use the</td>\n",
       "      <td>same every time. Okay, so We now need to create our model. So, the equivalent of the vision learner function that we've</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>15889</td>\n",
       "      <td>15889</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>same every time. Okay, so We now need to create our model. So, the equivalent of the vision learner function that we've</td>\n",
       "      <td>used to automatically create a reasonable vision model? In Hugging Face transformers,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>15890</td>\n",
       "      <td>15890</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>used to automatically create a reasonable vision model? In Hugging Face transformers,</td>\n",
       "      <td>they've got lots of different ones depending on what you're trying to do. So, we're trying to do classification as we've discussed, of sequences, so if we call AutoModelForSequenceClassification,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>15891</td>\n",
       "      <td>15891</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>they've got lots of different ones depending on what you're trying to do. So, we're trying to do classification as we've discussed, of sequences, so if we call AutoModelForSequenceClassification,</td>\n",
       "      <td>it will create a model that is appropriate for classifying sequences from a train pre-trained model, and this is the name of the model that we did earlier the deberta-v3.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>15892</td>\n",
       "      <td>15892</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>it will create a model that is appropriate for classifying sequences from a train pre-trained model, and this is the name of the model that we did earlier the deberta-v3.</td>\n",
       "      <td>It has to know when it adds that random matrix to the end, how many outputs it needs to have. So we have one label which is the score. So that's going to create our model, and then</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>15893</td>\n",
       "      <td>15893</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>It has to know when it adds that random matrix to the end, how many outputs it needs to have. So we have one label which is the score. So that's going to create our model, and then</td>\n",
       "      <td>this is the equivalent of creating a learner. It contains a model, and the data the training</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>15894</td>\n",
       "      <td>15894</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>this is the equivalent of creating a learner. It contains a model, and the data the training</td>\n",
       "      <td>data, and the test data. Again, there's a lot more boilerplate here than fast.ai, but you can kind of see the same basic steps here. We just have to do a little bit more manually,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>15895</td>\n",
       "      <td>15895</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>data, and the test data. Again, there's a lot more boilerplate here than fast.ai, but you can kind of see the same basic steps here. We just have to do a little bit more manually,</td>\n",
       "      <td>but it's not you know, it's nothing too crazy. So, it's going to tokenize it for us using that function, and then these are the metrics that will print out each time.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>15896</td>\n",
       "      <td>15896</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>but it's not you know, it's nothing too crazy. So, it's going to tokenize it for us using that function, and then these are the metrics that will print out each time.</td>\n",
       "      <td>That's that little function we created which returns a dictionary.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>15897</td>\n",
       "      <td>15897</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>That's that little function we created which returns a dictionary.</td>\n",
       "      <td>At the moment I find Hugging Face transformers very verbose. It spits out lots and lots and lots of text which you can ignore, and we can finally call train, which will spit out</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>15898</td>\n",
       "      <td>15898</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>At the moment I find Hugging Face transformers very verbose. It spits out lots and lots and lots of text which you can ignore, and we can finally call train, which will spit out</td>\n",
       "      <td>much more text again, which you can ignore, and as you can see, as it trains, it's printing</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>15899</td>\n",
       "      <td>15899</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>much more text again, which you can ignore, and as you can see, as it trains, it's printing</td>\n",
       "      <td>out the loss, and here's our Pearson correlation coefficient. So, it's training and we've got</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>15900</td>\n",
       "      <td>15900</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>out the loss, and here's our Pearson correlation coefficient. So, it's training and we've got</td>\n",
       "      <td>a 0.834 correlation, that's pretty cool! Right. I mean it took what ? Oh here we are five</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>15901</td>\n",
       "      <td>15901</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>a 0.834 correlation, that's pretty cool! Right. I mean it took what ? Oh here we are five</td>\n",
       "      <td>minutes to run. Maybe that's five minutes per epoch on Kaggle which doesn't have particularly great GPUs (but good for free) and we've got something that is you know got a very high</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>15902</td>\n",
       "      <td>15902</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>minutes to run. Maybe that's five minutes per epoch on Kaggle which doesn't have particularly great GPUs (but good for free) and we've got something that is you know got a very high</td>\n",
       "      <td>level of correlation in assessing how similar the two columns are, and the only reason it</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>15903</td>\n",
       "      <td>15903</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>level of correlation in assessing how similar the two columns are, and the only reason it</td>\n",
       "      <td>could do that is because it used a pre-trained model, right. There's no way you could just</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>15904</td>\n",
       "      <td>15904</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>could do that is because it used a pre-trained model, right. There's no way you could just</td>\n",
       "      <td>have that tiny amount of information and figure out whether those two columns are very similar.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>15905</td>\n",
       "      <td>15905</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>have that tiny amount of information and figure out whether those two columns are very similar.</td>\n",
       "      <td>This pre-trained model already knows a lot about language. It already has a good sense of whether two phrases are similar or not, and we've just fine-tuned it. You can see,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>15906</td>\n",
       "      <td>15906</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>This pre-trained model already knows a lot about language. It already has a good sense of whether two phrases are similar or not, and we've just fine-tuned it. You can see,</td>\n",
       "      <td>given that after one epoch it was already at 0.8. You know we this was a model that already did something pretty close to what we needed. It didn't really need that much</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>15907</td>\n",
       "      <td>15907</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>given that after one epoch it was already at 0.8. You know we this was a model that already did something pretty close to what we needed. It didn't really need that much</td>\n",
       "      <td>extra tuning for this particular task. We got any questions there John? Yeah we do! It's actually a bit back on the topic</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>15908</td>\n",
       "      <td>15908</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Training a model</td>\n",
       "      <td>extra tuning for this particular task. We got any questions there John? Yeah we do! It's actually a bit back on the topic</td>\n",
       "      <td>before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how</td>\n",
       "      <td>True</td>\n",
       "      <td>before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how</td>\n",
       "      <td>[Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>15909</td>\n",
       "      <td>15909</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how</td>\n",
       "      <td>do you decide when it's okay to remove outliers? Like, you pointed out some in that data</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interesting! it seems like there's a separate, group of districts wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>15910</td>\n",
       "      <td>15910</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>do you decide when it's okay to remove outliers? Like, you pointed out some in that data</td>\n",
       "      <td>set, and clearly your model is going to train a lot better if you clean that up; but I think</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be say...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>15911</td>\n",
       "      <td>15911</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>set, and clearly your model is going to train a lot better if you clean that up; but I think</td>\n",
       "      <td>Kevin's point here is, you know, those kinds of outliers will probably exist in the test</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be sa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>15912</td>\n",
       "      <td>15912</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>Kevin's point here is, you know, those kinds of outliers will probably exist in the test</td>\n",
       "      <td>set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interestin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>15913</td>\n",
       "      <td>15913</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense.</td>\n",
       "      <td>So, outliers should never just be removed, like, for modeling So if we take the example</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interestin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>15914</td>\n",
       "      <td>15914</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>So, outliers should never just be removed, like, for modeling So if we take the example</td>\n",
       "      <td>of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interesting! it seems like there's a separate</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., group of districts wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>15915</td>\n",
       "      <td>15915</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interesting! it seems like there's a separate</td>\n",
       "      <td>group of districts with a different kind of behavior. Yeah my guess is that they're going to be kind of like dorms or something like that, you know, probably low-income housing</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>15916</td>\n",
       "      <td>15916</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>group of districts with a different kind of behavior. Yeah my guess is that they're going to be kind of like dorms or something like that, you know, probably low-income housing</td>\n",
       "      <td>and so I would be saying like, oh clearly, from looking at this dataset, these two different</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>15917</td>\n",
       "      <td>15917</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>and so I would be saying like, oh clearly, from looking at this dataset, these two different</td>\n",
       "      <td>groups can't be treated the same way, they have very different behaviors, and I would probably split them into two separate analyses.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>15918</td>\n",
       "      <td>15918</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>groups can't be treated the same way, they have very different behaviors, and I would probably split them into two separate analyses.</td>\n",
       "      <td>You know the... the word outlier... it kind of exists in a statistical sense, right? There</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>15919</td>\n",
       "      <td>15919</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>You know the... the word outlier... it kind of exists in a statistical sense, right? There</td>\n",
       "      <td>can be things that are well outside our normal distribution and mess up our kind of metrics and things. It doesn't exist in a real sense. It doesn't exist in a sense of like... oh...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>15920</td>\n",
       "      <td>15920</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>can be things that are well outside our normal distribution and mess up our kind of metrics and things. It doesn't exist in a real sense. It doesn't exist in a sense of like... oh...</td>\n",
       "      <td>things that we should, like, ignore or throw away. You know, some of the most useful kind of insights I've had in my life in data projects</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>15921</td>\n",
       "      <td>15921</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>things that we should, like, ignore or throw away. You know, some of the most useful kind of insights I've had in my life in data projects</td>\n",
       "      <td>has been by digging into outliers...so-called outliers... and understanding: well, what</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>15922</td>\n",
       "      <td>15922</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>has been by digging into outliers...so-called outliers... and understanding: well, what</td>\n",
       "      <td>are they? And where did they come from? and it's kind of... often in those edge cases</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>15923</td>\n",
       "      <td>15923</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>are they? And where did they come from? and it's kind of... often in those edge cases</td>\n",
       "      <td>that you discover really important things about, like, where processes go wrong  or</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>15924</td>\n",
       "      <td>15924</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>that you discover really important things about, like, where processes go wrong  or</td>\n",
       "      <td>about, you know, kinds of behaviors you didn't even know existed, or indeed about, you know, kind of labeling problems or process problems which you really want to fix them at the source</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>15925</td>\n",
       "      <td>15925</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>about, you know, kinds of behaviors you didn't even know existed, or indeed about, you know, kind of labeling problems or process problems which you really want to fix them at the source</td>\n",
       "      <td>because otherwise when you go into production you're going to have more of those so-called outliers. So yeah. I'd say never delete outliers without investigating them and having a strategy</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>15926</td>\n",
       "      <td>15926</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>because otherwise when you go into production you're going to have more of those so-called outliers. So yeah. I'd say never delete outliers without investigating them and having a strategy</td>\n",
       "      <td>for ...like... understanding where they came from and ...like... what should you do about them. All right. So now that we've got a trained model, you'll see that it actually behaves,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>15927</td>\n",
       "      <td>15927</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: when is it ok to remove outliers?</td>\n",
       "      <td>for ...like... understanding where they came from and ...like... what should you do about them. All right. So now that we've got a trained model, you'll see that it actually behaves,</td>\n",
       "      <td>you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah</td>\n",
       "      <td>True</td>\n",
       "      <td>you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah</td>\n",
       "      <td>[before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>15928</td>\n",
       "      <td>15928</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah</td>\n",
       "      <td>this looks like stuff I've seen before, you know, like a bit more wordy and some slight</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,, I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>15929</td>\n",
       "      <td>15929</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>this looks like stuff I've seen before, you know, like a bit more wordy and some slight</td>\n",
       "      <td>changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,, I looked at quite a few Kaggle notebooks from other people, for this competition, and nea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>15930</td>\n",
       "      <td>15930</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're</td>\n",
       "      <td>going to pass in our dataset from the Kaggle test file  and so that's going to give</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, us our predictions, which we can cast to float., And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,, I looked at quite a few Kaggle notebooks from other people, for this competition, and...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>15931</td>\n",
       "      <td>15931</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>going to pass in our dataset from the Kaggle test file  and so that's going to give</td>\n",
       "      <td>us our predictions, which we can cast to float.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>15932</td>\n",
       "      <td>15932</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>us our predictions, which we can cast to float.</td>\n",
       "      <td>And here they are. So here are the predictions we made of similarity. Now again, not just</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, for your inputs but also for your outputs, always look at th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>15933</td>\n",
       "      <td>15933</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>And here they are. So here are the predictions we made of similarity. Now again, not just</td>\n",
       "      <td>for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., I looked at...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>15934</td>\n",
       "      <td>15934</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,</td>\n",
       "      <td>I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>15935</td>\n",
       "      <td>15935</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly</td>\n",
       "      <td>all of them had the problem we have right now, which is negative predictions and predictions</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>15936</td>\n",
       "      <td>15936</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>all of them had the problem we have right now, which is negative predictions and predictions</td>\n",
       "      <td>over one. So I'll be showing you how to fix this in a more proper way, maybe hopefully</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>15937</td>\n",
       "      <td>15937</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>over one. So I'll be showing you how to fix this in a more proper way, maybe hopefully</td>\n",
       "      <td>in the next lesson but for now you know we could at least just round these off ...right?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>15938</td>\n",
       "      <td>15938</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>in the next lesson but for now you know we could at least just round these off ...right?</td>\n",
       "      <td>because we know that none of the scores are going to be bigger than one or smaller than zero, so our correlation coefficient will definitely improve if we at least round</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>15939</td>\n",
       "      <td>15939</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>because we know that none of the scores are going to be bigger than one or smaller than zero, so our correlation coefficient will definitely improve if we at least round</td>\n",
       "      <td>this up to zero and round this down to one. As I said, there are better ways to do this</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>15940</td>\n",
       "      <td>15940</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>this up to zero and round this down to one. As I said, there are better ways to do this</td>\n",
       "      <td>but that's certainly better than nothing. So, in Pytorch, you might remember from when</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>15941</td>\n",
       "      <td>15941</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>but that's certainly better than nothing. So, in Pytorch, you might remember from when</td>\n",
       "      <td>we looked at ReLU, there's a thing called clip and that will clip everything under zero to zero and everything over one to one and so now that looks much better. So here's our</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>15942</td>\n",
       "      <td>15942</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>we looked at ReLU, there's a thing called clip and that will clip everything under zero to zero and everything over one to one and so now that looks much better. So here's our</td>\n",
       "      <td>predictions. So Kaggle expects submissions to generally be in a CSV file and Hugging</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>15943</td>\n",
       "      <td>15943</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>predictions. So Kaggle expects submissions to generally be in a CSV file and Hugging</td>\n",
       "      <td>Face datasets... it kind of looks a lot like pandas, really. We can create our submission</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>15944</td>\n",
       "      <td>15944</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>Face datasets... it kind of looks a lot like pandas, really. We can create our submission</td>\n",
       "      <td>file from... with our two columns called dot csv and there we go. That's basically it.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>15945</td>\n",
       "      <td>15945</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>file from... with our two columns called dot csv and there we go. That's basically it.</td>\n",
       "      <td>So yeah you know... it's... it's... it's kind of nice to see how... you know... it's a sense</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>15946</td>\n",
       "      <td>15946</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>So yeah you know... it's... it's... it's kind of nice to see how... you know... it's a sense</td>\n",
       "      <td>how far deep learning has come since we started this course a few years ago. That nowadays</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>15947</td>\n",
       "      <td>15947</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>how far deep learning has come since we started this course a few years ago. That nowadays</td>\n",
       "      <td>you know... there are multiple libraries around to kind of do this the same thing. We can, you know, use them in multiple application areas. They all look kind of pretty familiar.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>15948</td>\n",
       "      <td>15948</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>you know... there are multiple libraries around to kind of do this the same thing. We can, you know, use them in multiple application areas. They all look kind of pretty familiar.</td>\n",
       "      <td>They're reasonably beginner friendly. And NLP, because it's kind of like the most recent</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>15949</td>\n",
       "      <td>15949</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>They're reasonably beginner friendly. And NLP, because it's kind of like the most recent</td>\n",
       "      <td>area that's really become effective in the last year or two, is probably where the biggest</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>15950</td>\n",
       "      <td>15950</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>area that's really become effective in the last year or two, is probably where the biggest</td>\n",
       "      <td>opportunities are for, you know, big wins both in research and commercialization. And</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>15951</td>\n",
       "      <td>15951</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Predictions</td>\n",
       "      <td>opportunities are for, you know, big wins both in research and commercialization. And</td>\n",
       "      <td>so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would</td>\n",
       "      <td>True</td>\n",
       "      <td>so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would</td>\n",
       "      <td>[you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>15952</td>\n",
       "      <td>15952</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Opportunities for research and startups</td>\n",
       "      <td>so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would</td>\n",
       "      <td>you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[know, or it took 10 times more time or it took 10 times more money or whatever., So I think NLP is a huge opportunity area., It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>15953</td>\n",
       "      <td>15953</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Opportunities for research and startups</td>\n",
       "      <td>you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you</td>\n",
       "      <td>know, or it took 10 times more time or it took 10 times more money or whatever.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, So I think NLP is a huge opportunity area., It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>15954</td>\n",
       "      <td>15954</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Opportunities for research and startups</td>\n",
       "      <td>know, or it took 10 times more time or it took 10 times more money or whatever.</td>\n",
       "      <td>So I think NLP is a huge opportunity area.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you, It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>15955</td>\n",
       "      <td>15955</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Opportunities for research and startups</td>\n",
       "      <td>So I think NLP is a huge opportunity area.</td>\n",
       "      <td>It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you, know, or it took 10 times more time or it took 10 times more money or whatever.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>15956</td>\n",
       "      <td>15956</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Opportunities for research and startups</td>\n",
       "      <td>It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.</td>\n",
       "      <td>Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it.</td>\n",
       "      <td>True</td>\n",
       "      <td>Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it.</td>\n",
       "      <td>[so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you, know, or it took 10 times more time or it took 10 times more money or whatever., So I think NLP is a huge opportunity area.]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>15957</td>\n",
       "      <td>15957</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it.</td>\n",
       "      <td>So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which could create context, appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side, of an argument and...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>15958</td>\n",
       "      <td>15958</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?</td>\n",
       "      <td>And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which could create context, appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side, of an argument and you could scale that up such that 99% of twitter was these bots and, nobody woul...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>15959</td>\n",
       "      <td>15959</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much</td>\n",
       "      <td>much better now  so even then you could see these models were generating context appropriate</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, believable prose., You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which could create context, appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side, of an argument and you could scale that up such that 99% o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>15960</td>\n",
       "      <td>15960</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>much better now  so even then you could see these models were generating context appropriate</td>\n",
       "      <td>believable prose.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which coul...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>15961</td>\n",
       "      <td>15961</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>believable prose.</td>\n",
       "      <td>You know I would strongly believe that like any of our... kind of like... upper tier of</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, competent fast.ai alumni would be fairly easily able to create a bot whic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>15962</td>\n",
       "      <td>15962</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>You know I would strongly believe that like any of our... kind of like... upper tier of</td>\n",
       "      <td>competent fast.ai alumni would be fairly easily able to create a bot which could create context</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., appropriate prose on twitter or facebook groups or wha...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>15963</td>\n",
       "      <td>15963</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>competent fast.ai alumni would be fairly easily able to create a bot which could create context</td>\n",
       "      <td>appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>15964</td>\n",
       "      <td>15964</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side</td>\n",
       "      <td>of an argument and you could scale that up such that 99% of twitter was these bots and</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>15965</td>\n",
       "      <td>15965</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>of an argument and you could scale that up such that 99% of twitter was these bots and</td>\n",
       "      <td>nobody would know. You know, nobody would know. And that's very worrying to me because</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>15966</td>\n",
       "      <td>15966</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>nobody would know. You know, nobody would know. And that's very worrying to me because</td>\n",
       "      <td>a lot of, you know, a lot of...kind of... the way people see the world is now really</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>15967</td>\n",
       "      <td>15967</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>a lot of, you know, a lot of...kind of... the way people see the world is now really</td>\n",
       "      <td>coming out of their... their social media conversations, which at this point they're</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>15968</td>\n",
       "      <td>15968</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>coming out of their... their social media conversations, which at this point they're</td>\n",
       "      <td>controllable. Like... it would not be that hard to create something that's kind of optimized</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>15969</td>\n",
       "      <td>15969</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>controllable. Like... it would not be that hard to create something that's kind of optimized</td>\n",
       "      <td>towards moving a point of view amongst a billion people, you know, in a very subtle way, very</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>15970</td>\n",
       "      <td>15970</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>towards moving a point of view amongst a billion people, you know, in a very subtle way, very</td>\n",
       "      <td>gradually, over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>15971</td>\n",
       "      <td>15971</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>gradually, over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth.</td>\n",
       "      <td>Here is the start of an article in the Guardian which I'll let you read.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>15972</td>\n",
       "      <td>15972</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>Here is the start of an article in the Guardian which I'll let you read.</td>\n",
       "      <td>This article was, you know, quite long. These are just the first few paragraphs and at the end, it explains that this article was written by GPT3. It was given the instruction please</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>15973</td>\n",
       "      <td>15973</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>This article was, you know, quite long. These are just the first few paragraphs and at the end, it explains that this article was written by GPT3. It was given the instruction please</td>\n",
       "      <td>write a short op-ed around 500 words. Keep the language simple and concise. Focus on why humans have nothing to fear from AI. So GPT3 produced eight outputs and then they</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>15974</td>\n",
       "      <td>15974</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>write a short op-ed around 500 words. Keep the language simple and concise. Focus on why humans have nothing to fear from AI. So GPT3 produced eight outputs and then they</td>\n",
       "      <td>say, basically the... the editors at The Guardian did about the same level of editing that they would do for humans. In fact, they found it a bit less editing required than humans. So</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>15975</td>\n",
       "      <td>15975</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>say, basically the... the editors at The Guardian did about the same level of editing that they would do for humans. In fact, they found it a bit less editing required than humans. So</td>\n",
       "      <td>you know again, like, you can create longer pieces of context appropriate prose designed</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>15976</td>\n",
       "      <td>15976</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>you know again, like, you can create longer pieces of context appropriate prose designed</td>\n",
       "      <td>to argue a particular point of view. What kind of things might this be used for? You</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>15977</td>\n",
       "      <td>15977</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>to argue a particular point of view. What kind of things might this be used for? You</td>\n",
       "      <td>know, that we won't know probably for decades if ever but sometimes we get a clue based</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>15978</td>\n",
       "      <td>15978</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>know, that we won't know probably for decades if ever but sometimes we get a clue based</td>\n",
       "      <td>on older technology. Here's something from back 2017 and the pre ...kind of... deep learning</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>15979</td>\n",
       "      <td>15979</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>on older technology. Here's something from back 2017 and the pre ...kind of... deep learning</td>\n",
       "      <td>NLP days. There were millions of submissions to the FTC about the net neutrality situation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>15980</td>\n",
       "      <td>15980</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>NLP days. There were millions of submissions to the FTC about the net neutrality situation</td>\n",
       "      <td>in America. Very very heavily biased towards the point of view of saying we want to</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>15981</td>\n",
       "      <td>15981</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>in America. Very very heavily biased towards the point of view of saying we want to</td>\n",
       "      <td>get rid of net neutrality. An analysis by Jeff Kao showed that something like 99%</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>15982</td>\n",
       "      <td>15982</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>get rid of net neutrality. An analysis by Jeff Kao showed that something like 99%</td>\n",
       "      <td>of them and in particular, nearly all of the ones which were pro removal of net neutrality,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>15983</td>\n",
       "      <td>15983</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>of them and in particular, nearly all of the ones which were pro removal of net neutrality,</td>\n",
       "      <td>were clearly auto generated by basically ...if you look at the green, there's like, selecting</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>15984</td>\n",
       "      <td>15984</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>were clearly auto generated by basically ...if you look at the green, there's like, selecting</td>\n",
       "      <td>from a menu. So we've got... Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire individuals as opposed to Washington bureaucrats should</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>15985</td>\n",
       "      <td>15985</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>from a menu. So we've got... Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire individuals as opposed to Washington bureaucrats should</td>\n",
       "      <td>be blah...blah... people like me as opposed to so-called experts... and you get the idea. Now this is an example of a very very, you know, simple approach to auto-generating huge</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>15986</td>\n",
       "      <td>15986</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>be blah...blah... people like me as opposed to so-called experts... and you get the idea. Now this is an example of a very very, you know, simple approach to auto-generating huge</td>\n",
       "      <td>amounts of text. We don't know for sure but it looks like this might have been successful</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>15987</td>\n",
       "      <td>15987</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>amounts of text. We don't know for sure but it looks like this might have been successful</td>\n",
       "      <td>because this went through. You know, despite what seems to be actually overwhelming disagreement</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>15988</td>\n",
       "      <td>15988</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>because this went through. You know, despite what seems to be actually overwhelming disagreement</td>\n",
       "      <td>from the public that everybody, almost everybody, likes net neutrality, the FTC got rid of it</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>15989</td>\n",
       "      <td>15989</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>from the public that everybody, almost everybody, likes net neutrality, the FTC got rid of it</td>\n",
       "      <td>and this was a big part of the basis. Its like oh we got all these comments from the public and everybody said they don't want net neutrality. So imagine a similar thing</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>15990</td>\n",
       "      <td>15990</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>and this was a big part of the basis. Its like oh we got all these comments from the public and everybody said they don't want net neutrality. So imagine a similar thing</td>\n",
       "      <td>where you absolutely couldn't do this. You couldn't figure it out because everyone was really very compelling and very different. That's, you know, it's kind of worrying about</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>15991</td>\n",
       "      <td>15991</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>where you absolutely couldn't do this. You couldn't figure it out because everyone was really very compelling and very different. That's, you know, it's kind of worrying about</td>\n",
       "      <td>how we deal with that. You know, I will say... when I talk about this stuff, often people</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>15992</td>\n",
       "      <td>15992</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>how we deal with that. You know, I will say... when I talk about this stuff, often people</td>\n",
       "      <td>say ah no worries we'll build a model to recognize... you know... bot generated content</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>15993</td>\n",
       "      <td>15993</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>say ah no worries we'll build a model to recognize... you know... bot generated content</td>\n",
       "      <td>but, you know, if I put my black hat on, I'm like no that's not gonna work, right?. If you told me to build something that beats the bot classifiers, I'd say no worries,</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>15994</td>\n",
       "      <td>15994</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>but, you know, if I put my black hat on, I'm like no that's not gonna work, right?. If you told me to build something that beats the bot classifiers, I'd say no worries,</td>\n",
       "      <td>easy. You know, I will take the... the code or the service... or service... whatever that does the bot classifying and I will include beating that in my loss function and I will</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>15995</td>\n",
       "      <td>15995</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>easy. You know, I will take the... the code or the service... or service... whatever that does the bot classifying and I will include beating that in my loss function and I will</td>\n",
       "      <td>fine-tune my model until it beats the bot classifier. You know, when I used to run an</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>15996</td>\n",
       "      <td>15996</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>fine-tune my model until it beats the bot classifier. You know, when I used to run an</td>\n",
       "      <td>email company, we had a similar problem with spam prevention and our spammers could always</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>15997</td>\n",
       "      <td>15997</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>email company, we had a similar problem with spam prevention and our spammers could always</td>\n",
       "      <td>take a spam prevention algorithm and change their emails until it didn't get the spam</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>15998</td>\n",
       "      <td>15998</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>take a spam prevention algorithm and change their emails until it didn't get the spam</td>\n",
       "      <td>prevention algorithm anymore, for example. So yes, I... I'm really excited about the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>15999</td>\n",
       "      <td>15999</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>prevention algorithm anymore, for example. So yes, I... I'm really excited about the</td>\n",
       "      <td>opportunities for... for students in this course to build, you know, I think very valuable</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>16000</td>\n",
       "      <td>16000</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>opportunities for... for students in this course to build, you know, I think very valuable</td>\n",
       "      <td>businesses, really cool research and so forth using these pretty new NLP techniques that</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>16001</td>\n",
       "      <td>16001</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>businesses, really cool research and so forth using these pretty new NLP techniques that</td>\n",
       "      <td>are now pretty accessible and I'm also really worried about the things that might go wrong. I do think though that the more people that understand these capabilities the less chance</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>16002</td>\n",
       "      <td>16002</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>are now pretty accessible and I'm also really worried about the things that might go wrong. I do think though that the more people that understand these capabilities the less chance</td>\n",
       "      <td>they'll go wrong. John, was there some questions? Yeah I mean it's a throwback to the... to the workbook that you had before... yeah</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>16003</td>\n",
       "      <td>16003</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Misusing NLP</td>\n",
       "      <td>they'll go wrong. John, was there some questions? Yeah I mean it's a throwback to the... to the workbook that you had before... yeah</td>\n",
       "      <td>that's the one. The question Manikandan is asking... shouldn't num labels be five zero</td>\n",
       "      <td>True</td>\n",
       "      <td>that's the one. The question Manikandan is asking... shouldn't num labels be five zero</td>\n",
       "      <td>[Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>16004</td>\n",
       "      <td>16004</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>that's the one. The question Manikandan is asking... shouldn't num labels be five zero</td>\n",
       "      <td>zero point two five zero point five zero point seven five one instead of one? Isn't the target</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns i...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>16005</td>\n",
       "      <td>16005</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>zero point two five zero point five zero point seven five one instead of one? Isn't the target</td>\n",
       "      <td>a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which, is actually why we ended up wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>16006</td>\n",
       "      <td>16006</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if</td>\n",
       "      <td>this was being treated as a categorical problem with five categories, it's still considered one label.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which, is actually why we ended up with predi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>16007</td>\n",
       "      <td>16007</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>this was being treated as a categorical problem with five categories, it's still considered one label.</td>\n",
       "      <td>In this case though, we're actually treating it as a regression problem. It's one of the</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns it into a r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>16008</td>\n",
       "      <td>16008</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>In this case though, we're actually treating it as a regression problem. It's one of the</td>\n",
       "      <td>things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which, is actually why we ended up with predictions tha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>16009</td>\n",
       "      <td>16009</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if</td>\n",
       "      <td>you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, is actually why we ended up with predictions that were less than...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>16010</td>\n",
       "      <td>16010</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which</td>\n",
       "      <td>is actually why we ended up with predictions that were less than zero and bigger than one.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>16011</td>\n",
       "      <td>16011</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>is actually why we ended up with predictions that were less than zero and bigger than one.</td>\n",
       "      <td>So we'll be learning next time about the use of sigmoid functions to resolve this problem</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>16012</td>\n",
       "      <td>16012</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>So we'll be learning next time about the use of sigmoid functions to resolve this problem</td>\n",
       "      <td>and that should fix it up for us Okay, great. Well thanks everybody. I hope you enjoyed learning about NLP as much as</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>16013</td>\n",
       "      <td>16013</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>and that should fix it up for us Okay, great. Well thanks everybody. I hope you enjoyed learning about NLP as much as</td>\n",
       "      <td>I enjoyed putting this together. I'm really excited about it and can't wait for next week's</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>16014</td>\n",
       "      <td>16014</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>I enjoyed putting this together. I'm really excited about it and can't wait for next week's</td>\n",
       "      <td>lesson. See ya!</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>16015</td>\n",
       "      <td>16015</td>\n",
       "      <td>fast.ai 2022 - Part 1</td>\n",
       "      <td>4</td>\n",
       "      <td>Question: isnt the target categorical in this case?</td>\n",
       "      <td>lesson. See ya!</td>\n",
       "      <td>okay hi everybody and um welcome to practical deep learning for coders lesson five um we're uh at a stage now where we're going to be getting deeper and deeper into the details of how</td>\n",
       "      <td>True</td>\n",
       "      <td>okay hi everybody and um welcome to practical deep learning for coders lesson five um we're uh at a stage now where we're going to be getting deeper and deeper into the details of how</td>\n",
       "      <td>[that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0  index           course_title lesson_num  \\\n",
       "0      15307  15307  fast.ai 2022 - Part 1          4   \n",
       "1      15308  15308  fast.ai 2022 - Part 1          4   \n",
       "2      15309  15309  fast.ai 2022 - Part 1          4   \n",
       "3      15310  15310  fast.ai 2022 - Part 1          4   \n",
       "4      15311  15311  fast.ai 2022 - Part 1          4   \n",
       "5      15312  15312  fast.ai 2022 - Part 1          4   \n",
       "6      15313  15313  fast.ai 2022 - Part 1          4   \n",
       "7      15314  15314  fast.ai 2022 - Part 1          4   \n",
       "8      15315  15315  fast.ai 2022 - Part 1          4   \n",
       "9      15316  15316  fast.ai 2022 - Part 1          4   \n",
       "10     15317  15317  fast.ai 2022 - Part 1          4   \n",
       "11     15318  15318  fast.ai 2022 - Part 1          4   \n",
       "12     15319  15319  fast.ai 2022 - Part 1          4   \n",
       "13     15320  15320  fast.ai 2022 - Part 1          4   \n",
       "14     15321  15321  fast.ai 2022 - Part 1          4   \n",
       "15     15322  15322  fast.ai 2022 - Part 1          4   \n",
       "16     15323  15323  fast.ai 2022 - Part 1          4   \n",
       "17     15324  15324  fast.ai 2022 - Part 1          4   \n",
       "18     15325  15325  fast.ai 2022 - Part 1          4   \n",
       "19     15326  15326  fast.ai 2022 - Part 1          4   \n",
       "20     15327  15327  fast.ai 2022 - Part 1          4   \n",
       "21     15328  15328  fast.ai 2022 - Part 1          4   \n",
       "22     15329  15329  fast.ai 2022 - Part 1          4   \n",
       "23     15330  15330  fast.ai 2022 - Part 1          4   \n",
       "24     15331  15331  fast.ai 2022 - Part 1          4   \n",
       "25     15332  15332  fast.ai 2022 - Part 1          4   \n",
       "26     15333  15333  fast.ai 2022 - Part 1          4   \n",
       "27     15334  15334  fast.ai 2022 - Part 1          4   \n",
       "28     15335  15335  fast.ai 2022 - Part 1          4   \n",
       "29     15336  15336  fast.ai 2022 - Part 1          4   \n",
       "30     15337  15337  fast.ai 2022 - Part 1          4   \n",
       "31     15338  15338  fast.ai 2022 - Part 1          4   \n",
       "32     15339  15339  fast.ai 2022 - Part 1          4   \n",
       "33     15340  15340  fast.ai 2022 - Part 1          4   \n",
       "34     15341  15341  fast.ai 2022 - Part 1          4   \n",
       "35     15342  15342  fast.ai 2022 - Part 1          4   \n",
       "36     15343  15343  fast.ai 2022 - Part 1          4   \n",
       "37     15344  15344  fast.ai 2022 - Part 1          4   \n",
       "38     15345  15345  fast.ai 2022 - Part 1          4   \n",
       "39     15346  15346  fast.ai 2022 - Part 1          4   \n",
       "40     15347  15347  fast.ai 2022 - Part 1          4   \n",
       "41     15348  15348  fast.ai 2022 - Part 1          4   \n",
       "42     15349  15349  fast.ai 2022 - Part 1          4   \n",
       "43     15350  15350  fast.ai 2022 - Part 1          4   \n",
       "44     15351  15351  fast.ai 2022 - Part 1          4   \n",
       "45     15352  15352  fast.ai 2022 - Part 1          4   \n",
       "46     15353  15353  fast.ai 2022 - Part 1          4   \n",
       "47     15354  15354  fast.ai 2022 - Part 1          4   \n",
       "48     15355  15355  fast.ai 2022 - Part 1          4   \n",
       "49     15356  15356  fast.ai 2022 - Part 1          4   \n",
       "50     15357  15357  fast.ai 2022 - Part 1          4   \n",
       "51     15358  15358  fast.ai 2022 - Part 1          4   \n",
       "52     15359  15359  fast.ai 2022 - Part 1          4   \n",
       "53     15360  15360  fast.ai 2022 - Part 1          4   \n",
       "54     15361  15361  fast.ai 2022 - Part 1          4   \n",
       "55     15362  15362  fast.ai 2022 - Part 1          4   \n",
       "56     15363  15363  fast.ai 2022 - Part 1          4   \n",
       "57     15364  15364  fast.ai 2022 - Part 1          4   \n",
       "58     15365  15365  fast.ai 2022 - Part 1          4   \n",
       "59     15366  15366  fast.ai 2022 - Part 1          4   \n",
       "60     15367  15367  fast.ai 2022 - Part 1          4   \n",
       "61     15368  15368  fast.ai 2022 - Part 1          4   \n",
       "62     15369  15369  fast.ai 2022 - Part 1          4   \n",
       "63     15370  15370  fast.ai 2022 - Part 1          4   \n",
       "64     15371  15371  fast.ai 2022 - Part 1          4   \n",
       "65     15372  15372  fast.ai 2022 - Part 1          4   \n",
       "66     15373  15373  fast.ai 2022 - Part 1          4   \n",
       "67     15374  15374  fast.ai 2022 - Part 1          4   \n",
       "68     15375  15375  fast.ai 2022 - Part 1          4   \n",
       "69     15376  15376  fast.ai 2022 - Part 1          4   \n",
       "70     15377  15377  fast.ai 2022 - Part 1          4   \n",
       "71     15378  15378  fast.ai 2022 - Part 1          4   \n",
       "72     15379  15379  fast.ai 2022 - Part 1          4   \n",
       "73     15380  15380  fast.ai 2022 - Part 1          4   \n",
       "74     15381  15381  fast.ai 2022 - Part 1          4   \n",
       "75     15382  15382  fast.ai 2022 - Part 1          4   \n",
       "76     15383  15383  fast.ai 2022 - Part 1          4   \n",
       "77     15384  15384  fast.ai 2022 - Part 1          4   \n",
       "78     15385  15385  fast.ai 2022 - Part 1          4   \n",
       "79     15386  15386  fast.ai 2022 - Part 1          4   \n",
       "80     15387  15387  fast.ai 2022 - Part 1          4   \n",
       "81     15388  15388  fast.ai 2022 - Part 1          4   \n",
       "82     15389  15389  fast.ai 2022 - Part 1          4   \n",
       "83     15390  15390  fast.ai 2022 - Part 1          4   \n",
       "84     15391  15391  fast.ai 2022 - Part 1          4   \n",
       "85     15392  15392  fast.ai 2022 - Part 1          4   \n",
       "86     15393  15393  fast.ai 2022 - Part 1          4   \n",
       "87     15394  15394  fast.ai 2022 - Part 1          4   \n",
       "88     15395  15395  fast.ai 2022 - Part 1          4   \n",
       "89     15396  15396  fast.ai 2022 - Part 1          4   \n",
       "90     15397  15397  fast.ai 2022 - Part 1          4   \n",
       "91     15398  15398  fast.ai 2022 - Part 1          4   \n",
       "92     15399  15399  fast.ai 2022 - Part 1          4   \n",
       "93     15400  15400  fast.ai 2022 - Part 1          4   \n",
       "94     15401  15401  fast.ai 2022 - Part 1          4   \n",
       "95     15402  15402  fast.ai 2022 - Part 1          4   \n",
       "96     15403  15403  fast.ai 2022 - Part 1          4   \n",
       "97     15404  15404  fast.ai 2022 - Part 1          4   \n",
       "98     15405  15405  fast.ai 2022 - Part 1          4   \n",
       "99     15406  15406  fast.ai 2022 - Part 1          4   \n",
       "100    15407  15407  fast.ai 2022 - Part 1          4   \n",
       "101    15408  15408  fast.ai 2022 - Part 1          4   \n",
       "102    15409  15409  fast.ai 2022 - Part 1          4   \n",
       "103    15410  15410  fast.ai 2022 - Part 1          4   \n",
       "104    15411  15411  fast.ai 2022 - Part 1          4   \n",
       "105    15412  15412  fast.ai 2022 - Part 1          4   \n",
       "106    15413  15413  fast.ai 2022 - Part 1          4   \n",
       "107    15414  15414  fast.ai 2022 - Part 1          4   \n",
       "108    15415  15415  fast.ai 2022 - Part 1          4   \n",
       "109    15416  15416  fast.ai 2022 - Part 1          4   \n",
       "110    15417  15417  fast.ai 2022 - Part 1          4   \n",
       "111    15418  15418  fast.ai 2022 - Part 1          4   \n",
       "112    15419  15419  fast.ai 2022 - Part 1          4   \n",
       "113    15420  15420  fast.ai 2022 - Part 1          4   \n",
       "114    15421  15421  fast.ai 2022 - Part 1          4   \n",
       "115    15422  15422  fast.ai 2022 - Part 1          4   \n",
       "116    15423  15423  fast.ai 2022 - Part 1          4   \n",
       "117    15424  15424  fast.ai 2022 - Part 1          4   \n",
       "118    15425  15425  fast.ai 2022 - Part 1          4   \n",
       "119    15426  15426  fast.ai 2022 - Part 1          4   \n",
       "120    15427  15427  fast.ai 2022 - Part 1          4   \n",
       "121    15428  15428  fast.ai 2022 - Part 1          4   \n",
       "122    15429  15429  fast.ai 2022 - Part 1          4   \n",
       "123    15430  15430  fast.ai 2022 - Part 1          4   \n",
       "124    15431  15431  fast.ai 2022 - Part 1          4   \n",
       "125    15432  15432  fast.ai 2022 - Part 1          4   \n",
       "126    15433  15433  fast.ai 2022 - Part 1          4   \n",
       "127    15434  15434  fast.ai 2022 - Part 1          4   \n",
       "128    15435  15435  fast.ai 2022 - Part 1          4   \n",
       "129    15436  15436  fast.ai 2022 - Part 1          4   \n",
       "130    15437  15437  fast.ai 2022 - Part 1          4   \n",
       "131    15438  15438  fast.ai 2022 - Part 1          4   \n",
       "132    15439  15439  fast.ai 2022 - Part 1          4   \n",
       "133    15440  15440  fast.ai 2022 - Part 1          4   \n",
       "134    15441  15441  fast.ai 2022 - Part 1          4   \n",
       "135    15442  15442  fast.ai 2022 - Part 1          4   \n",
       "136    15443  15443  fast.ai 2022 - Part 1          4   \n",
       "137    15444  15444  fast.ai 2022 - Part 1          4   \n",
       "138    15445  15445  fast.ai 2022 - Part 1          4   \n",
       "139    15446  15446  fast.ai 2022 - Part 1          4   \n",
       "140    15447  15447  fast.ai 2022 - Part 1          4   \n",
       "141    15448  15448  fast.ai 2022 - Part 1          4   \n",
       "142    15449  15449  fast.ai 2022 - Part 1          4   \n",
       "143    15450  15450  fast.ai 2022 - Part 1          4   \n",
       "144    15451  15451  fast.ai 2022 - Part 1          4   \n",
       "145    15452  15452  fast.ai 2022 - Part 1          4   \n",
       "146    15453  15453  fast.ai 2022 - Part 1          4   \n",
       "147    15454  15454  fast.ai 2022 - Part 1          4   \n",
       "148    15455  15455  fast.ai 2022 - Part 1          4   \n",
       "149    15456  15456  fast.ai 2022 - Part 1          4   \n",
       "150    15457  15457  fast.ai 2022 - Part 1          4   \n",
       "151    15458  15458  fast.ai 2022 - Part 1          4   \n",
       "152    15459  15459  fast.ai 2022 - Part 1          4   \n",
       "153    15460  15460  fast.ai 2022 - Part 1          4   \n",
       "154    15461  15461  fast.ai 2022 - Part 1          4   \n",
       "155    15462  15462  fast.ai 2022 - Part 1          4   \n",
       "156    15463  15463  fast.ai 2022 - Part 1          4   \n",
       "157    15464  15464  fast.ai 2022 - Part 1          4   \n",
       "158    15465  15465  fast.ai 2022 - Part 1          4   \n",
       "159    15466  15466  fast.ai 2022 - Part 1          4   \n",
       "160    15467  15467  fast.ai 2022 - Part 1          4   \n",
       "161    15468  15468  fast.ai 2022 - Part 1          4   \n",
       "162    15469  15469  fast.ai 2022 - Part 1          4   \n",
       "163    15470  15470  fast.ai 2022 - Part 1          4   \n",
       "164    15471  15471  fast.ai 2022 - Part 1          4   \n",
       "165    15472  15472  fast.ai 2022 - Part 1          4   \n",
       "166    15473  15473  fast.ai 2022 - Part 1          4   \n",
       "167    15474  15474  fast.ai 2022 - Part 1          4   \n",
       "168    15475  15475  fast.ai 2022 - Part 1          4   \n",
       "169    15476  15476  fast.ai 2022 - Part 1          4   \n",
       "170    15477  15477  fast.ai 2022 - Part 1          4   \n",
       "171    15478  15478  fast.ai 2022 - Part 1          4   \n",
       "172    15479  15479  fast.ai 2022 - Part 1          4   \n",
       "173    15480  15480  fast.ai 2022 - Part 1          4   \n",
       "174    15481  15481  fast.ai 2022 - Part 1          4   \n",
       "175    15482  15482  fast.ai 2022 - Part 1          4   \n",
       "176    15483  15483  fast.ai 2022 - Part 1          4   \n",
       "177    15484  15484  fast.ai 2022 - Part 1          4   \n",
       "178    15485  15485  fast.ai 2022 - Part 1          4   \n",
       "179    15486  15486  fast.ai 2022 - Part 1          4   \n",
       "180    15487  15487  fast.ai 2022 - Part 1          4   \n",
       "181    15488  15488  fast.ai 2022 - Part 1          4   \n",
       "182    15489  15489  fast.ai 2022 - Part 1          4   \n",
       "183    15490  15490  fast.ai 2022 - Part 1          4   \n",
       "184    15491  15491  fast.ai 2022 - Part 1          4   \n",
       "185    15492  15492  fast.ai 2022 - Part 1          4   \n",
       "186    15493  15493  fast.ai 2022 - Part 1          4   \n",
       "187    15494  15494  fast.ai 2022 - Part 1          4   \n",
       "188    15495  15495  fast.ai 2022 - Part 1          4   \n",
       "189    15496  15496  fast.ai 2022 - Part 1          4   \n",
       "190    15497  15497  fast.ai 2022 - Part 1          4   \n",
       "191    15498  15498  fast.ai 2022 - Part 1          4   \n",
       "192    15499  15499  fast.ai 2022 - Part 1          4   \n",
       "193    15500  15500  fast.ai 2022 - Part 1          4   \n",
       "194    15501  15501  fast.ai 2022 - Part 1          4   \n",
       "195    15502  15502  fast.ai 2022 - Part 1          4   \n",
       "196    15503  15503  fast.ai 2022 - Part 1          4   \n",
       "197    15504  15504  fast.ai 2022 - Part 1          4   \n",
       "198    15505  15505  fast.ai 2022 - Part 1          4   \n",
       "199    15506  15506  fast.ai 2022 - Part 1          4   \n",
       "200    15507  15507  fast.ai 2022 - Part 1          4   \n",
       "201    15508  15508  fast.ai 2022 - Part 1          4   \n",
       "202    15509  15509  fast.ai 2022 - Part 1          4   \n",
       "203    15510  15510  fast.ai 2022 - Part 1          4   \n",
       "204    15511  15511  fast.ai 2022 - Part 1          4   \n",
       "205    15512  15512  fast.ai 2022 - Part 1          4   \n",
       "206    15513  15513  fast.ai 2022 - Part 1          4   \n",
       "207    15514  15514  fast.ai 2022 - Part 1          4   \n",
       "208    15515  15515  fast.ai 2022 - Part 1          4   \n",
       "209    15516  15516  fast.ai 2022 - Part 1          4   \n",
       "210    15517  15517  fast.ai 2022 - Part 1          4   \n",
       "211    15518  15518  fast.ai 2022 - Part 1          4   \n",
       "212    15519  15519  fast.ai 2022 - Part 1          4   \n",
       "213    15520  15520  fast.ai 2022 - Part 1          4   \n",
       "214    15521  15521  fast.ai 2022 - Part 1          4   \n",
       "215    15522  15522  fast.ai 2022 - Part 1          4   \n",
       "216    15523  15523  fast.ai 2022 - Part 1          4   \n",
       "217    15524  15524  fast.ai 2022 - Part 1          4   \n",
       "218    15525  15525  fast.ai 2022 - Part 1          4   \n",
       "219    15526  15526  fast.ai 2022 - Part 1          4   \n",
       "220    15527  15527  fast.ai 2022 - Part 1          4   \n",
       "221    15528  15528  fast.ai 2022 - Part 1          4   \n",
       "222    15529  15529  fast.ai 2022 - Part 1          4   \n",
       "223    15530  15530  fast.ai 2022 - Part 1          4   \n",
       "224    15531  15531  fast.ai 2022 - Part 1          4   \n",
       "225    15532  15532  fast.ai 2022 - Part 1          4   \n",
       "226    15533  15533  fast.ai 2022 - Part 1          4   \n",
       "227    15534  15534  fast.ai 2022 - Part 1          4   \n",
       "228    15535  15535  fast.ai 2022 - Part 1          4   \n",
       "229    15536  15536  fast.ai 2022 - Part 1          4   \n",
       "230    15537  15537  fast.ai 2022 - Part 1          4   \n",
       "231    15538  15538  fast.ai 2022 - Part 1          4   \n",
       "232    15539  15539  fast.ai 2022 - Part 1          4   \n",
       "233    15540  15540  fast.ai 2022 - Part 1          4   \n",
       "234    15541  15541  fast.ai 2022 - Part 1          4   \n",
       "235    15542  15542  fast.ai 2022 - Part 1          4   \n",
       "236    15543  15543  fast.ai 2022 - Part 1          4   \n",
       "237    15544  15544  fast.ai 2022 - Part 1          4   \n",
       "238    15545  15545  fast.ai 2022 - Part 1          4   \n",
       "239    15546  15546  fast.ai 2022 - Part 1          4   \n",
       "240    15547  15547  fast.ai 2022 - Part 1          4   \n",
       "241    15548  15548  fast.ai 2022 - Part 1          4   \n",
       "242    15549  15549  fast.ai 2022 - Part 1          4   \n",
       "243    15550  15550  fast.ai 2022 - Part 1          4   \n",
       "244    15551  15551  fast.ai 2022 - Part 1          4   \n",
       "245    15552  15552  fast.ai 2022 - Part 1          4   \n",
       "246    15553  15553  fast.ai 2022 - Part 1          4   \n",
       "247    15554  15554  fast.ai 2022 - Part 1          4   \n",
       "248    15555  15555  fast.ai 2022 - Part 1          4   \n",
       "249    15556  15556  fast.ai 2022 - Part 1          4   \n",
       "250    15557  15557  fast.ai 2022 - Part 1          4   \n",
       "251    15558  15558  fast.ai 2022 - Part 1          4   \n",
       "252    15559  15559  fast.ai 2022 - Part 1          4   \n",
       "253    15560  15560  fast.ai 2022 - Part 1          4   \n",
       "254    15561  15561  fast.ai 2022 - Part 1          4   \n",
       "255    15562  15562  fast.ai 2022 - Part 1          4   \n",
       "256    15563  15563  fast.ai 2022 - Part 1          4   \n",
       "257    15564  15564  fast.ai 2022 - Part 1          4   \n",
       "258    15565  15565  fast.ai 2022 - Part 1          4   \n",
       "259    15566  15566  fast.ai 2022 - Part 1          4   \n",
       "260    15567  15567  fast.ai 2022 - Part 1          4   \n",
       "261    15568  15568  fast.ai 2022 - Part 1          4   \n",
       "262    15569  15569  fast.ai 2022 - Part 1          4   \n",
       "263    15570  15570  fast.ai 2022 - Part 1          4   \n",
       "264    15571  15571  fast.ai 2022 - Part 1          4   \n",
       "265    15572  15572  fast.ai 2022 - Part 1          4   \n",
       "266    15573  15573  fast.ai 2022 - Part 1          4   \n",
       "267    15574  15574  fast.ai 2022 - Part 1          4   \n",
       "268    15575  15575  fast.ai 2022 - Part 1          4   \n",
       "269    15576  15576  fast.ai 2022 - Part 1          4   \n",
       "270    15577  15577  fast.ai 2022 - Part 1          4   \n",
       "271    15578  15578  fast.ai 2022 - Part 1          4   \n",
       "272    15579  15579  fast.ai 2022 - Part 1          4   \n",
       "273    15580  15580  fast.ai 2022 - Part 1          4   \n",
       "274    15581  15581  fast.ai 2022 - Part 1          4   \n",
       "275    15582  15582  fast.ai 2022 - Part 1          4   \n",
       "276    15583  15583  fast.ai 2022 - Part 1          4   \n",
       "277    15584  15584  fast.ai 2022 - Part 1          4   \n",
       "278    15585  15585  fast.ai 2022 - Part 1          4   \n",
       "279    15586  15586  fast.ai 2022 - Part 1          4   \n",
       "280    15587  15587  fast.ai 2022 - Part 1          4   \n",
       "281    15588  15588  fast.ai 2022 - Part 1          4   \n",
       "282    15589  15589  fast.ai 2022 - Part 1          4   \n",
       "283    15590  15590  fast.ai 2022 - Part 1          4   \n",
       "284    15591  15591  fast.ai 2022 - Part 1          4   \n",
       "285    15592  15592  fast.ai 2022 - Part 1          4   \n",
       "286    15593  15593  fast.ai 2022 - Part 1          4   \n",
       "287    15594  15594  fast.ai 2022 - Part 1          4   \n",
       "288    15595  15595  fast.ai 2022 - Part 1          4   \n",
       "289    15596  15596  fast.ai 2022 - Part 1          4   \n",
       "290    15597  15597  fast.ai 2022 - Part 1          4   \n",
       "291    15598  15598  fast.ai 2022 - Part 1          4   \n",
       "292    15599  15599  fast.ai 2022 - Part 1          4   \n",
       "293    15600  15600  fast.ai 2022 - Part 1          4   \n",
       "294    15601  15601  fast.ai 2022 - Part 1          4   \n",
       "295    15602  15602  fast.ai 2022 - Part 1          4   \n",
       "296    15603  15603  fast.ai 2022 - Part 1          4   \n",
       "297    15604  15604  fast.ai 2022 - Part 1          4   \n",
       "298    15605  15605  fast.ai 2022 - Part 1          4   \n",
       "299    15606  15606  fast.ai 2022 - Part 1          4   \n",
       "300    15607  15607  fast.ai 2022 - Part 1          4   \n",
       "301    15608  15608  fast.ai 2022 - Part 1          4   \n",
       "302    15609  15609  fast.ai 2022 - Part 1          4   \n",
       "303    15610  15610  fast.ai 2022 - Part 1          4   \n",
       "304    15611  15611  fast.ai 2022 - Part 1          4   \n",
       "305    15612  15612  fast.ai 2022 - Part 1          4   \n",
       "306    15613  15613  fast.ai 2022 - Part 1          4   \n",
       "307    15614  15614  fast.ai 2022 - Part 1          4   \n",
       "308    15615  15615  fast.ai 2022 - Part 1          4   \n",
       "309    15616  15616  fast.ai 2022 - Part 1          4   \n",
       "310    15617  15617  fast.ai 2022 - Part 1          4   \n",
       "311    15618  15618  fast.ai 2022 - Part 1          4   \n",
       "312    15619  15619  fast.ai 2022 - Part 1          4   \n",
       "313    15620  15620  fast.ai 2022 - Part 1          4   \n",
       "314    15621  15621  fast.ai 2022 - Part 1          4   \n",
       "315    15622  15622  fast.ai 2022 - Part 1          4   \n",
       "316    15623  15623  fast.ai 2022 - Part 1          4   \n",
       "317    15624  15624  fast.ai 2022 - Part 1          4   \n",
       "318    15625  15625  fast.ai 2022 - Part 1          4   \n",
       "319    15626  15626  fast.ai 2022 - Part 1          4   \n",
       "320    15627  15627  fast.ai 2022 - Part 1          4   \n",
       "321    15628  15628  fast.ai 2022 - Part 1          4   \n",
       "322    15629  15629  fast.ai 2022 - Part 1          4   \n",
       "323    15630  15630  fast.ai 2022 - Part 1          4   \n",
       "324    15631  15631  fast.ai 2022 - Part 1          4   \n",
       "325    15632  15632  fast.ai 2022 - Part 1          4   \n",
       "326    15633  15633  fast.ai 2022 - Part 1          4   \n",
       "327    15634  15634  fast.ai 2022 - Part 1          4   \n",
       "328    15635  15635  fast.ai 2022 - Part 1          4   \n",
       "329    15636  15636  fast.ai 2022 - Part 1          4   \n",
       "330    15637  15637  fast.ai 2022 - Part 1          4   \n",
       "331    15638  15638  fast.ai 2022 - Part 1          4   \n",
       "332    15639  15639  fast.ai 2022 - Part 1          4   \n",
       "333    15640  15640  fast.ai 2022 - Part 1          4   \n",
       "334    15641  15641  fast.ai 2022 - Part 1          4   \n",
       "335    15642  15642  fast.ai 2022 - Part 1          4   \n",
       "336    15643  15643  fast.ai 2022 - Part 1          4   \n",
       "337    15644  15644  fast.ai 2022 - Part 1          4   \n",
       "338    15645  15645  fast.ai 2022 - Part 1          4   \n",
       "339    15646  15646  fast.ai 2022 - Part 1          4   \n",
       "340    15647  15647  fast.ai 2022 - Part 1          4   \n",
       "341    15648  15648  fast.ai 2022 - Part 1          4   \n",
       "342    15649  15649  fast.ai 2022 - Part 1          4   \n",
       "343    15650  15650  fast.ai 2022 - Part 1          4   \n",
       "344    15651  15651  fast.ai 2022 - Part 1          4   \n",
       "345    15652  15652  fast.ai 2022 - Part 1          4   \n",
       "346    15653  15653  fast.ai 2022 - Part 1          4   \n",
       "347    15654  15654  fast.ai 2022 - Part 1          4   \n",
       "348    15655  15655  fast.ai 2022 - Part 1          4   \n",
       "349    15656  15656  fast.ai 2022 - Part 1          4   \n",
       "350    15657  15657  fast.ai 2022 - Part 1          4   \n",
       "351    15658  15658  fast.ai 2022 - Part 1          4   \n",
       "352    15659  15659  fast.ai 2022 - Part 1          4   \n",
       "353    15660  15660  fast.ai 2022 - Part 1          4   \n",
       "354    15661  15661  fast.ai 2022 - Part 1          4   \n",
       "355    15662  15662  fast.ai 2022 - Part 1          4   \n",
       "356    15663  15663  fast.ai 2022 - Part 1          4   \n",
       "357    15664  15664  fast.ai 2022 - Part 1          4   \n",
       "358    15665  15665  fast.ai 2022 - Part 1          4   \n",
       "359    15666  15666  fast.ai 2022 - Part 1          4   \n",
       "360    15667  15667  fast.ai 2022 - Part 1          4   \n",
       "361    15668  15668  fast.ai 2022 - Part 1          4   \n",
       "362    15669  15669  fast.ai 2022 - Part 1          4   \n",
       "363    15670  15670  fast.ai 2022 - Part 1          4   \n",
       "364    15671  15671  fast.ai 2022 - Part 1          4   \n",
       "365    15672  15672  fast.ai 2022 - Part 1          4   \n",
       "366    15673  15673  fast.ai 2022 - Part 1          4   \n",
       "367    15674  15674  fast.ai 2022 - Part 1          4   \n",
       "368    15675  15675  fast.ai 2022 - Part 1          4   \n",
       "369    15676  15676  fast.ai 2022 - Part 1          4   \n",
       "370    15677  15677  fast.ai 2022 - Part 1          4   \n",
       "371    15678  15678  fast.ai 2022 - Part 1          4   \n",
       "372    15679  15679  fast.ai 2022 - Part 1          4   \n",
       "373    15680  15680  fast.ai 2022 - Part 1          4   \n",
       "374    15681  15681  fast.ai 2022 - Part 1          4   \n",
       "375    15682  15682  fast.ai 2022 - Part 1          4   \n",
       "376    15683  15683  fast.ai 2022 - Part 1          4   \n",
       "377    15684  15684  fast.ai 2022 - Part 1          4   \n",
       "378    15685  15685  fast.ai 2022 - Part 1          4   \n",
       "379    15686  15686  fast.ai 2022 - Part 1          4   \n",
       "380    15687  15687  fast.ai 2022 - Part 1          4   \n",
       "381    15688  15688  fast.ai 2022 - Part 1          4   \n",
       "382    15689  15689  fast.ai 2022 - Part 1          4   \n",
       "383    15690  15690  fast.ai 2022 - Part 1          4   \n",
       "384    15691  15691  fast.ai 2022 - Part 1          4   \n",
       "385    15692  15692  fast.ai 2022 - Part 1          4   \n",
       "386    15693  15693  fast.ai 2022 - Part 1          4   \n",
       "387    15694  15694  fast.ai 2022 - Part 1          4   \n",
       "388    15695  15695  fast.ai 2022 - Part 1          4   \n",
       "389    15696  15696  fast.ai 2022 - Part 1          4   \n",
       "390    15697  15697  fast.ai 2022 - Part 1          4   \n",
       "391    15698  15698  fast.ai 2022 - Part 1          4   \n",
       "392    15699  15699  fast.ai 2022 - Part 1          4   \n",
       "393    15700  15700  fast.ai 2022 - Part 1          4   \n",
       "394    15701  15701  fast.ai 2022 - Part 1          4   \n",
       "395    15702  15702  fast.ai 2022 - Part 1          4   \n",
       "396    15703  15703  fast.ai 2022 - Part 1          4   \n",
       "397    15704  15704  fast.ai 2022 - Part 1          4   \n",
       "398    15705  15705  fast.ai 2022 - Part 1          4   \n",
       "399    15706  15706  fast.ai 2022 - Part 1          4   \n",
       "400    15707  15707  fast.ai 2022 - Part 1          4   \n",
       "401    15708  15708  fast.ai 2022 - Part 1          4   \n",
       "402    15709  15709  fast.ai 2022 - Part 1          4   \n",
       "403    15710  15710  fast.ai 2022 - Part 1          4   \n",
       "404    15711  15711  fast.ai 2022 - Part 1          4   \n",
       "405    15712  15712  fast.ai 2022 - Part 1          4   \n",
       "406    15713  15713  fast.ai 2022 - Part 1          4   \n",
       "407    15714  15714  fast.ai 2022 - Part 1          4   \n",
       "408    15715  15715  fast.ai 2022 - Part 1          4   \n",
       "409    15716  15716  fast.ai 2022 - Part 1          4   \n",
       "410    15717  15717  fast.ai 2022 - Part 1          4   \n",
       "411    15718  15718  fast.ai 2022 - Part 1          4   \n",
       "412    15719  15719  fast.ai 2022 - Part 1          4   \n",
       "413    15720  15720  fast.ai 2022 - Part 1          4   \n",
       "414    15721  15721  fast.ai 2022 - Part 1          4   \n",
       "415    15722  15722  fast.ai 2022 - Part 1          4   \n",
       "416    15723  15723  fast.ai 2022 - Part 1          4   \n",
       "417    15724  15724  fast.ai 2022 - Part 1          4   \n",
       "418    15725  15725  fast.ai 2022 - Part 1          4   \n",
       "419    15726  15726  fast.ai 2022 - Part 1          4   \n",
       "420    15727  15727  fast.ai 2022 - Part 1          4   \n",
       "421    15728  15728  fast.ai 2022 - Part 1          4   \n",
       "422    15729  15729  fast.ai 2022 - Part 1          4   \n",
       "423    15730  15730  fast.ai 2022 - Part 1          4   \n",
       "424    15731  15731  fast.ai 2022 - Part 1          4   \n",
       "425    15732  15732  fast.ai 2022 - Part 1          4   \n",
       "426    15733  15733  fast.ai 2022 - Part 1          4   \n",
       "427    15734  15734  fast.ai 2022 - Part 1          4   \n",
       "428    15735  15735  fast.ai 2022 - Part 1          4   \n",
       "429    15736  15736  fast.ai 2022 - Part 1          4   \n",
       "430    15737  15737  fast.ai 2022 - Part 1          4   \n",
       "431    15738  15738  fast.ai 2022 - Part 1          4   \n",
       "432    15739  15739  fast.ai 2022 - Part 1          4   \n",
       "433    15740  15740  fast.ai 2022 - Part 1          4   \n",
       "434    15741  15741  fast.ai 2022 - Part 1          4   \n",
       "435    15742  15742  fast.ai 2022 - Part 1          4   \n",
       "436    15743  15743  fast.ai 2022 - Part 1          4   \n",
       "437    15744  15744  fast.ai 2022 - Part 1          4   \n",
       "438    15745  15745  fast.ai 2022 - Part 1          4   \n",
       "439    15746  15746  fast.ai 2022 - Part 1          4   \n",
       "440    15747  15747  fast.ai 2022 - Part 1          4   \n",
       "441    15748  15748  fast.ai 2022 - Part 1          4   \n",
       "442    15749  15749  fast.ai 2022 - Part 1          4   \n",
       "443    15750  15750  fast.ai 2022 - Part 1          4   \n",
       "444    15751  15751  fast.ai 2022 - Part 1          4   \n",
       "445    15752  15752  fast.ai 2022 - Part 1          4   \n",
       "446    15753  15753  fast.ai 2022 - Part 1          4   \n",
       "447    15754  15754  fast.ai 2022 - Part 1          4   \n",
       "448    15755  15755  fast.ai 2022 - Part 1          4   \n",
       "449    15756  15756  fast.ai 2022 - Part 1          4   \n",
       "450    15757  15757  fast.ai 2022 - Part 1          4   \n",
       "451    15758  15758  fast.ai 2022 - Part 1          4   \n",
       "452    15759  15759  fast.ai 2022 - Part 1          4   \n",
       "453    15760  15760  fast.ai 2022 - Part 1          4   \n",
       "454    15761  15761  fast.ai 2022 - Part 1          4   \n",
       "455    15762  15762  fast.ai 2022 - Part 1          4   \n",
       "456    15763  15763  fast.ai 2022 - Part 1          4   \n",
       "457    15764  15764  fast.ai 2022 - Part 1          4   \n",
       "458    15765  15765  fast.ai 2022 - Part 1          4   \n",
       "459    15766  15766  fast.ai 2022 - Part 1          4   \n",
       "460    15767  15767  fast.ai 2022 - Part 1          4   \n",
       "461    15768  15768  fast.ai 2022 - Part 1          4   \n",
       "462    15769  15769  fast.ai 2022 - Part 1          4   \n",
       "463    15770  15770  fast.ai 2022 - Part 1          4   \n",
       "464    15771  15771  fast.ai 2022 - Part 1          4   \n",
       "465    15772  15772  fast.ai 2022 - Part 1          4   \n",
       "466    15773  15773  fast.ai 2022 - Part 1          4   \n",
       "467    15774  15774  fast.ai 2022 - Part 1          4   \n",
       "468    15775  15775  fast.ai 2022 - Part 1          4   \n",
       "469    15776  15776  fast.ai 2022 - Part 1          4   \n",
       "470    15777  15777  fast.ai 2022 - Part 1          4   \n",
       "471    15778  15778  fast.ai 2022 - Part 1          4   \n",
       "472    15779  15779  fast.ai 2022 - Part 1          4   \n",
       "473    15780  15780  fast.ai 2022 - Part 1          4   \n",
       "474    15781  15781  fast.ai 2022 - Part 1          4   \n",
       "475    15782  15782  fast.ai 2022 - Part 1          4   \n",
       "476    15783  15783  fast.ai 2022 - Part 1          4   \n",
       "477    15784  15784  fast.ai 2022 - Part 1          4   \n",
       "478    15785  15785  fast.ai 2022 - Part 1          4   \n",
       "479    15786  15786  fast.ai 2022 - Part 1          4   \n",
       "480    15787  15787  fast.ai 2022 - Part 1          4   \n",
       "481    15788  15788  fast.ai 2022 - Part 1          4   \n",
       "482    15789  15789  fast.ai 2022 - Part 1          4   \n",
       "483    15790  15790  fast.ai 2022 - Part 1          4   \n",
       "484    15791  15791  fast.ai 2022 - Part 1          4   \n",
       "485    15792  15792  fast.ai 2022 - Part 1          4   \n",
       "486    15793  15793  fast.ai 2022 - Part 1          4   \n",
       "487    15794  15794  fast.ai 2022 - Part 1          4   \n",
       "488    15795  15795  fast.ai 2022 - Part 1          4   \n",
       "489    15796  15796  fast.ai 2022 - Part 1          4   \n",
       "490    15797  15797  fast.ai 2022 - Part 1          4   \n",
       "491    15798  15798  fast.ai 2022 - Part 1          4   \n",
       "492    15799  15799  fast.ai 2022 - Part 1          4   \n",
       "493    15800  15800  fast.ai 2022 - Part 1          4   \n",
       "494    15801  15801  fast.ai 2022 - Part 1          4   \n",
       "495    15802  15802  fast.ai 2022 - Part 1          4   \n",
       "496    15803  15803  fast.ai 2022 - Part 1          4   \n",
       "497    15804  15804  fast.ai 2022 - Part 1          4   \n",
       "498    15805  15805  fast.ai 2022 - Part 1          4   \n",
       "499    15806  15806  fast.ai 2022 - Part 1          4   \n",
       "500    15807  15807  fast.ai 2022 - Part 1          4   \n",
       "501    15808  15808  fast.ai 2022 - Part 1          4   \n",
       "502    15809  15809  fast.ai 2022 - Part 1          4   \n",
       "503    15810  15810  fast.ai 2022 - Part 1          4   \n",
       "504    15811  15811  fast.ai 2022 - Part 1          4   \n",
       "505    15812  15812  fast.ai 2022 - Part 1          4   \n",
       "506    15813  15813  fast.ai 2022 - Part 1          4   \n",
       "507    15814  15814  fast.ai 2022 - Part 1          4   \n",
       "508    15815  15815  fast.ai 2022 - Part 1          4   \n",
       "509    15816  15816  fast.ai 2022 - Part 1          4   \n",
       "510    15817  15817  fast.ai 2022 - Part 1          4   \n",
       "511    15818  15818  fast.ai 2022 - Part 1          4   \n",
       "512    15819  15819  fast.ai 2022 - Part 1          4   \n",
       "513    15820  15820  fast.ai 2022 - Part 1          4   \n",
       "514    15821  15821  fast.ai 2022 - Part 1          4   \n",
       "515    15822  15822  fast.ai 2022 - Part 1          4   \n",
       "516    15823  15823  fast.ai 2022 - Part 1          4   \n",
       "517    15824  15824  fast.ai 2022 - Part 1          4   \n",
       "518    15825  15825  fast.ai 2022 - Part 1          4   \n",
       "519    15826  15826  fast.ai 2022 - Part 1          4   \n",
       "520    15827  15827  fast.ai 2022 - Part 1          4   \n",
       "521    15828  15828  fast.ai 2022 - Part 1          4   \n",
       "522    15829  15829  fast.ai 2022 - Part 1          4   \n",
       "523    15830  15830  fast.ai 2022 - Part 1          4   \n",
       "524    15831  15831  fast.ai 2022 - Part 1          4   \n",
       "525    15832  15832  fast.ai 2022 - Part 1          4   \n",
       "526    15833  15833  fast.ai 2022 - Part 1          4   \n",
       "527    15834  15834  fast.ai 2022 - Part 1          4   \n",
       "528    15835  15835  fast.ai 2022 - Part 1          4   \n",
       "529    15836  15836  fast.ai 2022 - Part 1          4   \n",
       "530    15837  15837  fast.ai 2022 - Part 1          4   \n",
       "531    15838  15838  fast.ai 2022 - Part 1          4   \n",
       "532    15839  15839  fast.ai 2022 - Part 1          4   \n",
       "533    15840  15840  fast.ai 2022 - Part 1          4   \n",
       "534    15841  15841  fast.ai 2022 - Part 1          4   \n",
       "535    15842  15842  fast.ai 2022 - Part 1          4   \n",
       "536    15843  15843  fast.ai 2022 - Part 1          4   \n",
       "537    15844  15844  fast.ai 2022 - Part 1          4   \n",
       "538    15845  15845  fast.ai 2022 - Part 1          4   \n",
       "539    15846  15846  fast.ai 2022 - Part 1          4   \n",
       "540    15847  15847  fast.ai 2022 - Part 1          4   \n",
       "541    15848  15848  fast.ai 2022 - Part 1          4   \n",
       "542    15849  15849  fast.ai 2022 - Part 1          4   \n",
       "543    15850  15850  fast.ai 2022 - Part 1          4   \n",
       "544    15851  15851  fast.ai 2022 - Part 1          4   \n",
       "545    15852  15852  fast.ai 2022 - Part 1          4   \n",
       "546    15853  15853  fast.ai 2022 - Part 1          4   \n",
       "547    15854  15854  fast.ai 2022 - Part 1          4   \n",
       "548    15855  15855  fast.ai 2022 - Part 1          4   \n",
       "549    15856  15856  fast.ai 2022 - Part 1          4   \n",
       "550    15857  15857  fast.ai 2022 - Part 1          4   \n",
       "551    15858  15858  fast.ai 2022 - Part 1          4   \n",
       "552    15859  15859  fast.ai 2022 - Part 1          4   \n",
       "553    15860  15860  fast.ai 2022 - Part 1          4   \n",
       "554    15861  15861  fast.ai 2022 - Part 1          4   \n",
       "555    15862  15862  fast.ai 2022 - Part 1          4   \n",
       "556    15863  15863  fast.ai 2022 - Part 1          4   \n",
       "557    15864  15864  fast.ai 2022 - Part 1          4   \n",
       "558    15865  15865  fast.ai 2022 - Part 1          4   \n",
       "559    15866  15866  fast.ai 2022 - Part 1          4   \n",
       "560    15867  15867  fast.ai 2022 - Part 1          4   \n",
       "561    15868  15868  fast.ai 2022 - Part 1          4   \n",
       "562    15869  15869  fast.ai 2022 - Part 1          4   \n",
       "563    15870  15870  fast.ai 2022 - Part 1          4   \n",
       "564    15871  15871  fast.ai 2022 - Part 1          4   \n",
       "565    15872  15872  fast.ai 2022 - Part 1          4   \n",
       "566    15873  15873  fast.ai 2022 - Part 1          4   \n",
       "567    15874  15874  fast.ai 2022 - Part 1          4   \n",
       "568    15875  15875  fast.ai 2022 - Part 1          4   \n",
       "569    15876  15876  fast.ai 2022 - Part 1          4   \n",
       "570    15877  15877  fast.ai 2022 - Part 1          4   \n",
       "571    15878  15878  fast.ai 2022 - Part 1          4   \n",
       "572    15879  15879  fast.ai 2022 - Part 1          4   \n",
       "573    15880  15880  fast.ai 2022 - Part 1          4   \n",
       "574    15881  15881  fast.ai 2022 - Part 1          4   \n",
       "575    15882  15882  fast.ai 2022 - Part 1          4   \n",
       "576    15883  15883  fast.ai 2022 - Part 1          4   \n",
       "577    15884  15884  fast.ai 2022 - Part 1          4   \n",
       "578    15885  15885  fast.ai 2022 - Part 1          4   \n",
       "579    15886  15886  fast.ai 2022 - Part 1          4   \n",
       "580    15887  15887  fast.ai 2022 - Part 1          4   \n",
       "581    15888  15888  fast.ai 2022 - Part 1          4   \n",
       "582    15889  15889  fast.ai 2022 - Part 1          4   \n",
       "583    15890  15890  fast.ai 2022 - Part 1          4   \n",
       "584    15891  15891  fast.ai 2022 - Part 1          4   \n",
       "585    15892  15892  fast.ai 2022 - Part 1          4   \n",
       "586    15893  15893  fast.ai 2022 - Part 1          4   \n",
       "587    15894  15894  fast.ai 2022 - Part 1          4   \n",
       "588    15895  15895  fast.ai 2022 - Part 1          4   \n",
       "589    15896  15896  fast.ai 2022 - Part 1          4   \n",
       "590    15897  15897  fast.ai 2022 - Part 1          4   \n",
       "591    15898  15898  fast.ai 2022 - Part 1          4   \n",
       "592    15899  15899  fast.ai 2022 - Part 1          4   \n",
       "593    15900  15900  fast.ai 2022 - Part 1          4   \n",
       "594    15901  15901  fast.ai 2022 - Part 1          4   \n",
       "595    15902  15902  fast.ai 2022 - Part 1          4   \n",
       "596    15903  15903  fast.ai 2022 - Part 1          4   \n",
       "597    15904  15904  fast.ai 2022 - Part 1          4   \n",
       "598    15905  15905  fast.ai 2022 - Part 1          4   \n",
       "599    15906  15906  fast.ai 2022 - Part 1          4   \n",
       "600    15907  15907  fast.ai 2022 - Part 1          4   \n",
       "601    15908  15908  fast.ai 2022 - Part 1          4   \n",
       "602    15909  15909  fast.ai 2022 - Part 1          4   \n",
       "603    15910  15910  fast.ai 2022 - Part 1          4   \n",
       "604    15911  15911  fast.ai 2022 - Part 1          4   \n",
       "605    15912  15912  fast.ai 2022 - Part 1          4   \n",
       "606    15913  15913  fast.ai 2022 - Part 1          4   \n",
       "607    15914  15914  fast.ai 2022 - Part 1          4   \n",
       "608    15915  15915  fast.ai 2022 - Part 1          4   \n",
       "609    15916  15916  fast.ai 2022 - Part 1          4   \n",
       "610    15917  15917  fast.ai 2022 - Part 1          4   \n",
       "611    15918  15918  fast.ai 2022 - Part 1          4   \n",
       "612    15919  15919  fast.ai 2022 - Part 1          4   \n",
       "613    15920  15920  fast.ai 2022 - Part 1          4   \n",
       "614    15921  15921  fast.ai 2022 - Part 1          4   \n",
       "615    15922  15922  fast.ai 2022 - Part 1          4   \n",
       "616    15923  15923  fast.ai 2022 - Part 1          4   \n",
       "617    15924  15924  fast.ai 2022 - Part 1          4   \n",
       "618    15925  15925  fast.ai 2022 - Part 1          4   \n",
       "619    15926  15926  fast.ai 2022 - Part 1          4   \n",
       "620    15927  15927  fast.ai 2022 - Part 1          4   \n",
       "621    15928  15928  fast.ai 2022 - Part 1          4   \n",
       "622    15929  15929  fast.ai 2022 - Part 1          4   \n",
       "623    15930  15930  fast.ai 2022 - Part 1          4   \n",
       "624    15931  15931  fast.ai 2022 - Part 1          4   \n",
       "625    15932  15932  fast.ai 2022 - Part 1          4   \n",
       "626    15933  15933  fast.ai 2022 - Part 1          4   \n",
       "627    15934  15934  fast.ai 2022 - Part 1          4   \n",
       "628    15935  15935  fast.ai 2022 - Part 1          4   \n",
       "629    15936  15936  fast.ai 2022 - Part 1          4   \n",
       "630    15937  15937  fast.ai 2022 - Part 1          4   \n",
       "631    15938  15938  fast.ai 2022 - Part 1          4   \n",
       "632    15939  15939  fast.ai 2022 - Part 1          4   \n",
       "633    15940  15940  fast.ai 2022 - Part 1          4   \n",
       "634    15941  15941  fast.ai 2022 - Part 1          4   \n",
       "635    15942  15942  fast.ai 2022 - Part 1          4   \n",
       "636    15943  15943  fast.ai 2022 - Part 1          4   \n",
       "637    15944  15944  fast.ai 2022 - Part 1          4   \n",
       "638    15945  15945  fast.ai 2022 - Part 1          4   \n",
       "639    15946  15946  fast.ai 2022 - Part 1          4   \n",
       "640    15947  15947  fast.ai 2022 - Part 1          4   \n",
       "641    15948  15948  fast.ai 2022 - Part 1          4   \n",
       "642    15949  15949  fast.ai 2022 - Part 1          4   \n",
       "643    15950  15950  fast.ai 2022 - Part 1          4   \n",
       "644    15951  15951  fast.ai 2022 - Part 1          4   \n",
       "645    15952  15952  fast.ai 2022 - Part 1          4   \n",
       "646    15953  15953  fast.ai 2022 - Part 1          4   \n",
       "647    15954  15954  fast.ai 2022 - Part 1          4   \n",
       "648    15955  15955  fast.ai 2022 - Part 1          4   \n",
       "649    15956  15956  fast.ai 2022 - Part 1          4   \n",
       "650    15957  15957  fast.ai 2022 - Part 1          4   \n",
       "651    15958  15958  fast.ai 2022 - Part 1          4   \n",
       "652    15959  15959  fast.ai 2022 - Part 1          4   \n",
       "653    15960  15960  fast.ai 2022 - Part 1          4   \n",
       "654    15961  15961  fast.ai 2022 - Part 1          4   \n",
       "655    15962  15962  fast.ai 2022 - Part 1          4   \n",
       "656    15963  15963  fast.ai 2022 - Part 1          4   \n",
       "657    15964  15964  fast.ai 2022 - Part 1          4   \n",
       "658    15965  15965  fast.ai 2022 - Part 1          4   \n",
       "659    15966  15966  fast.ai 2022 - Part 1          4   \n",
       "660    15967  15967  fast.ai 2022 - Part 1          4   \n",
       "661    15968  15968  fast.ai 2022 - Part 1          4   \n",
       "662    15969  15969  fast.ai 2022 - Part 1          4   \n",
       "663    15970  15970  fast.ai 2022 - Part 1          4   \n",
       "664    15971  15971  fast.ai 2022 - Part 1          4   \n",
       "665    15972  15972  fast.ai 2022 - Part 1          4   \n",
       "666    15973  15973  fast.ai 2022 - Part 1          4   \n",
       "667    15974  15974  fast.ai 2022 - Part 1          4   \n",
       "668    15975  15975  fast.ai 2022 - Part 1          4   \n",
       "669    15976  15976  fast.ai 2022 - Part 1          4   \n",
       "670    15977  15977  fast.ai 2022 - Part 1          4   \n",
       "671    15978  15978  fast.ai 2022 - Part 1          4   \n",
       "672    15979  15979  fast.ai 2022 - Part 1          4   \n",
       "673    15980  15980  fast.ai 2022 - Part 1          4   \n",
       "674    15981  15981  fast.ai 2022 - Part 1          4   \n",
       "675    15982  15982  fast.ai 2022 - Part 1          4   \n",
       "676    15983  15983  fast.ai 2022 - Part 1          4   \n",
       "677    15984  15984  fast.ai 2022 - Part 1          4   \n",
       "678    15985  15985  fast.ai 2022 - Part 1          4   \n",
       "679    15986  15986  fast.ai 2022 - Part 1          4   \n",
       "680    15987  15987  fast.ai 2022 - Part 1          4   \n",
       "681    15988  15988  fast.ai 2022 - Part 1          4   \n",
       "682    15989  15989  fast.ai 2022 - Part 1          4   \n",
       "683    15990  15990  fast.ai 2022 - Part 1          4   \n",
       "684    15991  15991  fast.ai 2022 - Part 1          4   \n",
       "685    15992  15992  fast.ai 2022 - Part 1          4   \n",
       "686    15993  15993  fast.ai 2022 - Part 1          4   \n",
       "687    15994  15994  fast.ai 2022 - Part 1          4   \n",
       "688    15995  15995  fast.ai 2022 - Part 1          4   \n",
       "689    15996  15996  fast.ai 2022 - Part 1          4   \n",
       "690    15997  15997  fast.ai 2022 - Part 1          4   \n",
       "691    15998  15998  fast.ai 2022 - Part 1          4   \n",
       "692    15999  15999  fast.ai 2022 - Part 1          4   \n",
       "693    16000  16000  fast.ai 2022 - Part 1          4   \n",
       "694    16001  16001  fast.ai 2022 - Part 1          4   \n",
       "695    16002  16002  fast.ai 2022 - Part 1          4   \n",
       "696    16003  16003  fast.ai 2022 - Part 1          4   \n",
       "697    16004  16004  fast.ai 2022 - Part 1          4   \n",
       "698    16005  16005  fast.ai 2022 - Part 1          4   \n",
       "699    16006  16006  fast.ai 2022 - Part 1          4   \n",
       "700    16007  16007  fast.ai 2022 - Part 1          4   \n",
       "701    16008  16008  fast.ai 2022 - Part 1          4   \n",
       "702    16009  16009  fast.ai 2022 - Part 1          4   \n",
       "703    16010  16010  fast.ai 2022 - Part 1          4   \n",
       "704    16011  16011  fast.ai 2022 - Part 1          4   \n",
       "705    16012  16012  fast.ai 2022 - Part 1          4   \n",
       "706    16013  16013  fast.ai 2022 - Part 1          4   \n",
       "707    16014  16014  fast.ai 2022 - Part 1          4   \n",
       "708    16015  16015  fast.ai 2022 - Part 1          4   \n",
       "\n",
       "                                                               topic  \\\n",
       "0                                                  Using Huggingface   \n",
       "1                                                  Using Huggingface   \n",
       "2                                                  Using Huggingface   \n",
       "3                                                  Using Huggingface   \n",
       "4                                                  Using Huggingface   \n",
       "5                                                  Using Huggingface   \n",
       "6                                                  Using Huggingface   \n",
       "7                                                  Using Huggingface   \n",
       "8                                                  Using Huggingface   \n",
       "9                                                  Using Huggingface   \n",
       "10                                                 Using Huggingface   \n",
       "11                                                 Using Huggingface   \n",
       "12                                                 Using Huggingface   \n",
       "13                                                 Using Huggingface   \n",
       "14                                                 Using Huggingface   \n",
       "15                                                 Using Huggingface   \n",
       "16                                                 Using Huggingface   \n",
       "17                                                 Using Huggingface   \n",
       "18                                                 Using Huggingface   \n",
       "19                                                 Using Huggingface   \n",
       "20                                                 Using Huggingface   \n",
       "21                                                 Using Huggingface   \n",
       "22                                                 Using Huggingface   \n",
       "23                                                 Using Huggingface   \n",
       "24                                                 Using Huggingface   \n",
       "25                                                 Using Huggingface   \n",
       "26                                                 Using Huggingface   \n",
       "27                                       Finetuning pretrained model   \n",
       "28                                       Finetuning pretrained model   \n",
       "29                                       Finetuning pretrained model   \n",
       "30                                       Finetuning pretrained model   \n",
       "31                                       Finetuning pretrained model   \n",
       "32                                       Finetuning pretrained model   \n",
       "33                                       Finetuning pretrained model   \n",
       "34                                       Finetuning pretrained model   \n",
       "35                                       Finetuning pretrained model   \n",
       "36                                       Finetuning pretrained model   \n",
       "37                                       Finetuning pretrained model   \n",
       "38                                       Finetuning pretrained model   \n",
       "39                                       Finetuning pretrained model   \n",
       "40                                       Finetuning pretrained model   \n",
       "41                                       Finetuning pretrained model   \n",
       "42                                                            ULMFit   \n",
       "43                                                            ULMFit   \n",
       "44                                                            ULMFit   \n",
       "45                                                            ULMFit   \n",
       "46                                                            ULMFit   \n",
       "47                                                            ULMFit   \n",
       "48                                                            ULMFit   \n",
       "49                                                            ULMFit   \n",
       "50                                                            ULMFit   \n",
       "51                                                            ULMFit   \n",
       "52                                                            ULMFit   \n",
       "53                                                            ULMFit   \n",
       "54                                                            ULMFit   \n",
       "55                                                            ULMFit   \n",
       "56                                                            ULMFit   \n",
       "57                                                            ULMFit   \n",
       "58                                                            ULMFit   \n",
       "59                                                            ULMFit   \n",
       "60                                                            ULMFit   \n",
       "61                                                            ULMFit   \n",
       "62                                                            ULMFit   \n",
       "63                                                            ULMFit   \n",
       "64                                                            ULMFit   \n",
       "65                                                            ULMFit   \n",
       "66                                                            ULMFit   \n",
       "67                                                            ULMFit   \n",
       "68                                                            ULMFit   \n",
       "69                                                            ULMFit   \n",
       "70                                                            ULMFit   \n",
       "71                                                       Transformer   \n",
       "72                                                       Transformer   \n",
       "73                                                       Transformer   \n",
       "74                                                       Transformer   \n",
       "75                                                       Transformer   \n",
       "76                                                       Transformer   \n",
       "77                                                       Transformer   \n",
       "78                                                       Transformer   \n",
       "79                                                       Transformer   \n",
       "80                                                       Transformer   \n",
       "81                                                       Transformer   \n",
       "82                                                       Transformer   \n",
       "83                                                   Zeiler & Fergus   \n",
       "84                                                   Zeiler & Fergus   \n",
       "85                                                   Zeiler & Fergus   \n",
       "86                                                   Zeiler & Fergus   \n",
       "87                                                   Zeiler & Fergus   \n",
       "88                                                   Zeiler & Fergus   \n",
       "89                                                   Zeiler & Fergus   \n",
       "90                                                   Zeiler & Fergus   \n",
       "91                                                   Zeiler & Fergus   \n",
       "92                                                   Zeiler & Fergus   \n",
       "93                                                   Zeiler & Fergus   \n",
       "94                                                   Zeiler & Fergus   \n",
       "95                                                   Zeiler & Fergus   \n",
       "96                                                   Zeiler & Fergus   \n",
       "97                                                   Zeiler & Fergus   \n",
       "98                                                   Zeiler & Fergus   \n",
       "99                                                   Zeiler & Fergus   \n",
       "100                                                  Zeiler & Fergus   \n",
       "101                                                  Zeiler & Fergus   \n",
       "102                                                  Zeiler & Fergus   \n",
       "103                                                  Zeiler & Fergus   \n",
       "104                                                  Zeiler & Fergus   \n",
       "105                                                  Zeiler & Fergus   \n",
       "106                                                  Zeiler & Fergus   \n",
       "107                                                  Zeiler & Fergus   \n",
       "108                                                  Zeiler & Fergus   \n",
       "109                                                  Zeiler & Fergus   \n",
       "110                                                  Zeiler & Fergus   \n",
       "111                                                  Zeiler & Fergus   \n",
       "112                                                  Zeiler & Fergus   \n",
       "113                                                  Zeiler & Fergus   \n",
       "114            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "115            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "116            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "117            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "118            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "119            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "120            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "121            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "122            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "123            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "124            US Patent Phrase to Phase Matching Kaggle competition   \n",
       "125                                               NLP Classification   \n",
       "126                                               NLP Classification   \n",
       "127                                               NLP Classification   \n",
       "128                                               NLP Classification   \n",
       "129                                               NLP Classification   \n",
       "130                                               NLP Classification   \n",
       "131                                               NLP Classification   \n",
       "132                                               NLP Classification   \n",
       "133                                               NLP Classification   \n",
       "134                                               NLP Classification   \n",
       "135                                               NLP Classification   \n",
       "136                                               NLP Classification   \n",
       "137                                               NLP Classification   \n",
       "138                                               NLP Classification   \n",
       "139                                               NLP Classification   \n",
       "140                                               NLP Classification   \n",
       "141                                               NLP Classification   \n",
       "142                                               NLP Classification   \n",
       "143                                               NLP Classification   \n",
       "144                                               NLP Classification   \n",
       "145                                               NLP Classification   \n",
       "146                                               NLP Classification   \n",
       "147                                               NLP Classification   \n",
       "148                                               NLP Classification   \n",
       "149                                               NLP Classification   \n",
       "150                                               NLP Classification   \n",
       "151                                               NLP Classification   \n",
       "152                                               NLP Classification   \n",
       "153                                               NLP Classification   \n",
       "154                                               NLP Classification   \n",
       "155                                               NLP Classification   \n",
       "156                                               NLP Classification   \n",
       "157                                               NLP Classification   \n",
       "158                                               NLP Classification   \n",
       "159                                               NLP Classification   \n",
       "160                                               NLP Classification   \n",
       "161                                               NLP Classification   \n",
       "162  Kaggle configs, insert python in bash, read competition website   \n",
       "163  Kaggle configs, insert python in bash, read competition website   \n",
       "164  Kaggle configs, insert python in bash, read competition website   \n",
       "165  Kaggle configs, insert python in bash, read competition website   \n",
       "166  Kaggle configs, insert python in bash, read competition website   \n",
       "167  Kaggle configs, insert python in bash, read competition website   \n",
       "168  Kaggle configs, insert python in bash, read competition website   \n",
       "169  Kaggle configs, insert python in bash, read competition website   \n",
       "170  Kaggle configs, insert python in bash, read competition website   \n",
       "171  Kaggle configs, insert python in bash, read competition website   \n",
       "172  Kaggle configs, insert python in bash, read competition website   \n",
       "173  Kaggle configs, insert python in bash, read competition website   \n",
       "174  Kaggle configs, insert python in bash, read competition website   \n",
       "175  Kaggle configs, insert python in bash, read competition website   \n",
       "176  Kaggle configs, insert python in bash, read competition website   \n",
       "177  Kaggle configs, insert python in bash, read competition website   \n",
       "178  Kaggle configs, insert python in bash, read competition website   \n",
       "179  Kaggle configs, insert python in bash, read competition website   \n",
       "180  Kaggle configs, insert python in bash, read competition website   \n",
       "181  Kaggle configs, insert python in bash, read competition website   \n",
       "182  Kaggle configs, insert python in bash, read competition website   \n",
       "183  Kaggle configs, insert python in bash, read competition website   \n",
       "184  Kaggle configs, insert python in bash, read competition website   \n",
       "185  Kaggle configs, insert python in bash, read competition website   \n",
       "186  Kaggle configs, insert python in bash, read competition website   \n",
       "187  Kaggle configs, insert python in bash, read competition website   \n",
       "188  Kaggle configs, insert python in bash, read competition website   \n",
       "189  Kaggle configs, insert python in bash, read competition website   \n",
       "190                             Pandas, numpy, matplotlib, & pytorch   \n",
       "191                             Pandas, numpy, matplotlib, & pytorch   \n",
       "192                             Pandas, numpy, matplotlib, & pytorch   \n",
       "193                             Pandas, numpy, matplotlib, & pytorch   \n",
       "194                             Pandas, numpy, matplotlib, & pytorch   \n",
       "195                             Pandas, numpy, matplotlib, & pytorch   \n",
       "196                             Pandas, numpy, matplotlib, & pytorch   \n",
       "197                             Pandas, numpy, matplotlib, & pytorch   \n",
       "198                             Pandas, numpy, matplotlib, & pytorch   \n",
       "199                             Pandas, numpy, matplotlib, & pytorch   \n",
       "200                             Pandas, numpy, matplotlib, & pytorch   \n",
       "201                             Pandas, numpy, matplotlib, & pytorch   \n",
       "202                             Pandas, numpy, matplotlib, & pytorch   \n",
       "203                             Pandas, numpy, matplotlib, & pytorch   \n",
       "204                             Pandas, numpy, matplotlib, & pytorch   \n",
       "205                             Pandas, numpy, matplotlib, & pytorch   \n",
       "206                             Pandas, numpy, matplotlib, & pytorch   \n",
       "207                             Pandas, numpy, matplotlib, & pytorch   \n",
       "208                             Pandas, numpy, matplotlib, & pytorch   \n",
       "209                             Pandas, numpy, matplotlib, & pytorch   \n",
       "210                             Pandas, numpy, matplotlib, & pytorch   \n",
       "211                             Pandas, numpy, matplotlib, & pytorch   \n",
       "212                             Pandas, numpy, matplotlib, & pytorch   \n",
       "213                             Pandas, numpy, matplotlib, & pytorch   \n",
       "214                             Pandas, numpy, matplotlib, & pytorch   \n",
       "215                             Pandas, numpy, matplotlib, & pytorch   \n",
       "216                             Pandas, numpy, matplotlib, & pytorch   \n",
       "217                             Pandas, numpy, matplotlib, & pytorch   \n",
       "218                             Pandas, numpy, matplotlib, & pytorch   \n",
       "219                             Pandas, numpy, matplotlib, & pytorch   \n",
       "220                             Pandas, numpy, matplotlib, & pytorch   \n",
       "221                             Pandas, numpy, matplotlib, & pytorch   \n",
       "222                             Pandas, numpy, matplotlib, & pytorch   \n",
       "223                             Pandas, numpy, matplotlib, & pytorch   \n",
       "224                                                     Tokenization   \n",
       "225                                                     Tokenization   \n",
       "226                                                     Tokenization   \n",
       "227                                                     Tokenization   \n",
       "228                                                     Tokenization   \n",
       "229                                                     Tokenization   \n",
       "230                                                     Tokenization   \n",
       "231                                                     Tokenization   \n",
       "232                                                     Tokenization   \n",
       "233                                                     Tokenization   \n",
       "234                                                     Tokenization   \n",
       "235                                                     Tokenization   \n",
       "236                                                     Tokenization   \n",
       "237                                                     Tokenization   \n",
       "238                                                     Tokenization   \n",
       "239                                                     Tokenization   \n",
       "240                                                     Tokenization   \n",
       "241                                                     Tokenization   \n",
       "242                                                     Tokenization   \n",
       "243                                                     Tokenization   \n",
       "244                                                     Tokenization   \n",
       "245                                                     Tokenization   \n",
       "246                                                     Tokenization   \n",
       "247                                                     Tokenization   \n",
       "248                                                     Tokenization   \n",
       "249                                                     Tokenization   \n",
       "250                                                     Tokenization   \n",
       "251                                                     Tokenization   \n",
       "252                                                     Tokenization   \n",
       "253                                                     Tokenization   \n",
       "254                                                     Tokenization   \n",
       "255                                            Huggingface model hub   \n",
       "256                                            Huggingface model hub   \n",
       "257                                            Huggingface model hub   \n",
       "258                                            Huggingface model hub   \n",
       "259                                            Huggingface model hub   \n",
       "260                                            Huggingface model hub   \n",
       "261                                            Huggingface model hub   \n",
       "262                                            Huggingface model hub   \n",
       "263                                            Huggingface model hub   \n",
       "264                                            Huggingface model hub   \n",
       "265                                            Huggingface model hub   \n",
       "266                                            Huggingface model hub   \n",
       "267                                            Huggingface model hub   \n",
       "268                                            Huggingface model hub   \n",
       "269                                            Huggingface model hub   \n",
       "270                                            Huggingface model hub   \n",
       "271                                            Huggingface model hub   \n",
       "272                                            Huggingface model hub   \n",
       "273                                            Huggingface model hub   \n",
       "274                                            Huggingface model hub   \n",
       "275                                            Huggingface model hub   \n",
       "276                                            Huggingface model hub   \n",
       "277                                            Huggingface model hub   \n",
       "278                                            Huggingface model hub   \n",
       "279                                            Huggingface model hub   \n",
       "280                                  Examples of tokenized sentences   \n",
       "281                                  Examples of tokenized sentences   \n",
       "282                                  Examples of tokenized sentences   \n",
       "283                                  Examples of tokenized sentences   \n",
       "284                                  Examples of tokenized sentences   \n",
       "285                                  Examples of tokenized sentences   \n",
       "286                                  Examples of tokenized sentences   \n",
       "287                                  Examples of tokenized sentences   \n",
       "288                                  Examples of tokenized sentences   \n",
       "289                                  Examples of tokenized sentences   \n",
       "290                                  Examples of tokenized sentences   \n",
       "291                                  Examples of tokenized sentences   \n",
       "292                                  Examples of tokenized sentences   \n",
       "293                                  Examples of tokenized sentences   \n",
       "294                                  Examples of tokenized sentences   \n",
       "295                                  Examples of tokenized sentences   \n",
       "296                                  Examples of tokenized sentences   \n",
       "297                                  Examples of tokenized sentences   \n",
       "298                                                 Numericalization   \n",
       "299                                                 Numericalization   \n",
       "300                                                 Numericalization   \n",
       "301                                                 Numericalization   \n",
       "302                                                 Numericalization   \n",
       "303                                                 Numericalization   \n",
       "304                                                 Numericalization   \n",
       "305                                                 Numericalization   \n",
       "306                                                 Numericalization   \n",
       "307                                                 Numericalization   \n",
       "308                                                 Numericalization   \n",
       "309                                                 Numericalization   \n",
       "310                                                 Numericalization   \n",
       "311                                                 Numericalization   \n",
       "312                                                 Numericalization   \n",
       "313                                                 Numericalization   \n",
       "314                                                 Numericalization   \n",
       "315                                                 Numericalization   \n",
       "316          Question: rationale behind how input data was formatted   \n",
       "317          Question: rationale behind how input data was formatted   \n",
       "318          Question: rationale behind how input data was formatted   \n",
       "319          Question: rationale behind how input data was formatted   \n",
       "320          Question: rationale behind how input data was formatted   \n",
       "321          Question: rationale behind how input data was formatted   \n",
       "322          Question: rationale behind how input data was formatted   \n",
       "323          Question: rationale behind how input data was formatted   \n",
       "324          Question: rationale behind how input data was formatted   \n",
       "325          Question: rationale behind how input data was formatted   \n",
       "326          Question: rationale behind how input data was formatted   \n",
       "327          Question: rationale behind how input data was formatted   \n",
       "328          Question: rationale behind how input data was formatted   \n",
       "329          Question: rationale behind how input data was formatted   \n",
       "330          Question: rationale behind how input data was formatted   \n",
       "331          Question: rationale behind how input data was formatted   \n",
       "332                               ULMFit fits large documents easily   \n",
       "333                               ULMFit fits large documents easily   \n",
       "334                               ULMFit fits large documents easily   \n",
       "335                               ULMFit fits large documents easily   \n",
       "336                               ULMFit fits large documents easily   \n",
       "337                               ULMFit fits large documents easily   \n",
       "338                               ULMFit fits large documents easily   \n",
       "339                               ULMFit fits large documents easily   \n",
       "340                               ULMFit fits large documents easily   \n",
       "341                               ULMFit fits large documents easily   \n",
       "342                               ULMFit fits large documents easily   \n",
       "343                               ULMFit fits large documents easily   \n",
       "344                               ULMFit fits large documents easily   \n",
       "345                               ULMFit fits large documents easily   \n",
       "346                               ULMFit fits large documents easily   \n",
       "347                               ULMFit fits large documents easily   \n",
       "348                               ULMFit fits large documents easily   \n",
       "349                                       Overfitting & underfitting   \n",
       "350                                       Overfitting & underfitting   \n",
       "351                                       Overfitting & underfitting   \n",
       "352                                       Overfitting & underfitting   \n",
       "353                                       Overfitting & underfitting   \n",
       "354                                       Overfitting & underfitting   \n",
       "355                                       Overfitting & underfitting   \n",
       "356                                       Overfitting & underfitting   \n",
       "357                                       Overfitting & underfitting   \n",
       "358                                       Overfitting & underfitting   \n",
       "359                                       Overfitting & underfitting   \n",
       "360                                       Overfitting & underfitting   \n",
       "361                                       Overfitting & underfitting   \n",
       "362                                       Overfitting & underfitting   \n",
       "363                                       Overfitting & underfitting   \n",
       "364                                       Overfitting & underfitting   \n",
       "365                                       Overfitting & underfitting   \n",
       "366                                       Overfitting & underfitting   \n",
       "367                                       Overfitting & underfitting   \n",
       "368                                       Overfitting & underfitting   \n",
       "369                                       Overfitting & underfitting   \n",
       "370                                       Overfitting & underfitting   \n",
       "371                                       Overfitting & underfitting   \n",
       "372                                       Overfitting & underfitting   \n",
       "373                                       Overfitting & underfitting   \n",
       "374                                       Overfitting & underfitting   \n",
       "375                                       Overfitting & underfitting   \n",
       "376                                       Overfitting & underfitting   \n",
       "377                                       Overfitting & underfitting   \n",
       "378                                       Overfitting & underfitting   \n",
       "379                                       Overfitting & underfitting   \n",
       "380                                       Overfitting & underfitting   \n",
       "381                                       Overfitting & underfitting   \n",
       "382                                       Overfitting & underfitting   \n",
       "383                                       Overfitting & underfitting   \n",
       "384                                       Overfitting & underfitting   \n",
       "385                                       Overfitting & underfitting   \n",
       "386                                            Splitting the dataset   \n",
       "387                                            Splitting the dataset   \n",
       "388                                            Splitting the dataset   \n",
       "389                                            Splitting the dataset   \n",
       "390                                            Splitting the dataset   \n",
       "391                                            Splitting the dataset   \n",
       "392                                            Splitting the dataset   \n",
       "393                                            Splitting the dataset   \n",
       "394                                            Splitting the dataset   \n",
       "395                                            Splitting the dataset   \n",
       "396                                            Splitting the dataset   \n",
       "397                                            Splitting the dataset   \n",
       "398                                   Creating a good validation set   \n",
       "399                                   Creating a good validation set   \n",
       "400                                   Creating a good validation set   \n",
       "401                                   Creating a good validation set   \n",
       "402                                   Creating a good validation set   \n",
       "403                                   Creating a good validation set   \n",
       "404                                   Creating a good validation set   \n",
       "405                                   Creating a good validation set   \n",
       "406                                   Creating a good validation set   \n",
       "407                                   Creating a good validation set   \n",
       "408                                   Creating a good validation set   \n",
       "409                                   Creating a good validation set   \n",
       "410                                   Creating a good validation set   \n",
       "411                                   Creating a good validation set   \n",
       "412                                   Creating a good validation set   \n",
       "413                                   Creating a good validation set   \n",
       "414                                   Creating a good validation set   \n",
       "415                                   Creating a good validation set   \n",
       "416                                   Creating a good validation set   \n",
       "417                                   Creating a good validation set   \n",
       "418                                   Creating a good validation set   \n",
       "419                                   Creating a good validation set   \n",
       "420                                   Creating a good validation set   \n",
       "421                                   Creating a good validation set   \n",
       "422                                   Creating a good validation set   \n",
       "423                                   Creating a good validation set   \n",
       "424                                   Creating a good validation set   \n",
       "425                                   Creating a good validation set   \n",
       "426                                   Creating a good validation set   \n",
       "427                                   Creating a good validation set   \n",
       "428                                   Creating a good validation set   \n",
       "429                                   Creating a good validation set   \n",
       "430                                   Creating a good validation set   \n",
       "431                                   Creating a good validation set   \n",
       "432                                   Creating a good validation set   \n",
       "433                                   Creating a good validation set   \n",
       "434                                   Creating a good validation set   \n",
       "435                                   Creating a good validation set   \n",
       "436                                                         Test set   \n",
       "437                                                         Test set   \n",
       "438                                                         Test set   \n",
       "439                                                         Test set   \n",
       "440                                                         Test set   \n",
       "441                                                         Test set   \n",
       "442                                                         Test set   \n",
       "443                                                         Test set   \n",
       "444                                                         Test set   \n",
       "445                                                         Test set   \n",
       "446                                                         Test set   \n",
       "447                                                         Test set   \n",
       "448                                                         Test set   \n",
       "449                                                         Test set   \n",
       "450                                                         Test set   \n",
       "451                                                         Test set   \n",
       "452                                                   Metric vs loss   \n",
       "453                                                   Metric vs loss   \n",
       "454                                                   Metric vs loss   \n",
       "455                                                   Metric vs loss   \n",
       "456                                                   Metric vs loss   \n",
       "457                                                   Metric vs loss   \n",
       "458                                                   Metric vs loss   \n",
       "459                                                   Metric vs loss   \n",
       "460                                                   Metric vs loss   \n",
       "461                                                   Metric vs loss   \n",
       "462                                                   Metric vs loss   \n",
       "463                                                   Metric vs loss   \n",
       "464                                                   Metric vs loss   \n",
       "465                                                   Metric vs loss   \n",
       "466                                                   Metric vs loss   \n",
       "467                                                   Metric vs loss   \n",
       "468                                                   Metric vs loss   \n",
       "469                                         The problem with metrics   \n",
       "470                                         The problem with metrics   \n",
       "471                                         The problem with metrics   \n",
       "472                                         The problem with metrics   \n",
       "473                                         The problem with metrics   \n",
       "474                                         The problem with metrics   \n",
       "475                                         The problem with metrics   \n",
       "476                                         The problem with metrics   \n",
       "477                                         The problem with metrics   \n",
       "478                                         The problem with metrics   \n",
       "479                                         The problem with metrics   \n",
       "480                                         The problem with metrics   \n",
       "481                                         The problem with metrics   \n",
       "482                                         The problem with metrics   \n",
       "483                                         The problem with metrics   \n",
       "484                                         The problem with metrics   \n",
       "485                                         The problem with metrics   \n",
       "486                                         The problem with metrics   \n",
       "487                                         The problem with metrics   \n",
       "488                                         The problem with metrics   \n",
       "489                                              Pearson correlation   \n",
       "490                                              Pearson correlation   \n",
       "491                                              Pearson correlation   \n",
       "492                                              Pearson correlation   \n",
       "493                                              Pearson correlation   \n",
       "494                                              Pearson correlation   \n",
       "495                                              Pearson correlation   \n",
       "496                                              Pearson correlation   \n",
       "497                                              Pearson correlation   \n",
       "498                                              Pearson correlation   \n",
       "499                                              Pearson correlation   \n",
       "500                                              Pearson correlation   \n",
       "501                                              Pearson correlation   \n",
       "502                                              Pearson correlation   \n",
       "503                                              Pearson correlation   \n",
       "504                                              Pearson correlation   \n",
       "505                                              Pearson correlation   \n",
       "506                                              Pearson correlation   \n",
       "507                                              Pearson correlation   \n",
       "508                                              Pearson correlation   \n",
       "509                                              Pearson correlation   \n",
       "510                                              Pearson correlation   \n",
       "511                                              Pearson correlation   \n",
       "512                                              Pearson correlation   \n",
       "513                                              Pearson correlation   \n",
       "514                                              Pearson correlation   \n",
       "515                                              Pearson correlation   \n",
       "516                                              Pearson correlation   \n",
       "517                                              Pearson correlation   \n",
       "518                                              Pearson correlation   \n",
       "519                                              Pearson correlation   \n",
       "520                                              Pearson correlation   \n",
       "521                                              Pearson correlation   \n",
       "522                                              Pearson correlation   \n",
       "523                                              Pearson correlation   \n",
       "524                                              Pearson correlation   \n",
       "525                                              Pearson correlation   \n",
       "526                                              Pearson correlation   \n",
       "527                                              Pearson correlation   \n",
       "528                                              Pearson correlation   \n",
       "529                                              Pearson correlation   \n",
       "530                                              Pearson correlation   \n",
       "531                                              Pearson correlation   \n",
       "532                                              Pearson correlation   \n",
       "533                                              Pearson correlation   \n",
       "534                                              Pearson correlation   \n",
       "535                                              Pearson correlation   \n",
       "536                             Correlation is sensitive to outliers   \n",
       "537                             Correlation is sensitive to outliers   \n",
       "538                             Correlation is sensitive to outliers   \n",
       "539                             Correlation is sensitive to outliers   \n",
       "540                             Correlation is sensitive to outliers   \n",
       "541                             Correlation is sensitive to outliers   \n",
       "542                             Correlation is sensitive to outliers   \n",
       "543                             Correlation is sensitive to outliers   \n",
       "544                             Correlation is sensitive to outliers   \n",
       "545                             Correlation is sensitive to outliers   \n",
       "546                             Correlation is sensitive to outliers   \n",
       "547                             Correlation is sensitive to outliers   \n",
       "548                             Correlation is sensitive to outliers   \n",
       "549                             Correlation is sensitive to outliers   \n",
       "550                             Correlation is sensitive to outliers   \n",
       "551                             Correlation is sensitive to outliers   \n",
       "552                             Correlation is sensitive to outliers   \n",
       "553                             Correlation is sensitive to outliers   \n",
       "554                             Correlation is sensitive to outliers   \n",
       "555                             Correlation is sensitive to outliers   \n",
       "556                             Correlation is sensitive to outliers   \n",
       "557                             Correlation is sensitive to outliers   \n",
       "558                             Correlation is sensitive to outliers   \n",
       "559                             Correlation is sensitive to outliers   \n",
       "560                             Correlation is sensitive to outliers   \n",
       "561                             Correlation is sensitive to outliers   \n",
       "562                             Correlation is sensitive to outliers   \n",
       "563                                                 Training a model   \n",
       "564                                                 Training a model   \n",
       "565                                                 Training a model   \n",
       "566                                                 Training a model   \n",
       "567                                                 Training a model   \n",
       "568                                                 Training a model   \n",
       "569                                                 Training a model   \n",
       "570                                                 Training a model   \n",
       "571                                                 Training a model   \n",
       "572                                                 Training a model   \n",
       "573                                                 Training a model   \n",
       "574                                                 Training a model   \n",
       "575                                                 Training a model   \n",
       "576                                                 Training a model   \n",
       "577                                                 Training a model   \n",
       "578                                                 Training a model   \n",
       "579                                                 Training a model   \n",
       "580                                                 Training a model   \n",
       "581                                                 Training a model   \n",
       "582                                                 Training a model   \n",
       "583                                                 Training a model   \n",
       "584                                                 Training a model   \n",
       "585                                                 Training a model   \n",
       "586                                                 Training a model   \n",
       "587                                                 Training a model   \n",
       "588                                                 Training a model   \n",
       "589                                                 Training a model   \n",
       "590                                                 Training a model   \n",
       "591                                                 Training a model   \n",
       "592                                                 Training a model   \n",
       "593                                                 Training a model   \n",
       "594                                                 Training a model   \n",
       "595                                                 Training a model   \n",
       "596                                                 Training a model   \n",
       "597                                                 Training a model   \n",
       "598                                                 Training a model   \n",
       "599                                                 Training a model   \n",
       "600                                                 Training a model   \n",
       "601                                                 Training a model   \n",
       "602                      Question: when is it ok to remove outliers?   \n",
       "603                      Question: when is it ok to remove outliers?   \n",
       "604                      Question: when is it ok to remove outliers?   \n",
       "605                      Question: when is it ok to remove outliers?   \n",
       "606                      Question: when is it ok to remove outliers?   \n",
       "607                      Question: when is it ok to remove outliers?   \n",
       "608                      Question: when is it ok to remove outliers?   \n",
       "609                      Question: when is it ok to remove outliers?   \n",
       "610                      Question: when is it ok to remove outliers?   \n",
       "611                      Question: when is it ok to remove outliers?   \n",
       "612                      Question: when is it ok to remove outliers?   \n",
       "613                      Question: when is it ok to remove outliers?   \n",
       "614                      Question: when is it ok to remove outliers?   \n",
       "615                      Question: when is it ok to remove outliers?   \n",
       "616                      Question: when is it ok to remove outliers?   \n",
       "617                      Question: when is it ok to remove outliers?   \n",
       "618                      Question: when is it ok to remove outliers?   \n",
       "619                      Question: when is it ok to remove outliers?   \n",
       "620                      Question: when is it ok to remove outliers?   \n",
       "621                                                      Predictions   \n",
       "622                                                      Predictions   \n",
       "623                                                      Predictions   \n",
       "624                                                      Predictions   \n",
       "625                                                      Predictions   \n",
       "626                                                      Predictions   \n",
       "627                                                      Predictions   \n",
       "628                                                      Predictions   \n",
       "629                                                      Predictions   \n",
       "630                                                      Predictions   \n",
       "631                                                      Predictions   \n",
       "632                                                      Predictions   \n",
       "633                                                      Predictions   \n",
       "634                                                      Predictions   \n",
       "635                                                      Predictions   \n",
       "636                                                      Predictions   \n",
       "637                                                      Predictions   \n",
       "638                                                      Predictions   \n",
       "639                                                      Predictions   \n",
       "640                                                      Predictions   \n",
       "641                                                      Predictions   \n",
       "642                                                      Predictions   \n",
       "643                                                      Predictions   \n",
       "644                                                      Predictions   \n",
       "645                          Opportunities for research and startups   \n",
       "646                          Opportunities for research and startups   \n",
       "647                          Opportunities for research and startups   \n",
       "648                          Opportunities for research and startups   \n",
       "649                          Opportunities for research and startups   \n",
       "650                                                     Misusing NLP   \n",
       "651                                                     Misusing NLP   \n",
       "652                                                     Misusing NLP   \n",
       "653                                                     Misusing NLP   \n",
       "654                                                     Misusing NLP   \n",
       "655                                                     Misusing NLP   \n",
       "656                                                     Misusing NLP   \n",
       "657                                                     Misusing NLP   \n",
       "658                                                     Misusing NLP   \n",
       "659                                                     Misusing NLP   \n",
       "660                                                     Misusing NLP   \n",
       "661                                                     Misusing NLP   \n",
       "662                                                     Misusing NLP   \n",
       "663                                                     Misusing NLP   \n",
       "664                                                     Misusing NLP   \n",
       "665                                                     Misusing NLP   \n",
       "666                                                     Misusing NLP   \n",
       "667                                                     Misusing NLP   \n",
       "668                                                     Misusing NLP   \n",
       "669                                                     Misusing NLP   \n",
       "670                                                     Misusing NLP   \n",
       "671                                                     Misusing NLP   \n",
       "672                                                     Misusing NLP   \n",
       "673                                                     Misusing NLP   \n",
       "674                                                     Misusing NLP   \n",
       "675                                                     Misusing NLP   \n",
       "676                                                     Misusing NLP   \n",
       "677                                                     Misusing NLP   \n",
       "678                                                     Misusing NLP   \n",
       "679                                                     Misusing NLP   \n",
       "680                                                     Misusing NLP   \n",
       "681                                                     Misusing NLP   \n",
       "682                                                     Misusing NLP   \n",
       "683                                                     Misusing NLP   \n",
       "684                                                     Misusing NLP   \n",
       "685                                                     Misusing NLP   \n",
       "686                                                     Misusing NLP   \n",
       "687                                                     Misusing NLP   \n",
       "688                                                     Misusing NLP   \n",
       "689                                                     Misusing NLP   \n",
       "690                                                     Misusing NLP   \n",
       "691                                                     Misusing NLP   \n",
       "692                                                     Misusing NLP   \n",
       "693                                                     Misusing NLP   \n",
       "694                                                     Misusing NLP   \n",
       "695                                                     Misusing NLP   \n",
       "696                                                     Misusing NLP   \n",
       "697             Question: isnt the target categorical in this case?   \n",
       "698             Question: isnt the target categorical in this case?   \n",
       "699             Question: isnt the target categorical in this case?   \n",
       "700             Question: isnt the target categorical in this case?   \n",
       "701             Question: isnt the target categorical in this case?   \n",
       "702             Question: isnt the target categorical in this case?   \n",
       "703             Question: isnt the target categorical in this case?   \n",
       "704             Question: isnt the target categorical in this case?   \n",
       "705             Question: isnt the target categorical in this case?   \n",
       "706             Question: isnt the target categorical in this case?   \n",
       "707             Question: isnt the target categorical in this case?   \n",
       "708             Question: isnt the target categorical in this case?   \n",
       "\n",
       "                                                                                                                                                                                                     seq  \\\n",
       "0                                                                                                             Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think   \n",
       "1                                                                                                                is the lesson that a lot of the regulars in the community have been most excited about,   \n",
       "2                                                                                                                because it's where we're gonna get some totally new material  totally new topic, we've   \n",
       "3                                                                                                               never covered before. We're going to cover natural language processing (NLP), and you'll   \n",
       "4                        find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the   \n",
       "5                                                                                                                                               fast.ai library, using recurrent neural networks (RNNs).   \n",
       "6                                                                                                             Today we're going to do something else, which is we're going to do Transformers, and we're   \n",
       "7                                                                                                                   not even going to use the fast.ai library at all in fact. So, what we're going to be   \n",
       "8                                                                                                            doing today is we're going to be fine-tuning a pre-trained NLP model using a library called   \n",
       "9                                                                                                                Hugging Face Transformers. Now given this is the fast.ai course, you might be wondering   \n",
       "10                                                                                                                                             why we'd be using a different library other than fast.ai.   \n",
       "11                                                                     The reason is that I think that It's really useful for everybody to have experience and practice of using more than one library.   \n",
       "12                                                                                                             Because you'll get to see the same concepts applied in different ways, and I think that's   \n",
       "13                                                                                                              great for your understanding of what these concepts are. Also, I really like the Hugging   \n",
       "14                                                                                                                 Face Transformers library. It's absolutely the state of the art in NLP, and it's well   \n",
       "15                                                                                                worth knowing. If you're watching this on video, by the time you're watching it, we will probably have   \n",
       "16                    completed our integration of the Transformers library into fast.ai. So it's in the process of becoming the main NLP (kind of) foundation for fast.ai. So you'll be able to combine   \n",
       "17                                                                                                                Transformers and fast.ai together. Yeah, so I think there's a lot of benefits to this,   \n",
       "18              and in the end you're going to know how to do NLP, you know, in a really fantastic library. Now the other thing is, Hugging Face Transformers doesn't have the same layered architecture   \n",
       "19                                                                                                             that fast.ai has, which means particularly for beginners, the kind of high level, height   \n",
       "20                                                                                                               you know top-tier API that you'll be using most of the time, is not as (kind of) ready   \n",
       "21                                                                                                                 to go for beginners, as you're used to from fast.ai. And so that's actually, I think,   \n",
       "22                      a good thing. You're up to Lesson Four, you know the basic idea now of how gradient descent works, and and you know, how parameters are learned as part of a flexible function,   \n",
       "23                                                                                                               I think you're ready to try using a somewhat lower level library that does a little bit   \n",
       "24                         less for you. So it's going to be, you know, a little bit more work. It's still it's a very well designed library, and it's still reasonably high level, but you're going to   \n",
       "25                    learn to go a little bit deeper. And that's kind of how the rest of the course in general is going to be. On the whole, is, we're going to get a bit deeper, and a bit deeper, and   \n",
       "26                                                                                                                a bit deeper. Now, so first of all, let's talk about what we're going to be doing with   \n",
       "27                fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now   \n",
       "28                                                                                                               you do. You played with these sliders last week, and hopefully you've all actually gone   \n",
       "29                     into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So   \n",
       "30                                                                                                            imagine that your job was to move these sliders, to get this as nice as possible, but when   \n",
       "31                                                                                                                 it was given to you, the person who gave it to you said, Oh! actually slider A, that   \n",
       "32                                                                                                           should be on 2.0, we know for sure. And slider B, we think it's like around two and a half.   \n",
       "33                                                                                                               Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because   \n",
       "34                        you could immediately start focusing on the one we have no idea about, get that in roughly the right spot, and then the one you kind of got a vague idea about, you could just   \n",
       "35                     tune it a little bit, and the one that they said was totally confident, you wouldn't move at all. You would probably tune these sliders really quickly. That's what a pre-trained   \n",
       "36                                                                                                                 model is. A pre-trained model is a bunch of parameters that have already been fitted,   \n",
       "37                                                                                                                    where some of them were already pretty confident of what they should be, and some   \n",
       "38                                                                                                              of them we really have no idea at all. And so fine-tuning is the process of taking those   \n",
       "39                                                                           ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit.   \n",
       "40                                                                                                             The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm   \n",
       "41                                                                                                          called ULMFiT which was first presented actually in a fast.ai course, I think the very first   \n",
       "42                                                                                                                fast.ai course. It was later turned into an academic paper by me in conjunction with a   \n",
       "43                                                                                                             then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers   \n",
       "44                                                                                                                 and went on to help inspire a huge change, you know, huge kind of step improvement in   \n",
       "45                                                                                                              NLP capabilities around the world, along with a number of other important innovations at   \n",
       "46                                                                                                            the time. This is the basic process that ULMFiT described. Step One was to build something   \n",
       "47                                                                                                           called a language model using basically nearly all of Wikipedia and what the language model   \n",
       "48                                                                                                                  did was it tried to predict the next word of a Wikipedia article. In fact every next   \n",
       "49                                                                                                          word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia   \n",
       "50                                                                                                                     articles which would say things like, you know the 17th prime number is dot dot   \n",
       "51                                                                                                                   dot or the 40th president of the United States, blah, said at his residence, blah   \n",
       "52                                                                                                                  that. You know, filling in these kinds of things requires understanding a lot about   \n",
       "53                                                                                                                  how language is structured, and about the world, and about math, and so forth. So to   \n",
       "54                                                                                                               get good at being a language model a neural network has to get good at a lot of things.   \n",
       "55                                                                                                         It has to understand how language works at a reasonably good level and it needs to understand   \n",
       "56                   what it's actually talking about, and what is actually true, and what is actually not true, and the different ways in which things are expressed, and so forth. So this was trained   \n",
       "57                   using a very similar approach to what we'll be looking at for fine-tuning but it started with random weights and at the end of it there was a model that could predict more than 30   \n",
       "58                                                                                                               percent of the time correctly what the next word of a Wikipedia article would be. So in   \n",
       "59                                                                                                                  this particular case, for the ULMFiT paper, we then took that and we were trying to   \n",
       "60                                                                                                              the first task I did actually, for the fast.ai course, back when I invented this, was to   \n",
       "61                 try and figure out whether IMDb movie reviews were positive or negative sentiment: Did the person like the movie or not? So what I did was I created a second language model so again   \n",
       "62                      the language model here is something that predicts the next word of a sentence but rather than using Wikipedia I took this pre-trained model that was trained on Wikipedia and I   \n",
       "63                                                                                                            ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next   \n",
       "64                                                                                                             word of an IMDb movie review. And then finally I took those weights and I fine-tuned them   \n",
       "65                                                                                                          for the task of predicting whether or not a movie review was positive or negative sentiment.   \n",
       "66                                                                                 So those were the three steps. This is a particularly interesting approach because this very first model, in fact the   \n",
       "67                       first two models, if you think about it they don't require any label. I didn't have to collect any kind of document categories, or do any kind of surveys, or collect anything.   \n",
       "68                                                                                                         All I needed was the actual text of Wikipedia and movie reviews themselves because the labels   \n",
       "69                                                               was: whats the next word of a sentence?. Now, since we built ULMFiT, and we used RNNs (recurrent neural networks) for this, at about   \n",
       "70       the same time-ish that we released this, a new kind of architecture particularly useful for NLP at the time was developed called Transformers. And Transformers were particularly built because   \n",
       "71                                                                                                                  they can take really good advantage of modern accelerators like, like Google's TPUs.   \n",
       "72                                                                                                                   They didn't really, kind of, allow you to predict the next word of a sentence. It's   \n",
       "73                just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,   \n",
       "74                   instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked   \n",
       "75                                                                                                              the model to predict which/what were the words that were deleted, essentially. So it's a   \n",
       "76                                                                                                          pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced   \n",
       "77                           our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the   \n",
       "78                                                                                                                 same. So today we're going to be looking at models using what's become the, you know,   \n",
       "79                    much more popular approach than ULMFiT which is this Transformers masked language model approach. Okay, John do we have any questions? And I should mention we do have a professor   \n",
       "80                                                                                                              from University of Queensland, John Williams, joining us, who will be asking the highest   \n",
       "81                                                  voted questions from the community. What have you got, John? Yeah thanks Jeremy. Look, and we might be jumping the gun here, I suspect this is where   \n",
       "82                       you're going tonight but we've got a good question here on the forum which is: How do you go from a model that's trained to predict the next word, to a model that can be used   \n",
       "83                                                                                         for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place   \n",
       "84                                                                                                                to start would be the next slide, kind of give you a sense of this. You might remember   \n",
       "85                                                                                                                  in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at   \n",
       "86                                                                                                           visualizations of the first layer of a imagenet classification model and Layer One had sets   \n",
       "87                                                                                                               of weights that found diagonal edges, and here are some examples of bits of photos that   \n",
       "88                                                                                                       successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's   \n",
       "89                                                                                                             some examples of bits of pictures that matched, and then Layer Two combined those and now   \n",
       "90               you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified linear units, the outputs of those, they're   \n",
       "91                         called activations, where then themselves run through a matrix multiply, a rectified linear unit, added together, so that now you don't just have to have edge detectors, but   \n",
       "92                       Layer Two had corner detectors. And here's some examples of some corners that that corner detector successfully found. And remember, these were not engineered in any way, they   \n",
       "93                                                                                                             just evolved from the gradient descent training process. Layer Two had examples of circle   \n",
       "94                                                                                                                detectors as it turns out, and skipping a bit, by the time we got to Layer Five we had   \n",
       "95                                                                                                                bird and lizard eyeball detectors, and dog face detectors, and flower detectors and so   \n",
       "96                                                                                                                forth. Now, you know, nowadays you'd have something like a resnet50 would be something   \n",
       "97                     you'd probably be training pretty regularly in this course so that, you know, you've got 50-layers, not just 5-layers. Now the later layers do things that are much more specific   \n",
       "98                                                                                                        to the training task which is, like, actually predicting really, what is it that we're looking   \n",
       "99                                                                                                               at? The early layers, pretty unlikely you're going to need to change them much, as long   \n",
       "100                                                                          as you're looking at, like, some kind of natural photos, right? You're going to need edge detectors and gradient detectors.   \n",
       "101                                                                                                                So what we do, in the fine-tuning process, is there's actually one extra layer after   \n",
       "102                    this, which is the layer that actually says: What is this?. You know, it's, it's a dog or a cat or whatever. We actually delete that, we throw it away. So now that last matrix   \n",
       "103                                                                                                                 multiply has one output, or one output per category you're predicting. We throw that   \n",
       "104                                                                                                              away, so the model now has that last matrix that's spitting out, you know, depends, but   \n",
       "105                                                                                                           generally a few hundred activations, and what we do is, as we'll learn more shortly in the   \n",
       "106                                                                                                                 coming lesson, we just stick a new random matrix on the end of that. And that's what   \n",
       "107                                                                                                            we initially train, so it learns to use these kinds of features to predict whatever it is   \n",
       "108                                                                                                       you're trying to predict. And then we gradually train all of those layers. So that's basically   \n",
       "109                                                                                                              how it's done and so it's a bit hand wavy but we'll, particularly in part two, actually   \n",
       "110                                                                                                                build that from scratch ourselves. And in fact in this lesson, time permitting, we're   \n",
       "111                                                                                                         actually going to start going down the process of actually building a real-world deep neural   \n",
       "112                                                                                                           net in python, so we'll be starting to actually make some progress towards that goal. Okay   \n",
       "113                                                                                                               so let's jump into the notebook. So we're going to look at a Kaggle competition that's   \n",
       "114                                                                                                                 actually on as I speak, and I created this notebook called Getting started with NLP   \n",
       "115                                                                                   for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition.   \n",
       "116                                                                                                          And, so I'm going to take you through, you know, a complete submission to this competition.   \n",
       "117             And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this   \n",
       "118                                                                                                    is an actual project, that an actual organization, is prepared to invest money in getting solved,   \n",
       "119                                                                                                             using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions   \n",
       "120                   as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data   \n",
       "121               about a real problem that real organizations really care about, and a very direct way to measure the, you know, accuracy of your solution, you can't really get better than this. Okay   \n",
       "122                                                                                                               so this is a good place, a good competition to experiment with for trying NLP. Now, as   \n",
       "123                                                                                                              I mentioned here, probably the most widely useful application for NLP is classification   \n",
       "124                                                                                                                 and as we've discussed in computer vision, classification refers to taking an object   \n",
       "125                                                                                                           and trying to identify a category that object belongs to. So, previously we've mainly been   \n",
       "126                                                                                                                 looking at images. Today we're going to be looking at documents. Now, in NLP when we   \n",
       "127                                                                                                                 say document, we don't specifically mean, you know, a 20 page long, you know, essay.   \n",
       "128                                                                                                             A document could be three or four words, or a document could be the entire encyclopedia.   \n",
       "129                                                                                                      So a document is just an input to an NLP model that contains text. Now, classifying a document,   \n",
       "130                                                                                                         so deciding what category a document belongs to, is a surprisingly rich thing to do. There's   \n",
       "131                     all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:   \n",
       "132                                                                                                  positive or negative sentiment. Author identification would be taking a document and trying to find   \n",
       "133                                                                                                              the category of author. Legal discovery would be taking documents and putting them into   \n",
       "134                                                                                                                categories according to in- or out-of-scope for a court case. Triaging inbound emails   \n",
       "135                                                                                                            would be putting them into categories of, you know, throw away, send to customer service,   \n",
       "136                                                                                                               send to sales, etc. Right? So classification is a very, very rich area, and for people   \n",
       "137                                                                                                               interested in trying out NLP in real life, I would suggest classification would be the   \n",
       "138                                                                                                               place I would start, for looking for, kind of, accessible, real world, useful problems   \n",
       "139                                                                              you can solve right away. Now, the Kaggle competition does not immediately look like a classification competition. What   \n",
       "140                                                                                                                                                              it contains Let me show you some data   \n",
       "141                               What it contains is data that looks like this. It has a thing that they call anchor, a thing they call target, a thing they call context, and a score. Now these   \n",
       "142                                                                                                                 are.. I can't remember exact details but I think these are from patents, and I think   \n",
       "143                                                                                                           on the patents there are various, like, things they have to fill in in the patent, and one   \n",
       "144                                  of those things is called anchor, one of those things is called target and in the competition the goal is to come up with a model that automatically determines   \n",
       "145                                                                                                               which anchor and target pairs are talking about the same thing. So a score of one here   \n",
       "146                                                                                                                        wood article and wooden article obviously talking about the same thing. A   \n",
       "147                         score of zero here abatement and forest region not talking about the same thing. So the basic idea is we're trying to guess the score. And it's kind of a classification   \n",
       "148                          problem, kind of not. We're basically trying to classify things into either these two things are the same or these two things aren't the same. It's kind of not because   \n",
       "149                                                                                                        we have not just 1 and 0 but also 0.25, 0.5 and 0.75. There's also a column called context,   \n",
       "150                                                                                                         which is, I believe, is like the category that this patent was filed in and my understanding   \n",
       "151                                                                                                              is that whether the anchor and the target count as similar or not depends on, you know,   \n",
       "152                                                                                                           what the patent was filed under. So how would we take this and turn it into something like   \n",
       "153                                                                               a classification problem? So the suggestion I make here is that we could basically say, okay, let's put the, you know,   \n",
       "154                                                                                                                 some constant string like TEXT1 or FIELD1 before the first column and then something   \n",
       "155                                                                                                                  else like TEXT2 before the second column. Oh, and maybe, also the context, I should   \n",
       "156                       have as well TEXT3 in the context, and then try to choose a category of meaning similarity: Different Similar or Identical. So you can basically concatenate those three   \n",
       "157                                                                                                                 pieces together, call that a document and then try to train a model that can predict   \n",
       "158                                                                                                            these categories. That would be an example of how we can take this, basically, similarity   \n",
       "159                   problem, and turn it into something that looks like a classification problem. And we tend to do this a lot in deep learning, is we kind of take problems that look a bit novel and   \n",
       "160                                                                                                           different, and turn them into a problem that looks like something we recognize. All right,   \n",
       "161                                                                                                            so on Kaggle this is a, you know, larger data set that you're going to need a GPU to run.   \n",
       "162            So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically.   \n",
       "163                                                                                                              Personally, you know, I like using things like Paperspace generally better than Kaggle,   \n",
       "164                       like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's   \n",
       "165                                                                                                            some information here, I won't go through but it basically describes how you can download   \n",
       "166                                                                                                               stuff to Paperspace or your own computer as well if you want to. So I basically create   \n",
       "167                       this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd   \n",
       "168                                                                                                                                                             say if iskaggle and put those changes.   \n",
       "169                                                                                                         So here, you can see here, if I'm not on Kaggle and I don't have the data yet, then download   \n",
       "170                                  it. And Kaggle has a little API which is quite handy for doing stuff like downloading data and uploading notebooks and stuff like that, submitting to competitions.   \n",
       "171                           If we are on Kaggle then the data's already going to be there for us which is actually a good reason for beginners to use Kaggle as you don't have to worry about grabbing   \n",
       "172                                                                                                                       the data at all  it's sitting there for you as soon as you open the notebook.   \n",
       "173                                                                                                            Kaggle has a lot of python packages installed, but not necessarily all the ones you want,   \n",
       "174                      and at the point I wrote this they didn't have the Hugging Faces datasets package, for some reason, so you can always just install stuff. So you might remember the exclamation   \n",
       "175                                                                                                               mark means this is not a python command, but a shell command, a bash command. But it's   \n",
       "176                                                                                                              quite neat you can even put bash commands inside python conditionals so that's a pretty   \n",
       "177                                                                                                                                                                      cool little trick in notebooks.   \n",
       "178                                                                                                                Another cool little trick in notebooks is that if you do use a bash command like ls   \n",
       "179                    but you then want to insert the contents of a python variable, just chuck it in parentheses. So, I've got a python variable called path and I can go ls {path} in parentheses   \n",
       "180                                                                                              and that will ls the contents of the python variable path. So there's another little trick for you.   \n",
       "181                                                                                                                  All right, so when we ls that we can see that there's some CSV files. So what I'm   \n",
       "182                         going to do is, kind of, take you through, roughly the process, the kind of process I, you know, went through as, you know when I first look at a competition. So the first   \n",
       "183                                                                                                           thing is like, already a data set, indeed, what's in it? Okay, so it's got some CSV files.   \n",
       "184                                                                                       You know, as well as looking at it here, the other thing I would do is I would go to the competition website   \n",
       "185                                                                                     and if you go to Data A lot of people skip over this, which is a terrible idea, because it actually tells you   \n",
       "186                    what the dependent variable means, what the different files are, what the columns are, and so forth. So don't just rely on looking at the data itself but look at the information   \n",
       "187                                                                             that you're given about the data. So, for CSV files, so CSV files are comma separated values, so they're just text files   \n",
       "188                                                                                                            with a comma between each field, and we can read them using pandas, which for some reason   \n",
       "189                                                                                                               is always called pd. Pandas is one of, I guess, like, (I'm trying to think) probably   \n",
       "190                                                                                                                          like four key libraries that you have to know to do data science in python.   \n",
       "191                                                                                                                  And specifically, those four libraries are: numpy matplotlib pandas and pytorch.   \n",
       "192                                                                                                             So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for   \n",
       "193                                                                                                                    plotting; pandas we use for tables of data; and pytorch we use for deep learning.   \n",
       "194                                                                                                             Those are all covered in a fantastic book by the author of pandas which, the new version   \n",
       "195                                                                                                                  is actually available for free, I believe. Python for data analysis. So if you're   \n",
       "196                                                                                                              not familiar with these libraries just read the whole book, it doesn't take too long to   \n",
       "197                                                                                                               get through, and it's got lots of cool tips and it's very readable. I do find a lot of   \n",
       "198                                                                                                                 people doing this course often I see people kind of, trying to jump ahead, and and   \n",
       "199                                     want to be like: Oh I want to know how to, like, create a new architecture or Build a speech recognition system or whatever. But it then turns out that they   \n",
       "200                don't know how to use these fundamental libraries. So it's always good to be bold and be trying to build things, but do also take the time to, you know, make sure you finish reading   \n",
       "201                                                                                                          the fast.ai book and read at least Wes McKinney's book. That would be enough to really give   \n",
       "202                                                                                                            you all the basic knowledge you need, I think. So, with pandas we can read a CSV file and   \n",
       "203                                                                                                                that creates something called a DataFrame, which is just a table of data, as you see.   \n",
       "204                                                                                                             So, now that we've got a DataFrame, we can see what we're working with, and when we ask   \n",
       "205                    when in jupyter we just put the name of a variable containing a DataFrame, we get the first five rows, the last five rows, and the size. So we've got 36,473 rows. Okay, so other   \n",
       "206                                                                                                                         things I like to use for understanding a DataFrame is the describe method.   \n",
       "207             If you pass include equals object that will describe, that will describe, basically all the kind of the string fields, the non-numeric fields. So, in this case there's four of those,   \n",
       "208                          and so you can see here that, that anchor field we looked at, there's actually only 733 unique values, okay, so this thing, you can see that there's lots of repetition out   \n",
       "209                                                                                                                of 36,000. So there's lots of repetition. This is the most common one: it appears 152   \n",
       "210                            times. And then context, we also see lots of repetition  there's 106 of those contexts. So, this is a nice little method, we can see a lot about the data in a glance.   \n",
       "211                                                                                                              And when I first saw this in this competition I thought: well this is actually not that   \n",
       "212                                                                                                             much language data, when you think about it. The, you know Each document is very short,   \n",
       "213                                                                                                                    you know, three or four words really, and lots of it is repeated. So that's like   \n",
       "214                            as I'm looking through it I'm thinking, like, what are some key features of this data set?. And that would be something, I'd be thinking, well, that's, you know, we've   \n",
       "215                                                                                                                                                 got to do a lot with not very much unique data here.   \n",
       "216                                                                                                       So here's how we can just go ahead and create a single string like I described which contains,   \n",
       "217                                                                                                              you know, some kind of field separator, plus the context, the target and the anchor. So   \n",
       "218                                                                                                              we're going to pop that into a field called input. Something slightly weird in pandas   \n",
       "219                     is there's two ways of referring to a column. You can use square brackets and a string to get the input column or you can just treat it as an attribute. When you're setting it,   \n",
       "220                                                  you should always use the form seen here (... df [input]= ) When reading it you can use either. I tend to use this one because it's less typing.   \n",
       "221                                                                                                         So you can see now we've got this/these concatenated rows. So, head() is the first few rows.   \n",
       "222                                                                                                                So we've now got some documents to do NLP with. Now, the problem is, as you know from   \n",
       "223                                                                                                              the last lesson, neural networks work with numbers. All right, we're going to take some   \n",
       "224                                                                                                           numbers and we're going to multiply them by matrices, we're going to replace the negatives   \n",
       "225                   with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that   \n",
       "226                                                                                                                               for these strings? So there's basically two steps we're going to take.   \n",
       "227                                                                                                              The first step is to split each of these into tokens. Tokens are basically words. We're   \n",
       "228                                                                                                               going to split it into words. There's a few problems with splitting things into words,   \n",
       "229                                                                                                           though. The first is that some languages like chinese don't have words, right, or at least   \n",
       "230                          certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even   \n",
       "231           the pieces are not next to each other. Another reason is that, what we're going to be doing is, after we've split it into words, or something like words, we're going to be getting a list   \n",
       "232              of all of the unique words that appear, which is called the vocabulary, and every one of those unique words is going to get a number. As you'll see later on the bigger the vocabulary,   \n",
       "233                                                                                                                  the more memory is going to get used, the more data we'll need to train. In general   \n",
       "234                                                                                                                                                            we don't want a vocabulary to be too big.   \n",
       "235                                                                                                         So instead, nowadays, people tend to tokenize into something called subwords which is pieces   \n",
       "236                                                                                                            of words  so I'll show you what it looks like. So the process of turning it into smaller   \n",
       "237                     units like words, it's called tokenization  and we call them tokens instead of words. The token is just like the more general concept of, like, whatever we're putting it into.   \n",
       "238                                                                                                         So we're going to get Hugging Face transformers and Hugging Face datasets doing our work for   \n",
       "239                                                                                                                us, and so, what we're going to do is we're going to turn our pandas DataFrame into a   \n",
       "240                                                                                                                    Hugging Face datasets Dataset. It's a bit confusing: pytorch has a class called   \n",
       "241                         Dataset and Hugging Face has a class called Dataset and they're different things, okay, so this is a Hugging Face Dataset. Hugging Face datasets dataset. So we can turn a   \n",
       "242                                                                                                       DataFrame into a Dataset just using the from_pandas method and so we've now got a Dataset. So,   \n",
       "243                                                                                                            if we take a look it just tells us: all right it's got these features, okay? And remember   \n",
       "244                                                                                                       input is the one we just created with the concatenated strings and here's those 36,000 rows.   \n",
       "245                                                                                                           Okay, so now we're going to do these two things. Tokenization, which is to split each text   \n",
       "246                                                                                                                up into tokens, and the numericalization, which is to turn each token into its unique   \n",
       "247                                                                                                               id based on where it is in the vocabulary. The vocabulary, remember, being the unique,   \n",
       "248                                                                                                              the list of unique tokens. Now, particularly in this stage: tokenization, there's a lot   \n",
       "249                                                                                                               of little decisions that have to be made. The good news is you don't have to make them   \n",
       "250                                                                                                      because whatever pre-trained model you used the people that pre-trained it made some decisions,   \n",
       "251                      and you're going to have to do exactly the same thing, otherwise you'll end up with a different vocabulary to them and that's going to mess everything up. So that means before   \n",
       "252                                                                                                              you start tokenizing you have to decide on what model to use. Hugging Face transformers   \n",
       "253                                                                                                                            is a lot like timm. It has a library of, I believe, hundreds of models.   \n",
       "254                                                                                                    I guess I shouldn't say Hugging Face transformers. It's really the Hugging Face model hub. 44,000   \n",
       "255                                                                                                             models, so even many more even than timm's image models. And so, these models, they vary   \n",
       "256                                                                                                               in a couple of ways. There's a variety of different architectures, just like in timm   \n",
       "257                      but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could   \n",
       "258                                                                                                        type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,   \n",
       "259                                                                                                               there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,   \n",
       "260                                                                                                            thanks to the Hugging Face model hub, you can start your pre-trained model with something   \n",
       "261                                                                               that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents.   \n",
       "262                                                                                                            Having said that, there are some just generally pretty good models that work for a lot of   \n",
       "263                                                                                                               things a lot of the time, and deberta-v3 is certainly one of those. This is a very new   \n",
       "264                                                                                                            area. NLP has been, like, practically, really effective for, you know, general users, for   \n",
       "265                                                                                                       only a year or two, whereas for computer vision it's been quite a while. So you'll see, you'll   \n",
       "266                                                                                                                find that a lot of things aren't quite as well bedded down. I don't have a picture to   \n",
       "267                                                                                                             show you of which models are the best or the fastest and the most accurate and whatever,   \n",
       "268                                                                                                               right? This, a lot of this stuff is, like stuff that we're figuring out as a community   \n",
       "269                                                                                                                using competitions like this, in fact. And this is one of the first NLP competitions,   \n",
       "270                                                                                                        actually, in the kind of modern NLP era. So, you know, we've been studying these competitions   \n",
       "271                                                                                                         closely and yes, I can tell you that deberta-v3 is actually a really good starting point for   \n",
       "272                         a lot of things so that's why we've picked it. So we pick our model and just like in timm for image, you know, models there's often going to be a small, a medium, a large   \n",
       "273                                                         and of course we should start with small, right, because small is going to be faster to train we're going to be able to do more iterations.   \n",
       "274                                                                                                                                                                                  and so forth. Okay.   \n",
       "275                                                                           So at this point remember the only reason we picked our model is because we have to make sure we tokenize in the same way.   \n",
       "276                  To tell transformers that we want to tokenize the same way that the people that built a model did, we use something called AutoTokenizer. It's nothing fancy, it's basically just a   \n",
       "277                                                                                       dictionary which says: oh, which model uses which tokenizer?. So when we say AutoTokenizer.from_pretrained   \n",
       "278                                                                                                            it will download the vocabulary and the details about how this particular model tokenized   \n",
       "279                                                                                                               the dataset. So, at this point we can now take that tokenizer and pass a string to it.   \n",
       "280                                                                                                                     So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's   \n",
       "281                                                                                                               kind of putting it into words, kind of not. So if you've ever wondered whether g'day   \n",
       "282                                                                                                                  is one word or two you know it's actually three tokens according to this tokenizer.   \n",
       "283                                                                                                              And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token.   \n",
       "284                                                                                                       And so, you kind of get the idea. These underscores here? That represents the start of a word,   \n",
       "285                           right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the   \n",
       "286                                                                                                      start of a word, that kind of means a different thing. So this is what happens when we tokenize   \n",
       "287                                                                                                                               this sentence using the tokenizer that the deberta-v3 developers used.   \n",
       "288                                                                                                           So here's a less common (unless you're a big platypus fan like me), less common sentence.:   \n",
       "289                                                                                                         A platypus is an ornithorhynchus anatinus. So okay, in this particular vocabulary platypus   \n",
       "290                                                                                                      got its own word, its own token, but ornithorhynchus didn't. And so I still remember grade one,   \n",
       "291                                  for some reason our teacher got us all to learn how to spell ornithorhynchus, so, one of my favorite words. So you can see here it's been split into _or, ni,   \n",
       "292                                                                                        tho, rhynch, us. So every one of these tokens you see here is going to be in the vocabulary, right? The   \n",
       "293                                                                                                       list of unique tokens that was created when this, when this particular model, this pre-trained   \n",
       "294                                                                                                                     model, was first trained. So somewhere in that list we'll find _A (underscore   \n",
       "295                                                                                                                   capital A), and it'll have a number and so that's how we'll be able to turn these   \n",
       "296                                                                                                          into numbers. So this first process is called tokenization and then the thing where we take   \n",
       "297                                         these tokens and turn them into numbers is called numericalization. So, our data set, remember we put our string into the input field so here's a function   \n",
       "298                                                                                                                that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our   \n",
       "299                                                                                                               tokenization function. Tokenization can take a minute or two so we may as well get all   \n",
       "300                                                                                                                 of our processes used doing it at the same time to save some time. So if you use the   \n",
       "301                              dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes   \n",
       "302                       this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with   \n",
       "303                                                                                                             batched=True it'll be able to do more stuff at once. So look it only took six seconds,   \n",
       "304                                                                                                                so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going   \n",
       "305                                                                                                              to contain exactly the same as our original data set.) No sorry, it's not going to take   \n",
       "306                    exactly the same as the original data set, it's going to contain exactly the same input as our original data set and it's also going to contain a bunch of numbers. These numbers   \n",
       "307                                                                                                                 are the position in the vocabulary of each of the tokens in the string, so we've now   \n",
       "308                                                                                                              successfully turned a string into a list of numbers. That is a great first step. We can   \n",
       "309                                                                                                                    see how this works, we can see for example that we've got of at this a separate   \n",
       "310                                                                                                                   word, so that's going to be an _of in the vocabulary we can grab the vocabulary,   \n",
       "311                                                                                                                  look up _of, find that it's 265 and check here: yep here it is 265. Okay, so it's   \n",
       "312                                                                                                             not rocket science right? It's just looking stuff up in a dictionary to get the numbers.   \n",
       "313                                                                                                   Okay, so that is the tokenization and numericalization necessary in NLP to turn our documents into   \n",
       "314                                                                                                                             numbers to allow us to put it into our model. Any questions so far John?   \n",
       "315                       Yeah, thanks Jeremy so there's a couple and this seems like a good time to throw them out  and it's related to how you've formatted your input data into these sentences that   \n",
       "316                                                                                                               you've just tokenized. So one question was really about: How you choose those keywords   \n",
       "317                                                                                                             and the order of the fields that you know, so I guess just interested in an explanation,   \n",
       "318                                                                is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,   \n",
       "319                                                                                                            you know, doesn't matter! We just want some way, something that it can learn from, right?   \n",
       "320                                                                                                                 So if I just concatenated it without these headers before each one, it wouldn't know   \n",
       "321                            where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're   \n",
       "322                                                                                            so flexible As long as you give it the information somehow, it doesn't really matter how you give it the   \n",
       "323                      information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't know, one semicolon here, and two here, and three here. Yeah it's not   \n",
       "324                                                                                                            a big deal. At the level where you're, like, trying to get an extra half a percent to get   \n",
       "325                                  up the leaderboard of a Kaggle competition you may find tweaking these things makes tiny differences, but in practice you won't generally find it matters too much.   \n",
       "326                                                                                                            Right, thank you. And I guess the second part of that, somebody's asking: If one of their   \n",
       "327                                                                                                                 fields was particularly long, say it was a thousand characters, is there any special   \n",
       "328                                                                                                         handling required there? Do you need to re-inject those kinds of special marker tokens? Does   \n",
       "329                                                                                                                    it change if you've got much bigger fields that you're trying to learn and query?   \n",
       "330                                                                                                              Yes. Long documents and ULMFiT require no special consideration. IMDb in fact has multi   \n",
       "331                                                                                                                 thousand word movie reviews, and it works great. To this day, ULMFiT is probably the   \n",
       "332                                                                                                         best approach for reasonably quickly and easily using large documents. Otherwise, if you use   \n",
       "333                                                                                                            transformer-based approaches, large documents are challenging. Specifically, transformers   \n",
       "334                   basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with   \n",
       "335             large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with   \n",
       "336                                                                                                             documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if   \n",
       "337                                                                                                              it works for you, but you know I'd certainly try both. For under 2,000 words, you know,   \n",
       "338                                                                                           transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory.   \n",
       "339                                                                                                                So, Hugging Face transformers has these, you know As I say it right now, I find them   \n",
       "340                        somewhat obscure and not particularly well documented expectations about your data, that you kind of have to figure out, and one of those is that it expects that your target   \n",
       "341                                                                                                              is a column called labels. So once I figured that out, I just went, got our tokenized   \n",
       "342                          DataSet, and renamed our score column to labels, and everything started working. I don't know if at some point they'll make this a bit more flexible, but its probably   \n",
       "343                                                       best to just call your target labels and life will be easy. You might have seen back when I went ls {path} that there was another data set   \n",
       "344                                                                                                                 there, called test.csv. And if you look at it, it looks a lot like our training set,   \n",
       "345                                                                                                           that's our other CSV that we've been working with, but it's missing the score. The labels.   \n",
       "346                                                                                                                   This is called a test set  and so we're going to talk a little bit about that now   \n",
       "347                                                                                                                 because my claim here is that perhaps the most important idea in machine learning is   \n",
       "348                                                                                                                                 the idea of having separate training, validation and test data sets.   \n",
       "349                                                                                                Test and validation sets are all about identifying and controlling for something called overfitting   \n",
       "350                                                                                                            and we're going to try and learn about this through example. This is the same information   \n",
       "351                                                                                                                               that's in that Kaggle notebook  I've just put it on some slides here.   \n",
       "352                                                                                                            So I'm going to create a function here called plot_poly and I'm actually going to use the   \n",
       "353                                                                                                              same data that, I don't know if you remember, we used it earlier for trying to fit this   \n",
       "354                                                                                                                       quadratic. We created some x and some y data. This is the data we're going   \n",
       "355                                                                                                                                           to use and we're going to use this to look at overfitting.   \n",
       "356                           The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for   \n",
       "357                                                                                                                those of you that remember, a first degree polynomial is just a line, it's y = a x.   \n",
       "358                                                                                                                A second degree polynomial will be y = ax^2 + bx + c, third degree polynomial we'll   \n",
       "359                                                                                                          have a cubic, fourth degree you know quartic, and so forth. And what I've done here is I've   \n",
       "360                                                                                                               plotted what happens if we try to fit a line to our data. It doesn't fit very well. So   \n",
       "361                                                                                                                   what happened here is we we did a linear regression and what we're using here is   \n",
       "362               a very cool library called scikit-learn. scikit-learn is something that, you know, I think it'd be fair to say it's mainly designed for kind of classic machine learning methods like,   \n",
       "363                        kind of linear regression and stuff like that  I mean, very advanced versions of these things, but it's also great for doing these quick and dirty things. So in this case I   \n",
       "364                    wanted to do a what's called a polynomial regression which is fitting the polynomial to data and it's just these two lines of code. It's a super nice library. So in this case,   \n",
       "365                                                                                                               a degree one polynomial is just a line, so I fit it, and then I show it with the data,   \n",
       "366                                                                                                      and there it is. Now that's what we call underfit, which is to say there's not enough, kind of,   \n",
       "367                                                                                                                                      complexity in this model I fit, to match the data that's there.   \n",
       "368            So an underfit model is a problem. It's got to be systematically biased, you know; all the stuff up here, we're going to be predicting too low; all the stuff down here, we're predicting   \n",
       "369                                                                                                        too low; all the stuff in the middle, well be predicting too high. A common misunderstanding   \n",
       "370                                                                                                         is that simpler models are kind of more reliable in some way, but models that are too simple   \n",
       "371                                                                                                                                                    will be systematically incorrect as you see here.   \n",
       "372                                                                                                                                                       What happens if we fit a 10 degree polynomial?   \n",
       "373                                                                                                           That's not great either! In this case it's not really showing us what the actual Remember   \n",
       "374                this was originally a quadratic. This is meant to match, right? And particularly at the ends here, it's predicting things that are way above what we would expect in real life right?   \n",
       "375                         And it's trying to get really it's trying really hard to get through this point, but clearly this point was just some noise, right? So this is what we call overfit. It's   \n",
       "376                                                                                                          done a good job of fitting to our exact data points, but if we sample some more data points   \n",
       "377                    from this distribution, honestly we probably would suspect they're not going to be very close to this, particularly if they're a bit beyond the edges. So that's what overfitting   \n",
       "378                                                        looks like. We don't want underfitting or overfitting. Now underfitting is actually pretty easy to recognize, because we can actually look at   \n",
       "379                                                                                                         our training data and see that it's not very close. Overfitting is a bit harder to recognize   \n",
       "380                                                                                                                                                    because the training data is actually very close.   \n",
       "381                                                                                                             Now on the other hand, here's what happens if we fit a quadratic. And here I've got both   \n",
       "382                                                                                                            the real-line and the fit-line and you can see they're pretty close, and that's of course   \n",
       "383                                                                                                                                                                               what we actually want.   \n",
       "384                                                                                                             So how do we tell whether we have something more like this, or something more like this.   \n",
       "385                                    Well what we do is we do something pretty straightforward is we take our original data set, these points, and we remove a few of them, so let's say 20% of them.   \n",
       "386                                                                                                            We then fit our model using only those points we haven't removed, and then we measure how   \n",
       "387                                                                                                                good it is by looking at only the points we removed. So in this case let's say we had   \n",
       "388                                                                                                       removed (I'm just trying to think) If I had removed this point here right, then it might have   \n",
       "389                                                                               kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away.   \n",
       "390                                                                                                              The model the data that we take away and don't let the model see it when it's training   \n",
       "391                                                                                                                    is called the validation set. So in fast.ai we've seen splitters before, right   \n",
       "392                           The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so   \n",
       "393                                                                                                              things like accuracy, measured only on the validation set. This is really unusual. Most   \n",
       "394                                                                                                         libraries make it really easy to shoot yourself in the foot, by not having a validation set,   \n",
       "395                         or accidentally not using it correctly. So fast.ai won't even let you do that. So you've got to be particularly careful when using other libraries. HuggingFace transformers   \n",
       "396                                                                                                             is good about this, so they make sure that they do show you your metrics on a validation   \n",
       "397                                                                                                          set. Now creating a good validation set is not generally as simple as just randomly pulling   \n",
       "398                                                        some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was   \n",
       "399                                                                                                                 the data you were trying to fit something to (okay) and you randomly remove some, so   \n",
       "400                                                                                                          it looks like this. That looks very easy doesn't it, because you've kind of like, still got   \n",
       "401                           all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So   \n",
       "402             if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate   \n",
       "403                                                                                                             and remove the last couple of weeks. So if this was your validation set and this is your   \n",
       "404                                                                                                                training set, that's going to be actually testing whether you can use this to predict   \n",
       "405                                                                                                                                                the future, rather than using it to predict the past.   \n",
       "406                                                                                                             Kaggle competitions are a fantastic way to test your ability to create a good validation   \n",
       "407                                                                                                              set, because Kaggle competitions only allow you to submit, generally, a couple of times   \n",
       "408                                                                                                            a day. The dataset that you are scored on in the leaderboard during that time is actually   \n",
       "409                        only a small subset in fact it's a totally separate subset to the one you'll be scored on, on the end of the competition. And so most beginners on Kaggle overfit. And it's   \n",
       "410                                                                                                                not until you've done it that you'll get that visceral feeling of like: oh my god, I   \n",
       "411                                                                                                          overfit. In the real world outside of Kaggle you will often not even know that you overfit   \n",
       "412                                                                                                             you just destroy value for your organization silently. So it's a really good idea to do   \n",
       "413                   this kind of stuff on Kaggle a few times first, in real competitions, to really make sure that you are confident you know how to avoid overfitting  how to find a good validation   \n",
       "414                                                                                              set and how to interpret it correctly. And you really don't get that until you screw it up a few times.   \n",
       "415                                                                                                              A good example of this was there was a distracted driver competition on Kaggle  there   \n",
       "416                                                                                                       are these kind of pictures from inside a car, and the idea was that you had to try and predict   \n",
       "417                                                                                                            whether somebody was driving in a distracted way or not, and on Kaggle they did something   \n",
       "418           pretty smart the test set, so the thing that they scored you on the leaderboard, contained people that didn't exist, at all, in the competition data that you train the model with. So if   \n",
       "419                        you wanted to create an effective validation set in this competition, you would have to make sure that you separated the photos, so that your validation set contained photos   \n",
       "420                                                                                                                                     of people that aren't in the data you're training your model on.   \n",
       "421                                                                                                                There's another one like that, the Kaggle fisheries competition, which had boats that   \n",
       "422            didn't appear so they were basically pictures of boats and you meant to try to guess/predict what fish were in the pictures. And it turned out that a lot of people accidentally figured   \n",
       "423                                                                                                         out what the fish were by looking at the boat, because certain boats tended to catch certain   \n",
       "424                                                                                                             kinds of fish. And so by messing up their validation set, they were really overconfident   \n",
       "425                                                                            of the accuracy of their model. I'll mention in passing, if you've been around Kaggle a bit, you'll see people talk about   \n",
       "426                                                                                                            cross-validation a lot. I'm just going to mention, be very very careful. Cross-validation   \n",
       "427                                                                                                      is explicitly not about building a good validation set, so you've got to be super super careful   \n",
       "428                                                                                                                                                                                 if you ever do that.   \n",
       "429                                                                                              Another thing I'll mention, is that scikit-learn conveniently offers something called train_test_split,   \n",
       "430                                                                                                           as does Hugging Face datasets, as does fast.ai  we have something called random splitter.   \n",
       "431                                                                                                              It can be encouraging it can almost feel like it's encouraging you to use a randomized   \n",
       "432                                                                                                            validation set because there are these methods that do it for you. But yeah, be very very   \n",
       "433                                                                                                               careful, because very very often that's not what you want, okay. So we've learned what   \n",
       "434                 a validation set is, so that's the bit that you pull out of your data that you don't train with, but you do measure your accuracy with. So what's a test set? It's basically another   \n",
       "435                                                                                                                 validation set, but you don't even use it for tracking your accuracy while you build   \n",
       "436                                                                                                                your model. Why not? Well imagine you tried two new models every day for three months   \n",
       "437                                                                                                                 (that's how long a Kaggle competition goes for.) So you would have tried 180 models,   \n",
       "438                                                                                                       and then you look at the accuracy on the validation set for each one. Some of those models you   \n",
       "439                                                                                                       would have got a good accuracy on the validation set, potentially because of pure chance, just   \n",
       "440                     a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually   \n",
       "441                                                                                                         overfit using the validation set. So you actually want to know whether you've really found a   \n",
       "442                                                                                                            good model or not. So in fact on Kaggle they have two test sets. They've got the one that   \n",
       "443                                                                                                             gives you feedback on the leaderboard during the competition and a second test set which   \n",
       "444                                                                                                          you don't get to see until after the competition is finished. So in real life you've got to   \n",
       "445                                                                                                             be very careful about this, not to try so many models during your model building process   \n",
       "446                                                                                                               that you accidentally find one that's good by coincidence. And only if you have a test   \n",
       "447           set that you've held out, or you know that. Now that leads to the obvious question which is very challenging, is you spent three months working on a model, worked well on your validation   \n",
       "448                     set, you did a good job of locking that test set away in a safe so you weren't allowed to use it, and at the end of the three months you finally checked it on the test set, and   \n",
       "449                                                                                                            it's terrible. What do you do? Honestly you have to go back to square one. You know there   \n",
       "450                                                                                                              really isn't any choice other than starting again. So this is tough, but it's better to   \n",
       "451                                                                                                                      know, right. Better to know than to not know, so that's what a test set is for.   \n",
       "452                   So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something   \n",
       "453                                                                                                                 like accuracy. It's a number that tells you: How good is your model? Now on Kaggle   \n",
       "454                                                                                                               this is very easy. What metric should we use? Well they tell us go to overview, click   \n",
       "455                                                                                                            on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation   \n",
       "456                                                  Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that   \n",
       "457                                                                                                        we will take the derivative of, and find the gradient, and use that to improve our parameters   \n",
       "458                                                                                                                 during training? And the answer is: maybe, sometimes, but probably not. For example,   \n",
       "459                                                                                                  consider accuracy. Now, if we were using accuracy to calculate our derivative and get the gradient,   \n",
       "460                 you could have a model that's actually slightly better, you know, it's slightly like it's doing a better job of recognizing dogs and cats, but not so much better that it's actually   \n",
       "461                                                                                                        caused any incorrectly classified cat to become a dog. So the accuracy doesn't change at all.   \n",
       "462                                                                                                              So the gradient is zero. You don't want stuff like that. You don't want bumpy functions   \n",
       "463                     because they don't have nice gradients  often they don't have gradients at all, they're basically zero nearly everywhere. You want a function that's nice and smooth. Something   \n",
       "464                                                                                                        like, for instance, the average absolute error, mean absolute error, which we've used before.   \n",
       "465                    So that's the difference between your metrics and your loss. Now be careful, right, because when you're training your model's spending all of its time trying to improve the loss   \n",
       "466                                         and most of the time that's not the same as a thing you actually care about, which is your metric. So you've got to keep those two different things in mind.   \n",
       "467                                                                                                                  The other thing to keep in mind is that in real life you can't go to a website and   \n",
       "468                                                                                                                 be told what metric to use. In real life the model that you choose, there isn't one   \n",
       "469                        number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex   \n",
       "470                                                                                                        process often involving humans, both as users and customers and as people, you know, involved   \n",
       "471                                                                                                                  in as part of the process. There's all kinds of things that are changing over time   \n",
       "472                                                                                                        and there's lots and lots of outcomes of decisions that are made. One metric is not enough to   \n",
       "473                                                                                                                capture all of that. Unfortunately, because it's so convenient to pick one metric and   \n",
       "474                                                                                                            use that to say: I've got a good model, that very often finds its way into industry, into   \n",
       "475                                                                                                                  government where people roll out these things that are good on the one metric that   \n",
       "476                                                                                                            happened to be easy to measure. And again and again we found people's lives turned upside   \n",
       "477                                                                                                          down because of how badly they get screwed up by models that have been incorrectly measured   \n",
       "478                                                                                                        using a single metric. So my partner Rachel Thomas has written this article which I recommend   \n",
       "479                                                                                                                                    you read about The problem with metrics is a big problem for AI   \n",
       "480                              It's not just an AI thing! There's actually this thing called Goodharts Law that states when a measure becomes a target, it ceases to be a good measure. The thing   \n",
       "481                                                                                                               is so when I was a management consultant, you know, 20 years ago, we were always kind   \n",
       "482                                                                                                                of part of these strategic things trying to like: find key performance indicators and   \n",
       "483                         ways to kind of, you know, set commission rates for sales people and we were really doing a lot of this, like, stuff which is basically about picking metrics and, you know,   \n",
       "484                                                                                                              we see that happen go wrong in industry all the time. AI is dramatically worse because   \n",
       "485                                                                                                            AI is so good at optimizing metrics, and so that's why you have to be extra, extra, extra   \n",
       "486                                      careful about metrics, when you are trying to use a model in real life. Anyway, as I said in Kaggle, we don't have to worry about any of that, we're just going   \n",
       "487                                                      to use the Pearson correlation coefficient which is all very well as long as you know what the hell the Pearson correlation coefficient is   \n",
       "488                                                                                                      If you don't, let's learn about it. So Pearson correlation coefficient is usually abbreviated   \n",
       "489                                                                                                                  using letter r and it's the most widely used measure of how similar two variables   \n",
       "490                                                                                                               are. And so, if your predictions are very similar to the real values then the Pearson   \n",
       "491                                                                                                                  correlation coefficient will be high, and that's what you want. r can be between   \n",
       "492                                                                                                         minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle   \n",
       "493                                        competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct.   \n",
       "494              Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function.   \n",
       "495                           I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical function, but how   \n",
       "496                                                                                                          it behaves; and I find most people, even who work in data science, have not actually looked   \n",
       "497                                                                                                                    at a bunch of data sets to understand how r behaves. So let's do that right now   \n",
       "498                                                                     so that you're not one of those people. The best way I find to understand how data behaves in real life, is to look at real-life   \n",
       "499                          data so there's a data set scikit-learn comes with a number of data sets, and one of them is called California housing and it's a data set where each row is a district   \n",
       "500                                                                                                                  and, it's kind of demographic, sorry it's information some demographic information   \n",
       "501                                                                                                                           about different districts, and about the value of houses in that district.   \n",
       "502                                                                                                       Im not going to try to plot the whole thing, it's too big, and this is a very common question   \n",
       "503                                                                                                                I have from people is: how do I plot data sets with far too many points? The answer   \n",
       "504                                                                                                                 is very simple: get less points. So I just randomly grab a thousand points. Whatever   \n",
       "505                         you see with a thousand points, is going to be the same as what you see with a million points. There's no point no reason, to plot huge amounts of data generally just grab   \n",
       "506                                                                                            a random sample. Now, numpy has something called corecoeff() to get the correlation coefficient between   \n",
       "507                                                                                                           every variable and every other variable, and it returns a matrix. So I can look down here,   \n",
       "508                    and so for example, here is the correlation coefficient between variable one, and variable one. Which of course is exactly perfectly 1.0. Right? because variable one is the same   \n",
       "509                                                                                                             as variable one. Here is the small inverse correlation between variable one and variable   \n",
       "510                                                                                                                two, and medium-sized positive correlation between variable one and variable 3 and so   \n",
       "511                                                                                                               forth. This is symmetric about the diagonal because the correlation between variable 1   \n",
       "512                                                                                                             and variable 8 is the same as the correlation between variable 8 and variable 1. So this   \n",
       "513                                                                                                                                                                 is a correlation coefficient matrix.   \n",
       "514              So that's great when we wanted to get a bunch of values all at once. For the Kaggle competition we don't want that. We just want a single correlation number. If we just pass in a pair   \n",
       "515                                                                                                                  of variables, we still get a matrix which is kind of weird.. it's kind of it's not   \n",
       "516                    weird, but it's not what we want! So we should grab one of these. So when I want to grab a correlation coefficient, I'll just return the zeroth row, first column. So that's what   \n",
       "517                                                                  core is. That's going to be our single correlation coefficient. So let's look at the correlation between two things; for example   \n",
       "518                                                                                                             median income, and median house value: 0.67. Okay? Is that high? medium? low? How big is   \n",
       "519                                                                                               that? What does it look like? So the main thing we need to understand is: what these things look like.   \n",
       "520                     So what I suggest we do is: we're going to take a 10 minute break nine minute break. We'll come back at half past, and then we're going to look at some examples of correlation   \n",
       "521                                                                                                    coefficients Okay. Welcome back! So what I've done here is I've created a little function called   \n",
       "522                                                                                                         show correlations, and I'm passing a DataFrame and a couple of columns as strings. I'm going   \n",
       "523                                                                                                           to grab each of those columns as series, do a scatter plot, and then show the correlation.   \n",
       "524                                                                                                                   So, we already mentioned median income and median house value of 0.68, so here   \n",
       "525                       it is here's what .68 looks like. So you know I don't know if you had some intuition about what you expected, but as you can see it's still plenty of variation, even at that   \n",
       "526                                                                              reasonably high correlation. Also, you can see here that visualizing your data is very important if you're working with   \n",
       "527                                                                                                             this data set, because you can immediately see all these dots along here. That's clearly   \n",
       "528                    truncation right? So this is like, when... it's not until you look at pictures like this, that you're gonna pick stuff like this up. Pictures are great! Oh! little trick: on the   \n",
       "529                                                                                                          scatter plot, I put alpha as 0.5, that creates some transparency. For these kind of scatter   \n",
       "530                                                                                                             plots, that really helps, because it like kind of creates darker areas in places where   \n",
       "531                                           there's lots of dots. So, yeah, alpha in scatter plots is nice. Okay, here's another pair. So this one's gone down from 0.68 to 0.43. Median income versus   \n",
       "532                                                                                                             the number of rooms per house. As you'd expect more rooms it's more income, but this is   \n",
       "533                                                                                                           a very weird looking thing. Now, you'll find that a lot of these statistical measures like   \n",
       "534                                                                                                          correlation rely on the square of the difference, and when you have big outliers like this,   \n",
       "535                 the square of the difference goes crazy, and so this is another place we'd want to look at the data first, and say oh that's that's going to be a bit of an issue. There's probably   \n",
       "536                                                                                                         more correlation here, but there's a few examples of some houses with lots and lots of rooms   \n",
       "537                                                                                     where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?   \n",
       "538                                                                                                                     So, r is very sensitive to outliers. So let's get rid of the houses the rooms   \n",
       "539                                                                                                                    with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up   \n",
       "540                                                                                                               from 0.43 to 0.68, even though we probably only got rid of one two three four five six   \n",
       "541                       seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,   \n",
       "542                  and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row.   \n",
       "543                                                             So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Because   \n",
       "544                                                                                                        0.34 sounds like quite a good relationship, but you almost can't see it! So this is something   \n",
       "545                                                                                                              I strongly suggest is, if you're working with a new metric, is: Draw some pictures of a   \n",
       "546                             few different levels of that metric to kind of try to get a feel for like what does it mean? You know, what does 0.6 look like? What does 0.3 look like? And so forth.   \n",
       "547                                                                                                               And here's an example of a correlation of minus 0.2. This very slight, negative slope.   \n",
       "548                       Okay, so there's just more of a kind of a general tip, of something I like to do when playing with a new metric, and I recommend you do as well. I think we've now got a sense   \n",
       "549                                                                              of what the correlation feels like. Now you can go look up the equation on Wikipedia if you're into that kind of thing.   \n",
       "550                                                                                                          We need to report the correlation after each epoch because we want to know how our training   \n",
       "551                                                                                                              is going. Hugging Face expects you to return a dictionary because it's going to use the   \n",
       "552                                                                                                                    keys of the dictionary to like label each metric. So here's something that gets   \n",
       "553                                                                                                                            the correlation, and returns it as a dictionary with the label pearson.   \n",
       "554                                                                                                                              Okay, so we've done metrics, we've done our training/ validation split.   \n",
       "555                                                                                                              Oh! we might have actually skipped over the bit where we actually did the split! Did I?   \n",
       "556                                                                                                 I did! So, to actually do the split, because in this Kaggle competition  I've got another notebook,   \n",
       "557                             we'll look at later, where we actually split this properly  but here we're just going to do a random split. Just to keep things simple for now, of 25 percent, will be   \n",
       "558                                                                                                              of the data will be a validation set. So, if we go tok_ds.train_test_split() it returns   \n",
       "559                                                                                                                  a data set dict; which has a train, and a test. So that looks a lot like a datasets   \n",
       "560                                                                                                                                                                object in fast.ai. Very similar idea!   \n",
       "561                      So this will be the thing that we'll be able to train with, so it's going to train with this data set, and return the metrics on this data set. This is really a validation set   \n",
       "562                                                                                                                                                           but Hugging Face datasets calls it test.   \n",
       "563                                                                                                           Okay. We're now ready to train our model. In fast.ai, we use something called a learner.   \n",
       "564                          The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch   \n",
       "565                                                                                                                  sizes. In short, each time we pass some data to our model for training, it's going   \n",
       "566                                                                                                                    to return it's going to send through a few rows at a time to the GPU, so that it   \n",
       "567                                                                                                                       can calculate those in parallel. Those a bunch of rows is called a batch or   \n",
       "568                            a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the   \n",
       "569                                                                                                          more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll   \n",
       "570                                                                                                                get an out of memory error on your GPU. So, you know, it's a bit of trial and error   \n",
       "571                                                                                                            to find a batch size that works. Epochs we've seen before. Then we've got the learning   \n",
       "572                                                                                                              rate. We'll talk in the next lesson  unless we get to this lesson  about a technique   \n",
       "573                      to automatically find a or semi-automatically find a good learning rate. We already know what a learning rate is from the last lesson. I've played around and found one that   \n",
       "574                                                                                                              seems to train quite quickly without falling apart, so I just tried a few. Generally, I   \n",
       "575                                                                                                                     kind of, you know, if I if I don't have a so Hugging Face transformers doesn't   \n",
       "576                        have something to help you find the learning rate. This the integration we're doing in fast.ai, will let you do that, but if you're using a framework that doesn't have that,   \n",
       "577                                                                           you can just start with a really low learning rate, and then kind of double it, and keep doubling it until it falls apart.   \n",
       "578                                      Hugging Face transformers uses this thing called training arguments which is a class where you just provide all of the kind of configuration so you have to   \n",
       "579                                                                                                            tell it what your learning rate is. This stuff here is the same as what we call basically   \n",
       "580                                                                                           fit_one_cycle() in fast.ai. You always want this to be true, because it's going to be faster pretty much   \n",
       "581           and then the this stuff here, you can probably use exactly the same every time. There's probably a lot of boilerplate compared to fast.ai as you see. This stuff you can probably use the   \n",
       "582                                                                             same every time. Okay, so We now need to create our model. So, the equivalent of the vision learner function that we've   \n",
       "583                                                                                                                used to automatically create a reasonable vision model? In Hugging Face transformers,   \n",
       "584  they've got lots of different ones depending on what you're trying to do. So, we're trying to do classification as we've discussed, of sequences, so if we call AutoModelForSequenceClassification,   \n",
       "585                          it will create a model that is appropriate for classifying sequences from a train pre-trained model, and this is the name of the model that we did earlier the deberta-v3.   \n",
       "586                 It has to know when it adds that random matrix to the end, how many outputs it needs to have. So we have one label which is the score. So that's going to create our model, and then   \n",
       "587                                                                                                        this is the equivalent of creating a learner. It contains a model, and the data the training   \n",
       "588                  data, and the test data. Again, there's a lot more boilerplate here than fast.ai, but you can kind of see the same basic steps here. We just have to do a little bit more manually,   \n",
       "589                             but it's not you know, it's nothing too crazy. So, it's going to tokenize it for us using that function, and then these are the metrics that will print out each time.   \n",
       "590                                                                                                                                   That's that little function we created which returns a dictionary.   \n",
       "591                    At the moment I find Hugging Face transformers very verbose. It spits out lots and lots and lots of text which you can ignore, and we can finally call train, which will spit out   \n",
       "592                                                                                                          much more text again, which you can ignore, and as you can see, as it trains, it's printing   \n",
       "593                                                                                                        out the loss, and here's our Pearson correlation coefficient. So, it's training and we've got   \n",
       "594                                                                                                           a 0.834 correlation, that's pretty cool! Right. I mean it took what ? Oh here we are five   \n",
       "595                minutes to run. Maybe that's five minutes per epoch on Kaggle which doesn't have particularly great GPUs (but good for free) and we've got something that is you know got a very high   \n",
       "596                                                                                                            level of correlation in assessing how similar the two columns are, and the only reason it   \n",
       "597                                                                                                           could do that is because it used a pre-trained model, right. There's no way you could just   \n",
       "598                                                                                                      have that tiny amount of information and figure out whether those two columns are very similar.   \n",
       "599                         This pre-trained model already knows a lot about language. It already has a good sense of whether two phrases are similar or not, and we've just fine-tuned it. You can see,   \n",
       "600                           given that after one epoch it was already at 0.8. You know we this was a model that already did something pretty close to what we needed. It didn't really need that much   \n",
       "601                                                                           extra tuning for this particular task. We got any questions there John? Yeah we do! It's actually a bit back on the topic   \n",
       "602                     before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how   \n",
       "603                                                                                                            do you decide when it's okay to remove outliers? Like, you pointed out some in that data   \n",
       "604                                                                                                         set, and clearly your model is going to train a lot better if you clean that up; but I think   \n",
       "605                                                                                                             Kevin's point here is, you know, those kinds of outliers will probably exist in the test   \n",
       "606                                                                              set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense.   \n",
       "607                                                                                                             So, outliers should never just be removed, like, for modeling So if we take the example   \n",
       "608                    of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interesting! it seems like there's a separate   \n",
       "609                    group of districts with a different kind of behavior. Yeah my guess is that they're going to be kind of like dorms or something like that, you know, probably low-income housing   \n",
       "610                                                                                                        and so I would be saying like, oh clearly, from looking at this dataset, these two different   \n",
       "611                                                                groups can't be treated the same way, they have very different behaviors, and I would probably split them into two separate analyses.   \n",
       "612                                                                                                           You know the... the word outlier... it kind of exists in a statistical sense, right? There   \n",
       "613               can be things that are well outside our normal distribution and mess up our kind of metrics and things. It doesn't exist in a real sense. It doesn't exist in a sense of like... oh...   \n",
       "614                                                           things that we should, like, ignore or throw away. You know, some of the most useful kind of insights I've had in my life in data projects   \n",
       "615                                                                                                              has been by digging into outliers...so-called outliers... and understanding: well, what   \n",
       "616                                                                                                               are they? And where did they come from? and it's kind of... often in those edge cases   \n",
       "617                                                                                                                 that you discover really important things about, like, where processes go wrong  or   \n",
       "618           about, you know, kinds of behaviors you didn't even know existed, or indeed about, you know, kind of labeling problems or process problems which you really want to fix them at the source   \n",
       "619         because otherwise when you go into production you're going to have more of those so-called outliers. So yeah. I'd say never delete outliers without investigating them and having a strategy   \n",
       "620               for ...like... understanding where they came from and ...like... what should you do about them. All right. So now that we've got a trained model, you'll see that it actually behaves,   \n",
       "621                  you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah   \n",
       "622                                                                                                             this looks like stuff I've seen before, you know, like a bit more wordy and some slight   \n",
       "623                     changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're   \n",
       "624                                                                                                                 going to pass in our dataset from the Kaggle test file  and so that's going to give   \n",
       "625                                                                                                                                                      us our predictions, which we can cast to float.   \n",
       "626                                                                                                            And here they are. So here are the predictions we made of similarity. Now again, not just   \n",
       "627                                                                                                    for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,   \n",
       "628                                                                                                         I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly   \n",
       "629                                                                                                         all of them had the problem we have right now, which is negative predictions and predictions   \n",
       "630                                                                                                               over one. So I'll be showing you how to fix this in a more proper way, maybe hopefully   \n",
       "631                                                                                                             in the next lesson but for now you know we could at least just round these off ...right?   \n",
       "632                           because we know that none of the scores are going to be bigger than one or smaller than zero, so our correlation coefficient will definitely improve if we at least round   \n",
       "633                                                                                                              this up to zero and round this down to one. As I said, there are better ways to do this   \n",
       "634                                                                                                               but that's certainly better than nothing. So, in Pytorch, you might remember from when   \n",
       "635                      we looked at ReLU, there's a thing called clip and that will clip everything under zero to zero and everything over one to one and so now that looks much better. So here's our   \n",
       "636                                                                                                                 predictions. So Kaggle expects submissions to generally be in a CSV file and Hugging   \n",
       "637                                                                                                            Face datasets... it kind of looks a lot like pandas, really. We can create our submission   \n",
       "638                                                                                                              file from... with our two columns called dot csv and there we go. That's basically it.   \n",
       "639                                                                                                         So yeah you know... it's... it's... it's kind of nice to see how... you know... it's a sense   \n",
       "640                                                                                                           how far deep learning has come since we started this course a few years ago. That nowadays   \n",
       "641                 you know... there are multiple libraries around to kind of do this the same thing. We can, you know, use them in multiple application areas. They all look kind of pretty familiar.   \n",
       "642                                                                                                             They're reasonably beginner friendly. And NLP, because it's kind of like the most recent   \n",
       "643                                                                                                           area that's really become effective in the last year or two, is probably where the biggest   \n",
       "644                                                                                                                opportunities are for, you know, big wins both in research and commercialization. And   \n",
       "645                      so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would   \n",
       "646                      you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you   \n",
       "647                                                                                                                  know, or it took 10 times more time or it took 10 times more money or whatever.   \n",
       "648                                                                                                                                                           So I think NLP is a huge opportunity area.   \n",
       "649                                                                                                     It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.   \n",
       "650                                                                                              Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it.   \n",
       "651                                                               So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?   \n",
       "652             And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much   \n",
       "653                                                                                                        much better now  so even then you could see these models were generating context appropriate   \n",
       "654                                                                                                                                                                                    believable prose.   \n",
       "655                                                                                                              You know I would strongly believe that like any of our... kind of like... upper tier of   \n",
       "656                                                                                                      competent fast.ai alumni would be fairly easily able to create a bot which could create context   \n",
       "657                                                                                                            appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side   \n",
       "658                                                                                                               of an argument and you could scale that up such that 99% of twitter was these bots and   \n",
       "659                                                                                                               nobody would know. You know, nobody would know. And that's very worrying to me because   \n",
       "660                                                                                                                 a lot of, you know, a lot of...kind of... the way people see the world is now really   \n",
       "661                                                                                                                 coming out of their... their social media conversations, which at this point they're   \n",
       "662                                                                                                         controllable. Like... it would not be that hard to create something that's kind of optimized   \n",
       "663                                                                                                        towards moving a point of view amongst a billion people, you know, in a very subtle way, very   \n",
       "664                                                 gradually, over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth.   \n",
       "665                                                                                                                             Here is the start of an article in the Guardian which I'll let you read.   \n",
       "666              This article was, you know, quite long. These are just the first few paragraphs and at the end, it explains that this article was written by GPT3. It was given the instruction please   \n",
       "667                          write a short op-ed around 500 words. Keep the language simple and concise. Focus on why humans have nothing to fear from AI. So GPT3 produced eight outputs and then they   \n",
       "668              say, basically the... the editors at The Guardian did about the same level of editing that they would do for humans. In fact, they found it a bit less editing required than humans. So   \n",
       "669                                                                                                             you know again, like, you can create longer pieces of context appropriate prose designed   \n",
       "670                                                                                                                 to argue a particular point of view. What kind of things might this be used for? You   \n",
       "671                                                                                                              know, that we won't know probably for decades if ever but sometimes we get a clue based   \n",
       "672                                                                                                         on older technology. Here's something from back 2017 and the pre ...kind of... deep learning   \n",
       "673                                                                                                           NLP days. There were millions of submissions to the FTC about the net neutrality situation   \n",
       "674                                                                                                                 in America. Very very heavily biased towards the point of view of saying we want to   \n",
       "675                                                                                                                   get rid of net neutrality. An analysis by Jeff Kao showed that something like 99%   \n",
       "676                                                                                                          of them and in particular, nearly all of the ones which were pro removal of net neutrality,   \n",
       "677                                                                                                        were clearly auto generated by basically ...if you look at the green, there's like, selecting   \n",
       "678                       from a menu. So we've got... Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire individuals as opposed to Washington bureaucrats should   \n",
       "679                   be blah...blah... people like me as opposed to so-called experts... and you get the idea. Now this is an example of a very very, you know, simple approach to auto-generating huge   \n",
       "680                                                                                                            amounts of text. We don't know for sure but it looks like this might have been successful   \n",
       "681                                                                                                     because this went through. You know, despite what seems to be actually overwhelming disagreement   \n",
       "682                                                                                                        from the public that everybody, almost everybody, likes net neutrality, the FTC got rid of it   \n",
       "683                         and this was a big part of the basis. Its like oh we got all these comments from the public and everybody said they don't want net neutrality. So imagine a similar thing   \n",
       "684                      where you absolutely couldn't do this. You couldn't figure it out because everyone was really very compelling and very different. That's, you know, it's kind of worrying about   \n",
       "685                                                                                                            how we deal with that. You know, I will say... when I talk about this stuff, often people   \n",
       "686                                                                                                            say ah no worries we'll build a model to recognize... you know... bot generated content   \n",
       "687                         but, you know, if I put my black hat on, I'm like no that's not gonna work, right?. If you told me to build something that beats the bot classifiers, I'd say no worries,   \n",
       "688                    easy. You know, I will take the... the code or the service... or service... whatever that does the bot classifying and I will include beating that in my loss function and I will   \n",
       "689                                                                                                                fine-tune my model until it beats the bot classifier. You know, when I used to run an   \n",
       "690                                                                                                           email company, we had a similar problem with spam prevention and our spammers could always   \n",
       "691                                                                                                                take a spam prevention algorithm and change their emails until it didn't get the spam   \n",
       "692                                                                                                                 prevention algorithm anymore, for example. So yes, I... I'm really excited about the   \n",
       "693                                                                                                           opportunities for... for students in this course to build, you know, I think very valuable   \n",
       "694                                                                                                             businesses, really cool research and so forth using these pretty new NLP techniques that   \n",
       "695                are now pretty accessible and I'm also really worried about the things that might go wrong. I do think though that the more people that understand these capabilities the less chance   \n",
       "696                                                                they'll go wrong. John, was there some questions? Yeah I mean it's a throwback to the... to the workbook that you had before... yeah   \n",
       "697                                                                                                               that's the one. The question Manikandan is asking... shouldn't num labels be five zero   \n",
       "698                                                                                                       zero point two five zero point five zero point seven five one instead of one? Isn't the target   \n",
       "699                                              a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if   \n",
       "700                                                                                               this was being treated as a categorical problem with five categories, it's still considered one label.   \n",
       "701                                                                                                             In this case though, we're actually treating it as a regression problem. It's one of the   \n",
       "702                      things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if   \n",
       "703                                                                                             you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which   \n",
       "704                                                                                                           is actually why we ended up with predictions that were less than zero and bigger than one.   \n",
       "705                                                                                                            So we'll be learning next time about the use of sigmoid functions to resolve this problem   \n",
       "706                                                                                and that should fix it up for us Okay, great. Well thanks everybody. I hope you enjoyed learning about NLP as much as   \n",
       "707                                                                                                          I enjoyed putting this together. I'm really excited about it and can't wait for next week's   \n",
       "708                                                                                                                                                                                      lesson. See ya!   \n",
       "\n",
       "                                                                                                                                                                                                next_seq  \\\n",
       "0                                                                                                                is the lesson that a lot of the regulars in the community have been most excited about,   \n",
       "1                                                                                                                because it's where we're gonna get some totally new material  totally new topic, we've   \n",
       "2                                                                                                               never covered before. We're going to cover natural language processing (NLP), and you'll   \n",
       "3                        find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the   \n",
       "4                                                                                                                                               fast.ai library, using recurrent neural networks (RNNs).   \n",
       "5                                                                                                             Today we're going to do something else, which is we're going to do Transformers, and we're   \n",
       "6                                                                                                                   not even going to use the fast.ai library at all in fact. So, what we're going to be   \n",
       "7                                                                                                            doing today is we're going to be fine-tuning a pre-trained NLP model using a library called   \n",
       "8                                                                                                                Hugging Face Transformers. Now given this is the fast.ai course, you might be wondering   \n",
       "9                                                                                                                                              why we'd be using a different library other than fast.ai.   \n",
       "10                                                                     The reason is that I think that It's really useful for everybody to have experience and practice of using more than one library.   \n",
       "11                                                                                                             Because you'll get to see the same concepts applied in different ways, and I think that's   \n",
       "12                                                                                                              great for your understanding of what these concepts are. Also, I really like the Hugging   \n",
       "13                                                                                                                 Face Transformers library. It's absolutely the state of the art in NLP, and it's well   \n",
       "14                                                                                                worth knowing. If you're watching this on video, by the time you're watching it, we will probably have   \n",
       "15                    completed our integration of the Transformers library into fast.ai. So it's in the process of becoming the main NLP (kind of) foundation for fast.ai. So you'll be able to combine   \n",
       "16                                                                                                                Transformers and fast.ai together. Yeah, so I think there's a lot of benefits to this,   \n",
       "17              and in the end you're going to know how to do NLP, you know, in a really fantastic library. Now the other thing is, Hugging Face Transformers doesn't have the same layered architecture   \n",
       "18                                                                                                             that fast.ai has, which means particularly for beginners, the kind of high level, height   \n",
       "19                                                                                                               you know top-tier API that you'll be using most of the time, is not as (kind of) ready   \n",
       "20                                                                                                                 to go for beginners, as you're used to from fast.ai. And so that's actually, I think,   \n",
       "21                      a good thing. You're up to Lesson Four, you know the basic idea now of how gradient descent works, and and you know, how parameters are learned as part of a flexible function,   \n",
       "22                                                                                                               I think you're ready to try using a somewhat lower level library that does a little bit   \n",
       "23                         less for you. So it's going to be, you know, a little bit more work. It's still it's a very well designed library, and it's still reasonably high level, but you're going to   \n",
       "24                    learn to go a little bit deeper. And that's kind of how the rest of the course in general is going to be. On the whole, is, we're going to get a bit deeper, and a bit deeper, and   \n",
       "25                                                                                                                a bit deeper. Now, so first of all, let's talk about what we're going to be doing with   \n",
       "26                fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now   \n",
       "27                                                                                                               you do. You played with these sliders last week, and hopefully you've all actually gone   \n",
       "28                     into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So   \n",
       "29                                                                                                            imagine that your job was to move these sliders, to get this as nice as possible, but when   \n",
       "30                                                                                                                 it was given to you, the person who gave it to you said, Oh! actually slider A, that   \n",
       "31                                                                                                           should be on 2.0, we know for sure. And slider B, we think it's like around two and a half.   \n",
       "32                                                                                                               Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because   \n",
       "33                        you could immediately start focusing on the one we have no idea about, get that in roughly the right spot, and then the one you kind of got a vague idea about, you could just   \n",
       "34                     tune it a little bit, and the one that they said was totally confident, you wouldn't move at all. You would probably tune these sliders really quickly. That's what a pre-trained   \n",
       "35                                                                                                                 model is. A pre-trained model is a bunch of parameters that have already been fitted,   \n",
       "36                                                                                                                    where some of them were already pretty confident of what they should be, and some   \n",
       "37                                                                                                              of them we really have no idea at all. And so fine-tuning is the process of taking those   \n",
       "38                                                                           ones we have no idea what they should be at all, and trying to get them right, and then moving the other ones a little bit.   \n",
       "39                                                                                                             The idea of fine-tuning a pre-trained NLP model in this way was pioneered by an algorithm   \n",
       "40                                                                                                          called ULMFiT which was first presented actually in a fast.ai course, I think the very first   \n",
       "41                                                                                                                fast.ai course. It was later turned into an academic paper by me in conjunction with a   \n",
       "42                                                                                                             then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers   \n",
       "43                                                                                                                 and went on to help inspire a huge change, you know, huge kind of step improvement in   \n",
       "44                                                                                                              NLP capabilities around the world, along with a number of other important innovations at   \n",
       "45                                                                                                            the time. This is the basic process that ULMFiT described. Step One was to build something   \n",
       "46                                                                                                           called a language model using basically nearly all of Wikipedia and what the language model   \n",
       "47                                                                                                                  did was it tried to predict the next word of a Wikipedia article. In fact every next   \n",
       "48                                                                                                          word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia   \n",
       "49                                                                                                                     articles which would say things like, you know the 17th prime number is dot dot   \n",
       "50                                                                                                                   dot or the 40th president of the United States, blah, said at his residence, blah   \n",
       "51                                                                                                                  that. You know, filling in these kinds of things requires understanding a lot about   \n",
       "52                                                                                                                  how language is structured, and about the world, and about math, and so forth. So to   \n",
       "53                                                                                                               get good at being a language model a neural network has to get good at a lot of things.   \n",
       "54                                                                                                         It has to understand how language works at a reasonably good level and it needs to understand   \n",
       "55                   what it's actually talking about, and what is actually true, and what is actually not true, and the different ways in which things are expressed, and so forth. So this was trained   \n",
       "56                   using a very similar approach to what we'll be looking at for fine-tuning but it started with random weights and at the end of it there was a model that could predict more than 30   \n",
       "57                                                                                                               percent of the time correctly what the next word of a Wikipedia article would be. So in   \n",
       "58                                                                                                                  this particular case, for the ULMFiT paper, we then took that and we were trying to   \n",
       "59                                                                                                              the first task I did actually, for the fast.ai course, back when I invented this, was to   \n",
       "60                 try and figure out whether IMDb movie reviews were positive or negative sentiment: Did the person like the movie or not? So what I did was I created a second language model so again   \n",
       "61                      the language model here is something that predicts the next word of a sentence but rather than using Wikipedia I took this pre-trained model that was trained on Wikipedia and I   \n",
       "62                                                                                                            ran a few more epochs using IMDb movie reviews. So it got very good at predicting the next   \n",
       "63                                                                                                             word of an IMDb movie review. And then finally I took those weights and I fine-tuned them   \n",
       "64                                                                                                          for the task of predicting whether or not a movie review was positive or negative sentiment.   \n",
       "65                                                                                 So those were the three steps. This is a particularly interesting approach because this very first model, in fact the   \n",
       "66                       first two models, if you think about it they don't require any label. I didn't have to collect any kind of document categories, or do any kind of surveys, or collect anything.   \n",
       "67                                                                                                         All I needed was the actual text of Wikipedia and movie reviews themselves because the labels   \n",
       "68                                                               was: whats the next word of a sentence?. Now, since we built ULMFiT, and we used RNNs (recurrent neural networks) for this, at about   \n",
       "69       the same time-ish that we released this, a new kind of architecture particularly useful for NLP at the time was developed called Transformers. And Transformers were particularly built because   \n",
       "70                                                                                                                  they can take really good advantage of modern accelerators like, like Google's TPUs.   \n",
       "71                                                                                                                   They didn't really, kind of, allow you to predict the next word of a sentence. It's   \n",
       "72                just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,   \n",
       "73                   instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked   \n",
       "74                                                                                                              the model to predict which/what were the words that were deleted, essentially. So it's a   \n",
       "75                                                                                                          pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced   \n",
       "76                           our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the   \n",
       "77                                                                                                                 same. So today we're going to be looking at models using what's become the, you know,   \n",
       "78                    much more popular approach than ULMFiT which is this Transformers masked language model approach. Okay, John do we have any questions? And I should mention we do have a professor   \n",
       "79                                                                                                              from University of Queensland, John Williams, joining us, who will be asking the highest   \n",
       "80                                                  voted questions from the community. What have you got, John? Yeah thanks Jeremy. Look, and we might be jumping the gun here, I suspect this is where   \n",
       "81                       you're going tonight but we've got a good question here on the forum which is: How do you go from a model that's trained to predict the next word, to a model that can be used   \n",
       "82                                                                                         for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place   \n",
       "83                                                                                                                to start would be the next slide, kind of give you a sense of this. You might remember   \n",
       "84                                                                                                                  in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at   \n",
       "85                                                                                                           visualizations of the first layer of a imagenet classification model and Layer One had sets   \n",
       "86                                                                                                               of weights that found diagonal edges, and here are some examples of bits of photos that   \n",
       "87                                                                                                       successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's   \n",
       "88                                                                                                             some examples of bits of pictures that matched, and then Layer Two combined those and now   \n",
       "89               you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified linear units, the outputs of those, they're   \n",
       "90                         called activations, where then themselves run through a matrix multiply, a rectified linear unit, added together, so that now you don't just have to have edge detectors, but   \n",
       "91                       Layer Two had corner detectors. And here's some examples of some corners that that corner detector successfully found. And remember, these were not engineered in any way, they   \n",
       "92                                                                                                             just evolved from the gradient descent training process. Layer Two had examples of circle   \n",
       "93                                                                                                                detectors as it turns out, and skipping a bit, by the time we got to Layer Five we had   \n",
       "94                                                                                                                bird and lizard eyeball detectors, and dog face detectors, and flower detectors and so   \n",
       "95                                                                                                                forth. Now, you know, nowadays you'd have something like a resnet50 would be something   \n",
       "96                     you'd probably be training pretty regularly in this course so that, you know, you've got 50-layers, not just 5-layers. Now the later layers do things that are much more specific   \n",
       "97                                                                                                        to the training task which is, like, actually predicting really, what is it that we're looking   \n",
       "98                                                                                                               at? The early layers, pretty unlikely you're going to need to change them much, as long   \n",
       "99                                                                           as you're looking at, like, some kind of natural photos, right? You're going to need edge detectors and gradient detectors.   \n",
       "100                                                                                                                So what we do, in the fine-tuning process, is there's actually one extra layer after   \n",
       "101                    this, which is the layer that actually says: What is this?. You know, it's, it's a dog or a cat or whatever. We actually delete that, we throw it away. So now that last matrix   \n",
       "102                                                                                                                 multiply has one output, or one output per category you're predicting. We throw that   \n",
       "103                                                                                                              away, so the model now has that last matrix that's spitting out, you know, depends, but   \n",
       "104                                                                                                           generally a few hundred activations, and what we do is, as we'll learn more shortly in the   \n",
       "105                                                                                                                 coming lesson, we just stick a new random matrix on the end of that. And that's what   \n",
       "106                                                                                                            we initially train, so it learns to use these kinds of features to predict whatever it is   \n",
       "107                                                                                                       you're trying to predict. And then we gradually train all of those layers. So that's basically   \n",
       "108                                                                                                              how it's done and so it's a bit hand wavy but we'll, particularly in part two, actually   \n",
       "109                                                                                                                build that from scratch ourselves. And in fact in this lesson, time permitting, we're   \n",
       "110                                                                                                         actually going to start going down the process of actually building a real-world deep neural   \n",
       "111                                                                                                           net in python, so we'll be starting to actually make some progress towards that goal. Okay   \n",
       "112                                                                                                               so let's jump into the notebook. So we're going to look at a Kaggle competition that's   \n",
       "113                                                                                                                 actually on as I speak, and I created this notebook called Getting started with NLP   \n",
       "114                                                                                   for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition.   \n",
       "115                                                                                                          And, so I'm going to take you through, you know, a complete submission to this competition.   \n",
       "116             And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this   \n",
       "117                                                                                                    is an actual project, that an actual organization, is prepared to invest money in getting solved,   \n",
       "118                                                                                                             using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions   \n",
       "119                   as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data   \n",
       "120               about a real problem that real organizations really care about, and a very direct way to measure the, you know, accuracy of your solution, you can't really get better than this. Okay   \n",
       "121                                                                                                               so this is a good place, a good competition to experiment with for trying NLP. Now, as   \n",
       "122                                                                                                              I mentioned here, probably the most widely useful application for NLP is classification   \n",
       "123                                                                                                                 and as we've discussed in computer vision, classification refers to taking an object   \n",
       "124                                                                                                           and trying to identify a category that object belongs to. So, previously we've mainly been   \n",
       "125                                                                                                                 looking at images. Today we're going to be looking at documents. Now, in NLP when we   \n",
       "126                                                                                                                 say document, we don't specifically mean, you know, a 20 page long, you know, essay.   \n",
       "127                                                                                                             A document could be three or four words, or a document could be the entire encyclopedia.   \n",
       "128                                                                                                      So a document is just an input to an NLP model that contains text. Now, classifying a document,   \n",
       "129                                                                                                         so deciding what category a document belongs to, is a surprisingly rich thing to do. There's   \n",
       "130                     all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:   \n",
       "131                                                                                                  positive or negative sentiment. Author identification would be taking a document and trying to find   \n",
       "132                                                                                                              the category of author. Legal discovery would be taking documents and putting them into   \n",
       "133                                                                                                                categories according to in- or out-of-scope for a court case. Triaging inbound emails   \n",
       "134                                                                                                            would be putting them into categories of, you know, throw away, send to customer service,   \n",
       "135                                                                                                               send to sales, etc. Right? So classification is a very, very rich area, and for people   \n",
       "136                                                                                                               interested in trying out NLP in real life, I would suggest classification would be the   \n",
       "137                                                                                                               place I would start, for looking for, kind of, accessible, real world, useful problems   \n",
       "138                                                                              you can solve right away. Now, the Kaggle competition does not immediately look like a classification competition. What   \n",
       "139                                                                                                                                                              it contains Let me show you some data   \n",
       "140                               What it contains is data that looks like this. It has a thing that they call anchor, a thing they call target, a thing they call context, and a score. Now these   \n",
       "141                                                                                                                 are.. I can't remember exact details but I think these are from patents, and I think   \n",
       "142                                                                                                           on the patents there are various, like, things they have to fill in in the patent, and one   \n",
       "143                                  of those things is called anchor, one of those things is called target and in the competition the goal is to come up with a model that automatically determines   \n",
       "144                                                                                                               which anchor and target pairs are talking about the same thing. So a score of one here   \n",
       "145                                                                                                                        wood article and wooden article obviously talking about the same thing. A   \n",
       "146                         score of zero here abatement and forest region not talking about the same thing. So the basic idea is we're trying to guess the score. And it's kind of a classification   \n",
       "147                          problem, kind of not. We're basically trying to classify things into either these two things are the same or these two things aren't the same. It's kind of not because   \n",
       "148                                                                                                        we have not just 1 and 0 but also 0.25, 0.5 and 0.75. There's also a column called context,   \n",
       "149                                                                                                         which is, I believe, is like the category that this patent was filed in and my understanding   \n",
       "150                                                                                                              is that whether the anchor and the target count as similar or not depends on, you know,   \n",
       "151                                                                                                           what the patent was filed under. So how would we take this and turn it into something like   \n",
       "152                                                                               a classification problem? So the suggestion I make here is that we could basically say, okay, let's put the, you know,   \n",
       "153                                                                                                                 some constant string like TEXT1 or FIELD1 before the first column and then something   \n",
       "154                                                                                                                  else like TEXT2 before the second column. Oh, and maybe, also the context, I should   \n",
       "155                       have as well TEXT3 in the context, and then try to choose a category of meaning similarity: Different Similar or Identical. So you can basically concatenate those three   \n",
       "156                                                                                                                 pieces together, call that a document and then try to train a model that can predict   \n",
       "157                                                                                                            these categories. That would be an example of how we can take this, basically, similarity   \n",
       "158                   problem, and turn it into something that looks like a classification problem. And we tend to do this a lot in deep learning, is we kind of take problems that look a bit novel and   \n",
       "159                                                                                                           different, and turn them into a problem that looks like something we recognize. All right,   \n",
       "160                                                                                                            so on Kaggle this is a, you know, larger data set that you're going to need a GPU to run.   \n",
       "161            So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically.   \n",
       "162                                                                                                              Personally, you know, I like using things like Paperspace generally better than Kaggle,   \n",
       "163                       like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's   \n",
       "164                                                                                                            some information here, I won't go through but it basically describes how you can download   \n",
       "165                                                                                                               stuff to Paperspace or your own computer as well if you want to. So I basically create   \n",
       "166                       this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd   \n",
       "167                                                                                                                                                             say if iskaggle and put those changes.   \n",
       "168                                                                                                         So here, you can see here, if I'm not on Kaggle and I don't have the data yet, then download   \n",
       "169                                  it. And Kaggle has a little API which is quite handy for doing stuff like downloading data and uploading notebooks and stuff like that, submitting to competitions.   \n",
       "170                           If we are on Kaggle then the data's already going to be there for us which is actually a good reason for beginners to use Kaggle as you don't have to worry about grabbing   \n",
       "171                                                                                                                       the data at all  it's sitting there for you as soon as you open the notebook.   \n",
       "172                                                                                                            Kaggle has a lot of python packages installed, but not necessarily all the ones you want,   \n",
       "173                      and at the point I wrote this they didn't have the Hugging Faces datasets package, for some reason, so you can always just install stuff. So you might remember the exclamation   \n",
       "174                                                                                                               mark means this is not a python command, but a shell command, a bash command. But it's   \n",
       "175                                                                                                              quite neat you can even put bash commands inside python conditionals so that's a pretty   \n",
       "176                                                                                                                                                                      cool little trick in notebooks.   \n",
       "177                                                                                                                Another cool little trick in notebooks is that if you do use a bash command like ls   \n",
       "178                    but you then want to insert the contents of a python variable, just chuck it in parentheses. So, I've got a python variable called path and I can go ls {path} in parentheses   \n",
       "179                                                                                              and that will ls the contents of the python variable path. So there's another little trick for you.   \n",
       "180                                                                                                                  All right, so when we ls that we can see that there's some CSV files. So what I'm   \n",
       "181                         going to do is, kind of, take you through, roughly the process, the kind of process I, you know, went through as, you know when I first look at a competition. So the first   \n",
       "182                                                                                                           thing is like, already a data set, indeed, what's in it? Okay, so it's got some CSV files.   \n",
       "183                                                                                       You know, as well as looking at it here, the other thing I would do is I would go to the competition website   \n",
       "184                                                                                     and if you go to Data A lot of people skip over this, which is a terrible idea, because it actually tells you   \n",
       "185                    what the dependent variable means, what the different files are, what the columns are, and so forth. So don't just rely on looking at the data itself but look at the information   \n",
       "186                                                                             that you're given about the data. So, for CSV files, so CSV files are comma separated values, so they're just text files   \n",
       "187                                                                                                            with a comma between each field, and we can read them using pandas, which for some reason   \n",
       "188                                                                                                               is always called pd. Pandas is one of, I guess, like, (I'm trying to think) probably   \n",
       "189                                                                                                                          like four key libraries that you have to know to do data science in python.   \n",
       "190                                                                                                                  And specifically, those four libraries are: numpy matplotlib pandas and pytorch.   \n",
       "191                                                                                                             So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for   \n",
       "192                                                                                                                    plotting; pandas we use for tables of data; and pytorch we use for deep learning.   \n",
       "193                                                                                                             Those are all covered in a fantastic book by the author of pandas which, the new version   \n",
       "194                                                                                                                  is actually available for free, I believe. Python for data analysis. So if you're   \n",
       "195                                                                                                              not familiar with these libraries just read the whole book, it doesn't take too long to   \n",
       "196                                                                                                               get through, and it's got lots of cool tips and it's very readable. I do find a lot of   \n",
       "197                                                                                                                 people doing this course often I see people kind of, trying to jump ahead, and and   \n",
       "198                                     want to be like: Oh I want to know how to, like, create a new architecture or Build a speech recognition system or whatever. But it then turns out that they   \n",
       "199                don't know how to use these fundamental libraries. So it's always good to be bold and be trying to build things, but do also take the time to, you know, make sure you finish reading   \n",
       "200                                                                                                          the fast.ai book and read at least Wes McKinney's book. That would be enough to really give   \n",
       "201                                                                                                            you all the basic knowledge you need, I think. So, with pandas we can read a CSV file and   \n",
       "202                                                                                                                that creates something called a DataFrame, which is just a table of data, as you see.   \n",
       "203                                                                                                             So, now that we've got a DataFrame, we can see what we're working with, and when we ask   \n",
       "204                    when in jupyter we just put the name of a variable containing a DataFrame, we get the first five rows, the last five rows, and the size. So we've got 36,473 rows. Okay, so other   \n",
       "205                                                                                                                         things I like to use for understanding a DataFrame is the describe method.   \n",
       "206             If you pass include equals object that will describe, that will describe, basically all the kind of the string fields, the non-numeric fields. So, in this case there's four of those,   \n",
       "207                          and so you can see here that, that anchor field we looked at, there's actually only 733 unique values, okay, so this thing, you can see that there's lots of repetition out   \n",
       "208                                                                                                                of 36,000. So there's lots of repetition. This is the most common one: it appears 152   \n",
       "209                            times. And then context, we also see lots of repetition  there's 106 of those contexts. So, this is a nice little method, we can see a lot about the data in a glance.   \n",
       "210                                                                                                              And when I first saw this in this competition I thought: well this is actually not that   \n",
       "211                                                                                                             much language data, when you think about it. The, you know Each document is very short,   \n",
       "212                                                                                                                    you know, three or four words really, and lots of it is repeated. So that's like   \n",
       "213                            as I'm looking through it I'm thinking, like, what are some key features of this data set?. And that would be something, I'd be thinking, well, that's, you know, we've   \n",
       "214                                                                                                                                                 got to do a lot with not very much unique data here.   \n",
       "215                                                                                                       So here's how we can just go ahead and create a single string like I described which contains,   \n",
       "216                                                                                                              you know, some kind of field separator, plus the context, the target and the anchor. So   \n",
       "217                                                                                                              we're going to pop that into a field called input. Something slightly weird in pandas   \n",
       "218                     is there's two ways of referring to a column. You can use square brackets and a string to get the input column or you can just treat it as an attribute. When you're setting it,   \n",
       "219                                                  you should always use the form seen here (... df [input]= ) When reading it you can use either. I tend to use this one because it's less typing.   \n",
       "220                                                                                                         So you can see now we've got this/these concatenated rows. So, head() is the first few rows.   \n",
       "221                                                                                                                So we've now got some documents to do NLP with. Now, the problem is, as you know from   \n",
       "222                                                                                                              the last lesson, neural networks work with numbers. All right, we're going to take some   \n",
       "223                                                                                                           numbers and we're going to multiply them by matrices, we're going to replace the negatives   \n",
       "224                   with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that   \n",
       "225                                                                                                                               for these strings? So there's basically two steps we're going to take.   \n",
       "226                                                                                                              The first step is to split each of these into tokens. Tokens are basically words. We're   \n",
       "227                                                                                                               going to split it into words. There's a few problems with splitting things into words,   \n",
       "228                                                                                                           though. The first is that some languages like chinese don't have words, right, or at least   \n",
       "229                          certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even   \n",
       "230           the pieces are not next to each other. Another reason is that, what we're going to be doing is, after we've split it into words, or something like words, we're going to be getting a list   \n",
       "231              of all of the unique words that appear, which is called the vocabulary, and every one of those unique words is going to get a number. As you'll see later on the bigger the vocabulary,   \n",
       "232                                                                                                                  the more memory is going to get used, the more data we'll need to train. In general   \n",
       "233                                                                                                                                                            we don't want a vocabulary to be too big.   \n",
       "234                                                                                                         So instead, nowadays, people tend to tokenize into something called subwords which is pieces   \n",
       "235                                                                                                            of words  so I'll show you what it looks like. So the process of turning it into smaller   \n",
       "236                     units like words, it's called tokenization  and we call them tokens instead of words. The token is just like the more general concept of, like, whatever we're putting it into.   \n",
       "237                                                                                                         So we're going to get Hugging Face transformers and Hugging Face datasets doing our work for   \n",
       "238                                                                                                                us, and so, what we're going to do is we're going to turn our pandas DataFrame into a   \n",
       "239                                                                                                                    Hugging Face datasets Dataset. It's a bit confusing: pytorch has a class called   \n",
       "240                         Dataset and Hugging Face has a class called Dataset and they're different things, okay, so this is a Hugging Face Dataset. Hugging Face datasets dataset. So we can turn a   \n",
       "241                                                                                                       DataFrame into a Dataset just using the from_pandas method and so we've now got a Dataset. So,   \n",
       "242                                                                                                            if we take a look it just tells us: all right it's got these features, okay? And remember   \n",
       "243                                                                                                       input is the one we just created with the concatenated strings and here's those 36,000 rows.   \n",
       "244                                                                                                           Okay, so now we're going to do these two things. Tokenization, which is to split each text   \n",
       "245                                                                                                                up into tokens, and the numericalization, which is to turn each token into its unique   \n",
       "246                                                                                                               id based on where it is in the vocabulary. The vocabulary, remember, being the unique,   \n",
       "247                                                                                                              the list of unique tokens. Now, particularly in this stage: tokenization, there's a lot   \n",
       "248                                                                                                               of little decisions that have to be made. The good news is you don't have to make them   \n",
       "249                                                                                                      because whatever pre-trained model you used the people that pre-trained it made some decisions,   \n",
       "250                      and you're going to have to do exactly the same thing, otherwise you'll end up with a different vocabulary to them and that's going to mess everything up. So that means before   \n",
       "251                                                                                                              you start tokenizing you have to decide on what model to use. Hugging Face transformers   \n",
       "252                                                                                                                            is a lot like timm. It has a library of, I believe, hundreds of models.   \n",
       "253                                                                                                    I guess I shouldn't say Hugging Face transformers. It's really the Hugging Face model hub. 44,000   \n",
       "254                                                                                                             models, so even many more even than timm's image models. And so, these models, they vary   \n",
       "255                                                                                                               in a couple of ways. There's a variety of different architectures, just like in timm   \n",
       "256                      but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could   \n",
       "257                                                                                                        type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,   \n",
       "258                                                                                                               there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,   \n",
       "259                                                                                                            thanks to the Hugging Face model hub, you can start your pre-trained model with something   \n",
       "260                                                                               that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents.   \n",
       "261                                                                                                            Having said that, there are some just generally pretty good models that work for a lot of   \n",
       "262                                                                                                               things a lot of the time, and deberta-v3 is certainly one of those. This is a very new   \n",
       "263                                                                                                            area. NLP has been, like, practically, really effective for, you know, general users, for   \n",
       "264                                                                                                       only a year or two, whereas for computer vision it's been quite a while. So you'll see, you'll   \n",
       "265                                                                                                                find that a lot of things aren't quite as well bedded down. I don't have a picture to   \n",
       "266                                                                                                             show you of which models are the best or the fastest and the most accurate and whatever,   \n",
       "267                                                                                                               right? This, a lot of this stuff is, like stuff that we're figuring out as a community   \n",
       "268                                                                                                                using competitions like this, in fact. And this is one of the first NLP competitions,   \n",
       "269                                                                                                        actually, in the kind of modern NLP era. So, you know, we've been studying these competitions   \n",
       "270                                                                                                         closely and yes, I can tell you that deberta-v3 is actually a really good starting point for   \n",
       "271                         a lot of things so that's why we've picked it. So we pick our model and just like in timm for image, you know, models there's often going to be a small, a medium, a large   \n",
       "272                                                         and of course we should start with small, right, because small is going to be faster to train we're going to be able to do more iterations.   \n",
       "273                                                                                                                                                                                  and so forth. Okay.   \n",
       "274                                                                           So at this point remember the only reason we picked our model is because we have to make sure we tokenize in the same way.   \n",
       "275                  To tell transformers that we want to tokenize the same way that the people that built a model did, we use something called AutoTokenizer. It's nothing fancy, it's basically just a   \n",
       "276                                                                                       dictionary which says: oh, which model uses which tokenizer?. So when we say AutoTokenizer.from_pretrained   \n",
       "277                                                                                                            it will download the vocabulary and the details about how this particular model tokenized   \n",
       "278                                                                                                               the dataset. So, at this point we can now take that tokenizer and pass a string to it.   \n",
       "279                                                                                                                     So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's   \n",
       "280                                                                                                               kind of putting it into words, kind of not. So if you've ever wondered whether g'day   \n",
       "281                                                                                                                  is one word or two you know it's actually three tokens according to this tokenizer.   \n",
       "282                                                                                                              And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token.   \n",
       "283                                                                                                       And so, you kind of get the idea. These underscores here? That represents the start of a word,   \n",
       "284                           right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the   \n",
       "285                                                                                                      start of a word, that kind of means a different thing. So this is what happens when we tokenize   \n",
       "286                                                                                                                               this sentence using the tokenizer that the deberta-v3 developers used.   \n",
       "287                                                                                                           So here's a less common (unless you're a big platypus fan like me), less common sentence.:   \n",
       "288                                                                                                         A platypus is an ornithorhynchus anatinus. So okay, in this particular vocabulary platypus   \n",
       "289                                                                                                      got its own word, its own token, but ornithorhynchus didn't. And so I still remember grade one,   \n",
       "290                                  for some reason our teacher got us all to learn how to spell ornithorhynchus, so, one of my favorite words. So you can see here it's been split into _or, ni,   \n",
       "291                                                                                        tho, rhynch, us. So every one of these tokens you see here is going to be in the vocabulary, right? The   \n",
       "292                                                                                                       list of unique tokens that was created when this, when this particular model, this pre-trained   \n",
       "293                                                                                                                     model, was first trained. So somewhere in that list we'll find _A (underscore   \n",
       "294                                                                                                                   capital A), and it'll have a number and so that's how we'll be able to turn these   \n",
       "295                                                                                                          into numbers. So this first process is called tokenization and then the thing where we take   \n",
       "296                                         these tokens and turn them into numbers is called numericalization. So, our data set, remember we put our string into the input field so here's a function   \n",
       "297                                                                                                                that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our   \n",
       "298                                                                                                               tokenization function. Tokenization can take a minute or two so we may as well get all   \n",
       "299                                                                                                                 of our processes used doing it at the same time to save some time. So if you use the   \n",
       "300                              dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes   \n",
       "301                       this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with   \n",
       "302                                                                                                             batched=True it'll be able to do more stuff at once. So look it only took six seconds,   \n",
       "303                                                                                                                so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going   \n",
       "304                                                                                                              to contain exactly the same as our original data set.) No sorry, it's not going to take   \n",
       "305                    exactly the same as the original data set, it's going to contain exactly the same input as our original data set and it's also going to contain a bunch of numbers. These numbers   \n",
       "306                                                                                                                 are the position in the vocabulary of each of the tokens in the string, so we've now   \n",
       "307                                                                                                              successfully turned a string into a list of numbers. That is a great first step. We can   \n",
       "308                                                                                                                    see how this works, we can see for example that we've got of at this a separate   \n",
       "309                                                                                                                   word, so that's going to be an _of in the vocabulary we can grab the vocabulary,   \n",
       "310                                                                                                                  look up _of, find that it's 265 and check here: yep here it is 265. Okay, so it's   \n",
       "311                                                                                                             not rocket science right? It's just looking stuff up in a dictionary to get the numbers.   \n",
       "312                                                                                                   Okay, so that is the tokenization and numericalization necessary in NLP to turn our documents into   \n",
       "313                                                                                                                             numbers to allow us to put it into our model. Any questions so far John?   \n",
       "314                       Yeah, thanks Jeremy so there's a couple and this seems like a good time to throw them out  and it's related to how you've formatted your input data into these sentences that   \n",
       "315                                                                                                               you've just tokenized. So one question was really about: How you choose those keywords   \n",
       "316                                                                                                             and the order of the fields that you know, so I guess just interested in an explanation,   \n",
       "317                                                                is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,   \n",
       "318                                                                                                            you know, doesn't matter! We just want some way, something that it can learn from, right?   \n",
       "319                                                                                                                 So if I just concatenated it without these headers before each one, it wouldn't know   \n",
       "320                            where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're   \n",
       "321                                                                                            so flexible As long as you give it the information somehow, it doesn't really matter how you give it the   \n",
       "322                      information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't know, one semicolon here, and two here, and three here. Yeah it's not   \n",
       "323                                                                                                            a big deal. At the level where you're, like, trying to get an extra half a percent to get   \n",
       "324                                  up the leaderboard of a Kaggle competition you may find tweaking these things makes tiny differences, but in practice you won't generally find it matters too much.   \n",
       "325                                                                                                            Right, thank you. And I guess the second part of that, somebody's asking: If one of their   \n",
       "326                                                                                                                 fields was particularly long, say it was a thousand characters, is there any special   \n",
       "327                                                                                                         handling required there? Do you need to re-inject those kinds of special marker tokens? Does   \n",
       "328                                                                                                                    it change if you've got much bigger fields that you're trying to learn and query?   \n",
       "329                                                                                                              Yes. Long documents and ULMFiT require no special consideration. IMDb in fact has multi   \n",
       "330                                                                                                                 thousand word movie reviews, and it works great. To this day, ULMFiT is probably the   \n",
       "331                                                                                                         best approach for reasonably quickly and easily using large documents. Otherwise, if you use   \n",
       "332                                                                                                            transformer-based approaches, large documents are challenging. Specifically, transformers   \n",
       "333                   basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with   \n",
       "334             large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with   \n",
       "335                                                                                                             documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if   \n",
       "336                                                                                                              it works for you, but you know I'd certainly try both. For under 2,000 words, you know,   \n",
       "337                                                                                           transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory.   \n",
       "338                                                                                                                So, Hugging Face transformers has these, you know As I say it right now, I find them   \n",
       "339                        somewhat obscure and not particularly well documented expectations about your data, that you kind of have to figure out, and one of those is that it expects that your target   \n",
       "340                                                                                                              is a column called labels. So once I figured that out, I just went, got our tokenized   \n",
       "341                          DataSet, and renamed our score column to labels, and everything started working. I don't know if at some point they'll make this a bit more flexible, but its probably   \n",
       "342                                                       best to just call your target labels and life will be easy. You might have seen back when I went ls {path} that there was another data set   \n",
       "343                                                                                                                 there, called test.csv. And if you look at it, it looks a lot like our training set,   \n",
       "344                                                                                                           that's our other CSV that we've been working with, but it's missing the score. The labels.   \n",
       "345                                                                                                                   This is called a test set  and so we're going to talk a little bit about that now   \n",
       "346                                                                                                                 because my claim here is that perhaps the most important idea in machine learning is   \n",
       "347                                                                                                                                 the idea of having separate training, validation and test data sets.   \n",
       "348                                                                                                Test and validation sets are all about identifying and controlling for something called overfitting   \n",
       "349                                                                                                            and we're going to try and learn about this through example. This is the same information   \n",
       "350                                                                                                                               that's in that Kaggle notebook  I've just put it on some slides here.   \n",
       "351                                                                                                            So I'm going to create a function here called plot_poly and I'm actually going to use the   \n",
       "352                                                                                                              same data that, I don't know if you remember, we used it earlier for trying to fit this   \n",
       "353                                                                                                                       quadratic. We created some x and some y data. This is the data we're going   \n",
       "354                                                                                                                                           to use and we're going to use this to look at overfitting.   \n",
       "355                           The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for   \n",
       "356                                                                                                                those of you that remember, a first degree polynomial is just a line, it's y = a x.   \n",
       "357                                                                                                                A second degree polynomial will be y = ax^2 + bx + c, third degree polynomial we'll   \n",
       "358                                                                                                          have a cubic, fourth degree you know quartic, and so forth. And what I've done here is I've   \n",
       "359                                                                                                               plotted what happens if we try to fit a line to our data. It doesn't fit very well. So   \n",
       "360                                                                                                                   what happened here is we we did a linear regression and what we're using here is   \n",
       "361               a very cool library called scikit-learn. scikit-learn is something that, you know, I think it'd be fair to say it's mainly designed for kind of classic machine learning methods like,   \n",
       "362                        kind of linear regression and stuff like that  I mean, very advanced versions of these things, but it's also great for doing these quick and dirty things. So in this case I   \n",
       "363                    wanted to do a what's called a polynomial regression which is fitting the polynomial to data and it's just these two lines of code. It's a super nice library. So in this case,   \n",
       "364                                                                                                               a degree one polynomial is just a line, so I fit it, and then I show it with the data,   \n",
       "365                                                                                                      and there it is. Now that's what we call underfit, which is to say there's not enough, kind of,   \n",
       "366                                                                                                                                      complexity in this model I fit, to match the data that's there.   \n",
       "367            So an underfit model is a problem. It's got to be systematically biased, you know; all the stuff up here, we're going to be predicting too low; all the stuff down here, we're predicting   \n",
       "368                                                                                                        too low; all the stuff in the middle, well be predicting too high. A common misunderstanding   \n",
       "369                                                                                                         is that simpler models are kind of more reliable in some way, but models that are too simple   \n",
       "370                                                                                                                                                    will be systematically incorrect as you see here.   \n",
       "371                                                                                                                                                       What happens if we fit a 10 degree polynomial?   \n",
       "372                                                                                                           That's not great either! In this case it's not really showing us what the actual Remember   \n",
       "373                this was originally a quadratic. This is meant to match, right? And particularly at the ends here, it's predicting things that are way above what we would expect in real life right?   \n",
       "374                         And it's trying to get really it's trying really hard to get through this point, but clearly this point was just some noise, right? So this is what we call overfit. It's   \n",
       "375                                                                                                          done a good job of fitting to our exact data points, but if we sample some more data points   \n",
       "376                    from this distribution, honestly we probably would suspect they're not going to be very close to this, particularly if they're a bit beyond the edges. So that's what overfitting   \n",
       "377                                                        looks like. We don't want underfitting or overfitting. Now underfitting is actually pretty easy to recognize, because we can actually look at   \n",
       "378                                                                                                         our training data and see that it's not very close. Overfitting is a bit harder to recognize   \n",
       "379                                                                                                                                                    because the training data is actually very close.   \n",
       "380                                                                                                             Now on the other hand, here's what happens if we fit a quadratic. And here I've got both   \n",
       "381                                                                                                            the real-line and the fit-line and you can see they're pretty close, and that's of course   \n",
       "382                                                                                                                                                                               what we actually want.   \n",
       "383                                                                                                             So how do we tell whether we have something more like this, or something more like this.   \n",
       "384                                    Well what we do is we do something pretty straightforward is we take our original data set, these points, and we remove a few of them, so let's say 20% of them.   \n",
       "385                                                                                                            We then fit our model using only those points we haven't removed, and then we measure how   \n",
       "386                                                                                                                good it is by looking at only the points we removed. So in this case let's say we had   \n",
       "387                                                                                                       removed (I'm just trying to think) If I had removed this point here right, then it might have   \n",
       "388                                                                               kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away.   \n",
       "389                                                                                                              The model the data that we take away and don't let the model see it when it's training   \n",
       "390                                                                                                                    is called the validation set. So in fast.ai we've seen splitters before, right   \n",
       "391                           The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so   \n",
       "392                                                                                                              things like accuracy, measured only on the validation set. This is really unusual. Most   \n",
       "393                                                                                                         libraries make it really easy to shoot yourself in the foot, by not having a validation set,   \n",
       "394                         or accidentally not using it correctly. So fast.ai won't even let you do that. So you've got to be particularly careful when using other libraries. HuggingFace transformers   \n",
       "395                                                                                                             is good about this, so they make sure that they do show you your metrics on a validation   \n",
       "396                                                                                                          set. Now creating a good validation set is not generally as simple as just randomly pulling   \n",
       "397                                                        some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was   \n",
       "398                                                                                                                 the data you were trying to fit something to (okay) and you randomly remove some, so   \n",
       "399                                                                                                          it looks like this. That looks very easy doesn't it, because you've kind of like, still got   \n",
       "400                           all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So   \n",
       "401             if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate   \n",
       "402                                                                                                             and remove the last couple of weeks. So if this was your validation set and this is your   \n",
       "403                                                                                                                training set, that's going to be actually testing whether you can use this to predict   \n",
       "404                                                                                                                                                the future, rather than using it to predict the past.   \n",
       "405                                                                                                             Kaggle competitions are a fantastic way to test your ability to create a good validation   \n",
       "406                                                                                                              set, because Kaggle competitions only allow you to submit, generally, a couple of times   \n",
       "407                                                                                                            a day. The dataset that you are scored on in the leaderboard during that time is actually   \n",
       "408                        only a small subset in fact it's a totally separate subset to the one you'll be scored on, on the end of the competition. And so most beginners on Kaggle overfit. And it's   \n",
       "409                                                                                                                not until you've done it that you'll get that visceral feeling of like: oh my god, I   \n",
       "410                                                                                                          overfit. In the real world outside of Kaggle you will often not even know that you overfit   \n",
       "411                                                                                                             you just destroy value for your organization silently. So it's a really good idea to do   \n",
       "412                   this kind of stuff on Kaggle a few times first, in real competitions, to really make sure that you are confident you know how to avoid overfitting  how to find a good validation   \n",
       "413                                                                                              set and how to interpret it correctly. And you really don't get that until you screw it up a few times.   \n",
       "414                                                                                                              A good example of this was there was a distracted driver competition on Kaggle  there   \n",
       "415                                                                                                       are these kind of pictures from inside a car, and the idea was that you had to try and predict   \n",
       "416                                                                                                            whether somebody was driving in a distracted way or not, and on Kaggle they did something   \n",
       "417           pretty smart the test set, so the thing that they scored you on the leaderboard, contained people that didn't exist, at all, in the competition data that you train the model with. So if   \n",
       "418                        you wanted to create an effective validation set in this competition, you would have to make sure that you separated the photos, so that your validation set contained photos   \n",
       "419                                                                                                                                     of people that aren't in the data you're training your model on.   \n",
       "420                                                                                                                There's another one like that, the Kaggle fisheries competition, which had boats that   \n",
       "421            didn't appear so they were basically pictures of boats and you meant to try to guess/predict what fish were in the pictures. And it turned out that a lot of people accidentally figured   \n",
       "422                                                                                                         out what the fish were by looking at the boat, because certain boats tended to catch certain   \n",
       "423                                                                                                             kinds of fish. And so by messing up their validation set, they were really overconfident   \n",
       "424                                                                            of the accuracy of their model. I'll mention in passing, if you've been around Kaggle a bit, you'll see people talk about   \n",
       "425                                                                                                            cross-validation a lot. I'm just going to mention, be very very careful. Cross-validation   \n",
       "426                                                                                                      is explicitly not about building a good validation set, so you've got to be super super careful   \n",
       "427                                                                                                                                                                                 if you ever do that.   \n",
       "428                                                                                              Another thing I'll mention, is that scikit-learn conveniently offers something called train_test_split,   \n",
       "429                                                                                                           as does Hugging Face datasets, as does fast.ai  we have something called random splitter.   \n",
       "430                                                                                                              It can be encouraging it can almost feel like it's encouraging you to use a randomized   \n",
       "431                                                                                                            validation set because there are these methods that do it for you. But yeah, be very very   \n",
       "432                                                                                                               careful, because very very often that's not what you want, okay. So we've learned what   \n",
       "433                 a validation set is, so that's the bit that you pull out of your data that you don't train with, but you do measure your accuracy with. So what's a test set? It's basically another   \n",
       "434                                                                                                                 validation set, but you don't even use it for tracking your accuracy while you build   \n",
       "435                                                                                                                your model. Why not? Well imagine you tried two new models every day for three months   \n",
       "436                                                                                                                 (that's how long a Kaggle competition goes for.) So you would have tried 180 models,   \n",
       "437                                                                                                       and then you look at the accuracy on the validation set for each one. Some of those models you   \n",
       "438                                                                                                       would have got a good accuracy on the validation set, potentially because of pure chance, just   \n",
       "439                     a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually   \n",
       "440                                                                                                         overfit using the validation set. So you actually want to know whether you've really found a   \n",
       "441                                                                                                            good model or not. So in fact on Kaggle they have two test sets. They've got the one that   \n",
       "442                                                                                                             gives you feedback on the leaderboard during the competition and a second test set which   \n",
       "443                                                                                                          you don't get to see until after the competition is finished. So in real life you've got to   \n",
       "444                                                                                                             be very careful about this, not to try so many models during your model building process   \n",
       "445                                                                                                               that you accidentally find one that's good by coincidence. And only if you have a test   \n",
       "446           set that you've held out, or you know that. Now that leads to the obvious question which is very challenging, is you spent three months working on a model, worked well on your validation   \n",
       "447                     set, you did a good job of locking that test set away in a safe so you weren't allowed to use it, and at the end of the three months you finally checked it on the test set, and   \n",
       "448                                                                                                            it's terrible. What do you do? Honestly you have to go back to square one. You know there   \n",
       "449                                                                                                              really isn't any choice other than starting again. So this is tough, but it's better to   \n",
       "450                                                                                                                      know, right. Better to know than to not know, so that's what a test set is for.   \n",
       "451                   So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something   \n",
       "452                                                                                                                 like accuracy. It's a number that tells you: How good is your model? Now on Kaggle   \n",
       "453                                                                                                               this is very easy. What metric should we use? Well they tell us go to overview, click   \n",
       "454                                                                                                            on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation   \n",
       "455                                                  Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that   \n",
       "456                                                                                                        we will take the derivative of, and find the gradient, and use that to improve our parameters   \n",
       "457                                                                                                                 during training? And the answer is: maybe, sometimes, but probably not. For example,   \n",
       "458                                                                                                  consider accuracy. Now, if we were using accuracy to calculate our derivative and get the gradient,   \n",
       "459                 you could have a model that's actually slightly better, you know, it's slightly like it's doing a better job of recognizing dogs and cats, but not so much better that it's actually   \n",
       "460                                                                                                        caused any incorrectly classified cat to become a dog. So the accuracy doesn't change at all.   \n",
       "461                                                                                                              So the gradient is zero. You don't want stuff like that. You don't want bumpy functions   \n",
       "462                     because they don't have nice gradients  often they don't have gradients at all, they're basically zero nearly everywhere. You want a function that's nice and smooth. Something   \n",
       "463                                                                                                        like, for instance, the average absolute error, mean absolute error, which we've used before.   \n",
       "464                    So that's the difference between your metrics and your loss. Now be careful, right, because when you're training your model's spending all of its time trying to improve the loss   \n",
       "465                                         and most of the time that's not the same as a thing you actually care about, which is your metric. So you've got to keep those two different things in mind.   \n",
       "466                                                                                                                  The other thing to keep in mind is that in real life you can't go to a website and   \n",
       "467                                                                                                                 be told what metric to use. In real life the model that you choose, there isn't one   \n",
       "468                        number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex   \n",
       "469                                                                                                        process often involving humans, both as users and customers and as people, you know, involved   \n",
       "470                                                                                                                  in as part of the process. There's all kinds of things that are changing over time   \n",
       "471                                                                                                        and there's lots and lots of outcomes of decisions that are made. One metric is not enough to   \n",
       "472                                                                                                                capture all of that. Unfortunately, because it's so convenient to pick one metric and   \n",
       "473                                                                                                            use that to say: I've got a good model, that very often finds its way into industry, into   \n",
       "474                                                                                                                  government where people roll out these things that are good on the one metric that   \n",
       "475                                                                                                            happened to be easy to measure. And again and again we found people's lives turned upside   \n",
       "476                                                                                                          down because of how badly they get screwed up by models that have been incorrectly measured   \n",
       "477                                                                                                        using a single metric. So my partner Rachel Thomas has written this article which I recommend   \n",
       "478                                                                                                                                    you read about The problem with metrics is a big problem for AI   \n",
       "479                              It's not just an AI thing! There's actually this thing called Goodharts Law that states when a measure becomes a target, it ceases to be a good measure. The thing   \n",
       "480                                                                                                               is so when I was a management consultant, you know, 20 years ago, we were always kind   \n",
       "481                                                                                                                of part of these strategic things trying to like: find key performance indicators and   \n",
       "482                         ways to kind of, you know, set commission rates for sales people and we were really doing a lot of this, like, stuff which is basically about picking metrics and, you know,   \n",
       "483                                                                                                              we see that happen go wrong in industry all the time. AI is dramatically worse because   \n",
       "484                                                                                                            AI is so good at optimizing metrics, and so that's why you have to be extra, extra, extra   \n",
       "485                                      careful about metrics, when you are trying to use a model in real life. Anyway, as I said in Kaggle, we don't have to worry about any of that, we're just going   \n",
       "486                                                      to use the Pearson correlation coefficient which is all very well as long as you know what the hell the Pearson correlation coefficient is   \n",
       "487                                                                                                      If you don't, let's learn about it. So Pearson correlation coefficient is usually abbreviated   \n",
       "488                                                                                                                  using letter r and it's the most widely used measure of how similar two variables   \n",
       "489                                                                                                               are. And so, if your predictions are very similar to the real values then the Pearson   \n",
       "490                                                                                                                  correlation coefficient will be high, and that's what you want. r can be between   \n",
       "491                                                                                                         minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle   \n",
       "492                                        competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct.   \n",
       "493              Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function.   \n",
       "494                           I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical function, but how   \n",
       "495                                                                                                          it behaves; and I find most people, even who work in data science, have not actually looked   \n",
       "496                                                                                                                    at a bunch of data sets to understand how r behaves. So let's do that right now   \n",
       "497                                                                     so that you're not one of those people. The best way I find to understand how data behaves in real life, is to look at real-life   \n",
       "498                          data so there's a data set scikit-learn comes with a number of data sets, and one of them is called California housing and it's a data set where each row is a district   \n",
       "499                                                                                                                  and, it's kind of demographic, sorry it's information some demographic information   \n",
       "500                                                                                                                           about different districts, and about the value of houses in that district.   \n",
       "501                                                                                                       Im not going to try to plot the whole thing, it's too big, and this is a very common question   \n",
       "502                                                                                                                I have from people is: how do I plot data sets with far too many points? The answer   \n",
       "503                                                                                                                 is very simple: get less points. So I just randomly grab a thousand points. Whatever   \n",
       "504                         you see with a thousand points, is going to be the same as what you see with a million points. There's no point no reason, to plot huge amounts of data generally just grab   \n",
       "505                                                                                            a random sample. Now, numpy has something called corecoeff() to get the correlation coefficient between   \n",
       "506                                                                                                           every variable and every other variable, and it returns a matrix. So I can look down here,   \n",
       "507                    and so for example, here is the correlation coefficient between variable one, and variable one. Which of course is exactly perfectly 1.0. Right? because variable one is the same   \n",
       "508                                                                                                             as variable one. Here is the small inverse correlation between variable one and variable   \n",
       "509                                                                                                                two, and medium-sized positive correlation between variable one and variable 3 and so   \n",
       "510                                                                                                               forth. This is symmetric about the diagonal because the correlation between variable 1   \n",
       "511                                                                                                             and variable 8 is the same as the correlation between variable 8 and variable 1. So this   \n",
       "512                                                                                                                                                                 is a correlation coefficient matrix.   \n",
       "513              So that's great when we wanted to get a bunch of values all at once. For the Kaggle competition we don't want that. We just want a single correlation number. If we just pass in a pair   \n",
       "514                                                                                                                  of variables, we still get a matrix which is kind of weird.. it's kind of it's not   \n",
       "515                    weird, but it's not what we want! So we should grab one of these. So when I want to grab a correlation coefficient, I'll just return the zeroth row, first column. So that's what   \n",
       "516                                                                  core is. That's going to be our single correlation coefficient. So let's look at the correlation between two things; for example   \n",
       "517                                                                                                             median income, and median house value: 0.67. Okay? Is that high? medium? low? How big is   \n",
       "518                                                                                               that? What does it look like? So the main thing we need to understand is: what these things look like.   \n",
       "519                     So what I suggest we do is: we're going to take a 10 minute break nine minute break. We'll come back at half past, and then we're going to look at some examples of correlation   \n",
       "520                                                                                                    coefficients Okay. Welcome back! So what I've done here is I've created a little function called   \n",
       "521                                                                                                         show correlations, and I'm passing a DataFrame and a couple of columns as strings. I'm going   \n",
       "522                                                                                                           to grab each of those columns as series, do a scatter plot, and then show the correlation.   \n",
       "523                                                                                                                   So, we already mentioned median income and median house value of 0.68, so here   \n",
       "524                       it is here's what .68 looks like. So you know I don't know if you had some intuition about what you expected, but as you can see it's still plenty of variation, even at that   \n",
       "525                                                                              reasonably high correlation. Also, you can see here that visualizing your data is very important if you're working with   \n",
       "526                                                                                                             this data set, because you can immediately see all these dots along here. That's clearly   \n",
       "527                    truncation right? So this is like, when... it's not until you look at pictures like this, that you're gonna pick stuff like this up. Pictures are great! Oh! little trick: on the   \n",
       "528                                                                                                          scatter plot, I put alpha as 0.5, that creates some transparency. For these kind of scatter   \n",
       "529                                                                                                             plots, that really helps, because it like kind of creates darker areas in places where   \n",
       "530                                           there's lots of dots. So, yeah, alpha in scatter plots is nice. Okay, here's another pair. So this one's gone down from 0.68 to 0.43. Median income versus   \n",
       "531                                                                                                             the number of rooms per house. As you'd expect more rooms it's more income, but this is   \n",
       "532                                                                                                           a very weird looking thing. Now, you'll find that a lot of these statistical measures like   \n",
       "533                                                                                                          correlation rely on the square of the difference, and when you have big outliers like this,   \n",
       "534                 the square of the difference goes crazy, and so this is another place we'd want to look at the data first, and say oh that's that's going to be a bit of an issue. There's probably   \n",
       "535                                                                                                         more correlation here, but there's a few examples of some houses with lots and lots of rooms   \n",
       "536                                                                                     where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?   \n",
       "537                                                                                                                     So, r is very sensitive to outliers. So let's get rid of the houses the rooms   \n",
       "538                                                                                                                    with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up   \n",
       "539                                                                                                               from 0.43 to 0.68, even though we probably only got rid of one two three four five six   \n",
       "540                       seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,   \n",
       "541                  and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row.   \n",
       "542                                                             So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Because   \n",
       "543                                                                                                        0.34 sounds like quite a good relationship, but you almost can't see it! So this is something   \n",
       "544                                                                                                              I strongly suggest is, if you're working with a new metric, is: Draw some pictures of a   \n",
       "545                             few different levels of that metric to kind of try to get a feel for like what does it mean? You know, what does 0.6 look like? What does 0.3 look like? And so forth.   \n",
       "546                                                                                                               And here's an example of a correlation of minus 0.2. This very slight, negative slope.   \n",
       "547                       Okay, so there's just more of a kind of a general tip, of something I like to do when playing with a new metric, and I recommend you do as well. I think we've now got a sense   \n",
       "548                                                                              of what the correlation feels like. Now you can go look up the equation on Wikipedia if you're into that kind of thing.   \n",
       "549                                                                                                          We need to report the correlation after each epoch because we want to know how our training   \n",
       "550                                                                                                              is going. Hugging Face expects you to return a dictionary because it's going to use the   \n",
       "551                                                                                                                    keys of the dictionary to like label each metric. So here's something that gets   \n",
       "552                                                                                                                            the correlation, and returns it as a dictionary with the label pearson.   \n",
       "553                                                                                                                              Okay, so we've done metrics, we've done our training/ validation split.   \n",
       "554                                                                                                              Oh! we might have actually skipped over the bit where we actually did the split! Did I?   \n",
       "555                                                                                                 I did! So, to actually do the split, because in this Kaggle competition  I've got another notebook,   \n",
       "556                             we'll look at later, where we actually split this properly  but here we're just going to do a random split. Just to keep things simple for now, of 25 percent, will be   \n",
       "557                                                                                                              of the data will be a validation set. So, if we go tok_ds.train_test_split() it returns   \n",
       "558                                                                                                                  a data set dict; which has a train, and a test. So that looks a lot like a datasets   \n",
       "559                                                                                                                                                                object in fast.ai. Very similar idea!   \n",
       "560                      So this will be the thing that we'll be able to train with, so it's going to train with this data set, and return the metrics on this data set. This is really a validation set   \n",
       "561                                                                                                                                                           but Hugging Face datasets calls it test.   \n",
       "562                                                                                                           Okay. We're now ready to train our model. In fast.ai, we use something called a learner.   \n",
       "563                          The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch   \n",
       "564                                                                                                                  sizes. In short, each time we pass some data to our model for training, it's going   \n",
       "565                                                                                                                    to return it's going to send through a few rows at a time to the GPU, so that it   \n",
       "566                                                                                                                       can calculate those in parallel. Those a bunch of rows is called a batch or   \n",
       "567                            a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the   \n",
       "568                                                                                                          more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll   \n",
       "569                                                                                                                get an out of memory error on your GPU. So, you know, it's a bit of trial and error   \n",
       "570                                                                                                            to find a batch size that works. Epochs we've seen before. Then we've got the learning   \n",
       "571                                                                                                              rate. We'll talk in the next lesson  unless we get to this lesson  about a technique   \n",
       "572                      to automatically find a or semi-automatically find a good learning rate. We already know what a learning rate is from the last lesson. I've played around and found one that   \n",
       "573                                                                                                              seems to train quite quickly without falling apart, so I just tried a few. Generally, I   \n",
       "574                                                                                                                     kind of, you know, if I if I don't have a so Hugging Face transformers doesn't   \n",
       "575                        have something to help you find the learning rate. This the integration we're doing in fast.ai, will let you do that, but if you're using a framework that doesn't have that,   \n",
       "576                                                                           you can just start with a really low learning rate, and then kind of double it, and keep doubling it until it falls apart.   \n",
       "577                                      Hugging Face transformers uses this thing called training arguments which is a class where you just provide all of the kind of configuration so you have to   \n",
       "578                                                                                                            tell it what your learning rate is. This stuff here is the same as what we call basically   \n",
       "579                                                                                           fit_one_cycle() in fast.ai. You always want this to be true, because it's going to be faster pretty much   \n",
       "580           and then the this stuff here, you can probably use exactly the same every time. There's probably a lot of boilerplate compared to fast.ai as you see. This stuff you can probably use the   \n",
       "581                                                                             same every time. Okay, so We now need to create our model. So, the equivalent of the vision learner function that we've   \n",
       "582                                                                                                                used to automatically create a reasonable vision model? In Hugging Face transformers,   \n",
       "583  they've got lots of different ones depending on what you're trying to do. So, we're trying to do classification as we've discussed, of sequences, so if we call AutoModelForSequenceClassification,   \n",
       "584                          it will create a model that is appropriate for classifying sequences from a train pre-trained model, and this is the name of the model that we did earlier the deberta-v3.   \n",
       "585                 It has to know when it adds that random matrix to the end, how many outputs it needs to have. So we have one label which is the score. So that's going to create our model, and then   \n",
       "586                                                                                                        this is the equivalent of creating a learner. It contains a model, and the data the training   \n",
       "587                  data, and the test data. Again, there's a lot more boilerplate here than fast.ai, but you can kind of see the same basic steps here. We just have to do a little bit more manually,   \n",
       "588                             but it's not you know, it's nothing too crazy. So, it's going to tokenize it for us using that function, and then these are the metrics that will print out each time.   \n",
       "589                                                                                                                                   That's that little function we created which returns a dictionary.   \n",
       "590                    At the moment I find Hugging Face transformers very verbose. It spits out lots and lots and lots of text which you can ignore, and we can finally call train, which will spit out   \n",
       "591                                                                                                          much more text again, which you can ignore, and as you can see, as it trains, it's printing   \n",
       "592                                                                                                        out the loss, and here's our Pearson correlation coefficient. So, it's training and we've got   \n",
       "593                                                                                                           a 0.834 correlation, that's pretty cool! Right. I mean it took what ? Oh here we are five   \n",
       "594                minutes to run. Maybe that's five minutes per epoch on Kaggle which doesn't have particularly great GPUs (but good for free) and we've got something that is you know got a very high   \n",
       "595                                                                                                            level of correlation in assessing how similar the two columns are, and the only reason it   \n",
       "596                                                                                                           could do that is because it used a pre-trained model, right. There's no way you could just   \n",
       "597                                                                                                      have that tiny amount of information and figure out whether those two columns are very similar.   \n",
       "598                         This pre-trained model already knows a lot about language. It already has a good sense of whether two phrases are similar or not, and we've just fine-tuned it. You can see,   \n",
       "599                           given that after one epoch it was already at 0.8. You know we this was a model that already did something pretty close to what we needed. It didn't really need that much   \n",
       "600                                                                           extra tuning for this particular task. We got any questions there John? Yeah we do! It's actually a bit back on the topic   \n",
       "601                     before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how   \n",
       "602                                                                                                            do you decide when it's okay to remove outliers? Like, you pointed out some in that data   \n",
       "603                                                                                                         set, and clearly your model is going to train a lot better if you clean that up; but I think   \n",
       "604                                                                                                             Kevin's point here is, you know, those kinds of outliers will probably exist in the test   \n",
       "605                                                                              set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense.   \n",
       "606                                                                                                             So, outliers should never just be removed, like, for modeling So if we take the example   \n",
       "607                    of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interesting! it seems like there's a separate   \n",
       "608                    group of districts with a different kind of behavior. Yeah my guess is that they're going to be kind of like dorms or something like that, you know, probably low-income housing   \n",
       "609                                                                                                        and so I would be saying like, oh clearly, from looking at this dataset, these two different   \n",
       "610                                                                groups can't be treated the same way, they have very different behaviors, and I would probably split them into two separate analyses.   \n",
       "611                                                                                                           You know the... the word outlier... it kind of exists in a statistical sense, right? There   \n",
       "612               can be things that are well outside our normal distribution and mess up our kind of metrics and things. It doesn't exist in a real sense. It doesn't exist in a sense of like... oh...   \n",
       "613                                                           things that we should, like, ignore or throw away. You know, some of the most useful kind of insights I've had in my life in data projects   \n",
       "614                                                                                                              has been by digging into outliers...so-called outliers... and understanding: well, what   \n",
       "615                                                                                                               are they? And where did they come from? and it's kind of... often in those edge cases   \n",
       "616                                                                                                                 that you discover really important things about, like, where processes go wrong  or   \n",
       "617           about, you know, kinds of behaviors you didn't even know existed, or indeed about, you know, kind of labeling problems or process problems which you really want to fix them at the source   \n",
       "618         because otherwise when you go into production you're going to have more of those so-called outliers. So yeah. I'd say never delete outliers without investigating them and having a strategy   \n",
       "619               for ...like... understanding where they came from and ...like... what should you do about them. All right. So now that we've got a trained model, you'll see that it actually behaves,   \n",
       "620                  you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah   \n",
       "621                                                                                                             this looks like stuff I've seen before, you know, like a bit more wordy and some slight   \n",
       "622                     changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're   \n",
       "623                                                                                                                 going to pass in our dataset from the Kaggle test file  and so that's going to give   \n",
       "624                                                                                                                                                      us our predictions, which we can cast to float.   \n",
       "625                                                                                                            And here they are. So here are the predictions we made of similarity. Now again, not just   \n",
       "626                                                                                                    for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,   \n",
       "627                                                                                                         I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly   \n",
       "628                                                                                                         all of them had the problem we have right now, which is negative predictions and predictions   \n",
       "629                                                                                                               over one. So I'll be showing you how to fix this in a more proper way, maybe hopefully   \n",
       "630                                                                                                             in the next lesson but for now you know we could at least just round these off ...right?   \n",
       "631                           because we know that none of the scores are going to be bigger than one or smaller than zero, so our correlation coefficient will definitely improve if we at least round   \n",
       "632                                                                                                              this up to zero and round this down to one. As I said, there are better ways to do this   \n",
       "633                                                                                                               but that's certainly better than nothing. So, in Pytorch, you might remember from when   \n",
       "634                      we looked at ReLU, there's a thing called clip and that will clip everything under zero to zero and everything over one to one and so now that looks much better. So here's our   \n",
       "635                                                                                                                 predictions. So Kaggle expects submissions to generally be in a CSV file and Hugging   \n",
       "636                                                                                                            Face datasets... it kind of looks a lot like pandas, really. We can create our submission   \n",
       "637                                                                                                              file from... with our two columns called dot csv and there we go. That's basically it.   \n",
       "638                                                                                                         So yeah you know... it's... it's... it's kind of nice to see how... you know... it's a sense   \n",
       "639                                                                                                           how far deep learning has come since we started this course a few years ago. That nowadays   \n",
       "640                 you know... there are multiple libraries around to kind of do this the same thing. We can, you know, use them in multiple application areas. They all look kind of pretty familiar.   \n",
       "641                                                                                                             They're reasonably beginner friendly. And NLP, because it's kind of like the most recent   \n",
       "642                                                                                                           area that's really become effective in the last year or two, is probably where the biggest   \n",
       "643                                                                                                                opportunities are for, you know, big wins both in research and commercialization. And   \n",
       "644                      so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would   \n",
       "645                      you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you   \n",
       "646                                                                                                                  know, or it took 10 times more time or it took 10 times more money or whatever.   \n",
       "647                                                                                                                                                           So I think NLP is a huge opportunity area.   \n",
       "648                                                                                                     It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.   \n",
       "649                                                                                              Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it.   \n",
       "650                                                               So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?   \n",
       "651             And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much   \n",
       "652                                                                                                        much better now  so even then you could see these models were generating context appropriate   \n",
       "653                                                                                                                                                                                    believable prose.   \n",
       "654                                                                                                              You know I would strongly believe that like any of our... kind of like... upper tier of   \n",
       "655                                                                                                      competent fast.ai alumni would be fairly easily able to create a bot which could create context   \n",
       "656                                                                                                            appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side   \n",
       "657                                                                                                               of an argument and you could scale that up such that 99% of twitter was these bots and   \n",
       "658                                                                                                               nobody would know. You know, nobody would know. And that's very worrying to me because   \n",
       "659                                                                                                                 a lot of, you know, a lot of...kind of... the way people see the world is now really   \n",
       "660                                                                                                                 coming out of their... their social media conversations, which at this point they're   \n",
       "661                                                                                                         controllable. Like... it would not be that hard to create something that's kind of optimized   \n",
       "662                                                                                                        towards moving a point of view amongst a billion people, you know, in a very subtle way, very   \n",
       "663                                                 gradually, over a long period of time by multiple bots each pretending to argue with each other and one of them getting the upper hand and so forth.   \n",
       "664                                                                                                                             Here is the start of an article in the Guardian which I'll let you read.   \n",
       "665              This article was, you know, quite long. These are just the first few paragraphs and at the end, it explains that this article was written by GPT3. It was given the instruction please   \n",
       "666                          write a short op-ed around 500 words. Keep the language simple and concise. Focus on why humans have nothing to fear from AI. So GPT3 produced eight outputs and then they   \n",
       "667              say, basically the... the editors at The Guardian did about the same level of editing that they would do for humans. In fact, they found it a bit less editing required than humans. So   \n",
       "668                                                                                                             you know again, like, you can create longer pieces of context appropriate prose designed   \n",
       "669                                                                                                                 to argue a particular point of view. What kind of things might this be used for? You   \n",
       "670                                                                                                              know, that we won't know probably for decades if ever but sometimes we get a clue based   \n",
       "671                                                                                                         on older technology. Here's something from back 2017 and the pre ...kind of... deep learning   \n",
       "672                                                                                                           NLP days. There were millions of submissions to the FTC about the net neutrality situation   \n",
       "673                                                                                                                 in America. Very very heavily biased towards the point of view of saying we want to   \n",
       "674                                                                                                                   get rid of net neutrality. An analysis by Jeff Kao showed that something like 99%   \n",
       "675                                                                                                          of them and in particular, nearly all of the ones which were pro removal of net neutrality,   \n",
       "676                                                                                                        were clearly auto generated by basically ...if you look at the green, there's like, selecting   \n",
       "677                       from a menu. So we've got... Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire individuals as opposed to Washington bureaucrats should   \n",
       "678                   be blah...blah... people like me as opposed to so-called experts... and you get the idea. Now this is an example of a very very, you know, simple approach to auto-generating huge   \n",
       "679                                                                                                            amounts of text. We don't know for sure but it looks like this might have been successful   \n",
       "680                                                                                                     because this went through. You know, despite what seems to be actually overwhelming disagreement   \n",
       "681                                                                                                        from the public that everybody, almost everybody, likes net neutrality, the FTC got rid of it   \n",
       "682                         and this was a big part of the basis. Its like oh we got all these comments from the public and everybody said they don't want net neutrality. So imagine a similar thing   \n",
       "683                      where you absolutely couldn't do this. You couldn't figure it out because everyone was really very compelling and very different. That's, you know, it's kind of worrying about   \n",
       "684                                                                                                            how we deal with that. You know, I will say... when I talk about this stuff, often people   \n",
       "685                                                                                                            say ah no worries we'll build a model to recognize... you know... bot generated content   \n",
       "686                         but, you know, if I put my black hat on, I'm like no that's not gonna work, right?. If you told me to build something that beats the bot classifiers, I'd say no worries,   \n",
       "687                    easy. You know, I will take the... the code or the service... or service... whatever that does the bot classifying and I will include beating that in my loss function and I will   \n",
       "688                                                                                                                fine-tune my model until it beats the bot classifier. You know, when I used to run an   \n",
       "689                                                                                                           email company, we had a similar problem with spam prevention and our spammers could always   \n",
       "690                                                                                                                take a spam prevention algorithm and change their emails until it didn't get the spam   \n",
       "691                                                                                                                 prevention algorithm anymore, for example. So yes, I... I'm really excited about the   \n",
       "692                                                                                                           opportunities for... for students in this course to build, you know, I think very valuable   \n",
       "693                                                                                                             businesses, really cool research and so forth using these pretty new NLP techniques that   \n",
       "694                are now pretty accessible and I'm also really worried about the things that might go wrong. I do think though that the more people that understand these capabilities the less chance   \n",
       "695                                                                they'll go wrong. John, was there some questions? Yeah I mean it's a throwback to the... to the workbook that you had before... yeah   \n",
       "696                                                                                                               that's the one. The question Manikandan is asking... shouldn't num labels be five zero   \n",
       "697                                                                                                       zero point two five zero point five zero point seven five one instead of one? Isn't the target   \n",
       "698                                              a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if   \n",
       "699                                                                                               this was being treated as a categorical problem with five categories, it's still considered one label.   \n",
       "700                                                                                                             In this case though, we're actually treating it as a regression problem. It's one of the   \n",
       "701                      things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if   \n",
       "702                                                                                             you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which   \n",
       "703                                                                                                           is actually why we ended up with predictions that were less than zero and bigger than one.   \n",
       "704                                                                                                            So we'll be learning next time about the use of sigmoid functions to resolve this problem   \n",
       "705                                                                                and that should fix it up for us Okay, great. Well thanks everybody. I hope you enjoyed learning about NLP as much as   \n",
       "706                                                                                                          I enjoyed putting this together. I'm really excited about it and can't wait for next week's   \n",
       "707                                                                                                                                                                                      lesson. See ya!   \n",
       "708              okay hi everybody and um welcome to practical deep learning for coders lesson five um we're uh at a stage now where we're going to be getting deeper and deeper into the details of how   \n",
       "\n",
       "     is_topic_end  \\\n",
       "0           False   \n",
       "1           False   \n",
       "2           False   \n",
       "3           False   \n",
       "4           False   \n",
       "5           False   \n",
       "6           False   \n",
       "7           False   \n",
       "8           False   \n",
       "9           False   \n",
       "10          False   \n",
       "11          False   \n",
       "12          False   \n",
       "13          False   \n",
       "14          False   \n",
       "15          False   \n",
       "16          False   \n",
       "17          False   \n",
       "18          False   \n",
       "19          False   \n",
       "20          False   \n",
       "21          False   \n",
       "22          False   \n",
       "23          False   \n",
       "24          False   \n",
       "25          False   \n",
       "26           True   \n",
       "27          False   \n",
       "28          False   \n",
       "29          False   \n",
       "30          False   \n",
       "31          False   \n",
       "32          False   \n",
       "33          False   \n",
       "34          False   \n",
       "35          False   \n",
       "36          False   \n",
       "37          False   \n",
       "38          False   \n",
       "39          False   \n",
       "40          False   \n",
       "41           True   \n",
       "42          False   \n",
       "43          False   \n",
       "44          False   \n",
       "45          False   \n",
       "46          False   \n",
       "47          False   \n",
       "48          False   \n",
       "49          False   \n",
       "50          False   \n",
       "51          False   \n",
       "52          False   \n",
       "53          False   \n",
       "54          False   \n",
       "55          False   \n",
       "56          False   \n",
       "57          False   \n",
       "58          False   \n",
       "59          False   \n",
       "60          False   \n",
       "61          False   \n",
       "62          False   \n",
       "63          False   \n",
       "64          False   \n",
       "65          False   \n",
       "66          False   \n",
       "67          False   \n",
       "68          False   \n",
       "69          False   \n",
       "70           True   \n",
       "71          False   \n",
       "72          False   \n",
       "73          False   \n",
       "74          False   \n",
       "75          False   \n",
       "76          False   \n",
       "77          False   \n",
       "78          False   \n",
       "79          False   \n",
       "80          False   \n",
       "81          False   \n",
       "82           True   \n",
       "83          False   \n",
       "84          False   \n",
       "85          False   \n",
       "86          False   \n",
       "87          False   \n",
       "88          False   \n",
       "89          False   \n",
       "90          False   \n",
       "91          False   \n",
       "92          False   \n",
       "93          False   \n",
       "94          False   \n",
       "95          False   \n",
       "96          False   \n",
       "97          False   \n",
       "98          False   \n",
       "99          False   \n",
       "100         False   \n",
       "101         False   \n",
       "102         False   \n",
       "103         False   \n",
       "104         False   \n",
       "105         False   \n",
       "106         False   \n",
       "107         False   \n",
       "108         False   \n",
       "109         False   \n",
       "110         False   \n",
       "111         False   \n",
       "112         False   \n",
       "113          True   \n",
       "114         False   \n",
       "115         False   \n",
       "116         False   \n",
       "117         False   \n",
       "118         False   \n",
       "119         False   \n",
       "120         False   \n",
       "121         False   \n",
       "122         False   \n",
       "123         False   \n",
       "124          True   \n",
       "125         False   \n",
       "126         False   \n",
       "127         False   \n",
       "128         False   \n",
       "129         False   \n",
       "130         False   \n",
       "131         False   \n",
       "132         False   \n",
       "133         False   \n",
       "134         False   \n",
       "135         False   \n",
       "136         False   \n",
       "137         False   \n",
       "138         False   \n",
       "139         False   \n",
       "140         False   \n",
       "141         False   \n",
       "142         False   \n",
       "143         False   \n",
       "144         False   \n",
       "145         False   \n",
       "146         False   \n",
       "147         False   \n",
       "148         False   \n",
       "149         False   \n",
       "150         False   \n",
       "151         False   \n",
       "152         False   \n",
       "153         False   \n",
       "154         False   \n",
       "155         False   \n",
       "156         False   \n",
       "157         False   \n",
       "158         False   \n",
       "159         False   \n",
       "160         False   \n",
       "161          True   \n",
       "162         False   \n",
       "163         False   \n",
       "164         False   \n",
       "165         False   \n",
       "166         False   \n",
       "167         False   \n",
       "168         False   \n",
       "169         False   \n",
       "170         False   \n",
       "171         False   \n",
       "172         False   \n",
       "173         False   \n",
       "174         False   \n",
       "175         False   \n",
       "176         False   \n",
       "177         False   \n",
       "178         False   \n",
       "179         False   \n",
       "180         False   \n",
       "181         False   \n",
       "182         False   \n",
       "183         False   \n",
       "184         False   \n",
       "185         False   \n",
       "186         False   \n",
       "187         False   \n",
       "188         False   \n",
       "189          True   \n",
       "190         False   \n",
       "191         False   \n",
       "192         False   \n",
       "193         False   \n",
       "194         False   \n",
       "195         False   \n",
       "196         False   \n",
       "197         False   \n",
       "198         False   \n",
       "199         False   \n",
       "200         False   \n",
       "201         False   \n",
       "202         False   \n",
       "203         False   \n",
       "204         False   \n",
       "205         False   \n",
       "206         False   \n",
       "207         False   \n",
       "208         False   \n",
       "209         False   \n",
       "210         False   \n",
       "211         False   \n",
       "212         False   \n",
       "213         False   \n",
       "214         False   \n",
       "215         False   \n",
       "216         False   \n",
       "217         False   \n",
       "218         False   \n",
       "219         False   \n",
       "220         False   \n",
       "221         False   \n",
       "222         False   \n",
       "223          True   \n",
       "224         False   \n",
       "225         False   \n",
       "226         False   \n",
       "227         False   \n",
       "228         False   \n",
       "229         False   \n",
       "230         False   \n",
       "231         False   \n",
       "232         False   \n",
       "233         False   \n",
       "234         False   \n",
       "235         False   \n",
       "236         False   \n",
       "237         False   \n",
       "238         False   \n",
       "239         False   \n",
       "240         False   \n",
       "241         False   \n",
       "242         False   \n",
       "243         False   \n",
       "244         False   \n",
       "245         False   \n",
       "246         False   \n",
       "247         False   \n",
       "248         False   \n",
       "249         False   \n",
       "250         False   \n",
       "251         False   \n",
       "252         False   \n",
       "253         False   \n",
       "254          True   \n",
       "255         False   \n",
       "256         False   \n",
       "257         False   \n",
       "258         False   \n",
       "259         False   \n",
       "260         False   \n",
       "261         False   \n",
       "262         False   \n",
       "263         False   \n",
       "264         False   \n",
       "265         False   \n",
       "266         False   \n",
       "267         False   \n",
       "268         False   \n",
       "269         False   \n",
       "270         False   \n",
       "271         False   \n",
       "272         False   \n",
       "273         False   \n",
       "274         False   \n",
       "275         False   \n",
       "276         False   \n",
       "277         False   \n",
       "278         False   \n",
       "279          True   \n",
       "280         False   \n",
       "281         False   \n",
       "282         False   \n",
       "283         False   \n",
       "284         False   \n",
       "285         False   \n",
       "286         False   \n",
       "287         False   \n",
       "288         False   \n",
       "289         False   \n",
       "290         False   \n",
       "291         False   \n",
       "292         False   \n",
       "293         False   \n",
       "294         False   \n",
       "295         False   \n",
       "296         False   \n",
       "297          True   \n",
       "298         False   \n",
       "299         False   \n",
       "300         False   \n",
       "301         False   \n",
       "302         False   \n",
       "303         False   \n",
       "304         False   \n",
       "305         False   \n",
       "306         False   \n",
       "307         False   \n",
       "308         False   \n",
       "309         False   \n",
       "310         False   \n",
       "311         False   \n",
       "312         False   \n",
       "313         False   \n",
       "314         False   \n",
       "315          True   \n",
       "316         False   \n",
       "317         False   \n",
       "318         False   \n",
       "319         False   \n",
       "320         False   \n",
       "321         False   \n",
       "322         False   \n",
       "323         False   \n",
       "324         False   \n",
       "325         False   \n",
       "326         False   \n",
       "327         False   \n",
       "328         False   \n",
       "329         False   \n",
       "330         False   \n",
       "331          True   \n",
       "332         False   \n",
       "333         False   \n",
       "334         False   \n",
       "335         False   \n",
       "336         False   \n",
       "337         False   \n",
       "338         False   \n",
       "339         False   \n",
       "340         False   \n",
       "341         False   \n",
       "342         False   \n",
       "343         False   \n",
       "344         False   \n",
       "345         False   \n",
       "346         False   \n",
       "347         False   \n",
       "348          True   \n",
       "349         False   \n",
       "350         False   \n",
       "351         False   \n",
       "352         False   \n",
       "353         False   \n",
       "354         False   \n",
       "355         False   \n",
       "356         False   \n",
       "357         False   \n",
       "358         False   \n",
       "359         False   \n",
       "360         False   \n",
       "361         False   \n",
       "362         False   \n",
       "363         False   \n",
       "364         False   \n",
       "365         False   \n",
       "366         False   \n",
       "367         False   \n",
       "368         False   \n",
       "369         False   \n",
       "370         False   \n",
       "371         False   \n",
       "372         False   \n",
       "373         False   \n",
       "374         False   \n",
       "375         False   \n",
       "376         False   \n",
       "377         False   \n",
       "378         False   \n",
       "379         False   \n",
       "380         False   \n",
       "381         False   \n",
       "382         False   \n",
       "383         False   \n",
       "384         False   \n",
       "385          True   \n",
       "386         False   \n",
       "387         False   \n",
       "388         False   \n",
       "389         False   \n",
       "390         False   \n",
       "391         False   \n",
       "392         False   \n",
       "393         False   \n",
       "394         False   \n",
       "395         False   \n",
       "396         False   \n",
       "397          True   \n",
       "398         False   \n",
       "399         False   \n",
       "400         False   \n",
       "401         False   \n",
       "402         False   \n",
       "403         False   \n",
       "404         False   \n",
       "405         False   \n",
       "406         False   \n",
       "407         False   \n",
       "408         False   \n",
       "409         False   \n",
       "410         False   \n",
       "411         False   \n",
       "412         False   \n",
       "413         False   \n",
       "414         False   \n",
       "415         False   \n",
       "416         False   \n",
       "417         False   \n",
       "418         False   \n",
       "419         False   \n",
       "420         False   \n",
       "421         False   \n",
       "422         False   \n",
       "423         False   \n",
       "424         False   \n",
       "425         False   \n",
       "426         False   \n",
       "427         False   \n",
       "428         False   \n",
       "429         False   \n",
       "430         False   \n",
       "431         False   \n",
       "432         False   \n",
       "433         False   \n",
       "434         False   \n",
       "435          True   \n",
       "436         False   \n",
       "437         False   \n",
       "438         False   \n",
       "439         False   \n",
       "440         False   \n",
       "441         False   \n",
       "442         False   \n",
       "443         False   \n",
       "444         False   \n",
       "445         False   \n",
       "446         False   \n",
       "447         False   \n",
       "448         False   \n",
       "449         False   \n",
       "450         False   \n",
       "451          True   \n",
       "452         False   \n",
       "453         False   \n",
       "454         False   \n",
       "455         False   \n",
       "456         False   \n",
       "457         False   \n",
       "458         False   \n",
       "459         False   \n",
       "460         False   \n",
       "461         False   \n",
       "462         False   \n",
       "463         False   \n",
       "464         False   \n",
       "465         False   \n",
       "466         False   \n",
       "467         False   \n",
       "468          True   \n",
       "469         False   \n",
       "470         False   \n",
       "471         False   \n",
       "472         False   \n",
       "473         False   \n",
       "474         False   \n",
       "475         False   \n",
       "476         False   \n",
       "477         False   \n",
       "478         False   \n",
       "479         False   \n",
       "480         False   \n",
       "481         False   \n",
       "482         False   \n",
       "483         False   \n",
       "484         False   \n",
       "485         False   \n",
       "486         False   \n",
       "487         False   \n",
       "488          True   \n",
       "489         False   \n",
       "490         False   \n",
       "491         False   \n",
       "492         False   \n",
       "493         False   \n",
       "494         False   \n",
       "495         False   \n",
       "496         False   \n",
       "497         False   \n",
       "498         False   \n",
       "499         False   \n",
       "500         False   \n",
       "501         False   \n",
       "502         False   \n",
       "503         False   \n",
       "504         False   \n",
       "505         False   \n",
       "506         False   \n",
       "507         False   \n",
       "508         False   \n",
       "509         False   \n",
       "510         False   \n",
       "511         False   \n",
       "512         False   \n",
       "513         False   \n",
       "514         False   \n",
       "515         False   \n",
       "516         False   \n",
       "517         False   \n",
       "518         False   \n",
       "519         False   \n",
       "520         False   \n",
       "521         False   \n",
       "522         False   \n",
       "523         False   \n",
       "524         False   \n",
       "525         False   \n",
       "526         False   \n",
       "527         False   \n",
       "528         False   \n",
       "529         False   \n",
       "530         False   \n",
       "531         False   \n",
       "532         False   \n",
       "533         False   \n",
       "534         False   \n",
       "535          True   \n",
       "536         False   \n",
       "537         False   \n",
       "538         False   \n",
       "539         False   \n",
       "540         False   \n",
       "541         False   \n",
       "542         False   \n",
       "543         False   \n",
       "544         False   \n",
       "545         False   \n",
       "546         False   \n",
       "547         False   \n",
       "548         False   \n",
       "549         False   \n",
       "550         False   \n",
       "551         False   \n",
       "552         False   \n",
       "553         False   \n",
       "554         False   \n",
       "555         False   \n",
       "556         False   \n",
       "557         False   \n",
       "558         False   \n",
       "559         False   \n",
       "560         False   \n",
       "561         False   \n",
       "562          True   \n",
       "563         False   \n",
       "564         False   \n",
       "565         False   \n",
       "566         False   \n",
       "567         False   \n",
       "568         False   \n",
       "569         False   \n",
       "570         False   \n",
       "571         False   \n",
       "572         False   \n",
       "573         False   \n",
       "574         False   \n",
       "575         False   \n",
       "576         False   \n",
       "577         False   \n",
       "578         False   \n",
       "579         False   \n",
       "580         False   \n",
       "581         False   \n",
       "582         False   \n",
       "583         False   \n",
       "584         False   \n",
       "585         False   \n",
       "586         False   \n",
       "587         False   \n",
       "588         False   \n",
       "589         False   \n",
       "590         False   \n",
       "591         False   \n",
       "592         False   \n",
       "593         False   \n",
       "594         False   \n",
       "595         False   \n",
       "596         False   \n",
       "597         False   \n",
       "598         False   \n",
       "599         False   \n",
       "600         False   \n",
       "601          True   \n",
       "602         False   \n",
       "603         False   \n",
       "604         False   \n",
       "605         False   \n",
       "606         False   \n",
       "607         False   \n",
       "608         False   \n",
       "609         False   \n",
       "610         False   \n",
       "611         False   \n",
       "612         False   \n",
       "613         False   \n",
       "614         False   \n",
       "615         False   \n",
       "616         False   \n",
       "617         False   \n",
       "618         False   \n",
       "619         False   \n",
       "620          True   \n",
       "621         False   \n",
       "622         False   \n",
       "623         False   \n",
       "624         False   \n",
       "625         False   \n",
       "626         False   \n",
       "627         False   \n",
       "628         False   \n",
       "629         False   \n",
       "630         False   \n",
       "631         False   \n",
       "632         False   \n",
       "633         False   \n",
       "634         False   \n",
       "635         False   \n",
       "636         False   \n",
       "637         False   \n",
       "638         False   \n",
       "639         False   \n",
       "640         False   \n",
       "641         False   \n",
       "642         False   \n",
       "643         False   \n",
       "644          True   \n",
       "645         False   \n",
       "646         False   \n",
       "647         False   \n",
       "648         False   \n",
       "649          True   \n",
       "650         False   \n",
       "651         False   \n",
       "652         False   \n",
       "653         False   \n",
       "654         False   \n",
       "655         False   \n",
       "656         False   \n",
       "657         False   \n",
       "658         False   \n",
       "659         False   \n",
       "660         False   \n",
       "661         False   \n",
       "662         False   \n",
       "663         False   \n",
       "664         False   \n",
       "665         False   \n",
       "666         False   \n",
       "667         False   \n",
       "668         False   \n",
       "669         False   \n",
       "670         False   \n",
       "671         False   \n",
       "672         False   \n",
       "673         False   \n",
       "674         False   \n",
       "675         False   \n",
       "676         False   \n",
       "677         False   \n",
       "678         False   \n",
       "679         False   \n",
       "680         False   \n",
       "681         False   \n",
       "682         False   \n",
       "683         False   \n",
       "684         False   \n",
       "685         False   \n",
       "686         False   \n",
       "687         False   \n",
       "688         False   \n",
       "689         False   \n",
       "690         False   \n",
       "691         False   \n",
       "692         False   \n",
       "693         False   \n",
       "694         False   \n",
       "695         False   \n",
       "696          True   \n",
       "697         False   \n",
       "698         False   \n",
       "699         False   \n",
       "700         False   \n",
       "701         False   \n",
       "702         False   \n",
       "703         False   \n",
       "704         False   \n",
       "705         False   \n",
       "706         False   \n",
       "707         False   \n",
       "708          True   \n",
       "\n",
       "                                                                                                                                                                          next_topic_begin_seq  \\\n",
       "0                                                                                                                                                                                          NaN   \n",
       "1                                                                                                                                                                                          NaN   \n",
       "2                                                                                                                                                                                          NaN   \n",
       "3                                                                                                                                                                                          NaN   \n",
       "4                                                                                                                                                                                          NaN   \n",
       "5                                                                                                                                                                                          NaN   \n",
       "6                                                                                                                                                                                          NaN   \n",
       "7                                                                                                                                                                                          NaN   \n",
       "8                                                                                                                                                                                          NaN   \n",
       "9                                                                                                                                                                                          NaN   \n",
       "10                                                                                                                                                                                         NaN   \n",
       "11                                                                                                                                                                                         NaN   \n",
       "12                                                                                                                                                                                         NaN   \n",
       "13                                                                                                                                                                                         NaN   \n",
       "14                                                                                                                                                                                         NaN   \n",
       "15                                                                                                                                                                                         NaN   \n",
       "16                                                                                                                                                                                         NaN   \n",
       "17                                                                                                                                                                                         NaN   \n",
       "18                                                                                                                                                                                         NaN   \n",
       "19                                                                                                                                                                                         NaN   \n",
       "20                                                                                                                                                                                         NaN   \n",
       "21                                                                                                                                                                                         NaN   \n",
       "22                                                                                                                                                                                         NaN   \n",
       "23                                                                                                                                                                                         NaN   \n",
       "24                                                                                                                                                                                         NaN   \n",
       "25                                                                                                                                                                                         NaN   \n",
       "26      fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now   \n",
       "27                                                                                                                                                                                         NaN   \n",
       "28                                                                                                                                                                                         NaN   \n",
       "29                                                                                                                                                                                         NaN   \n",
       "30                                                                                                                                                                                         NaN   \n",
       "31                                                                                                                                                                                         NaN   \n",
       "32                                                                                                                                                                                         NaN   \n",
       "33                                                                                                                                                                                         NaN   \n",
       "34                                                                                                                                                                                         NaN   \n",
       "35                                                                                                                                                                                         NaN   \n",
       "36                                                                                                                                                                                         NaN   \n",
       "37                                                                                                                                                                                         NaN   \n",
       "38                                                                                                                                                                                         NaN   \n",
       "39                                                                                                                                                                                         NaN   \n",
       "40                                                                                                                                                                                         NaN   \n",
       "41                                                                                                      fast.ai course. It was later turned into an academic paper by me in conjunction with a   \n",
       "42                                                                                                                                                                                         NaN   \n",
       "43                                                                                                                                                                                         NaN   \n",
       "44                                                                                                                                                                                         NaN   \n",
       "45                                                                                                                                                                                         NaN   \n",
       "46                                                                                                                                                                                         NaN   \n",
       "47                                                                                                                                                                                         NaN   \n",
       "48                                                                                                                                                                                         NaN   \n",
       "49                                                                                                                                                                                         NaN   \n",
       "50                                                                                                                                                                                         NaN   \n",
       "51                                                                                                                                                                                         NaN   \n",
       "52                                                                                                                                                                                         NaN   \n",
       "53                                                                                                                                                                                         NaN   \n",
       "54                                                                                                                                                                                         NaN   \n",
       "55                                                                                                                                                                                         NaN   \n",
       "56                                                                                                                                                                                         NaN   \n",
       "57                                                                                                                                                                                         NaN   \n",
       "58                                                                                                                                                                                         NaN   \n",
       "59                                                                                                                                                                                         NaN   \n",
       "60                                                                                                                                                                                         NaN   \n",
       "61                                                                                                                                                                                         NaN   \n",
       "62                                                                                                                                                                                         NaN   \n",
       "63                                                                                                                                                                                         NaN   \n",
       "64                                                                                                                                                                                         NaN   \n",
       "65                                                                                                                                                                                         NaN   \n",
       "66                                                                                                                                                                                         NaN   \n",
       "67                                                                                                                                                                                         NaN   \n",
       "68                                                                                                                                                                                         NaN   \n",
       "69                                                                                                                                                                                         NaN   \n",
       "70                                                                                                        they can take really good advantage of modern accelerators like, like Google's TPUs.   \n",
       "71                                                                                                                                                                                         NaN   \n",
       "72                                                                                                                                                                                         NaN   \n",
       "73                                                                                                                                                                                         NaN   \n",
       "74                                                                                                                                                                                         NaN   \n",
       "75                                                                                                                                                                                         NaN   \n",
       "76                                                                                                                                                                                         NaN   \n",
       "77                                                                                                                                                                                         NaN   \n",
       "78                                                                                                                                                                                         NaN   \n",
       "79                                                                                                                                                                                         NaN   \n",
       "80                                                                                                                                                                                         NaN   \n",
       "81                                                                                                                                                                                         NaN   \n",
       "82                                                                               for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place   \n",
       "83                                                                                                                                                                                         NaN   \n",
       "84                                                                                                                                                                                         NaN   \n",
       "85                                                                                                                                                                                         NaN   \n",
       "86                                                                                                                                                                                         NaN   \n",
       "87                                                                                                                                                                                         NaN   \n",
       "88                                                                                                                                                                                         NaN   \n",
       "89                                                                                                                                                                                         NaN   \n",
       "90                                                                                                                                                                                         NaN   \n",
       "91                                                                                                                                                                                         NaN   \n",
       "92                                                                                                                                                                                         NaN   \n",
       "93                                                                                                                                                                                         NaN   \n",
       "94                                                                                                                                                                                         NaN   \n",
       "95                                                                                                                                                                                         NaN   \n",
       "96                                                                                                                                                                                         NaN   \n",
       "97                                                                                                                                                                                         NaN   \n",
       "98                                                                                                                                                                                         NaN   \n",
       "99                                                                                                                                                                                         NaN   \n",
       "100                                                                                                                                                                                        NaN   \n",
       "101                                                                                                                                                                                        NaN   \n",
       "102                                                                                                                                                                                        NaN   \n",
       "103                                                                                                                                                                                        NaN   \n",
       "104                                                                                                                                                                                        NaN   \n",
       "105                                                                                                                                                                                        NaN   \n",
       "106                                                                                                                                                                                        NaN   \n",
       "107                                                                                                                                                                                        NaN   \n",
       "108                                                                                                                                                                                        NaN   \n",
       "109                                                                                                                                                                                        NaN   \n",
       "110                                                                                                                                                                                        NaN   \n",
       "111                                                                                                                                                                                        NaN   \n",
       "112                                                                                                                                                                                        NaN   \n",
       "113                                                                                                       actually on as I speak, and I created this notebook called Getting started with NLP   \n",
       "114                                                                                                                                                                                        NaN   \n",
       "115                                                                                                                                                                                        NaN   \n",
       "116                                                                                                                                                                                        NaN   \n",
       "117                                                                                                                                                                                        NaN   \n",
       "118                                                                                                                                                                                        NaN   \n",
       "119                                                                                                                                                                                        NaN   \n",
       "120                                                                                                                                                                                        NaN   \n",
       "121                                                                                                                                                                                        NaN   \n",
       "122                                                                                                                                                                                        NaN   \n",
       "123                                                                                                                                                                                        NaN   \n",
       "124                                                                                                 and trying to identify a category that object belongs to. So, previously we've mainly been   \n",
       "125                                                                                                                                                                                        NaN   \n",
       "126                                                                                                                                                                                        NaN   \n",
       "127                                                                                                                                                                                        NaN   \n",
       "128                                                                                                                                                                                        NaN   \n",
       "129                                                                                                                                                                                        NaN   \n",
       "130                                                                                                                                                                                        NaN   \n",
       "131                                                                                                                                                                                        NaN   \n",
       "132                                                                                                                                                                                        NaN   \n",
       "133                                                                                                                                                                                        NaN   \n",
       "134                                                                                                                                                                                        NaN   \n",
       "135                                                                                                                                                                                        NaN   \n",
       "136                                                                                                                                                                                        NaN   \n",
       "137                                                                                                                                                                                        NaN   \n",
       "138                                                                                                                                                                                        NaN   \n",
       "139                                                                                                                                                                                        NaN   \n",
       "140                                                                                                                                                                                        NaN   \n",
       "141                                                                                                                                                                                        NaN   \n",
       "142                                                                                                                                                                                        NaN   \n",
       "143                                                                                                                                                                                        NaN   \n",
       "144                                                                                                                                                                                        NaN   \n",
       "145                                                                                                                                                                                        NaN   \n",
       "146                                                                                                                                                                                        NaN   \n",
       "147                                                                                                                                                                                        NaN   \n",
       "148                                                                                                                                                                                        NaN   \n",
       "149                                                                                                                                                                                        NaN   \n",
       "150                                                                                                                                                                                        NaN   \n",
       "151                                                                                                                                                                                        NaN   \n",
       "152                                                                                                                                                                                        NaN   \n",
       "153                                                                                                                                                                                        NaN   \n",
       "154                                                                                                                                                                                        NaN   \n",
       "155                                                                                                                                                                                        NaN   \n",
       "156                                                                                                                                                                                        NaN   \n",
       "157                                                                                                                                                                                        NaN   \n",
       "158                                                                                                                                                                                        NaN   \n",
       "159                                                                                                                                                                                        NaN   \n",
       "160                                                                                                                                                                                        NaN   \n",
       "161  So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically.   \n",
       "162                                                                                                                                                                                        NaN   \n",
       "163                                                                                                                                                                                        NaN   \n",
       "164                                                                                                                                                                                        NaN   \n",
       "165                                                                                                                                                                                        NaN   \n",
       "166                                                                                                                                                                                        NaN   \n",
       "167                                                                                                                                                                                        NaN   \n",
       "168                                                                                                                                                                                        NaN   \n",
       "169                                                                                                                                                                                        NaN   \n",
       "170                                                                                                                                                                                        NaN   \n",
       "171                                                                                                                                                                                        NaN   \n",
       "172                                                                                                                                                                                        NaN   \n",
       "173                                                                                                                                                                                        NaN   \n",
       "174                                                                                                                                                                                        NaN   \n",
       "175                                                                                                                                                                                        NaN   \n",
       "176                                                                                                                                                                                        NaN   \n",
       "177                                                                                                                                                                                        NaN   \n",
       "178                                                                                                                                                                                        NaN   \n",
       "179                                                                                                                                                                                        NaN   \n",
       "180                                                                                                                                                                                        NaN   \n",
       "181                                                                                                                                                                                        NaN   \n",
       "182                                                                                                                                                                                        NaN   \n",
       "183                                                                                                                                                                                        NaN   \n",
       "184                                                                                                                                                                                        NaN   \n",
       "185                                                                                                                                                                                        NaN   \n",
       "186                                                                                                                                                                                        NaN   \n",
       "187                                                                                                                                                                                        NaN   \n",
       "188                                                                                                                                                                                        NaN   \n",
       "189                                                                                                                like four key libraries that you have to know to do data science in python.   \n",
       "190                                                                                                                                                                                        NaN   \n",
       "191                                                                                                                                                                                        NaN   \n",
       "192                                                                                                                                                                                        NaN   \n",
       "193                                                                                                                                                                                        NaN   \n",
       "194                                                                                                                                                                                        NaN   \n",
       "195                                                                                                                                                                                        NaN   \n",
       "196                                                                                                                                                                                        NaN   \n",
       "197                                                                                                                                                                                        NaN   \n",
       "198                                                                                                                                                                                        NaN   \n",
       "199                                                                                                                                                                                        NaN   \n",
       "200                                                                                                                                                                                        NaN   \n",
       "201                                                                                                                                                                                        NaN   \n",
       "202                                                                                                                                                                                        NaN   \n",
       "203                                                                                                                                                                                        NaN   \n",
       "204                                                                                                                                                                                        NaN   \n",
       "205                                                                                                                                                                                        NaN   \n",
       "206                                                                                                                                                                                        NaN   \n",
       "207                                                                                                                                                                                        NaN   \n",
       "208                                                                                                                                                                                        NaN   \n",
       "209                                                                                                                                                                                        NaN   \n",
       "210                                                                                                                                                                                        NaN   \n",
       "211                                                                                                                                                                                        NaN   \n",
       "212                                                                                                                                                                                        NaN   \n",
       "213                                                                                                                                                                                        NaN   \n",
       "214                                                                                                                                                                                        NaN   \n",
       "215                                                                                                                                                                                        NaN   \n",
       "216                                                                                                                                                                                        NaN   \n",
       "217                                                                                                                                                                                        NaN   \n",
       "218                                                                                                                                                                                        NaN   \n",
       "219                                                                                                                                                                                        NaN   \n",
       "220                                                                                                                                                                                        NaN   \n",
       "221                                                                                                                                                                                        NaN   \n",
       "222                                                                                                                                                                                        NaN   \n",
       "223                                                                                                 numbers and we're going to multiply them by matrices, we're going to replace the negatives   \n",
       "224                                                                                                                                                                                        NaN   \n",
       "225                                                                                                                                                                                        NaN   \n",
       "226                                                                                                                                                                                        NaN   \n",
       "227                                                                                                                                                                                        NaN   \n",
       "228                                                                                                                                                                                        NaN   \n",
       "229                                                                                                                                                                                        NaN   \n",
       "230                                                                                                                                                                                        NaN   \n",
       "231                                                                                                                                                                                        NaN   \n",
       "232                                                                                                                                                                                        NaN   \n",
       "233                                                                                                                                                                                        NaN   \n",
       "234                                                                                                                                                                                        NaN   \n",
       "235                                                                                                                                                                                        NaN   \n",
       "236                                                                                                                                                                                        NaN   \n",
       "237                                                                                                                                                                                        NaN   \n",
       "238                                                                                                                                                                                        NaN   \n",
       "239                                                                                                                                                                                        NaN   \n",
       "240                                                                                                                                                                                        NaN   \n",
       "241                                                                                                                                                                                        NaN   \n",
       "242                                                                                                                                                                                        NaN   \n",
       "243                                                                                                                                                                                        NaN   \n",
       "244                                                                                                                                                                                        NaN   \n",
       "245                                                                                                                                                                                        NaN   \n",
       "246                                                                                                                                                                                        NaN   \n",
       "247                                                                                                                                                                                        NaN   \n",
       "248                                                                                                                                                                                        NaN   \n",
       "249                                                                                                                                                                                        NaN   \n",
       "250                                                                                                                                                                                        NaN   \n",
       "251                                                                                                                                                                                        NaN   \n",
       "252                                                                                                                                                                                        NaN   \n",
       "253                                                                                                                                                                                        NaN   \n",
       "254                                                                                                   models, so even many more even than timm's image models. And so, these models, they vary   \n",
       "255                                                                                                                                                                                        NaN   \n",
       "256                                                                                                                                                                                        NaN   \n",
       "257                                                                                                                                                                                        NaN   \n",
       "258                                                                                                                                                                                        NaN   \n",
       "259                                                                                                                                                                                        NaN   \n",
       "260                                                                                                                                                                                        NaN   \n",
       "261                                                                                                                                                                                        NaN   \n",
       "262                                                                                                                                                                                        NaN   \n",
       "263                                                                                                                                                                                        NaN   \n",
       "264                                                                                                                                                                                        NaN   \n",
       "265                                                                                                                                                                                        NaN   \n",
       "266                                                                                                                                                                                        NaN   \n",
       "267                                                                                                                                                                                        NaN   \n",
       "268                                                                                                                                                                                        NaN   \n",
       "269                                                                                                                                                                                        NaN   \n",
       "270                                                                                                                                                                                        NaN   \n",
       "271                                                                                                                                                                                        NaN   \n",
       "272                                                                                                                                                                                        NaN   \n",
       "273                                                                                                                                                                                        NaN   \n",
       "274                                                                                                                                                                                        NaN   \n",
       "275                                                                                                                                                                                        NaN   \n",
       "276                                                                                                                                                                                        NaN   \n",
       "277                                                                                                                                                                                        NaN   \n",
       "278                                                                                                                                                                                        NaN   \n",
       "279                                                                                                           So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's   \n",
       "280                                                                                                                                                                                        NaN   \n",
       "281                                                                                                                                                                                        NaN   \n",
       "282                                                                                                                                                                                        NaN   \n",
       "283                                                                                                                                                                                        NaN   \n",
       "284                                                                                                                                                                                        NaN   \n",
       "285                                                                                                                                                                                        NaN   \n",
       "286                                                                                                                                                                                        NaN   \n",
       "287                                                                                                                                                                                        NaN   \n",
       "288                                                                                                                                                                                        NaN   \n",
       "289                                                                                                                                                                                        NaN   \n",
       "290                                                                                                                                                                                        NaN   \n",
       "291                                                                                                                                                                                        NaN   \n",
       "292                                                                                                                                                                                        NaN   \n",
       "293                                                                                                                                                                                        NaN   \n",
       "294                                                                                                                                                                                        NaN   \n",
       "295                                                                                                                                                                                        NaN   \n",
       "296                                                                                                                                                                                        NaN   \n",
       "297                                                                                                      that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our   \n",
       "298                                                                                                                                                                                        NaN   \n",
       "299                                                                                                                                                                                        NaN   \n",
       "300                                                                                                                                                                                        NaN   \n",
       "301                                                                                                                                                                                        NaN   \n",
       "302                                                                                                                                                                                        NaN   \n",
       "303                                                                                                                                                                                        NaN   \n",
       "304                                                                                                                                                                                        NaN   \n",
       "305                                                                                                                                                                                        NaN   \n",
       "306                                                                                                                                                                                        NaN   \n",
       "307                                                                                                                                                                                        NaN   \n",
       "308                                                                                                                                                                                        NaN   \n",
       "309                                                                                                                                                                                        NaN   \n",
       "310                                                                                                                                                                                        NaN   \n",
       "311                                                                                                                                                                                        NaN   \n",
       "312                                                                                                                                                                                        NaN   \n",
       "313                                                                                                                                                                                        NaN   \n",
       "314                                                                                                                                                                                        NaN   \n",
       "315                                                                                                     you've just tokenized. So one question was really about: How you choose those keywords   \n",
       "316                                                                                                                                                                                        NaN   \n",
       "317                                                                                                                                                                                        NaN   \n",
       "318                                                                                                                                                                                        NaN   \n",
       "319                                                                                                                                                                                        NaN   \n",
       "320                                                                                                                                                                                        NaN   \n",
       "321                                                                                                                                                                                        NaN   \n",
       "322                                                                                                                                                                                        NaN   \n",
       "323                                                                                                                                                                                        NaN   \n",
       "324                                                                                                                                                                                        NaN   \n",
       "325                                                                                                                                                                                        NaN   \n",
       "326                                                                                                                                                                                        NaN   \n",
       "327                                                                                                                                                                                        NaN   \n",
       "328                                                                                                                                                                                        NaN   \n",
       "329                                                                                                                                                                                        NaN   \n",
       "330                                                                                                                                                                                        NaN   \n",
       "331                                                                                               best approach for reasonably quickly and easily using large documents. Otherwise, if you use   \n",
       "332                                                                                                                                                                                        NaN   \n",
       "333                                                                                                                                                                                        NaN   \n",
       "334                                                                                                                                                                                        NaN   \n",
       "335                                                                                                                                                                                        NaN   \n",
       "336                                                                                                                                                                                        NaN   \n",
       "337                                                                                                                                                                                        NaN   \n",
       "338                                                                                                                                                                                        NaN   \n",
       "339                                                                                                                                                                                        NaN   \n",
       "340                                                                                                                                                                                        NaN   \n",
       "341                                                                                                                                                                                        NaN   \n",
       "342                                                                                                                                                                                        NaN   \n",
       "343                                                                                                                                                                                        NaN   \n",
       "344                                                                                                                                                                                        NaN   \n",
       "345                                                                                                                                                                                        NaN   \n",
       "346                                                                                                                                                                                        NaN   \n",
       "347                                                                                                                                                                                        NaN   \n",
       "348                                                                                      Test and validation sets are all about identifying and controlling for something called overfitting   \n",
       "349                                                                                                                                                                                        NaN   \n",
       "350                                                                                                                                                                                        NaN   \n",
       "351                                                                                                                                                                                        NaN   \n",
       "352                                                                                                                                                                                        NaN   \n",
       "353                                                                                                                                                                                        NaN   \n",
       "354                                                                                                                                                                                        NaN   \n",
       "355                                                                                                                                                                                        NaN   \n",
       "356                                                                                                                                                                                        NaN   \n",
       "357                                                                                                                                                                                        NaN   \n",
       "358                                                                                                                                                                                        NaN   \n",
       "359                                                                                                                                                                                        NaN   \n",
       "360                                                                                                                                                                                        NaN   \n",
       "361                                                                                                                                                                                        NaN   \n",
       "362                                                                                                                                                                                        NaN   \n",
       "363                                                                                                                                                                                        NaN   \n",
       "364                                                                                                                                                                                        NaN   \n",
       "365                                                                                                                                                                                        NaN   \n",
       "366                                                                                                                                                                                        NaN   \n",
       "367                                                                                                                                                                                        NaN   \n",
       "368                                                                                                                                                                                        NaN   \n",
       "369                                                                                                                                                                                        NaN   \n",
       "370                                                                                                                                                                                        NaN   \n",
       "371                                                                                                                                                                                        NaN   \n",
       "372                                                                                                                                                                                        NaN   \n",
       "373                                                                                                                                                                                        NaN   \n",
       "374                                                                                                                                                                                        NaN   \n",
       "375                                                                                                                                                                                        NaN   \n",
       "376                                                                                                                                                                                        NaN   \n",
       "377                                                                                                                                                                                        NaN   \n",
       "378                                                                                                                                                                                        NaN   \n",
       "379                                                                                                                                                                                        NaN   \n",
       "380                                                                                                                                                                                        NaN   \n",
       "381                                                                                                                                                                                        NaN   \n",
       "382                                                                                                                                                                                        NaN   \n",
       "383                                                                                                                                                                                        NaN   \n",
       "384                                                                                                                                                                                        NaN   \n",
       "385                                                                                                  We then fit our model using only those points we haven't removed, and then we measure how   \n",
       "386                                                                                                                                                                                        NaN   \n",
       "387                                                                                                                                                                                        NaN   \n",
       "388                                                                                                                                                                                        NaN   \n",
       "389                                                                                                                                                                                        NaN   \n",
       "390                                                                                                                                                                                        NaN   \n",
       "391                                                                                                                                                                                        NaN   \n",
       "392                                                                                                                                                                                        NaN   \n",
       "393                                                                                                                                                                                        NaN   \n",
       "394                                                                                                                                                                                        NaN   \n",
       "395                                                                                                                                                                                        NaN   \n",
       "396                                                                                                                                                                                        NaN   \n",
       "397                                              some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was   \n",
       "398                                                                                                                                                                                        NaN   \n",
       "399                                                                                                                                                                                        NaN   \n",
       "400                                                                                                                                                                                        NaN   \n",
       "401                                                                                                                                                                                        NaN   \n",
       "402                                                                                                                                                                                        NaN   \n",
       "403                                                                                                                                                                                        NaN   \n",
       "404                                                                                                                                                                                        NaN   \n",
       "405                                                                                                                                                                                        NaN   \n",
       "406                                                                                                                                                                                        NaN   \n",
       "407                                                                                                                                                                                        NaN   \n",
       "408                                                                                                                                                                                        NaN   \n",
       "409                                                                                                                                                                                        NaN   \n",
       "410                                                                                                                                                                                        NaN   \n",
       "411                                                                                                                                                                                        NaN   \n",
       "412                                                                                                                                                                                        NaN   \n",
       "413                                                                                                                                                                                        NaN   \n",
       "414                                                                                                                                                                                        NaN   \n",
       "415                                                                                                                                                                                        NaN   \n",
       "416                                                                                                                                                                                        NaN   \n",
       "417                                                                                                                                                                                        NaN   \n",
       "418                                                                                                                                                                                        NaN   \n",
       "419                                                                                                                                                                                        NaN   \n",
       "420                                                                                                                                                                                        NaN   \n",
       "421                                                                                                                                                                                        NaN   \n",
       "422                                                                                                                                                                                        NaN   \n",
       "423                                                                                                                                                                                        NaN   \n",
       "424                                                                                                                                                                                        NaN   \n",
       "425                                                                                                                                                                                        NaN   \n",
       "426                                                                                                                                                                                        NaN   \n",
       "427                                                                                                                                                                                        NaN   \n",
       "428                                                                                                                                                                                        NaN   \n",
       "429                                                                                                                                                                                        NaN   \n",
       "430                                                                                                                                                                                        NaN   \n",
       "431                                                                                                                                                                                        NaN   \n",
       "432                                                                                                                                                                                        NaN   \n",
       "433                                                                                                                                                                                        NaN   \n",
       "434                                                                                                                                                                                        NaN   \n",
       "435                                                                                                      your model. Why not? Well imagine you tried two new models every day for three months   \n",
       "436                                                                                                                                                                                        NaN   \n",
       "437                                                                                                                                                                                        NaN   \n",
       "438                                                                                                                                                                                        NaN   \n",
       "439                                                                                                                                                                                        NaN   \n",
       "440                                                                                                                                                                                        NaN   \n",
       "441                                                                                                                                                                                        NaN   \n",
       "442                                                                                                                                                                                        NaN   \n",
       "443                                                                                                                                                                                        NaN   \n",
       "444                                                                                                                                                                                        NaN   \n",
       "445                                                                                                                                                                                        NaN   \n",
       "446                                                                                                                                                                                        NaN   \n",
       "447                                                                                                                                                                                        NaN   \n",
       "448                                                                                                                                                                                        NaN   \n",
       "449                                                                                                                                                                                        NaN   \n",
       "450                                                                                                                                                                                        NaN   \n",
       "451         So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something   \n",
       "452                                                                                                                                                                                        NaN   \n",
       "453                                                                                                                                                                                        NaN   \n",
       "454                                                                                                                                                                                        NaN   \n",
       "455                                                                                                                                                                                        NaN   \n",
       "456                                                                                                                                                                                        NaN   \n",
       "457                                                                                                                                                                                        NaN   \n",
       "458                                                                                                                                                                                        NaN   \n",
       "459                                                                                                                                                                                        NaN   \n",
       "460                                                                                                                                                                                        NaN   \n",
       "461                                                                                                                                                                                        NaN   \n",
       "462                                                                                                                                                                                        NaN   \n",
       "463                                                                                                                                                                                        NaN   \n",
       "464                                                                                                                                                                                        NaN   \n",
       "465                                                                                                                                                                                        NaN   \n",
       "466                                                                                                                                                                                        NaN   \n",
       "467                                                                                                                                                                                        NaN   \n",
       "468              number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex   \n",
       "469                                                                                                                                                                                        NaN   \n",
       "470                                                                                                                                                                                        NaN   \n",
       "471                                                                                                                                                                                        NaN   \n",
       "472                                                                                                                                                                                        NaN   \n",
       "473                                                                                                                                                                                        NaN   \n",
       "474                                                                                                                                                                                        NaN   \n",
       "475                                                                                                                                                                                        NaN   \n",
       "476                                                                                                                                                                                        NaN   \n",
       "477                                                                                                                                                                                        NaN   \n",
       "478                                                                                                                                                                                        NaN   \n",
       "479                                                                                                                                                                                        NaN   \n",
       "480                                                                                                                                                                                        NaN   \n",
       "481                                                                                                                                                                                        NaN   \n",
       "482                                                                                                                                                                                        NaN   \n",
       "483                                                                                                                                                                                        NaN   \n",
       "484                                                                                                                                                                                        NaN   \n",
       "485                                                                                                                                                                                        NaN   \n",
       "486                                                                                                                                                                                        NaN   \n",
       "487                                                                                                                                                                                        NaN   \n",
       "488                                                                                                        using letter r and it's the most widely used measure of how similar two variables   \n",
       "489                                                                                                                                                                                        NaN   \n",
       "490                                                                                                                                                                                        NaN   \n",
       "491                                                                                                                                                                                        NaN   \n",
       "492                                                                                                                                                                                        NaN   \n",
       "493                                                                                                                                                                                        NaN   \n",
       "494                                                                                                                                                                                        NaN   \n",
       "495                                                                                                                                                                                        NaN   \n",
       "496                                                                                                                                                                                        NaN   \n",
       "497                                                                                                                                                                                        NaN   \n",
       "498                                                                                                                                                                                        NaN   \n",
       "499                                                                                                                                                                                        NaN   \n",
       "500                                                                                                                                                                                        NaN   \n",
       "501                                                                                                                                                                                        NaN   \n",
       "502                                                                                                                                                                                        NaN   \n",
       "503                                                                                                                                                                                        NaN   \n",
       "504                                                                                                                                                                                        NaN   \n",
       "505                                                                                                                                                                                        NaN   \n",
       "506                                                                                                                                                                                        NaN   \n",
       "507                                                                                                                                                                                        NaN   \n",
       "508                                                                                                                                                                                        NaN   \n",
       "509                                                                                                                                                                                        NaN   \n",
       "510                                                                                                                                                                                        NaN   \n",
       "511                                                                                                                                                                                        NaN   \n",
       "512                                                                                                                                                                                        NaN   \n",
       "513                                                                                                                                                                                        NaN   \n",
       "514                                                                                                                                                                                        NaN   \n",
       "515                                                                                                                                                                                        NaN   \n",
       "516                                                                                                                                                                                        NaN   \n",
       "517                                                                                                                                                                                        NaN   \n",
       "518                                                                                                                                                                                        NaN   \n",
       "519                                                                                                                                                                                        NaN   \n",
       "520                                                                                                                                                                                        NaN   \n",
       "521                                                                                                                                                                                        NaN   \n",
       "522                                                                                                                                                                                        NaN   \n",
       "523                                                                                                                                                                                        NaN   \n",
       "524                                                                                                                                                                                        NaN   \n",
       "525                                                                                                                                                                                        NaN   \n",
       "526                                                                                                                                                                                        NaN   \n",
       "527                                                                                                                                                                                        NaN   \n",
       "528                                                                                                                                                                                        NaN   \n",
       "529                                                                                                                                                                                        NaN   \n",
       "530                                                                                                                                                                                        NaN   \n",
       "531                                                                                                                                                                                        NaN   \n",
       "532                                                                                                                                                                                        NaN   \n",
       "533                                                                                                                                                                                        NaN   \n",
       "534                                                                                                                                                                                        NaN   \n",
       "535                                                                                               more correlation here, but there's a few examples of some houses with lots and lots of rooms   \n",
       "536                                                                                                                                                                                        NaN   \n",
       "537                                                                                                                                                                                        NaN   \n",
       "538                                                                                                                                                                                        NaN   \n",
       "539                                                                                                                                                                                        NaN   \n",
       "540                                                                                                                                                                                        NaN   \n",
       "541                                                                                                                                                                                        NaN   \n",
       "542                                                                                                                                                                                        NaN   \n",
       "543                                                                                                                                                                                        NaN   \n",
       "544                                                                                                                                                                                        NaN   \n",
       "545                                                                                                                                                                                        NaN   \n",
       "546                                                                                                                                                                                        NaN   \n",
       "547                                                                                                                                                                                        NaN   \n",
       "548                                                                                                                                                                                        NaN   \n",
       "549                                                                                                                                                                                        NaN   \n",
       "550                                                                                                                                                                                        NaN   \n",
       "551                                                                                                                                                                                        NaN   \n",
       "552                                                                                                                                                                                        NaN   \n",
       "553                                                                                                                                                                                        NaN   \n",
       "554                                                                                                                                                                                        NaN   \n",
       "555                                                                                                                                                                                        NaN   \n",
       "556                                                                                                                                                                                        NaN   \n",
       "557                                                                                                                                                                                        NaN   \n",
       "558                                                                                                                                                                                        NaN   \n",
       "559                                                                                                                                                                                        NaN   \n",
       "560                                                                                                                                                                                        NaN   \n",
       "561                                                                                                                                                                                        NaN   \n",
       "562                                                                                                 Okay. We're now ready to train our model. In fast.ai, we use something called a learner.   \n",
       "563                                                                                                                                                                                        NaN   \n",
       "564                                                                                                                                                                                        NaN   \n",
       "565                                                                                                                                                                                        NaN   \n",
       "566                                                                                                                                                                                        NaN   \n",
       "567                                                                                                                                                                                        NaN   \n",
       "568                                                                                                                                                                                        NaN   \n",
       "569                                                                                                                                                                                        NaN   \n",
       "570                                                                                                                                                                                        NaN   \n",
       "571                                                                                                                                                                                        NaN   \n",
       "572                                                                                                                                                                                        NaN   \n",
       "573                                                                                                                                                                                        NaN   \n",
       "574                                                                                                                                                                                        NaN   \n",
       "575                                                                                                                                                                                        NaN   \n",
       "576                                                                                                                                                                                        NaN   \n",
       "577                                                                                                                                                                                        NaN   \n",
       "578                                                                                                                                                                                        NaN   \n",
       "579                                                                                                                                                                                        NaN   \n",
       "580                                                                                                                                                                                        NaN   \n",
       "581                                                                                                                                                                                        NaN   \n",
       "582                                                                                                                                                                                        NaN   \n",
       "583                                                                                                                                                                                        NaN   \n",
       "584                                                                                                                                                                                        NaN   \n",
       "585                                                                                                                                                                                        NaN   \n",
       "586                                                                                                                                                                                        NaN   \n",
       "587                                                                                                                                                                                        NaN   \n",
       "588                                                                                                                                                                                        NaN   \n",
       "589                                                                                                                                                                                        NaN   \n",
       "590                                                                                                                                                                                        NaN   \n",
       "591                                                                                                                                                                                        NaN   \n",
       "592                                                                                                                                                                                        NaN   \n",
       "593                                                                                                                                                                                        NaN   \n",
       "594                                                                                                                                                                                        NaN   \n",
       "595                                                                                                                                                                                        NaN   \n",
       "596                                                                                                                                                                                        NaN   \n",
       "597                                                                                                                                                                                        NaN   \n",
       "598                                                                                                                                                                                        NaN   \n",
       "599                                                                                                                                                                                        NaN   \n",
       "600                                                                                                                                                                                        NaN   \n",
       "601           before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how   \n",
       "602                                                                                                                                                                                        NaN   \n",
       "603                                                                                                                                                                                        NaN   \n",
       "604                                                                                                                                                                                        NaN   \n",
       "605                                                                                                                                                                                        NaN   \n",
       "606                                                                                                                                                                                        NaN   \n",
       "607                                                                                                                                                                                        NaN   \n",
       "608                                                                                                                                                                                        NaN   \n",
       "609                                                                                                                                                                                        NaN   \n",
       "610                                                                                                                                                                                        NaN   \n",
       "611                                                                                                                                                                                        NaN   \n",
       "612                                                                                                                                                                                        NaN   \n",
       "613                                                                                                                                                                                        NaN   \n",
       "614                                                                                                                                                                                        NaN   \n",
       "615                                                                                                                                                                                        NaN   \n",
       "616                                                                                                                                                                                        NaN   \n",
       "617                                                                                                                                                                                        NaN   \n",
       "618                                                                                                                                                                                        NaN   \n",
       "619                                                                                                                                                                                        NaN   \n",
       "620        you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah   \n",
       "621                                                                                                                                                                                        NaN   \n",
       "622                                                                                                                                                                                        NaN   \n",
       "623                                                                                                                                                                                        NaN   \n",
       "624                                                                                                                                                                                        NaN   \n",
       "625                                                                                                                                                                                        NaN   \n",
       "626                                                                                                                                                                                        NaN   \n",
       "627                                                                                                                                                                                        NaN   \n",
       "628                                                                                                                                                                                        NaN   \n",
       "629                                                                                                                                                                                        NaN   \n",
       "630                                                                                                                                                                                        NaN   \n",
       "631                                                                                                                                                                                        NaN   \n",
       "632                                                                                                                                                                                        NaN   \n",
       "633                                                                                                                                                                                        NaN   \n",
       "634                                                                                                                                                                                        NaN   \n",
       "635                                                                                                                                                                                        NaN   \n",
       "636                                                                                                                                                                                        NaN   \n",
       "637                                                                                                                                                                                        NaN   \n",
       "638                                                                                                                                                                                        NaN   \n",
       "639                                                                                                                                                                                        NaN   \n",
       "640                                                                                                                                                                                        NaN   \n",
       "641                                                                                                                                                                                        NaN   \n",
       "642                                                                                                                                                                                        NaN   \n",
       "643                                                                                                                                                                                        NaN   \n",
       "644            so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would   \n",
       "645                                                                                                                                                                                        NaN   \n",
       "646                                                                                                                                                                                        NaN   \n",
       "647                                                                                                                                                                                        NaN   \n",
       "648                                                                                                                                                                                        NaN   \n",
       "649                                                                                    Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it.   \n",
       "650                                                                                                                                                                                        NaN   \n",
       "651                                                                                                                                                                                        NaN   \n",
       "652                                                                                                                                                                                        NaN   \n",
       "653                                                                                                                                                                                        NaN   \n",
       "654                                                                                                                                                                                        NaN   \n",
       "655                                                                                                                                                                                        NaN   \n",
       "656                                                                                                                                                                                        NaN   \n",
       "657                                                                                                                                                                                        NaN   \n",
       "658                                                                                                                                                                                        NaN   \n",
       "659                                                                                                                                                                                        NaN   \n",
       "660                                                                                                                                                                                        NaN   \n",
       "661                                                                                                                                                                                        NaN   \n",
       "662                                                                                                                                                                                        NaN   \n",
       "663                                                                                                                                                                                        NaN   \n",
       "664                                                                                                                                                                                        NaN   \n",
       "665                                                                                                                                                                                        NaN   \n",
       "666                                                                                                                                                                                        NaN   \n",
       "667                                                                                                                                                                                        NaN   \n",
       "668                                                                                                                                                                                        NaN   \n",
       "669                                                                                                                                                                                        NaN   \n",
       "670                                                                                                                                                                                        NaN   \n",
       "671                                                                                                                                                                                        NaN   \n",
       "672                                                                                                                                                                                        NaN   \n",
       "673                                                                                                                                                                                        NaN   \n",
       "674                                                                                                                                                                                        NaN   \n",
       "675                                                                                                                                                                                        NaN   \n",
       "676                                                                                                                                                                                        NaN   \n",
       "677                                                                                                                                                                                        NaN   \n",
       "678                                                                                                                                                                                        NaN   \n",
       "679                                                                                                                                                                                        NaN   \n",
       "680                                                                                                                                                                                        NaN   \n",
       "681                                                                                                                                                                                        NaN   \n",
       "682                                                                                                                                                                                        NaN   \n",
       "683                                                                                                                                                                                        NaN   \n",
       "684                                                                                                                                                                                        NaN   \n",
       "685                                                                                                                                                                                        NaN   \n",
       "686                                                                                                                                                                                        NaN   \n",
       "687                                                                                                                                                                                        NaN   \n",
       "688                                                                                                                                                                                        NaN   \n",
       "689                                                                                                                                                                                        NaN   \n",
       "690                                                                                                                                                                                        NaN   \n",
       "691                                                                                                                                                                                        NaN   \n",
       "692                                                                                                                                                                                        NaN   \n",
       "693                                                                                                                                                                                        NaN   \n",
       "694                                                                                                                                                                                        NaN   \n",
       "695                                                                                                                                                                                        NaN   \n",
       "696                                                                                                     that's the one. The question Manikandan is asking... shouldn't num labels be five zero   \n",
       "697                                                                                                                                                                                        NaN   \n",
       "698                                                                                                                                                                                        NaN   \n",
       "699                                                                                                                                                                                        NaN   \n",
       "700                                                                                                                                                                                        NaN   \n",
       "701                                                                                                                                                                                        NaN   \n",
       "702                                                                                                                                                                                        NaN   \n",
       "703                                                                                                                                                                                        NaN   \n",
       "704                                                                                                                                                                                        NaN   \n",
       "705                                                                                                                                                                                        NaN   \n",
       "706                                                                                                                                                                                        NaN   \n",
       "707                                                                                                                                                                                        NaN   \n",
       "708    okay hi everybody and um welcome to practical deep learning for coders lesson five um we're uh at a stage now where we're going to be getting deeper and deeper into the details of how   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            other_topic_seqs  \\\n",
       "0    [because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doi...   \n",
       "1    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, ...   \n",
       "2    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, d...   \n",
       "3    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, fast.ai library, using recurrent neural networks (RNNs)., Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-trained NLP model using a library call...   \n",
       "4    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, Today we're going to do something else, which is we're going to do Transformers, and we're, not even going to use the fast.ai library at all in fact. So, what we're going to be, doing today is we're going to be fine-tuning a pre-traine...   \n",
       "5    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, not even going to use the fast.ai library at all in fact. ...   \n",
       "6    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "7    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "8    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "9    [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "10   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "11   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "12   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "13   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "14   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "15   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "16   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "17   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "18   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "19   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "20   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "21   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "22   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "23   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "24   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "25   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "26   [Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think, is the lesson that a lot of the regulars in the community have been most excited about,, because it's where we're gonna get some totally new material  totally new topic, we've, never covered before. We're going to cover natural language processing (NLP), and you'll, find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the, fast.ai library, using recurrent neural networks (RNNs)., ...   \n",
       "27   [into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you said, Oh! actually slider A, that, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because, you could immediately start focusing on the one we have...   \n",
       "28   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you said, Oh! actually slider A, that, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because, you could immediately start focusing on the one we...   \n",
       "29   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, it was given to you, the person who gave it to you said, Oh! actually slider A, that, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty helpful, wouldn't it, right? Because, you could immediately start focusing on the one we ha...   \n",
       "30   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, should be on 2.0, we know for sure. And slider B, we think it's like around two and a half., Slider C, we've got no idea. Now that'd be pretty...   \n",
       "31   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, Slider C, we've got no idea. Now that'd be pretty ...   \n",
       "32   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "33   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "34   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "35   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "36   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "37   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "38   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "39   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "40   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "41   [fine-tuning a pre-trained model. We've talked about that in passing before, but we haven't really been able to describe it in any detail, because you haven't had the foundations. Now, you do. You played with these sliders last week, and hopefully you've all actually gone, into this notebook, and dragged them around, and tried to get an intuition for, like this idea of, like, moving them up and down, makes the loss go up and down, and so forth. So, imagine that your job was to move these sliders, to get this as nice as possible, but when, it was given to you, the person who gave it to you ...   \n",
       "42   [and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 1...   \n",
       "43   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the ...   \n",
       "44   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the...   \n",
       "45   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 17th...   \n",
       "46   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, did was it tried to predict the next word of a Wikipedia article. In fact every next, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 17th pr...   \n",
       "47   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, word of every Wikipedia article. Doing that is very difficult. You know, there are Wikipedia, articles which would say things like, you know the 1...   \n",
       "48   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, articles which would say things like, you know the 17...   \n",
       "49   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "50   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "51   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "52   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "53   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "54   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "55   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "56   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "57   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "58   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "59   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "60   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "61   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "62   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "63   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "64   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "65   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "66   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "67   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "68   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "69   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "70   [fast.ai course. It was later turned into an academic paper by me in conjunction with a, then PhD student named Sebastian Ruder, who is now one of the world's top NLP researchers, and went on to help inspire a huge change, you know, huge kind of step improvement in, NLP capabilities around the world, along with a number of other important innovations at, the time. This is the basic process that ULMFiT described. Step One was to build something, called a language model using basically nearly all of Wikipedia and what the language model, did was it tried to predict the next word of a Wikiped...   \n",
       "71   [just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were deleted, essentially. So it's a, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, the...   \n",
       "72   [they can take really good advantage of modern accelerators like, like Google's TPUs., instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were deleted, essentially. So it's a, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than ...   \n",
       "73   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, the model to predict which/what were the words that were deleted, essentially. So it's a, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than that the basic idea was the, same. So today we're going to be looking at models using what's bec...   \n",
       "74   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, pretty similar idea. Other than that the basic concept was the same as ULMFiT. They replaced, our RNN approach with a Transformer model, they replaced our language model approach with what's called a masked language model, but other than th...   \n",
       "75   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, our RNN approach with a Transformer model, they replaced ou...   \n",
       "76   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "77   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "78   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "79   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "80   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "81   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "82   [they can take really good advantage of modern accelerators like, like Google's TPUs., They didn't really, kind of, allow you to predict the next word of a sentence. It's, just not how they're structured, for reasons we'll talk about properly in part two of the course. So they threw away the idea of predicting the next word of a sentence and then they,, instead they did something just as good and pretty clever. They took, kind of, chunks of Wikipedia, or whatever text they're looking at and deleted at random a few words and asked, the model to predict which/what were the words that were de...   \n",
       "83   [in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of those rectified...   \n",
       "84   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And the...   \n",
       "85   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then set...   \n",
       "86   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets o...   \n",
       "87   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, some examples of bits of pictures that matched, and then Layer Two combined those and now, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of th...   \n",
       "88   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, you know how those were combined, right, these were rectified linear units that were added together, okay? And then sets of thos...   \n",
       "89   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, called activations, where then ...   \n",
       "90   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "91   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "92   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "93   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "94   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "95   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "96   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "97   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "98   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "99   [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "100  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "101  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "102  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "103  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "104  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "105  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "106  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "107  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "108  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "109  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "110  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "111  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "112  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "113  [for classification? Sure. So yeah we will be getting into that in more detail and in fact maybe a good place, to start would be the next slide, kind of give you a sense of this. You might remember, in Lesson One we looked at this fantastic Zeiler and Fergus paper where we looked at, visualizations of the first layer of a imagenet classification model and Layer One had sets, of weights that found diagonal edges, and here are some examples of bits of photos that, successfully matched with, and opposite diagonal edges, and kind of color gradients, and here's, some examples of bits of pictur...   \n",
       "114  [And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model....   \n",
       "115  [actually on as I speak, and I created this notebook called Getting started with NLP, And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, y...   \n",
       "116  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data, about a real problem tha...   \n",
       "117  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., using their actual data. So, a lot of people are a bit dismissive of Kaggle competitions, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizing the model. But, you know, in terms of like, getting real data, about a real problem that real...   \n",
       "118  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, as being, kind of, like, not very real, and it's certainly true you're not worrying about stuff like productionizi...   \n",
       "119  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, about a real pr...   \n",
       "120  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...   \n",
       "121  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...   \n",
       "122  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...   \n",
       "123  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...   \n",
       "124  [actually on as I speak, and I created this notebook called Getting started with NLP, for absolute beginners. And so the competition is called the U.S. Patent Phrase to Phrase Matching Competition., And, so I'm going to take you through, you know, a complete submission to this competition., And Kaggle competitions are interesting, particularly the ones that are not playground competitions, but the real competitions with real money applied they're interesting because this, is an actual project, that an actual organization, is prepared to invest money in getting solved,, using their act...   \n",
       "125  [say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identificat...   \n",
       "126  [and trying to identify a category that object belongs to. So, previously we've mainly been, A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author ident...   \n",
       "127  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identific...   \n",
       "128  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identification would...   \n",
       "129  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., all kinds of stuff you could do with that. So, for example we've already mentioned sentiment analysis. That's ~(a ca) a classification task  we try to decide on the category:, positive or negative sentiment. Author identification would be ...   \n",
       "130  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, positive or negative sentiment. Author identification would be taking a document and trying to find, the category of author. Legal discovery wou...   \n",
       "131  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, the category of author. Legal discovery would be t...   \n",
       "132  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "133  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "134  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "135  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "136  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "137  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "138  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "139  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "140  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "141  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "142  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "143  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "144  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "145  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "146  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "147  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "148  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "149  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "150  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "151  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "152  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "153  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "154  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "155  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "156  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "157  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "158  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "159  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "160  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "161  [and trying to identify a category that object belongs to. So, previously we've mainly been, looking at images. Today we're going to be looking at documents. Now, in NLP when we, say document, we don't specifically mean, you know, a 20 page long, you know, essay., A document could be three or four words, or a document could be the entire encyclopedia., So a document is just an input to an NLP model that contains text. Now, classifying a document,, so deciding what category a document belongs to, is a surprisingly rich thing to do. There's, all kinds of stuff you could do with that. So, for...   \n",
       "162  [like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if you want to. So I basically create, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd, say if iskaggle and put those changes., So here, you can see h...   \n",
       "163  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if you want to. So I basically create, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd, say if iskaggle and put those changes., So here, yo...   \n",
       "164  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, stuff to Paperspace or your own computer as well if you want to. So I basically create, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any little changes I need to make I'd, say if iskaggle and put those changes., So here, you ...   \n",
       "165  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, this little boolean, always, in my notebooks called iskaggle which is going to be True if it's running on Kaggle and False otherwise and any li...   \n",
       "166  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, say if iskaggle and put those changes., So here, y...   \n",
       "167  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "168  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "169  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "170  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "171  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "172  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "173  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "174  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "175  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "176  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "177  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "178  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "179  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "180  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "181  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "182  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "183  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "184  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "185  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "186  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "187  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "188  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "189  [So you can click on the accelerator button and choose GPU to make sure that you're using a GPU. If you click copy and edit on my document I think that will happen for you automatically., Personally, you know, I like using things like Paperspace generally better than Kaggle,, like, Kaggle's pretty good but you know you only get 30 hours a week of GPU time, and the notebook editor for me is not as good as the real JupyterLab environment. So there's, some information here, I won't go through but it basically describes how you can download, stuff to Paperspace or your own computer as well if ...   \n",
       "190  [So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump a...   \n",
       "191  [like four key libraries that you have to know to do data science in python., plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and an...   \n",
       "192  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and ...   \n",
       "193  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and ...   \n",
       "194  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., not familiar with these libraries just read the whole book, it doesn't take too long to, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and an...   \n",
       "195  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, get through, and it's got lots of cool tips and it's very readable. I do find a lot of, people doing this course often I see people kind of, trying to jump ahead, and a...   \n",
       "196  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, people doing this course often I see people kind of, trying to jump ahead, and and,...   \n",
       "197  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "198  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "199  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "200  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "201  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "202  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "203  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "204  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "205  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "206  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "207  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "208  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "209  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "210  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "211  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "212  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "213  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "214  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "215  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "216  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "217  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "218  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "219  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "220  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "221  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "222  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "223  [like four key libraries that you have to know to do data science in python., And specifically, those four libraries are: numpy matplotlib pandas and pytorch., So numpy is what we use for basic, kind of, numerical programming; matplotlib we use for, plotting; pandas we use for tables of data; and pytorch we use for deep learning., Those are all covered in a fantastic book by the author of pandas which, the new version, is actually available for free, I believe. Python for data analysis. So if you're, not familiar with these libraries just read the whole book, it doesn't take too long ...   \n",
       "224  [for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even, the pieces are not next to each other. Another reason is that, what we're going t...   \n",
       "225  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of not even, the pieces are not next to each other. Another reason is that...   \n",
       "226  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some...   \n",
       "227  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., though. The first is that some languages like chinese don't have words, right, or at least, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind ...   \n",
       "228  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, certainly not space separated words. And in fact in chinese it's sometimes it's a bit fuzzy to even say where a word begins and ends. And some words are kind of ...   \n",
       "229  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, the pieces are not next to each other. Another reason is that, what we're ...   \n",
       "230  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "231  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "232  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "233  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "234  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "235  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "236  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "237  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "238  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "239  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "240  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "241  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "242  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "243  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "244  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "245  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "246  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "247  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "248  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "249  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "250  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "251  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "252  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "253  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "254  [numbers and we're going to multiply them by matrices, we're going to replace the negatives, with zeros and add them up, and we're going to do that a few times. That's our neural network. With some little wrinkles, but that's the basic idea. So how on earth do we do that, for these strings? So there's basically two steps we're going to take., The first step is to split each of these into tokens. Tokens are basically words. We're, going to split it into words. There's a few problems with splitting things into words,, though. The first is that some languages like chinese don't have words, ri...   \n",
       "255  [but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there ...   \n",
       "256  [models, so even many more even than timm's image models. And so, these models, they vary, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there are some just generally pretty good models that work for a lot of, things a lot of the ...   \n",
       "257  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there are some just generally pretty good models that work for a lot of, things a lot of the time, a...   \n",
       "258  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, thanks to the Hugging Face model hub, you can start your pre-trained model with something, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there are s...   \n",
       "259  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, that's actually pretty similar to what you actually want to do, or at least was trained on the same kind of documents., Having said that, there a...   \n",
       "260  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, Having said that, there are some just generally pretty go...   \n",
       "261  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "262  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "263  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "264  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "265  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "266  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "267  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "268  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "269  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "270  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "271  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "272  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "273  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "274  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "275  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "276  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "277  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "278  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "279  [models, so even many more even than timm's image models. And so, these models, they vary, in a couple of ways. There's a variety of different architectures, just like in timm, but then something which is different to timm is that each of those architectures can be trained on different corpuses for solving different problems. So for example I could, type patent and see if there's any pre-trained patent: there is. Okay, so there's a patent,, there's a whole lot of pre-trained patent models. Isn't that amazing? So, quite often,, thanks to the Hugging Face model hub, you can start your ...   \n",
       "280  [is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 de...   \n",
       "281  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 devel...   \n",
       "282  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 develo...   \n",
       "283  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a word versus the, start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 developers used.,...   \n",
       "284  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., start of a word, that kind of means a different thing. So this is what happens when we tokenize, this sentence using the tokenizer that the deberta-v3 developers used., So here's a less common (unless you're a big platypus fan like me), less common se...   \n",
       "285  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, this sentence using the tokenizer that the deberta-v3 developers used., So here's a less common (unless you're a big platypus fan like me), less common sen...   \n",
       "286  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "287  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "288  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "289  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "290  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "291  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "292  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "293  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "294  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "295  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "296  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "297  [So, if I pass the string Gday folks, Im Jeremy from fast.ai! you'll see it's, kind of putting it into words, kind of not. So if you've ever wondered whether g'day, is one word or two you know it's actually three tokens according to this tokenizer., And I'm is three tokens. And fast.ai has three tokens. This punctuation is a token., And so, you kind of get the idea. These underscores here? That represents the start of a word,, right. So that's kind of there's this concept that, like, the start of a word is kind of part of the token. So if you see a capital I in the middle of a ...   \n",
       "298  [of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ~...   \n",
       "299  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ...   \n",
       "300  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth, so with, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going, to contain exactly the same as our original data set.) No sorry, it...   \n",
       "301  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, batched=True it'll be able to do more stuff at once. So look it only took six seconds,, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going, to contain exactly the same as our original data set.) No sorry, it's not going to take, exactly the same as the original data set, it's going to contain exa...   \n",
       "302  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, so pretty fast. So now, when we look at a row of our tokenized data set, ~(it's going, to contain exactly the same as our original data set.) No sorry, it's not goin...   \n",
       "303  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "304  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "305  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "306  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "307  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "308  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "309  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "310  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "311  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "312  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "313  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "314  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "315  [that takes a document, grabs its input, and tokenizes it. Okay so we'll call this our, tokenization function. Tokenization can take a minute or two so we may as well get all, of our processes used doing it at the same time to save some time. So if you use the, dataset dot map it will parallelize that process, and just pass in your function. Make sure you pass batched=True so it can do a bunch at a time. Behind the scenes, this is going through something called the tokenizers library which is a pretty optimized Rust library that uses, you know, SIMD and parallel processing and so forth...   \n",
       "316  [is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, infor...   \n",
       "317  [you've just tokenized. So one question was really about: How you choose those keywords, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, information, as long as it's there, right? I could h...   \n",
       "318  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, information, as long as it's there, right? I could ha...   \n",
       "319  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, where abatement of pollution ended and where abatement started, right? So I did just something that I can learn from. This is a nice thing about neural nets, they're, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, info...   \n",
       "320  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, so flexible As long as you give it the information somehow, it doesn't really matter how you give it the, information, as long as it's there, right? I could have used punctuation, I could ha...   \n",
       "321  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, information, as long as it's there, right? I could have used punctuation, I could have put, like, I don't...   \n",
       "322  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "323  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "324  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "325  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "326  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "327  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "328  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "329  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "330  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "331  [you've just tokenized. So one question was really about: How you choose those keywords, and the order of the fields that you know, so I guess just interested in an explanation,, is it more art or science? how are you No, it's arbitrary! I tried a few things I tried X, you know, I tried putting them backwards,, you know, doesn't matter! We just want some way, something that it can learn from, right?, So if I just concatenated it without these headers before each one, it wouldn't know, where abatement of pollution ended and where abatement started, right? So I did just something that I...   \n",
       "332  [basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got noth...   \n",
       "333  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory., So, Hugging Face transformer...   \n",
       "334  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, documents of over 2,000 words you might want to look at ULMFiT. Try transformers, see if, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory., So, Hugging Face transformers has these, you know As I say it right now, I find them, somewhat obscure and not particularl...   \n",
       "335  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, it works for you, but you know I'd certainly try both. For under 2,000 words, you know,, transformers should be fine unless you've got nothing but a laptop GPU, or something with not much memory., So, Hugging Face transformers has...   \n",
       "336  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, transformers should be fine unless you've go...   \n",
       "337  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "338  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "339  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "340  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "341  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "342  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "343  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "344  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "345  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "346  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "347  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "348  [best approach for reasonably quickly and easily using large documents. Otherwise, if you use, transformer-based approaches, large documents are challenging. Specifically, transformers, basically have to do the whole document at once, whereas ULMFiT can split it into multiple pieces and read it gradually. So that means you'll find that people trying to work with, large documents tend to spend a lot of money on GPUs because they need the big fancy ones with lots of memory. So yes, generally speaking, I would say if you're trying to do stuff with, documents of over 2,000 words you might want...   \n",
       "349  [that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, those of you that remember, a f...   \n",
       "350  [Test and validation sets are all about identifying and controlling for something called overfitting, So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, ...   \n",
       "351  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, ...   \n",
       "352  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, those of you that...   \n",
       "353  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, to use and we're going to use this to look at overfitting., The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of a polynomial. So for, those ...   \n",
       "354  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, The details of this function don't matter too much, what matters is what we do with it, which is that it allows us to basically pass in the degree of...   \n",
       "355  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, those of you that remember, a first degree polynomial is just a line,...   \n",
       "356  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., A second ...   \n",
       "357  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "358  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "359  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "360  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "361  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "362  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "363  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "364  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "365  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "366  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "367  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "368  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "369  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "370  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "371  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "372  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "373  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "374  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "375  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "376  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "377  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "378  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "379  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "380  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "381  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "382  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "383  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "384  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "385  [Test and validation sets are all about identifying and controlling for something called overfitting, and we're going to try and learn about this through example. This is the same information, that's in that Kaggle notebook  I've just put it on some slides here., So I'm going to create a function here called plot_poly and I'm actually going to use the, same data that, I don't know if you remember, we used it earlier for trying to fit this, quadratic. We created some x and some y data. This is the data we're going, to use and we're going to use this to look at overfitting., The detai...   \n",
       "386  [removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only...   \n",
       "387  [We then fit our model using only those points we haven't removed, and then we measure how, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only on t...   \n",
       "388  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only on the validation set. This is really...   \n",
       "389  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measured only on the validation set. This is...   \n",
       "390  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The splitters are the things that separate out the validation set. Fast.ai won't let you train a model without a validation set. Fast.ai always shows you your metrics, so, things like accuracy, measure...   \n",
       "391  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, things like accuracy, measured only on the validation set. This is really unusual. Most, libraries make it reall...   \n",
       "392  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, libraries make it really easy...   \n",
       "393  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...   \n",
       "394  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...   \n",
       "395  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...   \n",
       "396  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...   \n",
       "397  [We then fit our model using only those points we haven't removed, and then we measure how, good it is by looking at only the points we removed. So in this case let's say we had, removed (I'm just trying to think) If I had removed this point here right, then it might have, kind of gone off down over here, and so then when we look at how well it fits we would say: oh! This one's miles away., The model the data that we take away and don't let the model see it when it's training, is called the validation set. So in fast.ai we've seen splitters before, right, The splitters are the things ...   \n",
       "398  [it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate, and remove the last couple of weeks. So if this was your validation set and this is your, training set, that's going to be actually testing whet...   \n",
       "399  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate, and remove the last couple of weeks. So if this was your validation set and this is your, trai...   \n",
       "400  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, if you created your validation set by randomly removing stuff from the middle, it's not really a good indication of how you're going to be using this model. Instead you should truncate, and remove the last couple of weeks. So if this was your validation set and this is your, training set, that's going to be actually testing whether you can use this to predict, the...   \n",
       "401  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, and remove the last couple of weeks. So if this was your validation set and this is your, training set, that's going to be actually testing whether you can use this to predict, the future, rather than using it to predict the past., Kaggle competitions are a fantastic way t...   \n",
       "402  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, training set, that's going to be actually testing whether you can use this to predict, the future, ra...   \n",
       "403  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "404  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "405  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "406  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "407  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "408  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "409  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "410  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "411  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "412  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "413  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "414  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "415  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "416  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "417  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "418  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "419  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "420  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "421  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "422  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "423  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "424  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "425  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "426  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "427  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "428  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "429  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "430  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "431  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "432  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "433  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "434  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "435  [some of your data out of your model, out of the data that you passed that you train for your model. The reason why is imagine that this was, the data you were trying to fit something to (okay) and you randomly remove some, so, it looks like this. That looks very easy doesn't it, because you've kind of like, still got, all the data you'd want around the points, and in a time series like this this is dates and sales in real life you're probably going to want to predict future dates. So, if you created your validation set by randomly removing stuff from the middle, it's not really a good...   \n",
       "436  [and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard du...   \n",
       "437  [your model. Why not? Well imagine you tried two new models every day for three months, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the ...   \n",
       "438  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the competitio...   \n",
       "439  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, overfit using the validation set. So you actually want to know whether you've really found a, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the competition and a second test set which, you don't get to see until after the competition is...   \n",
       "440  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, good model or not. So in fact on Kaggle they have two test sets. They've got the one that, gives you feedback on the leaderboard during the competition and a second test set which, you don't get to see until after the competition ...   \n",
       "441  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, gives you feedback on the leaderboard during the com...   \n",
       "442  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "443  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "444  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "445  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "446  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "447  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "448  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "449  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "450  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "451  [your model. Why not? Well imagine you tried two new models every day for three months, (that's how long a Kaggle competition goes for.) So you would have tried 180 models,, and then you look at the accuracy on the validation set for each one. Some of those models you, would have got a good accuracy on the validation set, potentially because of pure chance, just, a coincidence, and then you get all excited and you submit that to Kaggle and you think you're going to win the competition, and you mess it up! And that's because you actually, overfit using the validation set. So you actually wa...   \n",
       "452  [this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For example,, consider accuracy. Now, if we were using accuracy to calculate our derivative and get ...   \n",
       "453  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For exam...   \n",
       "454  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For example,,...   \n",
       "455  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, we will take the derivative of, and find the gradient, and use that to improve our parameters, during training? And the answer is: maybe, sometimes, but probably not. For example,, consider accuracy. Now, if we were using accuracy to calcula...   \n",
       "456  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, during training? And the answer is: maybe, sometimes, but probably not. For example,, consider accuracy. Now, if we were using accuracy to calculate o...   \n",
       "457  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, c...   \n",
       "458  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "459  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "460  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "461  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "462  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "463  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "464  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "465  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "466  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "467  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "468  [So you've got a validation set, what are you going to do with it? What you're going to do with a validation set, is you're going to measure some metrics. So a metric is something, like accuracy. It's a number that tells you: How good is your model? Now on Kaggle, this is very easy. What metric should we use? Well they tell us go to overview, click, on evaluation, and find out and it says: oh, we will evaluate on the Pearson Correlation, Coefficient. Therefore this is the metric you care about So, one obvious question is: Is this the same as the loss function? Is this the thing that, w...   \n",
       "469  [in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found people's lives turned upside, down because of how badly they get screwed up by models that ...   \n",
       "470  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found p...   \n",
       "471  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found p...   \n",
       "472  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, use that to say: I've got a good model, that very often finds its way into industry, into, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found peo...   \n",
       "473  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, government where people roll out these things that are good on the one metric that, happened to be easy to measure. And again and again we found...   \n",
       "474  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, happened to be easy to measure. And again and again we fou...   \n",
       "475  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "476  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "477  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "478  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "479  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "480  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "481  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "482  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "483  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "484  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "485  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "486  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "487  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "488  [number that tells you whether it's good or bad and even if there was you wouldn't be able to find it out ahead of time. In real life the model you use is a part of a complex, process often involving humans, both as users and customers and as people, you know, involved, in as part of the process. There's all kinds of things that are changing over time, and there's lots and lots of outcomes of decisions that are made. One metric is not enough to, capture all of that. Unfortunately, because it's so convenient to pick one metric and, use that to say: I've got a good model, that very often fi...   \n",
       "489  [correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pears...   \n",
       "490  [using letter r and it's the most widely used measure of how similar two variables, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pears...   \n",
       "491  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pearson Cor...   \n",
       "492  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, Generally speaking, in courses or textbooks, when they teach you about the Pearson Correlation Coefficient, at that point at this point, they will show you a mathematical function., I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical...   \n",
       "493  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, I'm not going to do that because that tells you nothing about the Pearson Correlation Coefficient. What we actually care about is not the mathematical function, but how, it behaves; and I find most people, even who work in data science, have...   \n",
       "494  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., it behaves; and I find most people, even who work in data science, have not actually...   \n",
       "495  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "496  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "497  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "498  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "499  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "500  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "501  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "502  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "503  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "504  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "505  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "506  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "507  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "508  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "509  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "510  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "511  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "512  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "513  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "514  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "515  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "516  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "517  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "518  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "519  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "520  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "521  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "522  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "523  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "524  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "525  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "526  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "527  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "528  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "529  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "530  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "531  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "532  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "533  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "534  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "535  [using letter r and it's the most widely used measure of how similar two variables, are. And so, if your predictions are very similar to the real values then the Pearson, correlation coefficient will be high, and that's what you want. r can be between, minus-one and one. Minus-one means you predicted exactly the wrong answer, which in a Kaggle, competition could be great because then you can just reverse all of your answers and you'll be perfect. Plus-one means you've got everything exactly correct., Generally speaking, in courses or textbooks, when they teach you about the Pearson ...   \n",
       "536  [So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job ...   \n",
       "537  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pret...   \n",
       "538  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got ...   \n",
       "539  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle competition where the metric is correlation,, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to mak...   \n",
       "540  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, and you just get a couple of rows really badly wrong, then it's going to be a disaster to your score, right. So you've got to make sure that you do a pretty good job of every row., So there's what a correlation of 0.68 loo...   \n",
       "541  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, So there's what a correlation of 0.68 looks like. Okay, here's a correlation of 0.34, and this is kind of interesting, isn't it? Becau...   \n",
       "542  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "543  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "544  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "545  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "546  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "547  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "548  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "549  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "550  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "551  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "552  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "553  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "554  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "555  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "556  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "557  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "558  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "559  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "560  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "561  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "562  [more correlation here, but there's a few examples of some houses with lots and lots of rooms, where people that aren't very rich live. Maybe these are some kind of shared shared accommodation or something?, So, r is very sensitive to outliers. So let's get rid of the houses the rooms, with 15 rooms the houses with 15 rooms or more, and now you can see it's gone up, from 0.43 to 0.68, even though we probably only got rid of one two three four five six, seven got rid of seven data points! So we've got to be very careful of outliers! And that means if you're trying to win a Kaggle comp...   \n",
       "563  [sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll, get an out of memory error on your GPU. So, you know, it's a bit of trial and err...   \n",
       "564  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll, get an out of memory error on your GPU. So, you know, it's a bit of trial ...   \n",
       "565  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make it to...   \n",
       "566  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, a mini batch'' and the number of rows is called the batch size. So here, we're going to set the batch size to 128. Generally speaking, the larger your batch size, the, more it can do in parallel (at once) and it'll be faster, but if you make ...   \n",
       "567  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, more it can do in parallel (at once) and it'll be faster, but if you make it too big you'll, get an out of memory error on your GPU. So, you know, it's a bit of...   \n",
       "568  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, get an out of memory error on your GPU. So, you know, it's a bit of trial and er...   \n",
       "569  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "570  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "571  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "572  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "573  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "574  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "575  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "576  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "577  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "578  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "579  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "580  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "581  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "582  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "583  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "584  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "585  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "586  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "587  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "588  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "589  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "590  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "591  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "592  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "593  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "594  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "595  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "596  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "597  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "598  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "599  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "600  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "601  [Okay. We're now ready to train our model. In fast.ai, we use something called a learner., The equivalent in Hugging Face transformers is called trainer. So we'll bring that in; something we'll learn about quite shortly is the idea of mini batches and batch, sizes. In short, each time we pass some data to our model for training, it's going, to return it's going to send through a few rows at a time to the GPU, so that it, can calculate those in parallel. Those a bunch of rows is called a batch or, a mini batch'' and the number of rows is called the batch size. So here, we're ...   \n",
       "602  [set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interesting! it seems like there's a separate, group of districts wi...   \n",
       "603  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be say...   \n",
       "604  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be sa...   \n",
       "605  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, So, outliers should never just be removed, like, for modeling So if we take the example, of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interestin...   \n",
       "606  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, of the California housing data set, you know, if I was really working with that dataset in real life, I would be saying, oh that's interestin...   \n",
       "607  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., group of districts wi...   \n",
       "608  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "609  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "610  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "611  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "612  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "613  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "614  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "615  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "616  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "617  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "618  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "619  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "620  [before when you were showing us the visual interpretation of the Pearson Coefficient, and you were talking about outliers. And we've got a question here from Kevin, asking: how, do you decide when it's okay to remove outliers? Like, you pointed out some in that data, set, and clearly your model is going to train a lot better if you clean that up; but I think, Kevin's point here is, you know, those kinds of outliers will probably exist in the test, set as well, so I think he's just looking for some practical advice on on how you handle that in a more general sense., So, outliers should n...   \n",
       "621  [changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,, I looked at quite a few Kaggle notebooks from other people, for this competition, and nearly...   \n",
       "622  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,, I looked at quite a few Kaggle notebooks from other people, for this competition, and nea...   \n",
       "623  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, us our predictions, which we can cast to float., And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look at them. Always. Right? And interestingly,, I looked at quite a few Kaggle notebooks from other people, for this competition, and...   \n",
       "624  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, And here they are. So here are the predictions we made of similarity. Now again, not just, for your inputs but also for your outputs, always look ...   \n",
       "625  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, for your inputs but also for your outputs, always look at th...   \n",
       "626  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., I looked at...   \n",
       "627  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "628  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "629  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "630  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "631  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "632  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "633  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "634  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "635  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "636  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "637  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "638  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "639  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "640  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "641  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "642  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "643  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "644  [you know, really a lot like a fast.ai learner and you know hopefully the impression you'll get from going through this process is largely a sense of familiarity, of like, oh yeah, this looks like stuff I've seen before, you know, like a bit more wordy and some slight, changes but it really is very very similar to the way we've done it before. Because now that we've got a trained trainer, rather than learner, we can call predict and now we're, going to pass in our dataset from the Kaggle test file  and so that's going to give, us our predictions, which we can cast to float., And here th...   \n",
       "645                                                                                                                                                                                                                                                                                                                                                                                      [know, or it took 10 times more time or it took 10 times more money or whatever., So I think NLP is a huge opportunity area., It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.]   \n",
       "646                                                                                                                                                                                                                                                                                          [so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, So I think NLP is a huge opportunity area., It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.]   \n",
       "647                                                                                                                                                     [so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you, It's worth thinking about both use and misuse of modern NLP, and I want to show you a subreddit.]   \n",
       "648                                                                                                                                                                  [so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you, know, or it took 10 times more time or it took 10 times more money or whatever.]   \n",
       "649                                                                                                                      [so if you're looking to build a startup, for example, one of the key things that VCs look for, you know, that they'll ask is like oh why now?.. you know... why... why would, you build this company now? And of course you know with NLP, the answer is really simple. It's like... it can often be like... well until last year this wasn't possible you, know, or it took 10 times more time or it took 10 times more money or whatever., So I think NLP is a huge opportunity area.]   \n",
       "650  [And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which could create context, appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side, of an argument and...   \n",
       "651  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which could create context, appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side, of an argument and you could scale that up such that 99% of twitter was these bots and, nobody woul...   \n",
       "652  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, believable prose., You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which could create context, appropriate prose on twitter or facebook groups or whatever, you know, arguing for a side, of an argument and you could scale that up such that 99% o...   \n",
       "653  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, You know I would strongly believe that like any of our... kind of like... upper tier of, competent fast.ai alumni would be fairly easily able to create a bot which coul...   \n",
       "654  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, competent fast.ai alumni would be fairly easily able to create a bot whic...   \n",
       "655  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., appropriate prose on twitter or facebook groups or wha...   \n",
       "656  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "657  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "658  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "659  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "660  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "661  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "662  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "663  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "664  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "665  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "666  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "667  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "668  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "669  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "670  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "671  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "672  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "673  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "674  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "675  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "676  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "677  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "678  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "679  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "680  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "681  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "682  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "683  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "684  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "685  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "686  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "687  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "688  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "689  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "690  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "691  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "692  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "693  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "694  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "695  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "696  [Here is a conversation on a subreddit from a couple of years ago. I'll let you have a quick read of it., So the question I want you to be thinking about is: What subreddit do you think this comes from? this debate about military spending?, And the answer is it comes from a subreddit that posts automatically generated conversations between GPT2 models. Now this is like a totally previous generation of model  they're much, much better now  so even then you could see these models were generating context appropriate, believable prose., You know I would strongly believe that like any of our...   \n",
       "697  [a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns i...   \n",
       "698  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which, is actually why we ended up wi...   \n",
       "699  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which, is actually why we ended up with predi...   \n",
       "700  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, things that's a bit tricky. I was trying to figure this out just the other day. It's not documented as far as I can tell but on the Hugging Face transformers website... but if, you pass in one label to AutoModelForSequenceClassification, it turns it into a r...   \n",
       "701  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., you pass in one label to AutoModelForSequenceClassification, it turns it into a regression problem which, is actually why we ended up with predictions tha...   \n",
       "702  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, is actually why we ended up with predictions that were less than...   \n",
       "703  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...   \n",
       "704  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...   \n",
       "705  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...   \n",
       "706  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...   \n",
       "707  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...   \n",
       "708  [that's the one. The question Manikandan is asking... shouldn't num labels be five zero, zero point two five zero point five zero point seven five one instead of one? Isn't the target, a categorical, or are we considering this as a regression problem? Yeah it's a good question. So there's one label because there's one column. Even if, this was being treated as a categorical problem with five categories, it's still considered one label., In this case though, we're actually treating it as a regression problem. It's one of the, things that's a bit tricky. I was trying to figure this out just...   \n",
       "\n",
       "     pred_start  \n",
       "0          True  \n",
       "1         False  \n",
       "2         False  \n",
       "3         False  \n",
       "4         False  \n",
       "5         False  \n",
       "6         False  \n",
       "7         False  \n",
       "8         False  \n",
       "9         False  \n",
       "10        False  \n",
       "11        False  \n",
       "12        False  \n",
       "13        False  \n",
       "14        False  \n",
       "15        False  \n",
       "16        False  \n",
       "17        False  \n",
       "18        False  \n",
       "19        False  \n",
       "20        False  \n",
       "21        False  \n",
       "22        False  \n",
       "23         True  \n",
       "24         True  \n",
       "25        False  \n",
       "26        False  \n",
       "27        False  \n",
       "28        False  \n",
       "29        False  \n",
       "30        False  \n",
       "31        False  \n",
       "32        False  \n",
       "33        False  \n",
       "34        False  \n",
       "35        False  \n",
       "36        False  \n",
       "37        False  \n",
       "38        False  \n",
       "39        False  \n",
       "40        False  \n",
       "41        False  \n",
       "42        False  \n",
       "43        False  \n",
       "44        False  \n",
       "45        False  \n",
       "46        False  \n",
       "47         True  \n",
       "48        False  \n",
       "49        False  \n",
       "50        False  \n",
       "51        False  \n",
       "52        False  \n",
       "53        False  \n",
       "54         True  \n",
       "55        False  \n",
       "56        False  \n",
       "57        False  \n",
       "58        False  \n",
       "59        False  \n",
       "60        False  \n",
       "61        False  \n",
       "62        False  \n",
       "63        False  \n",
       "64        False  \n",
       "65        False  \n",
       "66        False  \n",
       "67         True  \n",
       "68        False  \n",
       "69         True  \n",
       "70        False  \n",
       "71        False  \n",
       "72        False  \n",
       "73        False  \n",
       "74        False  \n",
       "75        False  \n",
       "76        False  \n",
       "77        False  \n",
       "78        False  \n",
       "79        False  \n",
       "80        False  \n",
       "81        False  \n",
       "82        False  \n",
       "83        False  \n",
       "84        False  \n",
       "85         True  \n",
       "86        False  \n",
       "87        False  \n",
       "88        False  \n",
       "89        False  \n",
       "90         True  \n",
       "91        False  \n",
       "92         True  \n",
       "93        False  \n",
       "94        False  \n",
       "95        False  \n",
       "96        False  \n",
       "97        False  \n",
       "98        False  \n",
       "99        False  \n",
       "100       False  \n",
       "101        True  \n",
       "102       False  \n",
       "103       False  \n",
       "104       False  \n",
       "105        True  \n",
       "106       False  \n",
       "107       False  \n",
       "108       False  \n",
       "109       False  \n",
       "110       False  \n",
       "111        True  \n",
       "112       False  \n",
       "113       False  \n",
       "114       False  \n",
       "115       False  \n",
       "116       False  \n",
       "117       False  \n",
       "118       False  \n",
       "119       False  \n",
       "120       False  \n",
       "121       False  \n",
       "122       False  \n",
       "123       False  \n",
       "124       False  \n",
       "125       False  \n",
       "126       False  \n",
       "127       False  \n",
       "128       False  \n",
       "129       False  \n",
       "130       False  \n",
       "131       False  \n",
       "132       False  \n",
       "133       False  \n",
       "134       False  \n",
       "135       False  \n",
       "136       False  \n",
       "137       False  \n",
       "138       False  \n",
       "139       False  \n",
       "140        True  \n",
       "141       False  \n",
       "142       False  \n",
       "143       False  \n",
       "144        True  \n",
       "145       False  \n",
       "146       False  \n",
       "147       False  \n",
       "148        True  \n",
       "149       False  \n",
       "150       False  \n",
       "151       False  \n",
       "152        True  \n",
       "153       False  \n",
       "154        True  \n",
       "155       False  \n",
       "156       False  \n",
       "157       False  \n",
       "158       False  \n",
       "159        True  \n",
       "160       False  \n",
       "161       False  \n",
       "162       False  \n",
       "163       False  \n",
       "164       False  \n",
       "165        True  \n",
       "166       False  \n",
       "167       False  \n",
       "168       False  \n",
       "169        True  \n",
       "170        True  \n",
       "171        True  \n",
       "172       False  \n",
       "173       False  \n",
       "174       False  \n",
       "175       False  \n",
       "176       False  \n",
       "177       False  \n",
       "178       False  \n",
       "179       False  \n",
       "180       False  \n",
       "181       False  \n",
       "182       False  \n",
       "183       False  \n",
       "184       False  \n",
       "185       False  \n",
       "186       False  \n",
       "187       False  \n",
       "188       False  \n",
       "189       False  \n",
       "190       False  \n",
       "191       False  \n",
       "192       False  \n",
       "193       False  \n",
       "194       False  \n",
       "195       False  \n",
       "196       False  \n",
       "197       False  \n",
       "198       False  \n",
       "199       False  \n",
       "200       False  \n",
       "201       False  \n",
       "202       False  \n",
       "203       False  \n",
       "204       False  \n",
       "205       False  \n",
       "206       False  \n",
       "207       False  \n",
       "208       False  \n",
       "209       False  \n",
       "210       False  \n",
       "211       False  \n",
       "212       False  \n",
       "213       False  \n",
       "214       False  \n",
       "215       False  \n",
       "216       False  \n",
       "217       False  \n",
       "218        True  \n",
       "219       False  \n",
       "220       False  \n",
       "221       False  \n",
       "222       False  \n",
       "223       False  \n",
       "224       False  \n",
       "225       False  \n",
       "226       False  \n",
       "227       False  \n",
       "228       False  \n",
       "229       False  \n",
       "230       False  \n",
       "231       False  \n",
       "232       False  \n",
       "233       False  \n",
       "234       False  \n",
       "235       False  \n",
       "236        True  \n",
       "237       False  \n",
       "238       False  \n",
       "239       False  \n",
       "240       False  \n",
       "241       False  \n",
       "242       False  \n",
       "243       False  \n",
       "244       False  \n",
       "245       False  \n",
       "246        True  \n",
       "247       False  \n",
       "248       False  \n",
       "249        True  \n",
       "250       False  \n",
       "251       False  \n",
       "252       False  \n",
       "253       False  \n",
       "254       False  \n",
       "255       False  \n",
       "256       False  \n",
       "257       False  \n",
       "258       False  \n",
       "259       False  \n",
       "260       False  \n",
       "261       False  \n",
       "262       False  \n",
       "263       False  \n",
       "264        True  \n",
       "265       False  \n",
       "266       False  \n",
       "267       False  \n",
       "268       False  \n",
       "269        True  \n",
       "270       False  \n",
       "271       False  \n",
       "272       False  \n",
       "273       False  \n",
       "274       False  \n",
       "275       False  \n",
       "276        True  \n",
       "277       False  \n",
       "278       False  \n",
       "279        True  \n",
       "280       False  \n",
       "281       False  \n",
       "282       False  \n",
       "283       False  \n",
       "284       False  \n",
       "285       False  \n",
       "286       False  \n",
       "287       False  \n",
       "288       False  \n",
       "289       False  \n",
       "290       False  \n",
       "291       False  \n",
       "292       False  \n",
       "293       False  \n",
       "294       False  \n",
       "295       False  \n",
       "296       False  \n",
       "297       False  \n",
       "298       False  \n",
       "299       False  \n",
       "300       False  \n",
       "301       False  \n",
       "302       False  \n",
       "303       False  \n",
       "304       False  \n",
       "305        True  \n",
       "306       False  \n",
       "307       False  \n",
       "308       False  \n",
       "309       False  \n",
       "310       False  \n",
       "311       False  \n",
       "312       False  \n",
       "313       False  \n",
       "314       False  \n",
       "315       False  \n",
       "316       False  \n",
       "317       False  \n",
       "318       False  \n",
       "319       False  \n",
       "320       False  \n",
       "321       False  \n",
       "322        True  \n",
       "323       False  \n",
       "324       False  \n",
       "325       False  \n",
       "326       False  \n",
       "327       False  \n",
       "328       False  \n",
       "329       False  \n",
       "330       False  \n",
       "331       False  \n",
       "332       False  \n",
       "333        True  \n",
       "334       False  \n",
       "335       False  \n",
       "336       False  \n",
       "337       False  \n",
       "338       False  \n",
       "339       False  \n",
       "340       False  \n",
       "341       False  \n",
       "342       False  \n",
       "343       False  \n",
       "344       False  \n",
       "345       False  \n",
       "346       False  \n",
       "347       False  \n",
       "348        True  \n",
       "349       False  \n",
       "350       False  \n",
       "351       False  \n",
       "352       False  \n",
       "353       False  \n",
       "354       False  \n",
       "355       False  \n",
       "356       False  \n",
       "357       False  \n",
       "358       False  \n",
       "359       False  \n",
       "360       False  \n",
       "361       False  \n",
       "362       False  \n",
       "363       False  \n",
       "364       False  \n",
       "365       False  \n",
       "366       False  \n",
       "367       False  \n",
       "368       False  \n",
       "369       False  \n",
       "370        True  \n",
       "371       False  \n",
       "372       False  \n",
       "373       False  \n",
       "374       False  \n",
       "375       False  \n",
       "376        True  \n",
       "377       False  \n",
       "378        True  \n",
       "379       False  \n",
       "380       False  \n",
       "381        True  \n",
       "382       False  \n",
       "383       False  \n",
       "384       False  \n",
       "385        True  \n",
       "386        True  \n",
       "387        True  \n",
       "388       False  \n",
       "389       False  \n",
       "390       False  \n",
       "391        True  \n",
       "392        True  \n",
       "393       False  \n",
       "394       False  \n",
       "395       False  \n",
       "396       False  \n",
       "397       False  \n",
       "398        True  \n",
       "399       False  \n",
       "400       False  \n",
       "401       False  \n",
       "402       False  \n",
       "403       False  \n",
       "404       False  \n",
       "405       False  \n",
       "406       False  \n",
       "407       False  \n",
       "408       False  \n",
       "409       False  \n",
       "410       False  \n",
       "411       False  \n",
       "412       False  \n",
       "413       False  \n",
       "414       False  \n",
       "415       False  \n",
       "416       False  \n",
       "417       False  \n",
       "418       False  \n",
       "419       False  \n",
       "420       False  \n",
       "421       False  \n",
       "422       False  \n",
       "423       False  \n",
       "424       False  \n",
       "425       False  \n",
       "426        True  \n",
       "427       False  \n",
       "428       False  \n",
       "429       False  \n",
       "430       False  \n",
       "431       False  \n",
       "432       False  \n",
       "433        True  \n",
       "434       False  \n",
       "435       False  \n",
       "436       False  \n",
       "437       False  \n",
       "438       False  \n",
       "439       False  \n",
       "440       False  \n",
       "441       False  \n",
       "442        True  \n",
       "443       False  \n",
       "444       False  \n",
       "445        True  \n",
       "446       False  \n",
       "447       False  \n",
       "448       False  \n",
       "449       False  \n",
       "450       False  \n",
       "451       False  \n",
       "452       False  \n",
       "453       False  \n",
       "454       False  \n",
       "455       False  \n",
       "456       False  \n",
       "457       False  \n",
       "458       False  \n",
       "459       False  \n",
       "460        True  \n",
       "461       False  \n",
       "462        True  \n",
       "463       False  \n",
       "464       False  \n",
       "465       False  \n",
       "466       False  \n",
       "467       False  \n",
       "468        True  \n",
       "469       False  \n",
       "470       False  \n",
       "471       False  \n",
       "472        True  \n",
       "473       False  \n",
       "474       False  \n",
       "475       False  \n",
       "476       False  \n",
       "477       False  \n",
       "478       False  \n",
       "479       False  \n",
       "480       False  \n",
       "481       False  \n",
       "482       False  \n",
       "483       False  \n",
       "484       False  \n",
       "485        True  \n",
       "486       False  \n",
       "487       False  \n",
       "488       False  \n",
       "489       False  \n",
       "490       False  \n",
       "491       False  \n",
       "492       False  \n",
       "493       False  \n",
       "494       False  \n",
       "495       False  \n",
       "496       False  \n",
       "497       False  \n",
       "498       False  \n",
       "499       False  \n",
       "500       False  \n",
       "501       False  \n",
       "502       False  \n",
       "503       False  \n",
       "504       False  \n",
       "505       False  \n",
       "506       False  \n",
       "507       False  \n",
       "508        True  \n",
       "509       False  \n",
       "510       False  \n",
       "511       False  \n",
       "512       False  \n",
       "513       False  \n",
       "514       False  \n",
       "515        True  \n",
       "516       False  \n",
       "517       False  \n",
       "518       False  \n",
       "519       False  \n",
       "520       False  \n",
       "521       False  \n",
       "522        True  \n",
       "523       False  \n",
       "524       False  \n",
       "525        True  \n",
       "526       False  \n",
       "527       False  \n",
       "528       False  \n",
       "529       False  \n",
       "530       False  \n",
       "531       False  \n",
       "532       False  \n",
       "533       False  \n",
       "534       False  \n",
       "535       False  \n",
       "536        True  \n",
       "537       False  \n",
       "538       False  \n",
       "539       False  \n",
       "540       False  \n",
       "541       False  \n",
       "542        True  \n",
       "543       False  \n",
       "544       False  \n",
       "545       False  \n",
       "546       False  \n",
       "547       False  \n",
       "548       False  \n",
       "549       False  \n",
       "550       False  \n",
       "551       False  \n",
       "552       False  \n",
       "553       False  \n",
       "554       False  \n",
       "555       False  \n",
       "556       False  \n",
       "557       False  \n",
       "558       False  \n",
       "559       False  \n",
       "560       False  \n",
       "561       False  \n",
       "562       False  \n",
       "563        True  \n",
       "564       False  \n",
       "565       False  \n",
       "566       False  \n",
       "567       False  \n",
       "568        True  \n",
       "569       False  \n",
       "570       False  \n",
       "571       False  \n",
       "572        True  \n",
       "573       False  \n",
       "574       False  \n",
       "575       False  \n",
       "576        True  \n",
       "577       False  \n",
       "578       False  \n",
       "579       False  \n",
       "580       False  \n",
       "581       False  \n",
       "582       False  \n",
       "583       False  \n",
       "584       False  \n",
       "585       False  \n",
       "586       False  \n",
       "587       False  \n",
       "588       False  \n",
       "589       False  \n",
       "590       False  \n",
       "591        True  \n",
       "592       False  \n",
       "593        True  \n",
       "594       False  \n",
       "595       False  \n",
       "596        True  \n",
       "597       False  \n",
       "598        True  \n",
       "599       False  \n",
       "600       False  \n",
       "601       False  \n",
       "602       False  \n",
       "603       False  \n",
       "604        True  \n",
       "605       False  \n",
       "606       False  \n",
       "607       False  \n",
       "608        True  \n",
       "609       False  \n",
       "610       False  \n",
       "611       False  \n",
       "612       False  \n",
       "613       False  \n",
       "614       False  \n",
       "615       False  \n",
       "616       False  \n",
       "617       False  \n",
       "618       False  \n",
       "619       False  \n",
       "620       False  \n",
       "621       False  \n",
       "622       False  \n",
       "623       False  \n",
       "624       False  \n",
       "625       False  \n",
       "626       False  \n",
       "627       False  \n",
       "628       False  \n",
       "629       False  \n",
       "630       False  \n",
       "631       False  \n",
       "632        True  \n",
       "633       False  \n",
       "634       False  \n",
       "635       False  \n",
       "636       False  \n",
       "637       False  \n",
       "638       False  \n",
       "639       False  \n",
       "640       False  \n",
       "641       False  \n",
       "642       False  \n",
       "643       False  \n",
       "644       False  \n",
       "645       False  \n",
       "646       False  \n",
       "647       False  \n",
       "648       False  \n",
       "649       False  \n",
       "650        True  \n",
       "651       False  \n",
       "652       False  \n",
       "653       False  \n",
       "654       False  \n",
       "655        True  \n",
       "656       False  \n",
       "657       False  \n",
       "658       False  \n",
       "659        True  \n",
       "660       False  \n",
       "661       False  \n",
       "662       False  \n",
       "663       False  \n",
       "664       False  \n",
       "665        True  \n",
       "666        True  \n",
       "667       False  \n",
       "668       False  \n",
       "669       False  \n",
       "670       False  \n",
       "671       False  \n",
       "672       False  \n",
       "673       False  \n",
       "674        True  \n",
       "675       False  \n",
       "676       False  \n",
       "677       False  \n",
       "678       False  \n",
       "679       False  \n",
       "680        True  \n",
       "681        True  \n",
       "682       False  \n",
       "683       False  \n",
       "684       False  \n",
       "685       False  \n",
       "686       False  \n",
       "687       False  \n",
       "688       False  \n",
       "689       False  \n",
       "690        True  \n",
       "691       False  \n",
       "692        True  \n",
       "693       False  \n",
       "694       False  \n",
       "695        True  \n",
       "696       False  \n",
       "697       False  \n",
       "698       False  \n",
       "699       False  \n",
       "700       False  \n",
       "701       False  \n",
       "702       False  \n",
       "703       False  \n",
       "704       False  \n",
       "705       False  \n",
       "706       False  \n",
       "707       False  \n",
       "708       False  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learner = load_learner(\"topic_segmentation_learner.pkl\")\n",
    "inf_dl = inf_learner.dls.test_dl(inf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, b in enumerate(inf_dl):\n",
    "    print(hf_tokenizer.decode(b[0][\"input_ids\"][0]))\n",
    "    print(hf_tokenizer.decode(b[1][\"input_ids\"][0]))\n",
    "    print(\"---\")\n",
    "    print(hf_tokenizer.decode(b[0][\"input_ids\"][1]))\n",
    "    print(hf_tokenizer.decode(b[1][\"input_ids\"][1]))\n",
    "    print(\"---\")\n",
    "    print(hf_tokenizer.decode(b[0][\"input_ids\"][2]))\n",
    "    print(hf_tokenizer.decode(b[1][\"input_ids\"][2]))\n",
    "    print(\"---\")\n",
    "    print(len(b[0][\"input_ids\"]))\n",
    "\n",
    "    scores = inf_learner.model.hf_model(b[0][\"input_ids\"], attention_mask=b[0][\"attention_mask\"])\n",
    "    print(scores)\n",
    "    print(torch.sigmoid(scores[0][:, 0]).detach().cpu().numpy().tolist())\n",
    "    if idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _ = inf_learner.get_preds(dl=inf_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preds), len(preds[0]), len(preds[1]))\n",
    "print(preds[0][:5])\n",
    "print(preds[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics, segeval\n",
    "from sklearn.metrics import mean_absolute_error, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.sigmoid(preds[0]).detach().cpu().numpy().tolist()\n",
    "\n",
    "print(len(scores))\n",
    "print(scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_np = np.array(scores)\n",
    "print(scores_np.mean(), scores_np.std())\n",
    "\n",
    "scores_np.mean() - (0.1 * scores_np.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_score_cal(scores):\n",
    "    output_scores = []\n",
    "    for i in range(len(scores)):\n",
    "        lflag = scores[i]\n",
    "        rflag = scores[i]\n",
    "        if i == 0:\n",
    "            hl = scores[i]\n",
    "            for r in range(i + 1, len(scores)):\n",
    "                if rflag <= scores[r]:\n",
    "                    rflag = scores[r]\n",
    "                else:\n",
    "                    break\n",
    "        elif i == len(scores):\n",
    "            hr = scores[i]\n",
    "            for l in range(i - 1, -1, -1):\n",
    "                if lflag <= scores[l]:\n",
    "                    lflag = scores[l]\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            for r in range(i + 1, len(scores)):\n",
    "                if rflag <= scores[r]:\n",
    "                    rflag = scores[r]\n",
    "                else:\n",
    "                    break\n",
    "            for l in range(i - 1, -1, -1):\n",
    "                if lflag <= scores[l]:\n",
    "                    lflag = scores[l]\n",
    "                else:\n",
    "                    break\n",
    "        depth_score = 0.5 * (lflag + rflag - 2 * scores[i])\n",
    "        output_scores.append(depth_score)\n",
    "\n",
    "    return output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_scores = depth_score_cal(scores)\n",
    "\n",
    "print(len(depth_scores))\n",
    "print(depth_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_scores_np = np.array(depth_scores)\n",
    "depth_scores_np.mean(), depth_scores_np.std(), depth_scores_np.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = sum(depth_scores) / (len(depth_scores)) - 0.1 * statistics.stdev(depth_scores)\n",
    "print(threshold)\n",
    "\n",
    "# TODO: A higher threshold seems better in every case; explore a different calculation than the above\n",
    "# threshold = 0.755 # 0.755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# pick_num = 3\n",
    "# score_wd = 0\n",
    "# score_mae = 0\n",
    "# score_f1 = 0\n",
    "# score_pk = 0\n",
    "# dp_var = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_var = [statistics.stdev(depth_scores)]\n",
    "\n",
    "boundary_indice = []\n",
    "\n",
    "seg_p_labels = [0] * (len(depth_scores))\n",
    "\n",
    "# TODO: I'm pretty sure we should start with a 1\n",
    "seg_p_labels[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(depth_scores)):\n",
    "    if depth_scores[i] > threshold:\n",
    "        boundary_indice.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in boundary_indice:\n",
    "    seg_p_labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seg_p_labels))\n",
    "print(seg_p_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 0\n",
    "seg_p = []\n",
    "for idx, fake in enumerate(seg_p_labels):\n",
    "    if fake == 1 and idx != 0:\n",
    "        # tmp += 1\n",
    "        seg_p.append(tmp)\n",
    "        tmp = 1\n",
    "    else:\n",
    "        tmp += 1\n",
    "seg_p.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for el in seg_p:\n",
    "    total += el\n",
    "\n",
    "print(len(seg_p))\n",
    "print(seg_p)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_r_labels = []\n",
    "seg_r = []\n",
    "tmp = 1\n",
    "\n",
    "for r_idx, r in inf_df.iterrows():\n",
    "    current_topic = r[\"topic\"]\n",
    "    if r_idx == 0:\n",
    "        last_seen_topic = r[\"topic\"]\n",
    "\n",
    "    if last_seen_topic != current_topic:\n",
    "        last_seen_topic = current_topic\n",
    "        seg_r_labels.append(1)\n",
    "        seg_r.append(tmp)\n",
    "        tmp = 1\n",
    "    else:\n",
    "        seg_r_labels.append(0 if r_idx != 0 else 1)\n",
    "        tmp += 1 if r_idx != 0 else 0\n",
    "\n",
    "seg_r.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate window_dff (WD) and PK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seg_r))\n",
    "print(seg_r[:20])\n",
    "print(seg_r[-20:])\n",
    "print(\"\")\n",
    "print(len(seg_r_labels))\n",
    "print(seg_r_labels[:20])\n",
    "print(seg_r_labels[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seg_r), len(seg_p))\n",
    "\n",
    "score_wd = segeval.window_diff(seg_p, seg_r)\n",
    "print(score_wd)\n",
    "\n",
    "score_pk = segeval.pk(seg_p, seg_r)\n",
    "print(score_pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate MAE and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seg_r_labels), len(seg_p_labels))\n",
    "print(seg_r_labels[:20])\n",
    "print(seg_p_labels[:20])\n",
    "print(\"\")\n",
    "print(seg_r_labels[-20:])\n",
    "print(seg_p_labels[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seg_r_labels), len(seg_p_labels))\n",
    "\n",
    "score_mae = sum(list(map(abs, np.array(seg_r_labels) - np.array(seg_p_labels))))\n",
    "print(score_mae)\n",
    "\n",
    "print(f1_score(seg_r_labels, seg_p_labels, average=\"macro\"))\n",
    "print(precision_score(seg_r_labels, seg_p_labels, average=\"macro\"))\n",
    "print(recall_score(seg_r_labels, seg_p_labels, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_idxs = [seg_idx for seg_idx, v in enumerate(seg_p_labels) if v == 1]\n",
    "seg_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.iloc[seg_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fsdl_2022_course_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c27d0eb0116998fc328b5a00abe6956c11e30aa3cb3ca27ff0ca511f067786d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
