# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_transcription.ipynb.

# %% ../nbs/10_transcription.ipynb 4
from __future__ import annotations

import csv, os
from pathlib import Path

import pandas as pd
from pytube import YouTube
import torch
import webvtt
import whisper
from whisper.utils import write_vtt

from . import utils
from .preprocessing import convert_duration_to_seconds

# %% auto 0
__all__ = ['fetch_youtube_audio', 'fetch_transcription', 'transcription_to_df']

# %% ../nbs/10_transcription.ipynb 7
def fetch_youtube_audio(
    # The ID for the YouTube video you want transcribed
    yt_id: str,
    # The location to store the audio file
    audio_files_fpath: Path = Path("./transcription/audio_files")
    # Returns the path of the created audio file
) -> Path:
    """This method isolates the audio from a YouTube video and saves it to the filesystem"""
    ext = "mp4"
    order = "abr"

    yt = YouTube(f"https://www.youtube.com/watch?v={yt_id}")
    yt.check_availability()

    filename = f"{yt.video_id}.{ext}"

    audio_files_fpath.mkdir(exist_ok=True, parents=True)
    download_path = audio_files_fpath / filename

    audio_streams = yt.streams.filter(only_audio=True, file_extension=ext).order_by(order).desc()

    # download it
    audio_streams.first().download(filename=download_path, skip_existing=True)
    return download_path

# %% ../nbs/10_transcription.ipynb 10
def fetch_transcription(
    # The path to the audio file we want to predict transcriptions from
    audio_fpath: Path,
    # The path to store predicted transcriptions
    transcription_fpath: Path = Path("./transcription/transcriptions"),
    # The path where our Whisper models are stored
    model_fpath: Path = Path("./transcription/models"),
    # The model checkpoint we want to use
    model_checkptoint: str = "base",
    # What device to run transcription on. Note: A GPU will be much faster
    device="cpu",
    # The path to our transcribed file (saved in VTT format)
) -> Path:

    transcription_fpath.mkdir(exist_ok=True, parents=True)
    model_fpath.mkdir(exist_ok=True, parents=True)

    torch_device = device if torch.cuda.is_available() and device != "cpu" else "cpu"
    model = whisper.load_model(model_checkptoint, device=torch_device, download_root=model_fpath)

    stem = audio_fpath.stem
    ext = "vtt"

    filename = f"{audio_fpath.stem}.{ext}"
    vtt_path = transcription_fpath / filename

    fields = ["start", "end", "text"]

    result = model.transcribe(str(audio_fpath))
    segments = result["segments"]

    with open(vtt_path, "w", encoding="utf-8") as vtt:
        write_vtt(segments, file=vtt)

    return vtt_path

# %% ../nbs/10_transcription.ipynb 13
def transcription_to_df(
    # The path to our saved VTT transcription
    transcription_fpath,
    # Returns a DataFrame with our transcription including timestamp and elapsed seconds
) -> pd.DataFrame:
    transcription_d = []
    for caption in webvtt.read(transcription_fpath):
        transcription_d.append({"timestamp": caption.start, "transcript": caption.text})

    df = pd.DataFrame(transcription_d)
    df["timestamp"] = df["timestamp"].astype(str)
    df.insert(0, "elapsed_seconds", df["timestamp"].apply(convert_duration_to_seconds))

    return df
