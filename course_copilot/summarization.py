# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/30_summarization.ipynb.

# %% ../nbs/30_summarization.ipynb 3
from __future__ import annotations

import datetime
import gc
import os
import time
import warnings

import wandb
from blurr.text.data.seq2seq.core import Seq2SeqBatchTokenizeTransform, Seq2SeqTextBlock, default_text_gen_kwargs
from blurr.text.modeling.core import BaseModelCallback, BaseModelWrapper
from blurr.text.modeling.seq2seq.core import Seq2SeqMetricsCallback, blurr_seq2seq_splitter
from blurr.text.utils import get_hf_objects
from blurr.utils import PreCalculatedCrossEntropyLoss
from fastcore.all import *
from fastai.data.block import DataBlock, ColReader, ItemGetter, ColSplitter, RandomSplitter
from fastai.callback.wandb import WandbCallback
from fastai.callback.schedule import *
from fastai.imports import *
from fastai.learner import *
from fastai.losses import CrossEntropyLossFlat
from fastai.optimizer import Adam, ranger
from fastai.torch_core import *
from fastai.torch_imports import *
from transformers.utils import logging as hf_logging
from transformers import PegasusForConditionalGeneration, BartForConditionalGeneration, T5ForConditionalGeneration

from . import utils, training, preprocessing

# %% auto 0
__all__ = ['headline_length', 'content_length', 'SummarizationConfig', 'ContentSummarizationConfig',
           'HeadlineSummarizationConfig', 'SummarizationModelTrainer']

# %% ../nbs/30_summarization.ipynb 6
# silence all the HF warnings
warnings.simplefilter("ignore")
hf_logging.set_verbosity_error()

# %% ../nbs/30_summarization.ipynb 10
class SummarizationConfig(training.TrainConfig):
    hf_model_cls = PegasusForConditionalGeneration
    hf_model_checkpoint = "sshleifer/distill-pegasus-cnn-16-4"

    # datablock/dataloaders
    text_gen_kwargs = {}
    tok_kwargs = {}

    # learner
    input_sequence_size = 512
    max_target_length = 10

    batch_size = 8
    use_fp16 = True
    use_wandb = True

# %% ../nbs/30_summarization.ipynb 11
headline_length = 5
content_length = 80


class ContentSummarizationConfig(SummarizationConfig):
    max_target_length = content_length
    text_gen_kwargs = {"do_sample": True, "max_length": 100, "top_k": 50, "top_p": 0.95}

# %% ../nbs/30_summarization.ipynb 13
class HeadlineSummarizationConfig(SummarizationConfig):
    max_target_length = headline_length

# %% ../nbs/30_summarization.ipynb 16
def _get_training_data(cfg: SummarizationConfig, data_dir="data"):  # configuration for summarization  # data directory
    segmentation_df, summarization_df = preprocessing.preprocess_data(
        ds="train", data_path=data_dir, return_file=True, save_file=False
    )
    return summarization_df

# %% ../nbs/30_summarization.ipynb 19
def _get_task_hf_objects(cfg: SummarizationConfig):
    hf_tok_kwargs = {}
    if cfg.hf_model_checkpoint == "sshleifer/tiny-mbart":
        hf_tok_kwargs["src_lang"], hf_tok_kwargs["tgt_lang"] = "en_XX", "en_XX"

    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(
        pretrained_model_name_or_path=cfg.hf_model_checkpoint,
        model_cls=cfg.hf_model_cls,
        tokenizer_kwargs=hf_tok_kwargs,
    )
    return hf_arch, hf_config, hf_tokenizer, hf_model

# %% ../nbs/30_summarization.ipynb 22
def _get_dls(cfg: SummarizationConfig, df, hf_arch, hf_config, hf_tokenizer, hf_model):
    if hf_arch in ["bart", "t5"]:
        cfg.text_gen_kwargs = {**hf_config.task_specific_params["summarization"], **{"max_length": 40, "min_length": 5}}

    # TODO: add text_gen_kwargs dynamically
    generate_func_args = list(inspect.signature(hf_model.generate).parameters.keys())
    for k in cfg.text_gen_kwargs.copy():
        if k not in generate_func_args:
            del text_gen_kwargs[k]

    if hf_arch == "mbart":
        cfg.text_gen_kwargs["decoder_start_token_id"] = hf_tokenizer.get_vocab()["en_XX"]

    def add_t5_prefix(inp):
        return f"summarize: {inp}" if (hf_arch == "t5") else inp

    batch_tokenize_tfm = Seq2SeqBatchTokenizeTransform(
        hf_arch,
        hf_config,
        hf_tokenizer,
        hf_model,
        padding="max_length",
        max_length=cfg.input_sequence_size,
        max_target_length=cfg.max_target_length,
        text_gen_kwargs=cfg.text_gen_kwargs,
    )

    blocks = (Seq2SeqTextBlock(batch_tokenize_tfm=batch_tokenize_tfm), noop)
    dblock = DataBlock(
        blocks=blocks, get_x=ColReader("transcript"), get_y=ColReader("topic"), splitter=RandomSplitter()
    )

    dls = dblock.dataloaders(df, bs=cfg.batch_size)
    return dls

# %% ../nbs/30_summarization.ipynb 27
def _get_learner(cfg: SummarizationConfig, dls, hf_config, hf_model, hf_arch):
    if cfg.random_seed:
        set_seed(cfg.random_seed)

    model = BaseModelWrapper(hf_model)
    learn_cbs = [BaseModelCallback]

    learn = Learner(
        dls,
        model,
        opt_func=ranger,
        loss_func=PreCalculatedCrossEntropyLoss(),
        cbs=learn_cbs,
        splitter=partial(blurr_seq2seq_splitter, arch=hf_arch),
    )

    learn.create_opt()
    learn.freeze()

    if cfg.use_fp16:
        learn = learn.to_fp16()

    return learn

# %% ../nbs/30_summarization.ipynb 37
def _get_preds(model_or_learner, text_data: str, gen_algo, max_length):
    if gen_algo == "greedy":
        return model_or_learner.blurr_generate(text_data, key="summary_texts", max_length=max_length)[0][
            "summary_texts"
        ]
    elif gen_algo == "topp":
        return model_or_learner.blurr_generate(
            text_data,
            key="summary_texts",
            max_length=max_length,
            top_k=50,
            top_p=0.95,
        )[0]["summary_texts"]
    elif gen_algo == "topk":
        return model_or_learner.blurr_generate(text_data, key="summary_texts", max_length=max_length, top_k=50)[0][
            "summary_texts"
        ]

# %% ../nbs/30_summarization.ipynb 41
class SummarizationModelTrainer(training.ModelTrainer):
    def __init__(
        self,
        experiment_name,
        train_config: SummarizationConfig,
        data_path="data",
        model_output_path="models",
        log_output_path="logs",
        log_preds=False,
        log_n_preds=None,
        use_wandb=False,
        verbose=False,
        **kwargs,
    ):
        super().__init__(
            experiment_name=experiment_name,
            train_config=train_config,
            data_path=data_path,
            model_output_path=model_output_path,
            log_output_path=log_output_path,
            log_preds=log_preds,
            log_n_preds=log_n_preds,
            use_wandb=use_wandb,
            verbose=verbose,
            **kwargs,
        )

    def get_training_data(self):
        return _get_training_data(cfg=self.train_config, data_dir=self.data_path)

    def load_learner_or_model(self, model_learner_fpath: str | Path = None, device="cpu"):
        if model_learner_fpath is None:
            model_learner_fpath = f"{self.model_output_path}/{self.experiment_name}.pkl"

        learn = load_learner(model_learner_fpath, cpu=device == "cpu")
        return learn

# %% ../nbs/30_summarization.ipynb 43
@patch
def train(self: SummarizationModelTrainer, sweep_config: dict = None):
    # setup
    start = time.time()

    yyyymmddHm = datetime.today().strftime("%Y%m%d_%H%m")
    seed = self.train_config.random_seed

    # --- step 0: init the WANDB run if logging to wandb and update the training config from the sweep config if doing a sweep
    is_sweep = True if sweep_config is not None else False

    if self.use_wandb:
        run = self.init_wandb_run(is_sweep)

    # --- BEGIN TRAINING ---
    if self.verbose:
        print(f"Experiment: {self.experiment_name}")
        print(f"Training config: f{self.get_train_config_props()}")

    # --- step 1: get our TRAINING DATA ---
    if self.verbose:
        print("Preparing training data ...")

    summarization_df = self.get_training_data()

    # --- step 2: get our HF OBJECTS ---
    if self.verbose:
        print("Building HF objects ...")

    hf_arch, hf_config, hf_tokenizer, hf_model = _get_task_hf_objects(self.train_config)

    # --- step 3: DATALOADERS ---
    if self.verbose:
        print("Building DataLoaders ...")

    dls = _get_dls(self.train_config, summarization_df, hf_arch, hf_config, hf_tokenizer, hf_model)

    # --- step 4: LEARNER ---
    if self.verbose:
        print("Building Learner ...")

    learn = _get_learner(cfg=self.train_config, dls=dls, hf_config=hf_config, hf_model=hf_model, hf_arch=hf_arch)

    seq2seq_metrics = {
        "rouge": {
            "compute_kwargs": {"rouge_types": ["rouge1", "rouge2", "rougeL", "rougeLsum"], "use_stemmer": True},
            "returns": ["rouge1", "rouge2", "rougeL", "rougeLsum"],
        }
    }
    fit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]

    # add any learner callbacks req. by the `ModelTrainer`
    if self.use_wandb and not is_sweep:
        fit_cbs.append(WandbCallback(log_preds=False))

    if self.train_config.random_seed and not self.train_config.only_seed_splits:
        set_seed(self.train_config.random_seed)

    learn.fit_one_cycle(
        self.train_config.n_unfrozen_epochs,
        cbs=fit_cbs,
        lr_max=1e-4,
    )

    end = time.time()
    if self.verbose:
        print(f"Time took for training is: {end - start}")

    learn.metrics = None

    if self.verbose:
        print("Saving model ...")

    learn.export(self.model_output_path / f"{self.experiment_name}.pkl")

    # clean up
    super(self.__class__, self).train()

    if self.verbose:
        print("End training")

    del learn, dls, hf_model, hf_tokenizer, hf_config
    torch.cuda.empty_cache()
    gc.collect()

    return None

# %% ../nbs/30_summarization.ipynb 45
@patch
def get_preds(self: SummarizationModelTrainer, model_or_learner, df, **kwargs):
    max_length = kwargs.get("max_target_length", 10)
    gen_algo = kwargs.get("gen_algo", "greedy")

    # To convey it's a headline summarization model
    data = df.copy()

    if self.train_config.max_target_length == headline_length:
        headings = []
        for i in range(len(data)):
            headings.append(_get_preds(model_or_learner, data.iloc[i]["transcript"], "greedy", headline_length))
        data.loc[:, "topic_prediction"] = headings

    # To convey it's a content summarization model
    elif self.train_config.max_target_length == content_length:
        content_preds = []
        for i in range(len(data)):
            content_preds.append(_get_preds(model_or_learner, data.iloc[i]["transcript"], "topp", content_length))
        data.loc[:, "content_highlights"] = content_preds

    return data
