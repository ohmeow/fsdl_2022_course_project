[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fsdl_2022_course_project",
    "section": "",
    "text": "Our project is to create an augmented ML approach course creators can use to streamline the generation of lecture summaries and chapter markers based on lesson videos.\nThe basic workflow is:"
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "fsdl_2022_course_project",
    "section": "Why",
    "text": "Why\nIn our own experience, we have noticed that such content either doesn’t get done, is time consuming, and/or requires work from outside parties. In particular, we noted in the below courses we’ve been a part of:\n\nFast.ai course - During the course students manually create youtube chapter markers, lesson transcripts, and summaries on the forums.\nFSDL course - The chapter markers and lesson notes are later created manually and then shared on the FSDL website usually 1 week after the each lesson."
  },
  {
    "objectID": "index.html#how-our-application-is-structured",
    "href": "index.html#how-our-application-is-structured",
    "title": "fsdl_2022_course_project",
    "section": "How our application is structured?",
    "text": "How our application is structured?\n\n\n\nSystem Diagram"
  },
  {
    "objectID": "index.html#what-have-we-done-so-far",
    "href": "index.html#what-have-we-done-so-far",
    "title": "fsdl_2022_course_project",
    "section": "What have we done so far?",
    "text": "What have we done so far?\nLet’s look at the dataset, ML library, API, and web application we created for our prototype system\n\nDataset\nSince we had to train summarization models and topic segmentation models, we manually created our dataset from a bunch of youtube videos ranging from videos from fastai lessons, FSDL lesson to random videos teaching something.\nDataset Link\n\n\n\nDataset Schema\n\n\n\n\nML library: course_copilot\nWe leveraged nbdev framework to create a python package which acted as our framework for Model training and model serving. We integrated Wandb for experiment tracking and fine tuning models with sweeps. We created Model trainers for task of topic segmentation and summarization. The timing of our project coincided with release whisper which we used for creating transcription of youtube video URL you are passing. This helps to provide the required data for creating topic segments and summaries.\nfsdl_2022_course_project\n\n\n\nnbdev based Model Trainer for Topic Segmentation, Experiment tracking with W&B\n\n\n\n\nBackend API\nFor the backend, we used FastAPI for creating APIs. Our API is leveraging dagster as the workflow engine to create tasks for running inference jobs from creating transcripts of video with whisper, running topic segmentation and running the summarization models.\nfsdl-2022-group-007-app\n\n\n\nCourse Copilot APIs\n\n\n\n\nWeb Application\nWe created our front-end web application using Vue3 and Quasar. It is deployed to github pages from our repo.\nfsdl-2022-group-007-web\n\n\n\nTopic summaries and chapter summaries generated"
  },
  {
    "objectID": "index.html#future-plans",
    "href": "index.html#future-plans",
    "title": "fsdl_2022_course_project",
    "section": "Future Plans",
    "text": "Future Plans\n\nImprove quality of training data\nAllow users to save their corrected headlines and summaries\nAdd ability for users to update topic spans\nImplement data flywheel\nImplement chapter marker and quarto export features\nAdd authentication/authorization"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "fsdl_2022_course_project",
    "section": "Install",
    "text": "Install\npip install course_copilot"
  },
  {
    "objectID": "index.html#setting-up-your-development-environment",
    "href": "index.html#setting-up-your-development-environment",
    "title": "fsdl_2022_course_project",
    "section": "Setting up your development environment",
    "text": "Setting up your development environment\nPlease take some time reading up on nbdev … how it works, directives, etc… by checking out the walk-thrus and tutorials on the nbdev website"
  },
  {
    "objectID": "index.html#step-1-create-conda-environment",
    "href": "index.html#step-1-create-conda-environment",
    "title": "fsdl_2022_course_project",
    "section": "Step 1: Create conda environment",
    "text": "Step 1: Create conda environment\nAfter cloning the repo, create a conda environment. This will install nbdev alongside other libraries likely required for this project.\nmamba env create -f environment.yml"
  },
  {
    "objectID": "index.html#step-2-install-quarto",
    "href": "index.html#step-2-install-quarto",
    "title": "fsdl_2022_course_project",
    "section": "Step 2: Install Quarto:",
    "text": "Step 2: Install Quarto:\nnbdev_install_quarto"
  },
  {
    "objectID": "index.html#step-3-install-hooks",
    "href": "index.html#step-3-install-hooks",
    "title": "fsdl_2022_course_project",
    "section": "Step 3: Install hooks",
    "text": "Step 3: Install hooks\nnbdev_install_hooks"
  },
  {
    "objectID": "index.html#step-4-add-pre-commit-hooks-optional",
    "href": "index.html#step-4-add-pre-commit-hooks-optional",
    "title": "fsdl_2022_course_project",
    "section": "Step 4: Add pre-commit hooks (optional)",
    "text": "Step 4: Add pre-commit hooks (optional)\nIf using VSCode, you can install pre-commit hooks “to catch and fix uncleaned and unexported notebooks” before pushing to get. See the instructions in the nbdev documentation if you want to use this feature. https://nbdev.fast.ai/tutorials/pre_commit.html"
  },
  {
    "objectID": "index.html#step-5-install-our-library",
    "href": "index.html#step-5-install-our-library",
    "title": "fsdl_2022_course_project",
    "section": "Step 5: Install our library",
    "text": "Step 5: Install our library\npip install -e '.[dev]'"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "training",
    "section": "",
    "text": "What we're running with at the time this documentation was generated:\ntorch: 1.12.1+cu102\nfastai: 2.7.9\ntransformers: 4.22.0\nblurr: 1.0.5"
  },
  {
    "objectID": "training.html#configuration",
    "href": "training.html#configuration",
    "title": "training",
    "section": "Configuration",
    "text": "Configuration\n\nsource\n\nTrainConfig\n\n TrainConfig ()\n\nThis class defines a base class for each task to implement. Contains class properties relevant to the training process\n\nsource\n\n\nget_train_config_props\n\n get_train_config_props (cfg:__main__.TrainConfig)\n\nReturns a dictionary of all the class properties in cfg\n\nget_train_config_props(TrainConfig)\n\n{'only_seed_splits': True,\n 'preprocess_strategy': None,\n 'random_seed': 2022,\n 'training_subset': 1.0,\n 'val_pct': 0.25}"
  },
  {
    "objectID": "training.html#model-trainer",
    "href": "training.html#model-trainer",
    "title": "training",
    "section": "Model Trainer",
    "text": "Model Trainer\n\nsource\n\nModelTrainer\n\n ModelTrainer (task:str, experiment_name:str,\n               train_config:__main__.TrainConfig, data_path:str='data',\n               model_output_path:str='models', log_output_path:str='logs',\n               log_preds:bool=False, log_n_preds:int=None,\n               use_wandb:bool=False, verbose:bool=False, **kwargs)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask\nstr\n\nThe ML task to run (e.g., topic_segmentation, summarization)\n\n\nexperiment_name\nstr\n\nThe name of your experiment (e.g., deberta_v3_large). This value is used in conjunction with task whenlogging information with W&B or else saving data releveant to training/evaluation runs\n\n\ntrain_config\nTrainConfig\n\nThe TrainConfig for your task\n\n\ndata_path\nstr\ndata\nWhere the project’s data is stored\n\n\nmodel_output_path\nstr\nmodels\nWhere exported Learners and other models should stored\n\n\nlog_output_path\nstr\nlogs\nWhere any logged data should be stored\n\n\nlog_preds\nbool\nFalse\nWhether predictions should be logged\n\n\nlog_n_preds\nint\nNone\nThe number of predictions that should be logged. It is left to each subclass to define what that means\n\n\nuse_wandb\nbool\nFalse\nWhether or not to log experiments and sweeps to W&B\n\n\nverbose\nbool\nFalse\nWhether or not you want to have printed out everything during a training/evaulation run\n\n\nkwargs"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "Application-wide defaults"
  },
  {
    "objectID": "utils.html#developmentrun-environment",
    "href": "utils.html#developmentrun-environment",
    "title": "utils",
    "section": "Development/Run environment",
    "text": "Development/Run environment\nInformation about where your code is running and your compute capabilities\n\nsource\n\ndetect_env\n\n detect_env ()\n\nA helper function that detects where you are running code\n\nprint(run_env)\n\nlocal_nb\n\n\n\nsource\n\n\nprint_dev_environment\n\n print_dev_environment ()\n\nProvides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc.\n\nprint_dev_environment()"
  },
  {
    "objectID": "transcription.html",
    "href": "transcription.html",
    "title": "transcription",
    "section": "",
    "text": "youtube_id = \"Jsz4E2iNXUA\"\n\naudio_fpath = fetch_youtube_audio(youtube_id, Path(\"../transcription/audio_files\"))\naudio_fpath\n\nPath('../transcription/audio_files/Jsz4E2iNXUA.mp4')"
  },
  {
    "objectID": "transcription.html#whisper-transcription",
    "href": "transcription.html#whisper-transcription",
    "title": "transcription",
    "section": "Whisper transcription",
    "text": "Whisper transcription\n\ntranscription_fpath = fetch_transcription(\n    audio_fpath, Path(\"../transcription/transcriptions\"), Path(\"../transcription/models\"), device=\"cuda\"\n)\ntranscription_fpath\n\nPath('../transcription/transcriptions/Jsz4E2iNXUA.vtt')"
  },
  {
    "objectID": "transcription.html#transcription-rendering",
    "href": "transcription.html#transcription-rendering",
    "title": "transcription",
    "section": "Transcription rendering",
    "text": "Transcription rendering\n\ndf = transcription_to_df(transcription_fpath)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      elapsed_seconds\n      timestamp\n      transcript\n    \n  \n  \n    \n      0\n      0.00\n      00:00:00.000\n      Let me make sure everything is as it should be.\n    \n    \n      1\n      3.56\n      00:00:03.560\n      I'm always fascinated by the fact that people are waiting.\n    \n    \n      2\n      6.72\n      00:00:06.720\n      It's like it's it's so surprising that people are like on here\n    \n    \n      3\n      12.70\n      00:00:12.700\n      sometimes early for you.\n    \n    \n      4\n      14.12\n      00:00:14.120\n      It makes sense for my session."
  },
  {
    "objectID": "tasks.html",
    "href": "tasks.html",
    "title": "tasks",
    "section": "",
    "text": "source\n\nbuild_train_config\n\n build_train_config (train_config_cls:type, args)\n\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_config_cls\ntype\nOur task’s TrainConfig\n\n\nargs\n\nThe arguments (name and values) we want to update our TrainConfig with\n\n\nReturns\ntraining.TrainConfig\n\n\n\n\n\nsource\n\n\nrun_experiment\n\n run_experiment (task:str, experiment_name:str, data_path:str='data',\n                 model_output_path:str='models',\n                 log_output_path:str='logs', log_preds:bool=False,\n                 log_n_preds:int=None, use_wandb:bool=False,\n                 verbose:bool=False, args=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask\nstr\n\nThe ML task to run (e.g., topic_segmentation, summarization)\n\n\nexperiment_name\nstr\n\nThe name of your experiment (e.g., deberta_v3_large). This value is used in conjunction with task whenlogging information with W&B or else saving data releveant to training/evaluation runs\n\n\ndata_path\nstr\ndata\nWhere the project’s data is stored\n\n\nmodel_output_path\nstr\nmodels\nWhere exported Learners and other models should stored\n\n\nlog_output_path\nstr\nlogs\nWhere any logged data should be stored\n\n\nlog_preds\nbool\nFalse\nWhether predictions should be logged\n\n\nlog_n_preds\nint\nNone\nThe number of predictions that should be logged. It is left to each subclass to define what that means\n\n\nuse_wandb\nbool\nFalse\nWhether or not to log experiments and sweeps to W&B\n\n\nverbose\nbool\nFalse\nWhether or not you want to have printed out everything during a training/evaulation run\n\n\nargs\nNoneType\nNone\nAny args/values we want to use to update our TrainConfig with\n\n\n\n\nsource\n\n\nprepare_tuning\n\n prepare_tuning (task:str, experiment_name:str, data_path:str,\n                 model_output_path:str, log_output_path:str,\n                 log_preds:bool, log_n_preds:int, use_wandb:bool,\n                 verbose:bool, args=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask\nstr\n\nThe ML task to run (e.g., topic_segmentation, summarization)\n\n\nexperiment_name\nstr\n\nThe name of your experiment (e.g., deberta_v3_large). This value is used in conjunction with task whenlogging information with W&B or else saving data releveant to training/evaluation runs\n\n\ndata_path\nstr\n\nWhere the project’s data is stored\n\n\nmodel_output_path\nstr\n\nWhere exported Learners and other models should stored\n\n\nlog_output_path\nstr\n\nWhere any logged data should be stored\n\n\nlog_preds\nbool\n\nWhether predictions should be logged\n\n\nlog_n_preds\nint\n\nThe number of predictions that should be logged. It is left to each subclass to define what that means\n\n\nuse_wandb\nbool\n\nWhether or not to log experiments and sweeps to W&B\n\n\nverbose\nbool\n\nWhether or not you want to have printed out everything during a training/evaulation run\n\n\nargs\nNoneType\nNone\nAny args/values we want to use to update our TrainConfig with\n\n\n\n\nsource\n\n\nadd_required_args\n\n add_required_args (parser)"
  },
  {
    "objectID": "summarization.html",
    "href": "summarization.html",
    "title": "summarization",
    "section": "",
    "text": "What we're running with at the time this documentation was generated:\ntorch: 1.12.1+cu102\nfastai: 2.7.9\ntransformers: 4.22.1"
  },
  {
    "objectID": "summarization.html#setup",
    "href": "summarization.html#setup",
    "title": "summarization",
    "section": "Setup",
    "text": "Setup\n\nsource\n\nSummarizationConfig\n\n SummarizationConfig ()\n\nThis class defines a base class for each task to implement. Contains class properties relevant to the training process\n\nsource\n\n\nContentSummarizationConfig\n\n ContentSummarizationConfig ()\n\nThis class defines a base class for each task to implement. Contains class properties relevant to the training process\n\nContentSummarizationConfig.max_target_length\n\n80\n\n\n\nsource\n\n\nHeadlineSummarizationConfig\n\n HeadlineSummarizationConfig ()\n\nThis class defines a base class for each task to implement. Contains class properties relevant to the training process\n\nclass XsumCFG(ContentSummarizationConfig):\n    training_subset = 0.25\n    n_frozen_epochs = 0\n    n_unfrozen_epochs = 1\n\n\n[f\"{k}: {v}\" for k, v in training.get_train_config_props(XsumCFG).items()]\n\n['batch_size: 8',\n 'hf_model_checkpoint: sshleifer/distill-pegasus-cnn-16-4',\n 'hf_model_cls: PegasusForConditionalGeneration',\n 'input_sequence_size: 512',\n 'max_target_length: 80',\n 'n_frozen_epochs: 0',\n 'n_unfrozen_epochs: 1',\n 'only_seed_splits: True',\n 'preprocess_strategy: None',\n 'random_seed: 2022',\n \"text_gen_kwargs: {'do_sample': True, 'max_length': 100, 'top_k': 50, 'top_p': 0.95}\",\n 'tok_kwargs: {}',\n 'training_subset: 0.25',\n 'use_fp16: True',\n 'use_wandb: True',\n 'val_pct: 0.25']"
  },
  {
    "objectID": "summarization.html#data",
    "href": "summarization.html#data",
    "title": "summarization",
    "section": "Data",
    "text": "Data\n\nsdf = _get_training_data(XsumCFG, data_dir=\"../data\")\nsdf.head()\n\n\n\n\n\n  \n    \n      \n      course_title\n      lesson_num\n      start_seconds\n      topic\n      transcript\n    \n  \n  \n    \n      0\n      C-Squared Podcast\n      1\n      0.0\n      Intro\n      [Music] welcome everybody to episode one of a chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up christian well not so much fabi uh it's first of all great um to finally start a podcast the chess podcast i know that um there's a lot of podcasts out there but i wanted to bring our own tune to the mix and i think uh yeah i'm excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's been a while at your home it's good to be here it's my first time in uh visiting here and uh yeah it's been an intere...\n    \n    \n      1\n      C-Squared Podcast\n      1\n      137.0\n      Candidates 2018\n      camps look like in general yeah well you mentioned the 2018 cycle uh where we worked together we started with the training before the candidates and for me it's interesting because i've i've played a lot of these candidates tournaments and i'm always doing it a bit differently trying different things trying to improve it but sometimes it goes less or more successfully you never know what will work out i think what we did in 2018 not just for the candidates but also for the world championship because i qualified for that i think what we did then was extremely successful um we we arranged it...\n    \n    \n      2\n      C-Squared Podcast\n      1\n      464.0\n      Candidates training\n      going in the candidates like how was the experience yeah i think the preparation was pretty serious it included a bunch of uh camps and preparation devoted to players as i assume i think everyone has the same sort of general approach which is to think about their openings their strategy look at the opponents try to get in shape make sure that you're not you know rusty or blundering things or hallucinating variations uh but there's a lot of nerves and i i felt a lot of nerves before the tournament and i think possibly i you know overworked over trained a bit because it was yeah it was like ...\n    \n    \n      3\n      C-Squared Podcast\n      1\n      610.0\n      Playing for 2nd place\n      were you just like focused on grabbing first well i was only focused on first but of course there were always these thoughts that well maybe second is enough but you can't play for second like let's say once i had achieved plus three in the tournament and john was plus four and i tried to go and go into like full like risk reverse mode which is still difficult to do but let's say i had gone that mode and and achieved it and like finished second with like plus three and john got plus five uh and then like magnus says well i'm going to play right then you also feel kind of stupid you know li...\n    \n    \n      4\n      C-Squared Podcast\n      1\n      916.0\n      Magnus' WC decision\n      know you can't uh you can't tell him you have to do something i i guess let me rephrase that fair to let you guys play the tournament first and then tell you the decision well i think he said it in a strange way which was that i'll play against alireza which to me is strange because if you don't want to play world championship match i fully understand you know but did he say that did he actually name him yeah that's kind of what he said um yeah he more he like he didn't say definitively like i won't play against anyone but he was like i probably won't play unless it's frozen right and yeah..."
  },
  {
    "objectID": "summarization.html#huggingface-objects",
    "href": "summarization.html#huggingface-objects",
    "title": "summarization",
    "section": "Huggingface objects",
    "text": "Huggingface objects\n\nhf_arch, hf_config, hf_tokenizer, hf_model = _get_task_hf_objects(XsumCFG)\nhf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n\n('pegasus',\n transformers.models.pegasus.configuration_pegasus.PegasusConfig,\n transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast,\n transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration)"
  },
  {
    "objectID": "summarization.html#dataloaders",
    "href": "summarization.html#dataloaders",
    "title": "summarization",
    "section": "Dataloaders",
    "text": "Dataloaders\n\ndls = _get_dls(XsumCFG, sdf, hf_arch, hf_config, hf_tokenizer, hf_model)\nb = dls.one_batch()\n\n\nlen(b), len(b[0]), b[0][\"input_ids\"].shape, len(b[1]), b[1].shape\n\n\nprint(hf_tokenizer.decode(b[0][\"input_ids\"][0]))"
  },
  {
    "objectID": "summarization.html#models",
    "href": "summarization.html#models",
    "title": "summarization",
    "section": "Models",
    "text": "Models\n\nlearn = _get_learner(cfg=XsumCFG, dls=dls, hf_config=hf_config, hf_model=hf_model, hf_arch=hf_arch)\n\n\nlearn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n\n\nseq2seq_metrics = {\n    \"rouge\": {\n        \"compute_kwargs\": {\"rouge_types\": [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], \"use_stemmer\": True},\n        \"returns\": [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n    }\n}\nfit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n\n\nlearn.fit_one_cycle(1, lr_max=5e-4, cbs=fit_cbs)\n\n\nlearn.metrics = None\n\n\nexport_fname = \"summarize_export\"\nlearn.export(fname=f\"{export_fname}.pkl\")\n\n\ndel learn, dls, hf_model, hf_tokenizer, hf_config\ntorch.cuda.empty_cache()\ngc.collect()\n\n\nlearn = load_learner(\"summarize_export.pkl\")\n\n\ntest_article = \"\"\"hey everybody welcome back this week we're going to talk about something a little bit different than we do most weeks most weeks we talk about specific\ntechnical aspects of building machine learning powered products but this week we're going to focus on some of the\norganizational things that you need to do in order to work together on ml-powered products as part of an\ninterdisciplinary team so the the reality of building ml Power Products is that building any product well is really\ndifficult you have to figure out how to hire grade people you need to be able to manage those people and get the best out\nof them you need to make sure that your team is all working together towards a shared goal you need to make good\nlong-term technical choices manage technical debt over time you need to make sure that you're managing\nexpectations not just of your own team but also of leadership of your organization and you need to be able to make sure\nthat you're working well within the confines of the requirements of the rest of the org that you're understanding\nthose requirements well and communicating back to your progress to the rest of the organization against those requirements\nbut machine learning adds even more additional complexity to this machine learning Talent tends to be very scarce\nand expensive to attract machine learning teams are not just a\nsingle role but today they tend to be pretty interdisciplinary which makes managing them an even bigger challenge\nmachine learning projects often have unclear timelines and there's a high\ndegree of uncertainty to those timelines machine learning itself is moving super fast and machine learning as we've\ncovered before you can think of as like the high interest credit card of technical debt so keeping up with making\ngood long-term decisions and not incurring too much technical debt is especially difficult in ml unlike\ntraditional software ml is so new that in most organizations leadership tends not to be that well educated in it they\nmight not understand some of the core differences between ML and other technology that you're working with machine learning products tend to fail\nin ways that are really hard for Lay people to understand and so that makes it very difficult to help the rest of\nthe stakeholders in your organization understand what they could really expect from the technology that you're building\nand what is realistic for us to achieve so throughout the rest rest of this lecture we're going to kind of touch on\nsome of these themes and cover different aspects of this problem of working together to build ml Power Products as\nan organization so here are the pieces that we're going to cover we're going to talk about different roles that are involved in building ml products we're\ngoing to talk about some of the unique aspects involved in hiring ml Talent\nwe're going to talk about organization of teams and how the ml team tends to fit into the rest of the org and some of\nthe pros and cons of different ways of setting that up we'll talk about managing ml teams and\nml product management and then lastly we'll talk about some of the design considerations for how to design a\nproduct that is well suited to having a good ml model that backs it so let's dive in and talk about rules the most\ncommon ml rules that you might hear of are things like ml product manager ml\n\"\"\"\nlearn.blurr_generate(\n    test_article, key=\"summary_texts\", do_sample=True, max_length=100, top_k=50, top_p=0.95, num_return_sequences=3\n)\n\n[{'summary_texts': [\"This week we're going to talk about some of the organizational things that you need to do in order to work together on ml-powered products .\",\n   \"This week we're going to talk about some of the organizational things that you need to do in order to work together on ml-powered products .\",\n   \"This week we're going to talk about some of the organizational things that you need to do in order to work together on ml-powered products .\"]}]\n\n\n\nresponse = _get_preds(learn, test_article, \"greedy\", 100)\nassert isinstance(response, str)\n\n\n_get_preds(learn, test_article, \"topp\", 15)\n\n\"This week we're going to talk about some of the organizational\""
  },
  {
    "objectID": "summarization.html#model-trainer",
    "href": "summarization.html#model-trainer",
    "title": "summarization",
    "section": "Model Trainer",
    "text": "Model Trainer\n\nsource\n\nSummarizationModelTrainer\n\n SummarizationModelTrainer (experiment_name,\n                            train_config:__main__.SummarizationConfig,\n                            data_path='data', model_output_path='models',\n                            log_output_path='logs', log_preds=False,\n                            log_n_preds=None, use_wandb=False,\n                            verbose=False, **kwargs)\n\nHelper class that provides a standard way to create an ABC using inheritance."
  },
  {
    "objectID": "summarization.html#train",
    "href": "summarization.html#train",
    "title": "summarization",
    "section": "Train",
    "text": "Train\n\nsource\n\nSummarizationModelTrainer.train\n\n SummarizationModelTrainer.train (sweep_config:dict=None)"
  },
  {
    "objectID": "summarization.html#get_preds",
    "href": "summarization.html#get_preds",
    "title": "summarization",
    "section": "get_preds",
    "text": "get_preds\n\nsource\n\nSummarizationModelTrainer.get_preds\n\n SummarizationModelTrainer.get_preds (model_or_learner, df, **kwargs)\n\n\ntrainer = SummarizationModelTrainer(\n    task=\"train\",\n    experiment_name=\"content_summarization\",\n    train_config=XsumCFG,\n    data_path=\"../data\",\n    model_output_path=\"../models\",\n    log_output_path=\"../logs\",\n    log_preds=True,\n    log_n_preds=2,\n    use_wandb=True,\n)"
  },
  {
    "objectID": "summarization.html#example-training-code",
    "href": "summarization.html#example-training-code",
    "title": "summarization",
    "section": "Example Training code",
    "text": "Example Training code\n\ntrainer.train()"
  },
  {
    "objectID": "summarization.html#example-inference-code",
    "href": "summarization.html#example-inference-code",
    "title": "summarization",
    "section": "Example inference code",
    "text": "Example inference code\n\ninf_learn = trainer.load_learner_or_model(\"../models/content_summarization.pkl\", device=\"cpu\")\npredicted_df = trainer.get_preds(inf_learn, sdf[:3])\npredicted_df\n\nCPU times: user 11min 4s, sys: 21.8 s, total: 11min 26s\nWall time: 10.4 s\n\n\n\n\n\n\n  \n    \n      \n      course_title\n      lesson_num\n      start_seconds\n      topic\n      transcript\n      content_highlights\n    \n  \n  \n    \n      0\n      C-Squared Podcast\n      1\n      0.0\n      Intro\n      [Music] welcome everybody to episode one of a chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up christian well not so much fabi uh it's first of all great um to finally start a podcast the chess podcast i know that um there's a lot of podcasts out there but i wanted to bring our own tune to the mix and i think uh yeah i'm excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's been a while at your home it's good to be here it's my first time in uh visiting here and uh yeah it's been an intere...\n      I'm back in the states after it's been a few months playing a lot of chess . It's been an interesting few months playing a lot of chess which is pretty cool but also a bit difficult at times his home .\n    \n    \n      1\n      C-Squared Podcast\n      1\n      137.0\n      Candidates 2018\n      camps look like in general yeah well you mentioned the 2018 cycle uh where we worked together we started with the training before the candidates and for me it's interesting because i've i've played a lot of these candidates tournaments and i'm always doing it a bit differently trying different things trying to improve it but sometimes it goes less or more successfully you never know what will work out i think what we did in 2018 not just for the candidates but also for the world championship because i qualified for that i think what we did then was extremely successful um we we arranged it...\n      Maxine vachelagrav will be replaced by maxine vachelagrav .\n    \n    \n      2\n      C-Squared Podcast\n      1\n      464.0\n      Candidates training\n      going in the candidates like how was the experience yeah i think the preparation was pretty serious it included a bunch of uh camps and preparation devoted to players as i assume i think everyone has the same sort of general approach which is to think about their openings their strategy look at the opponents try to get in shape make sure that you're not you know rusty or blundering things or hallucinating variations uh but there's a lot of nerves and i i felt a lot of nerves before the tournament and i think possibly i you know overworked over trained a bit because it was yeah it was like ...\n      The preparation included a bunch of camps and preparation devoted to players . It was like work that pretty much led up straight to the tournament and nerfs more than you usually feel . It was like work that pretty much led up straight to the tournament and nerfs more than you usually feel .\n    \n  \n\n\n\n\n\n# cleanup resources\ndel inf_learn\ntorch.cuda.empty_cache()\ngc.collect()\n\n20803"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "preprocessing",
    "section": "",
    "text": "source\n\n\n\n convert_duration_to_seconds (v:str)\n\nTakes a string representation of a duration in the format of ‘hh:mm:ss’ and returns the number of seconds it represents\n\n\n\n\nType\nDetails\n\n\n\n\nv\nstr\nA duration/iterval with the format of “hh:mm:ss”\n\n\nReturns\nint\nThe total number of seconds\n\n\n\n\nsource\n\n\n\n\n build_train_df (data_path:str|pathlib.Path='../data/')\n\nThis method performs the core preprocessing required to turn our user collected raw data into something we can use for training both topic segmentation and summarization models\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_path\nstr | Path\n../data/\nThe path to the data dir\n\n\nReturns\npd.DataFrame\n\nA preprocessed DataFrame suitable for both segmentation and summarization training\n\n\n\n\nsource\n\n\n\n\n build_segmentation_train_df (train_df:pandas.core.frame.DataFrame)\n\nFor segmentation, we want to create a dataset of seq, seq +1 examples, but also include the ability to gather negative samples from either sequences in that topic or not\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_df\npd.DataFrame\nThe preprocessed training DataFrame from build_train_df\n\n\nReturns\npd.DataFrame\nA DataFrame suitable for topic segmentation training\n\n\n\n\nsource\n\n\n\n\n build_summarization_train_df (train_df:pandas.core.frame.DataFrame)\n\nFor summarization, we want to concatenate all the sequences in a topic and use the resulting string to predict the topic\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_df\npd.DataFrame\nThe preprocessed training DataFrame from build_train_df\n\n\nReturns\npd.DataFrame\nA DataFrame for suitable for summarization training"
  },
  {
    "objectID": "preprocessing.html#preprocessing",
    "href": "preprocessing.html#preprocessing",
    "title": "preprocessing",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nsource\n\npreprocess_data\n\n preprocess_data (ds:str='train', data_path:str|pathlib.Path='../data/',\n                  return_file:bool=True, save_file:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\nstr\ntrain\nWhat dataset do we want to preprocess in the data/raw folder\n\n\ndata_path\nstr | Path\n../data/\nThe path to the data folder\n\n\nreturn_file\nbool\nTrue\nDetermines whether or not we save the cleaned Dataframes to data/clean\n\n\nsave_file\nbool\nFalse\nDetermines whether or not we return the cleaned Dataframes\n\n\n\n\nsegmentation_train_df, summarization_train_df = preprocess_data(\"train\", save_file=True)\n\nlen(segmentation_train_df), len(summarization_train_df)\n\n(25383, 597)\n\n\n\nsegmentation_train_df.head()\n\n\n\n\n\n  \n    \n      \n      course_title\n      lesson_num\n      topic\n      seq\n      prev_seq\n      next_seq\n      is_topic_end\n      next_topic_begin_seq\n      other_topic_seqs\n    \n  \n  \n    \n      0\n      C-Squared Podcast\n      1\n      Intro\n      [Music] welcome everybody to episode one of a\n      xxBEGIN_TOPICxx\n      chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up\n      False\n      None\n      [christian well not so much fabi uh it's first of all great um to finally start a, podcast the chess podcast i know that um there's a lot of podcasts out there but, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to m...\n    \n    \n      1\n      C-Squared Podcast\n      1\n      Intro\n      chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up\n      [Music] welcome everybody to episode one of a\n      christian well not so much fabi uh it's first of all great um to finally start a\n      False\n      None\n      [[Music] welcome everybody to episode one of a, podcast the chess podcast i know that um there's a lot of podcasts out there but, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location because, those ...\n    \n    \n      2\n      C-Squared Podcast\n      1\n      Intro\n      christian well not so much fabi uh it's first of all great um to finally start a\n      chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up\n      podcast the chess podcast i know that um there's a lot of podcasts out there but\n      False\n      None\n      [[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, i wanted to bring our own tune to the mix and i think uh yeah i'm, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location beca...\n    \n    \n      3\n      C-Squared Podcast\n      1\n      Intro\n      podcast the chess podcast i know that um there's a lot of podcasts out there but\n      christian well not so much fabi uh it's first of all great um to finally start a\n      i wanted to bring our own tune to the mix and i think uh yeah i'm\n      False\n      None\n      [[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, christian well not so much fabi uh it's first of all great um to finally start a, excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention th...\n    \n    \n      4\n      C-Squared Podcast\n      1\n      Intro\n      i wanted to bring our own tune to the mix and i think uh yeah i'm\n      podcast the chess podcast i know that um there's a lot of podcasts out there but\n      excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's\n      False\n      None\n      [[Music] welcome everybody to episode one of a, chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up, christian well not so much fabi uh it's first of all great um to finally start a, podcast the chess podcast i know that um there's a lot of podcasts out there but, been a while at your home it's good to be here it's my first time in uh visiting here and uh, yeah it's been an interesting few months played a lot of chess which is pretty cool but, also a bit difficult at times my home uh here we are not going to mention the location because, those uh cra...\n    \n  \n\n\n\n\n\nsummarization_train_df.head()\n\n\n\n\n\n  \n    \n      \n      course_title\n      lesson_num\n      start_seconds\n      topic\n      transcript\n    \n  \n  \n    \n      0\n      C-Squared Podcast\n      1\n      0.0\n      Intro\n      [Music] welcome everybody to episode one of a chess themed podcast with myself christian kirilla and i'm fighting on caruana so what's up christian well not so much fabi uh it's first of all great um to finally start a podcast the chess podcast i know that um there's a lot of podcasts out there but i wanted to bring our own tune to the mix and i think uh yeah i'm excited about that so that's uh the first thing how about yourself fabian well i'm back in the states after it's been a while at your home it's good to be here it's my first time in uh visiting here and uh yeah it's been an intere...\n    \n    \n      1\n      C-Squared Podcast\n      1\n      137.0\n      Candidates 2018\n      camps look like in general yeah well you mentioned the 2018 cycle uh where we worked together we started with the training before the candidates and for me it's interesting because i've i've played a lot of these candidates tournaments and i'm always doing it a bit differently trying different things trying to improve it but sometimes it goes less or more successfully you never know what will work out i think what we did in 2018 not just for the candidates but also for the world championship because i qualified for that i think what we did then was extremely successful um we we arranged it...\n    \n    \n      2\n      C-Squared Podcast\n      1\n      464.0\n      Candidates training\n      going in the candidates like how was the experience yeah i think the preparation was pretty serious it included a bunch of uh camps and preparation devoted to players as i assume i think everyone has the same sort of general approach which is to think about their openings their strategy look at the opponents try to get in shape make sure that you're not you know rusty or blundering things or hallucinating variations uh but there's a lot of nerves and i i felt a lot of nerves before the tournament and i think possibly i you know overworked over trained a bit because it was yeah it was like ...\n    \n    \n      3\n      C-Squared Podcast\n      1\n      610.0\n      Playing for 2nd place\n      were you just like focused on grabbing first well i was only focused on first but of course there were always these thoughts that well maybe second is enough but you can't play for second like let's say once i had achieved plus three in the tournament and john was plus four and i tried to go and go into like full like risk reverse mode which is still difficult to do but let's say i had gone that mode and and achieved it and like finished second with like plus three and john got plus five uh and then like magnus says well i'm going to play right then you also feel kind of stupid you know li...\n    \n    \n      4\n      C-Squared Podcast\n      1\n      916.0\n      Magnus' WC decision\n      know you can't uh you can't tell him you have to do something i i guess let me rephrase that fair to let you guys play the tournament first and then tell you the decision well i think he said it in a strange way which was that i'll play against alireza which to me is strange because if you don't want to play world championship match i fully understand you know but did he say that did he actually name him yeah that's kind of what he said um yeah he more he like he didn't say definitively like i won't play against anyone but he was like i probably won't play unless it's frozen right and yeah...\n    \n  \n\n\n\n\n\n# TODO\n# segmentation_test_df,  summarization_test_df = preprocess_data(\"test\", save_file=True)"
  },
  {
    "objectID": "topic_segmentation.html",
    "href": "topic_segmentation.html",
    "title": "topic_segmentation",
    "section": "",
    "text": "What we're running with at the time this documentation was generated:\ntorch: 1.12.1+cu102\nfastai: 2.7.9\ntransformers: 4.22.0"
  },
  {
    "objectID": "topic_segmentation.html#configuration",
    "href": "topic_segmentation.html#configuration",
    "title": "topic_segmentation",
    "section": "Configuration",
    "text": "Configuration\n\nprint(os.environ[\"WANDB_PROJECT_NAME\"])\nprint(os.environ[\"WANDB_TEAM\"])\n\ncourse-copilot-ml\ncourse-copilot\n\n\n\nsource\n\nTopicSegmentationConfig\n\n TopicSegmentationConfig ()\n\nA training.TrainConfig object training and tuning our segmentation models. Uses fastai and huggingface defaults by default\n\nclass ExampleCFG(TopicSegmentationConfig):\n    training_subset = 0.25\n    n_frozen_epochs = 0\n    n_unfrozen_epochs = 2\n\n\n[f\"{k}: {v}\" for k, v in training.get_train_config_props(ExampleCFG).items()][:5]\n\n['accum: None',\n 'adam_beta2: 0.99',\n 'adam_eps: 1e-07',\n 'batch_size: 8',\n \"custom_model_kwargs: {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}\"]"
  },
  {
    "objectID": "topic_segmentation.html#data",
    "href": "topic_segmentation.html#data",
    "title": "topic_segmentation",
    "section": "Data",
    "text": "Data\n\ntrain_df, trn_idxs, val_idxs, raw_train_df = _get_training_data(\n    ExampleCFG, data_dir=\"../data\", on_the_fly=False, split_type=\"cross_validation\"\n)\n\n\nprint(len(train_df))\nprint(len(trn_idxs), len(val_idxs))\ntrain_df.head(2)\n\n6196\n4981 1215\n\n\n\n\n\n\n  \n    \n      \n      index\n      course_title\n      lesson_num\n      topic\n      seq\n      prev_seq\n      next_seq\n      is_topic_end\n      next_topic_begin_seq\n      other_topic_seqs\n    \n  \n  \n    \n      0\n      23538\n      parker - learn photography\n      1\n      Apertures Deep Dive\n      particular the detroit tigers so their wedding day was in that area\n      and if you recognize this letter d you know that's from detroit and in\n      and we were near the detroit tiger stadium if i had completely blurred out\n      False\n      NaN\n      [you fully understand how they work and how they can alter your final image now previously i demonstrated that the, larger the aperture the more the background is blurred out when it comes to the area in focus this is referred to, as the depth of field so the depth of field is the zone within a photo that, appears sharp and in focus when focusing on your subject that is considered the point of focus beyond that, how much appears in focus corresponds to the depth of field so here are two more, images and the amount of the depth of field in one is greater than the other, the first image i ca...\n    \n    \n      1\n      6466\n      Full Stack Deep Learning - Spring 2021\n      9\n      Long Term Ethical Problems in AI\n      we basically need robots in order to have a functioning economy in the next few decades\n      brilliant person and this article which i recommend you click on talks about how\n      an interesting spin on this worry though is ai not necessarily replacing human labor\n      False\n      NaN\n      [a.i so the first i think a lot of people's minds go to autonomous weapons and maybe they go into a place that, is a little easy to dismiss as maybe far-fetched not realistic we don't have to worry about it it's just a movie, but of course as the saying goes the future is already here it's just not evenly distributed so, israel apparently has autonomous robo snipers on their borders today and just, i think last weekend or something there was an article about the new york city police deploying, the boston dynamics spot robot which actually anyone can buy now i think it's only like, 60 000 o..."
  },
  {
    "objectID": "topic_segmentation.html#hugging-face-transformers-objects",
    "href": "topic_segmentation.html#hugging-face-transformers-objects",
    "title": "topic_segmentation",
    "section": "Hugging Face transformers objects",
    "text": "Hugging Face transformers objects\n\nhf_arch, hf_config, hf_tokenizer, hf_model = _get_task_hf_objects(ExampleCFG)"
  },
  {
    "objectID": "topic_segmentation.html#dataloaders",
    "href": "topic_segmentation.html#dataloaders",
    "title": "topic_segmentation",
    "section": "DataLoaders",
    "text": "DataLoaders\n\nsource\n\nSiameseBatchTokenizeTransform\n\n SiameseBatchTokenizeTransform (use_next_pos_prob:float=0.75,\n                                use_adjacent_neg_prob:float=0.5, **kwargs)\n\nHandles everything you need to assemble a mini-batch of inputs and targets, as well as decode the dictionary produced as a byproduct of the tokenization process in the encodes method.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nuse_next_pos_prob\nfloat\n0.75\nThe probability we’ll use the positive examples next sequence vs another sequence in the same topic\n\n\nuse_adjacent_neg_prob\nfloat\n0.5\nThe probability we’ll use the negative example’s sequence in another topic vs. sequence in a completely different course\n\n\nkwargs\n\n\n\n\n\n\n\ndls = _get_dls(ExampleCFG, train_df, hf_arch, hf_config, hf_tokenizer, hf_model, val_idxs_or_fold=val_idxs)\n\n\nb = dls.one_batch()\nprint(len(b))\nprint(\"\")\nprint(hf_tokenizer.decode(b[0][\"input_ids\"][0]))\nprint(\"\")\nprint(hf_tokenizer.decode(b[1][\"input_ids\"][0]))\nprint(\"\")\nprint(b[2])\n\n3\n\n[CLS] of three of them so, you know, you kind of... this is the idea, right?, as if somebody says to you: “i like this movie, this movie, this movie” and you're like: “oh, they like those movies too” what[SEP] other movies do you like? and they'll say: “oh, how about this?” there's a chance, good chance, that you're going to like the same thing. that's the basis of collaborative filtering, okay, it's...[SEP]\n\n[CLS] of three of them so, you know, you kind of... this is the idea, right?, as if somebody says to you: “i like this movie, this movie, this movie” and you're like: “oh, they like those movies too” what[SEP] you're going to be really hungry and[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n\nTensorCategory([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:1')"
  },
  {
    "objectID": "topic_segmentation.html#models",
    "href": "topic_segmentation.html#models",
    "title": "topic_segmentation",
    "section": "Models",
    "text": "Models\n\nsource\n\nblurr_splitter_on_backbone\n\n blurr_splitter_on_backbone (m:fastai.torch_core.Module)\n\nCreates two layer groups: One for the backbone and one for the pooler/classification head\n\nsource\n\n\nblurr_splitter_with_head\n\n blurr_splitter_with_head (m:fastai.torch_core.Module)\n\nSimply adds an additional layer group to the classification head\n\nsource\n\n\nMarginRankingLoss\n\n MarginRankingLoss (pos_neg_scores, targs)\n\n\nsource\n\n\ntopic_seg_f1_score\n\n topic_seg_f1_score (inps, targs)\n\n\nsource\n\n\nTopicSegmentationModelWrapper\n\n TopicSegmentationModelWrapper (hf_config:PretrainedConfig,\n                                hf_model:PreTrainedModel,\n                                dropout_cls:type=<class\n                                'torch.nn.modules.dropout.Dropout'>,\n                                p:float=0.1, hf_model_kwargs:dict={})\n\nThis custom BaseModelWrapper allows to score our positive examples against our negative examples. A good model will be one where the former are assigned a value greater than the later.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhf_config\nPretrainedConfig\n\nOur transformer’s configuration object\n\n\nhf_model\nPreTrainedModel\n\nOur transformer model\n\n\ndropout_cls\ntype\nDropout\nThe class we want to use for applying dropout\n\n\np\nfloat\n0.1\nThe amount of dropout to apply prior to our classification head\n\n\nhf_model_kwargs\ndict\n{}\nAny kwargs we want to be passed when the model is used for prediction"
  },
  {
    "objectID": "topic_segmentation.html#learner",
    "href": "topic_segmentation.html#learner",
    "title": "topic_segmentation",
    "section": "Learner",
    "text": "Learner\n\nlearn = _get_learner(cfg=ExampleCFG, dls=dls, hf_config=hf_config, hf_model=hf_model, learner_path=\"../models\")\n\nfit_cbs = []\nif ExampleCFG.max_grad_norm:\n    fit_cbs.append(GradientClip(max_norm=ExampleCFG.max_grad_norm))\n\nif ExampleCFG.include_gradient_checkpointing:\n    fit_cbs.append(GradientCheckpointing())\n\nif ExampleCFG.save_best_model:\n    fit_cbs.append(\n        SaveModelCallback(\n            monitor=\"valid_loss\",\n            comp=np.less,\n            fname=f\"temp_best_loss_topic_segmentation\",\n            reset_on_fit=False,\n        )\n    )\n\n\nprint(len(learn.opt.param_groups))\n\n2\n\n\n\n# learn.unfreeze()\n# learn.summary()\n\n\nlearn.unfreeze()\nlearn.lr_find(suggest_funcs=[minimum, steep, valley, slide])\n\n\n\n\n\n\n\n\nSuggestedLRs(minimum=1.0964781722577755e-07, steep=0.005248074419796467, valley=0.00013182566908653826, slide=0.0014454397605732083)\n\n\n\n\n\n\nif ExampleCFG.random_seed:\n    set_seed(ExampleCFG.random_seed)\n\nlearn.fit_one_cycle(2, slice(1e-5, 1e-3), cbs=fit_cbs)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.456956\n      0.556683\n      0.781070\n      01:25\n    \n    \n      1\n      0.335876\n      0.568177\n      0.772840\n      01:25\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.5566829442977905.\n\n\n\n[f\"{k}: {v}\" for k, v in training.get_train_config_props(ExampleCFG).items()][:5]\n\n['accum: None',\n 'adam_beta2: 0.99',\n 'adam_eps: 1e-07',\n 'batch_size: 8',\n \"custom_model_kwargs: {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}\"]\n\n\n\nfor m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n    print(m_name, m_val)\n\ntrain_loss 0.33587607741355896\nvalid_loss 0.5681771039962769\ntopic_seg_f1_score 0.7728395061728395\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.export(\"../models/test_topic_segmentation.pkl\")"
  },
  {
    "objectID": "topic_segmentation.html#validation",
    "href": "topic_segmentation.html#validation",
    "title": "topic_segmentation",
    "section": "Validation",
    "text": "Validation\n\nsource\n\ndepth_score_cal\n\n depth_score_cal (scores)\n\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nscores\nThe original scores assigned to each sequence in a course’s transcript\n\n\n\n\nval_course_titles = train_df.iloc[val_idxs][\"course_title\"].unique().tolist()\npreds_df = _get_validation_preds(\n    hf_model,\n    hf_tokenizer,\n    raw_train_df,\n    val_course_titles[:2],\n    batch_size=16,\n    threshold_std_coeff=1.0,\n)\n\nprint(len(preds_df))\npreds_df.head()\n\n4628\n\n\n\n\n\n\n  \n    \n      \n      level_0\n      index\n      course_title\n      lesson_num\n      topic\n      seq\n      prev_seq\n      next_seq\n      is_topic_end\n      next_topic_begin_seq\n      other_topic_seqs\n      depth_score\n      threshold\n      pred_start\n    \n  \n  \n    \n      0\n      19012\n      19012\n      markowskyart - begginer drawing course\n      1\n      Introduction\n      okay here we are welcome everybody wherever you are here on planet Earth\n      xxBEGIN_TOPICxx\n      I'm welcoming you into my studio here in Vancouver British Columbia Canada my\n      False\n      NaN\n      [name is Michael Markowski and I'm gonna be your drawing instructor for the next, hour or month depending on how many classes you decide to watch so it's, really exciting that there's I see 50 people who are watching right now which is really exciting to have a nice big, audience worth of people and I don't know how many comments have been pouring in here I was looking at a few of those, right started and people from a lot of people from British Columbia and it's great also to see a bunch of people who, have taken classes with me in the past so I guess that might mean I'm possibly, have do...\n      0.001695\n      0.00322\n      True\n    \n    \n      1\n      19013\n      19013\n      markowskyart - begginer drawing course\n      1\n      Introduction\n      I'm welcoming you into my studio here in Vancouver British Columbia Canada my\n      okay here we are welcome everybody wherever you are here on planet Earth\n      name is Michael Markowski and I'm gonna be your drawing instructor for the next\n      False\n      NaN\n      [okay here we are welcome everybody wherever you are here on planet Earth, hour or month depending on how many classes you decide to watch so it's, really exciting that there's I see 50 people who are watching right now which is really exciting to have a nice big, audience worth of people and I don't know how many comments have been pouring in here I was looking at a few of those, right started and people from a lot of people from British Columbia and it's great also to see a bunch of people who, have taken classes with me in the past so I guess that might mean I'm possibly, have done some...\n      0.000806\n      0.00322\n      False\n    \n    \n      2\n      19014\n      19014\n      markowskyart - begginer drawing course\n      1\n      Introduction\n      name is Michael Markowski and I'm gonna be your drawing instructor for the next\n      I'm welcoming you into my studio here in Vancouver British Columbia Canada my\n      hour or month depending on how many classes you decide to watch so it's\n      False\n      NaN\n      [okay here we are welcome everybody wherever you are here on planet Earth, I'm welcoming you into my studio here in Vancouver British Columbia Canada my, really exciting that there's I see 50 people who are watching right now which is really exciting to have a nice big, audience worth of people and I don't know how many comments have been pouring in here I was looking at a few of those, right started and people from a lot of people from British Columbia and it's great also to see a bunch of people who, have taken classes with me in the past so I guess that might mean I'm possibly, have don...\n      0.000616\n      0.00322\n      False\n    \n    \n      3\n      19015\n      19015\n      markowskyart - begginer drawing course\n      1\n      Introduction\n      hour or month depending on how many classes you decide to watch so it's\n      name is Michael Markowski and I'm gonna be your drawing instructor for the next\n      really exciting that there's I see 50 people who are watching right now which is really exciting to have a nice big\n      False\n      NaN\n      [okay here we are welcome everybody wherever you are here on planet Earth, I'm welcoming you into my studio here in Vancouver British Columbia Canada my, name is Michael Markowski and I'm gonna be your drawing instructor for the next, audience worth of people and I don't know how many comments have been pouring in here I was looking at a few of those, right started and people from a lot of people from British Columbia and it's great also to see a bunch of people who, have taken classes with me in the past so I guess that might mean I'm possibly, have done something right in the past and pe...\n      0.000000\n      0.00322\n      False\n    \n    \n      4\n      19016\n      19016\n      markowskyart - begginer drawing course\n      1\n      Introduction\n      really exciting that there's I see 50 people who are watching right now which is really exciting to have a nice big\n      hour or month depending on how many classes you decide to watch so it's\n      audience worth of people and I don't know how many comments have been pouring in here I was looking at a few of those\n      False\n      NaN\n      [okay here we are welcome everybody wherever you are here on planet Earth, I'm welcoming you into my studio here in Vancouver British Columbia Canada my, name is Michael Markowski and I'm gonna be your drawing instructor for the next, hour or month depending on how many classes you decide to watch so it's, right started and people from a lot of people from British Columbia and it's great also to see a bunch of people who, have taken classes with me in the past so I guess that might mean I'm possibly, have done something right in the past and people who've taken my photography, classes and ...\n      0.004914\n      0.00322\n      True"
  },
  {
    "objectID": "topic_segmentation.html#inference",
    "href": "topic_segmentation.html#inference",
    "title": "topic_segmentation",
    "section": "Inference",
    "text": "Inference\n\nraw_train_df = pd.read_csv(f\"../data/clean/segmentation_train.csv\", index_col=None)\n\nval_course_title = \"fast.ai 2022 - Part 1\"\nval_lesson_num = \"4\"\n\ninf_df = raw_train_df[\n    (raw_train_df[\"course_title\"] == val_course_title) & (raw_train_df[\"lesson_num\"] == val_lesson_num)\n].copy()\ninf_df[\"transcript\"] = inf_df[\"seq\"]\ninf_df.reset_index(inplace=True)\n\nprint(len(inf_df))\ninf_df.head(2)\n\n709\n\n\n\n\n\n\n  \n    \n      \n      index\n      course_title\n      lesson_num\n      topic\n      seq\n      prev_seq\n      next_seq\n      is_topic_end\n      next_topic_begin_seq\n      other_topic_seqs\n      transcript\n    \n  \n  \n    \n      0\n      15307\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n      xxBEGIN_TOPICxx\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      False\n      NaN\n      [\"because it's where we're gonna get some totally new material — totally new topic, we've\", \"never covered before. We're going to cover natural language processing (NLP), and you'll\", \"find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\", 'fast.ai library, using recurrent neural networks (RNNs).', \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're going...\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n    \n    \n      1\n      15308\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n      because it's where we're gonna get some totally new material — totally new topic, we've\n      False\n      NaN\n      ['Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think', \"never covered before. We're going to cover natural language processing (NLP), and you'll\", \"find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\", 'fast.ai library, using recurrent neural networks (RNNs).', \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're go...\n      is the lesson that a lot of the regulars in the community have been most excited about,\n    \n  \n\n\n\n\n\npreds_df, seg_idxs = _get_preds(learn, inf_df, threshold_std_coeff=1.0)\n\nprint(seg_idxs[:10])\nprint(len(preds_df))\npreds_df.head()\n\n[0, 4, 16, 20, 23, 26, 30, 44, 47, 54]\n709\n\n\n\n\n\n\n  \n    \n      \n      index\n      course_title\n      lesson_num\n      topic\n      seq\n      prev_seq\n      next_seq\n      is_topic_end\n      next_topic_begin_seq\n      other_topic_seqs\n      transcript\n      depth_score\n      threshold\n      pred_start\n    \n  \n  \n    \n      0\n      15307\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n      xxBEGIN_TOPICxx\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      False\n      NaN\n      [\"because it's where we're gonna get some totally new material — totally new topic, we've\", \"never covered before. We're going to cover natural language processing (NLP), and you'll\", \"find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\", 'fast.ai library, using recurrent neural networks (RNNs).', \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're going...\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n      0.001369\n      0.002255\n      True\n    \n    \n      1\n      15308\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n      because it's where we're gonna get some totally new material — totally new topic, we've\n      False\n      NaN\n      ['Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think', \"never covered before. We're going to cover natural language processing (NLP), and you'll\", \"find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\", 'fast.ai library, using recurrent neural networks (RNNs).', \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're go...\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      0.000000\n      0.002255\n      False\n    \n    \n      2\n      15309\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      because it's where we're gonna get some totally new material — totally new topic, we've\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      never covered before. We're going to cover natural language processing (NLP), and you'll\n      False\n      NaN\n      ['Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think', 'is the lesson that a lot of the regulars in the community have been most excited about,', \"find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\", 'fast.ai library, using recurrent neural networks (RNNs).', \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're goi...\n      because it's where we're gonna get some totally new material — totally new topic, we've\n      0.000203\n      0.002255\n      False\n    \n    \n      3\n      15310\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      never covered before. We're going to cover natural language processing (NLP), and you'll\n      because it's where we're gonna get some totally new material — totally new topic, we've\n      find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\n      False\n      NaN\n      ['Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think', 'is the lesson that a lot of the regulars in the community have been most excited about,', \"because it's where we're gonna get some totally new material — totally new topic, we've\", 'fast.ai library, using recurrent neural networks (RNNs).', \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're going to be\", \"doing today is we're going to be fine-tuning a pre-trained NLP model using a...\n      never covered before. We're going to cover natural language processing (NLP), and you'll\n      0.001102\n      0.002255\n      False\n    \n    \n      4\n      15311\n      fast.ai 2022 - Part 1\n      4\n      Using Huggingface\n      find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\n      never covered before. We're going to cover natural language processing (NLP), and you'll\n      fast.ai library, using recurrent neural networks (RNNs).\n      False\n      NaN\n      ['Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think', 'is the lesson that a lot of the regulars in the community have been most excited about,', \"because it's where we're gonna get some totally new material — totally new topic, we've\", \"never covered before. We're going to cover natural language processing (NLP), and you'll\", \"Today we're going to do something else, which is we're going to do Transformers, and we're\", \"not even going to use the fast.ai library at all in fact. So, what we're going to be\", \"doing today is we're going to be fine-tuning...\n      find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\n      0.002383\n      0.002255\n      True\n    \n  \n\n\n\n\n\ntry:\n    del learn\n    gc.collect()\n    torch.cuda.empty_cache()\nexcept:\n    pass"
  },
  {
    "objectID": "topic_segmentation.html#model-trainer",
    "href": "topic_segmentation.html#model-trainer",
    "title": "topic_segmentation",
    "section": "Model Trainer",
    "text": "Model Trainer\n\nsource\n\nTopicSegmentationModelTrainer\n\n TopicSegmentationModelTrainer (experiment_name:str,\n                                train_config:__main__.TopicSegmentationCon\n                                fig, data_path:str='data',\n                                model_output_path:str='models',\n                                log_output_path:str='logs',\n                                log_preds:bool=False,\n                                log_n_preds:int=None,\n                                use_wandb:bool=False, verbose:bool=False,\n                                **kwargs)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiment_name\nstr\n\nThe name of your experiment (e.g., deberta_v3_large). This value is used in conjunction with task whenlogging information with W&B or else saving data releveant to training/evaluation runs\n\n\ntrain_config\nTopicSegmentationConfig\n\nThe TopicSegmentationConfig\n\n\ndata_path\nstr\ndata\nWhere the project’s data is stored\n\n\nmodel_output_path\nstr\nmodels\nWhere exported Learners and other models should stored\n\n\nlog_output_path\nstr\nlogs\nWhere any logged data should be stored\n\n\nlog_preds\nbool\nFalse\nWhether predictions should be logged\n\n\nlog_n_preds\nint\nNone\nThe number of course predictions that should be logged\n\n\nuse_wandb\nbool\nFalse\nWhether or not to log experiments and sweeps to W&B\n\n\nverbose\nbool\nFalse\nWhether or not you want to have printed out everything during a training/evaulation run\n\n\nkwargs\n\n\n\n\n\n\n\n\ntrain\n\nsource\n\n\nTopicSegmentationModelTrainer.train\n\n TopicSegmentationModelTrainer.train (sweep_config:dict=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsweep_config\ndict\nNone\nA dictionary providing the parameters and ranges for our sweep\n\n\n\nExample training code\n\ntrainer = TopicSegmentationModelTrainer(\n    experiment_name=\"test_topic_segmentation\",\n    train_config=ExampleCFG,\n    data_path=\"../data\",\n    model_output_path=\"../models\",\n    log_output_path=\"../logs\",\n    log_preds=True,\n    log_n_preds=2,\n    use_wandb=True,\n    verbose=True,\n)\n\nresults_df, raw_df, train_df, train_val_idxs = trainer.train()\nresults_df.head()\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: ohmeow. Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: ohmeow (course-copilot). Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in ../logs/wandb/run-20221010_131156-mz9oyjue\n\n\nSyncing run tough-eon-1 to Weights & Biases (docs)\n\n\nExperiment: test_topic_segmentation\nTraining config: f{'accum': None, 'adam_beta2': 0.99, 'adam_eps': 1e-07, 'batch_size': 8, 'custom_model_kwargs': {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}, 'frozen_lr': 0, 'hf_config_kwargs': {'num_labels': 2}, 'hf_model_checkpoint': 'microsoft/deberta-v3-small', 'hf_model_cls': 'AutoModelForSequenceClassification', 'hf_model_kwargs': {}, 'hf_tokenizer_kwargs': {}, 'include_gradient_checkpointing': False, 'include_labels': False, 'lower_case': True, 'max_grad_norm': None, 'max_length': True, 'n_frozen_epochs': 0, 'n_unfrozen_epochs': 2, 'new_special_tokens': None, 'one_cycle_moms_end': 0.8, 'one_cycle_moms_min': 0.7, 'one_cycle_moms_start': 0.8, 'only_seed_splits': True, 'preprocess_strategy': None, 'random_seed': 2022, 'save_best_model': True, 'tok_kwargs': {}, 'training_subset': 0.25, 'truncation_strategy': True, 'unfrozen_lr_max': 0.001, 'unfrozen_lr_min': 1e-05, 'use_adjacent_neg_prob': 0.5, 'use_fp16': True, 'use_next_pos_prob': 0.75, 'val_pct': 0.25, 'weight_decay': 0.0}\nPreparing training data ...\nBuilding HF objects ...\nBuilding DataLoaders ...\nBuilding Learner ...\nTraining ...\nCould not gather input dimensions\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.462488\n      0.582567\n      0.768724\n      01:27\n    \n    \n      1\n      0.384275\n      0.571569\n      0.759671\n      01:28\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.5825673341751099.\nBetter model found at epoch 1 with valid_loss value: 0.5715688467025757.\nCould not gather input dimensions\n\n\n\n\n\n\n\n\n\nLogging results ...\nGetting predictions for validatation set ...\nprocessing markowskyart - begginer drawing course: 1\nprocessing markowskyart - begginer drawing course: 2\nprocessing markowskyart - begginer drawing course: 3\nprocessing markowskyart - begginer drawing course: 4\nprocessing markowskyart - begginer drawing course: 5\nprocessing cc - how to invest in stocks: 1\nSaving model ...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:epoch▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███eps_0▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁eps_1▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁lr_0▁▁▂▃▄▅▆▇▇██████▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁lr_1▁▁▂▃▄▅▆▇▇██████▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁mom_0██▇▇▅▄▃▂▂▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇█████mom_1██▇▇▅▄▃▂▂▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇█████raw_loss███▇▇▅█▃▄▂▂█▇▇▃▃▁▂▃▁▁▄▅▁▂▃▄▃▅▃▆▂▃▄▃▃▁▆▅▆sqr_mom_0▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁sqr_mom_1▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁topic_seg_f1_score█▁train_loss█████▇▆▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▂▂▂▂train_samples_per_sec▁▄▅█▂▅█▄▆▄▇▅▄█▅█▅▃▇▇▃▄▃▅▅▃▄▃▄▅▃▅▆▆▇▅▅▆▂▂valid_loss█▁wd_0▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁wd_1▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁Run summary:epoch2eps_00.0eps_10.0lr_00.0lr_10.0mom_00.8mom_10.8raw_loss0.19263sqr_mom_00.99sqr_mom_10.99statecompletedtopic_seg_f1_score0.75802train_loss0.38427train_samples_per_sec28.17576valid_loss0.5866wd_00.0wd_10.0\n\n\nSynced tough-eon-1: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/runs/mz9oyjueSynced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ../logs/wandb/run-20221010_131156-mz9oyjue/logs\n\n\nEnd training\n\n\n\n\n\n\n  \n    \n      \n      accum\n      adam_beta2\n      adam_eps\n      batch_size\n      custom_model_kwargs\n      frozen_lr\n      hf_config_kwargs\n      hf_model_checkpoint\n      hf_model_cls\n      hf_model_kwargs\n      ...\n      unfrozen_lr_max\n      unfrozen_lr_min\n      use_adjacent_neg_prob\n      use_fp16\n      use_next_pos_prob\n      val_pct\n      weight_decay\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      None\n      0.99\n      1.000000e-07\n      8\n      {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}\n      0\n      {'num_labels': 2}\n      microsoft/deberta-v3-small\n      AutoModelForSequenceClassification\n      {}\n      ...\n      0.001\n      0.00001\n      0.5\n      True\n      0.75\n      0.25\n      0.0\n      0.586599\n      0.758025\n      202.285863\n    \n  \n\n1 rows × 39 columns\n\n\n\n\n\nget_preds\n\nsource\n\n\nTopicSegmentationModelTrainer.get_preds\n\n TopicSegmentationModelTrainer.get_preds (model_or_learner,\n                                          data:pandas.core.frame.DataFrame\n                                          , **kwargs)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmodel_or_learner\n\nThe Learner we want to use for inference\n\n\ndata\npd.DataFrame\nThe data we want to get predictions on (must include a column named ‘transcript’ at minimum)\n\n\nkwargs\n\n\n\n\nReturns\ntuple[pd.DataFrame, list[int]]\nReturns data (including topic segmentation boundaris and scores) and indicies of predicted topic starts\n\n\n\nExample inference code\n\ntrainer = TopicSegmentationModelTrainer(\n    \"test_topic_segmentation\", ExampleCFG, \"../data\", \"../models\", \"../logs\", verbose=True\n)\n# results_df, train_df, val_idxs = trainer.train()\ninf_learn = trainer.load_learner_or_model(device=\"cpu\")\npreds_df, pred_topic_idxs = trainer.get_preds(inf_learn, inf_df[[\"topic\", \"transcript\"]].copy())\n\n# cleanup resources\ndel inf_learn\ntorch.cuda.empty_cache()\ngc.collect()\n\n# shows final results\nprint(pred_topic_idxs[:10])\npreds_df.head()\n\n[0, 4, 16, 20, 23, 26, 30, 44, 47, 54]\n\n\n\n\n\n\n  \n    \n      \n      topic\n      transcript\n      depth_score\n      threshold\n      pred_start\n    \n  \n  \n    \n      0\n      Using Huggingface\n      Hi everybody, and welcome to Practical Deep Learning for Coders Lesson Four, which I think\n      0.001369\n      0.002255\n      True\n    \n    \n      1\n      Using Huggingface\n      is the lesson that a lot of the regulars in the community have been most excited about,\n      0.000000\n      0.002255\n      False\n    \n    \n      2\n      Using Huggingface\n      because it's where we're gonna get some totally new material — totally new topic, we've\n      0.000203\n      0.002255\n      False\n    \n    \n      3\n      Using Huggingface\n      never covered before. We're going to cover natural language processing (NLP), and you'll\n      0.001102\n      0.002255\n      False\n    \n    \n      4\n      Using Huggingface\n      find there, there is indeed a chapter about that in the book, but we're going to do it in a totally different way to how it's done in the book. In the book we do NLP using the\n      0.002383\n      0.002255\n      True\n    \n  \n\n\n\n\n\n\ntune\n\ntrainer = TopicSegmentationModelTrainer(\n    experiment_name=\"test_topic_segmentation\",\n    train_config=ExampleCFG,\n    data_path=\"../data\",\n    model_output_path=\"../models\",\n    log_output_path=\"../logs\",\n    log_preds=False,\n    log_n_preds=None,\n    use_wandb=True,\n    verbose=True,\n)\n\nsweep_id = trainer.configure_sweep(sweep_config=default_sweep_config)\n\nCreate sweep with ID: 2nqt78ll\nSweep URL: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/sweeps/2nqt78ll\n\n\n\nwandb.agent(sweep_id, function=partial(trainer.train, sweep_config=default_sweep_config), count=5)\n\nwandb: Agent Starting Run: wvxi3n4n with config:\nwandb:  unfrozen_lr_max: 0.0002964642653616338\nwandb:  unfrozen_lr_min: 8.666728243468805e-05\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in ../logs/wandb/run-20221010_131806-wvxi3n4n\n\n\nSyncing run earthy-sweep-1 to Weights & Biases (docs)Sweep page:  https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/sweeps/2nqt78ll\n\n\nExperiment: test_topic_segmentation\nTraining config: f{'accum': None, 'adam_beta2': 0.99, 'adam_eps': 1e-07, 'batch_size': 8, 'custom_model_kwargs': {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}, 'frozen_lr': 0, 'hf_config_kwargs': {'num_labels': 2}, 'hf_model_checkpoint': 'microsoft/deberta-v3-small', 'hf_model_cls': 'AutoModelForSequenceClassification', 'hf_model_kwargs': {}, 'hf_tokenizer_kwargs': {}, 'include_gradient_checkpointing': False, 'include_labels': False, 'lower_case': True, 'max_grad_norm': None, 'max_length': True, 'n_frozen_epochs': 0, 'n_unfrozen_epochs': 2, 'new_special_tokens': None, 'one_cycle_moms_end': 0.8, 'one_cycle_moms_min': 0.7, 'one_cycle_moms_start': 0.8, 'only_seed_splits': True, 'preprocess_strategy': None, 'random_seed': 2022, 'save_best_model': True, 'tok_kwargs': {}, 'training_subset': 0.25, 'truncation_strategy': True, 'unfrozen_lr_max': 0.0002964642653616338, 'unfrozen_lr_min': 8.666728243468805e-05, 'use_adjacent_neg_prob': 0.5, 'use_fp16': True, 'use_next_pos_prob': 0.75, 'val_pct': 0.25, 'weight_decay': 0.0}\nPreparing training data ...\nBuilding HF objects ...\nBuilding DataLoaders ...\nBuilding Learner ...\nTraining ...\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.434889\n      0.607325\n      0.763786\n      01:27\n    \n    \n      1\n      0.297782\n      0.579850\n      0.768724\n      01:27\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.6073254942893982.\nBetter model found at epoch 1 with valid_loss value: 0.5798501968383789.\n\n\n\n\n\n\n\n\n\nEnd training\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:topic_seg_f1_score▁valid_loss▁Run summary:topic_seg_f1_score0.77366valid_loss0.58265\n\n\nSynced earthy-sweep-1: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/runs/wvxi3n4nSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ../logs/wandb/run-20221010_131806-wvxi3n4n/logs\n\n\nwandb: Agent Starting Run: l5r5816d with config:\nwandb:  unfrozen_lr_max: 0.0002667648067607438\nwandb:  unfrozen_lr_min: 4.856146061559808e-05\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in ../logs/wandb/run-20221010_132215-l5r5816d\n\n\nSyncing run rich-sweep-2 to Weights & Biases (docs)Sweep page:  https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/sweeps/2nqt78ll\n\n\nExperiment: test_topic_segmentation\nTraining config: f{'accum': None, 'adam_beta2': 0.99, 'adam_eps': 1e-07, 'batch_size': 8, 'custom_model_kwargs': {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}, 'frozen_lr': 0, 'hf_config_kwargs': {'num_labels': 2}, 'hf_model_checkpoint': 'microsoft/deberta-v3-small', 'hf_model_cls': 'AutoModelForSequenceClassification', 'hf_model_kwargs': {}, 'hf_tokenizer_kwargs': {}, 'include_gradient_checkpointing': False, 'include_labels': False, 'lower_case': True, 'max_grad_norm': None, 'max_length': True, 'n_frozen_epochs': 0, 'n_unfrozen_epochs': 2, 'new_special_tokens': None, 'one_cycle_moms_end': 0.8, 'one_cycle_moms_min': 0.7, 'one_cycle_moms_start': 0.8, 'only_seed_splits': True, 'preprocess_strategy': None, 'random_seed': 2022, 'save_best_model': True, 'tok_kwargs': {}, 'training_subset': 0.25, 'truncation_strategy': True, 'unfrozen_lr_max': 0.0002667648067607438, 'unfrozen_lr_min': 4.856146061559808e-05, 'use_adjacent_neg_prob': 0.5, 'use_fp16': True, 'use_next_pos_prob': 0.75, 'val_pct': 0.25, 'weight_decay': 0.0}\nPreparing training data ...\nBuilding HF objects ...\nBuilding DataLoaders ...\nBuilding Learner ...\nTraining ...\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.395206\n      0.567065\n      0.772840\n      01:25\n    \n    \n      1\n      0.289024\n      0.579308\n      0.767901\n      01:25\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.5670649409294128.\n\n\n\n\n\n\n\n\n\nEnd training\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:topic_seg_f1_score▁valid_loss▁Run summary:topic_seg_f1_score0.75802valid_loss0.59401\n\n\nSynced rich-sweep-2: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/runs/l5r5816dSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ../logs/wandb/run-20221010_132215-l5r5816d/logs\n\n\nwandb: Agent Starting Run: 3x9bljp5 with config:\nwandb:  unfrozen_lr_max: 0.00014688870658243158\nwandb:  unfrozen_lr_min: 9.855246064537064e-05\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in ../logs/wandb/run-20221010_132551-3x9bljp5\n\n\nSyncing run true-sweep-3 to Weights & Biases (docs)Sweep page:  https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/sweeps/2nqt78ll\n\n\nExperiment: test_topic_segmentation\nTraining config: f{'accum': None, 'adam_beta2': 0.99, 'adam_eps': 1e-07, 'batch_size': 8, 'custom_model_kwargs': {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}, 'frozen_lr': 0, 'hf_config_kwargs': {'num_labels': 2}, 'hf_model_checkpoint': 'microsoft/deberta-v3-small', 'hf_model_cls': 'AutoModelForSequenceClassification', 'hf_model_kwargs': {}, 'hf_tokenizer_kwargs': {}, 'include_gradient_checkpointing': False, 'include_labels': False, 'lower_case': True, 'max_grad_norm': None, 'max_length': True, 'n_frozen_epochs': 0, 'n_unfrozen_epochs': 2, 'new_special_tokens': None, 'one_cycle_moms_end': 0.8, 'one_cycle_moms_min': 0.7, 'one_cycle_moms_start': 0.8, 'only_seed_splits': True, 'preprocess_strategy': None, 'random_seed': 2022, 'save_best_model': True, 'tok_kwargs': {}, 'training_subset': 0.25, 'truncation_strategy': True, 'unfrozen_lr_max': 0.00014688870658243158, 'unfrozen_lr_min': 9.855246064537064e-05, 'use_adjacent_neg_prob': 0.5, 'use_fp16': True, 'use_next_pos_prob': 0.75, 'val_pct': 0.25, 'weight_decay': 0.0}\nPreparing training data ...\nBuilding HF objects ...\nBuilding DataLoaders ...\nBuilding Learner ...\nTraining ...\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.457043\n      0.659170\n      0.729218\n      01:25\n    \n    \n      1\n      0.266419\n      0.602764\n      0.770370\n      01:25\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.6591702103614807.\nBetter model found at epoch 1 with valid_loss value: 0.6027644276618958.\n\n\n\n\n\n\n\n\n\nEnd training\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:topic_seg_f1_score▁valid_loss▁Run summary:topic_seg_f1_score0.75473valid_loss0.62178\n\n\nSynced true-sweep-3: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/runs/3x9bljp5Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ../logs/wandb/run-20221010_132551-3x9bljp5/logs\n\n\nwandb: Agent Starting Run: xf8n2hrc with config:\nwandb:  unfrozen_lr_max: 0.0008400970402462487\nwandb:  unfrozen_lr_min: 8.196114939228642e-05\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in ../logs/wandb/run-20221010_132924-xf8n2hrc\n\n\nSyncing run atomic-sweep-4 to Weights & Biases (docs)Sweep page:  https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/sweeps/2nqt78ll\n\n\nExperiment: test_topic_segmentation\nTraining config: f{'accum': None, 'adam_beta2': 0.99, 'adam_eps': 1e-07, 'batch_size': 8, 'custom_model_kwargs': {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}, 'frozen_lr': 0, 'hf_config_kwargs': {'num_labels': 2}, 'hf_model_checkpoint': 'microsoft/deberta-v3-small', 'hf_model_cls': 'AutoModelForSequenceClassification', 'hf_model_kwargs': {}, 'hf_tokenizer_kwargs': {}, 'include_gradient_checkpointing': False, 'include_labels': False, 'lower_case': True, 'max_grad_norm': None, 'max_length': True, 'n_frozen_epochs': 0, 'n_unfrozen_epochs': 2, 'new_special_tokens': None, 'one_cycle_moms_end': 0.8, 'one_cycle_moms_min': 0.7, 'one_cycle_moms_start': 0.8, 'only_seed_splits': True, 'preprocess_strategy': None, 'random_seed': 2022, 'save_best_model': True, 'tok_kwargs': {}, 'training_subset': 0.25, 'truncation_strategy': True, 'unfrozen_lr_max': 0.0008400970402462487, 'unfrozen_lr_min': 8.196114939228642e-05, 'use_adjacent_neg_prob': 0.5, 'use_fp16': True, 'use_next_pos_prob': 0.75, 'val_pct': 0.25, 'weight_decay': 0.0}\nPreparing training data ...\nBuilding HF objects ...\nBuilding DataLoaders ...\nBuilding Learner ...\nTraining ...\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.396438\n      0.553289\n      0.798354\n      01:25\n    \n    \n      1\n      0.272380\n      0.460504\n      0.811523\n      01:25\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.5532887578010559.\nBetter model found at epoch 1 with valid_loss value: 0.4605042636394501.\n\n\n\n\n\n\n\n\n\nEnd training\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:topic_seg_f1_score▁valid_loss▁Run summary:topic_seg_f1_score0.80658valid_loss0.46773\n\n\nSynced atomic-sweep-4: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/runs/xf8n2hrcSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ../logs/wandb/run-20221010_132924-xf8n2hrc/logs\n\n\nwandb: Agent Starting Run: 6kb77a8z with config:\nwandb:  unfrozen_lr_max: 0.0006178585838591866\nwandb:  unfrozen_lr_min: 1.6556729698366223e-05\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in ../logs/wandb/run-20221010_133257-6kb77a8z\n\n\nSyncing run splendid-sweep-5 to Weights & Biases (docs)Sweep page:  https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/sweeps/2nqt78ll\n\n\nExperiment: test_topic_segmentation\nTraining config: f{'accum': None, 'adam_beta2': 0.99, 'adam_eps': 1e-07, 'batch_size': 8, 'custom_model_kwargs': {'p': 0.1, 'dropout_cls': <class 'torch.nn.modules.dropout.Dropout'>}, 'frozen_lr': 0, 'hf_config_kwargs': {'num_labels': 2}, 'hf_model_checkpoint': 'microsoft/deberta-v3-small', 'hf_model_cls': 'AutoModelForSequenceClassification', 'hf_model_kwargs': {}, 'hf_tokenizer_kwargs': {}, 'include_gradient_checkpointing': False, 'include_labels': False, 'lower_case': True, 'max_grad_norm': None, 'max_length': True, 'n_frozen_epochs': 0, 'n_unfrozen_epochs': 2, 'new_special_tokens': None, 'one_cycle_moms_end': 0.8, 'one_cycle_moms_min': 0.7, 'one_cycle_moms_start': 0.8, 'only_seed_splits': True, 'preprocess_strategy': None, 'random_seed': 2022, 'save_best_model': True, 'tok_kwargs': {}, 'training_subset': 0.25, 'truncation_strategy': True, 'unfrozen_lr_max': 0.0006178585838591866, 'unfrozen_lr_min': 1.6556729698366223e-05, 'use_adjacent_neg_prob': 0.5, 'use_fp16': True, 'use_next_pos_prob': 0.75, 'val_pct': 0.25, 'weight_decay': 0.0}\nPreparing training data ...\nBuilding HF objects ...\nBuilding DataLoaders ...\nBuilding Learner ...\nTraining ...\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      topic_seg_f1_score\n      time\n    \n  \n  \n    \n      0\n      0.423982\n      0.563763\n      0.781893\n      01:26\n    \n    \n      1\n      0.333860\n      0.548688\n      0.783539\n      01:26\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.5637626647949219.\nBetter model found at epoch 1 with valid_loss value: 0.5486881136894226.\n\n\n\n\n\n\n\n\n\nEnd training\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:topic_seg_f1_score▁valid_loss▁Run summary:topic_seg_f1_score0.78354valid_loss0.56118\n\n\nSynced splendid-sweep-5: https://wandb.ai/course-copilot/course-copilot-ml-topic_segmentation/runs/6kb77a8zSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ../logs/wandb/run-20221010_133257-6kb77a8z/logs\n\n\n\n\ntune_threshold"
  }
]